Added_lines_fixing_commit,Added_lines_bug_inducing_commit,Fixing_Script_MisconfigurationFoundbyLLM,Bug_Inducing_Script_MisconfigurationFoundbyLLM,Fixing_Script_MisconfigurationFoundbyLLM_Label,Bug_Inducing_Script_MisconfigurationFoundbyLLM_Label
"command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""","- name: ""[NC] - Set Nextcloud settings in config.php""
  shell: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_config_settings }}""
- name: ""[NC] - Set Redis Server""
  command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_redis_settings }}""
  when: nextcloud_install_redis_server == True

    mode: 0750",command: php occ config:system:set {{ item.name }} --value={{ item.value }},No misconfigured_snippet Found,1,0
when: (not _nextcloud_conf.stat.exists) or (_nextcloud_configured.rc is defined and _nextcloud_configured.rc != 0),"check_mode: no
  when: _nextcloud_configured.rc != 0",when: (not _nextcloud_conf.stat.exists) or (_nextcloud_configured.rc is defined and _nextcloud_configured.rc!= 0),check_mode: no\nwhen: _nextcloud_configured.rc!= 0\n,1,1
when: (nc_sudo_installed_result.rc is defined) and (nc_sudo_installed_result.rc != 0),"register: nc_sudo_installed_result
  when: nc_sudo_installed_result is defined and nc_sudo_installed_result.rc != 0",when: (nc_sudo_installed_result.rc is defined) and (nc_sudo_installed_result.rc!= 0),No misconfigured_snippet Found,1,0
"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""","action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""","##
 # BROWSER
 #
 
- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
#TODO sudo ln -s /usr/lib/chromium-browser/chromedriver /var/lib/chromedriver
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
#TODO sudo ln -s /usr/bin/chromedrive /var/lib/chromedriver  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""","action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""","##
 # BROWSER
 #
 
- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
#TODO sudo ln -s /usr/lib/chromium-browser/chromedriver /var/lib/chromedriver
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
#TODO sudo ln -s /usr/bin/chromedrive /var/lib/chromedriver  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare(8, '<'))","- (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare
(8, '<'))",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"owner: ""{{ consul_user }}""
    group: ""{{ consul_group }}""","file:
    dest: ""{{ consul_tls_dir }}""
    state: directory
    owner: root
    group: root
    mode: 0755
    copy:
      src: ""{{ consul_src_files }}/{{ consul_ca_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_ca_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_key }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_key }}""
  template:
    src: config_server_tls.json.j2
    dest: ""{{ consul_config_path }}/server/config_server_tls.json""",owner: \,No misconfigured_snippet Found,1,0
"url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ item }}""
    url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script""","url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}/service/siesta/rest/v1/script/{{ item }}""",http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ item }},No misconfigured_snippet Found,1,0
"- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: ""modify auth user admin password {{admin_password}}""
  register: change_password
  until: change_password is not failed
  retries: 5","- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: modify auth user admin password admin",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""{{ec2_info.arista.filter}}""
        architecture: ""x86_64""
    register: arista_amis","#### CISCO AMI
- name: BLOCK FOR CISCO AMI
  block:
  - name: find ami for cisco (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""cisco-CSR*BYOL*""
        architecture: ""x86_64""
    register: cisco_ami_list

  - name: save ami for cisco (NETWORKING MODE)
    set_fact:
      cisco_ami: >
        {{ cisco_ami_list.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""cisco""'

#### ARISTA AMI
- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""*EOS*""
        architecture: ""x86_64""
    register: arista_amis

  - name: save ami for arista eos (NETWORKING MODE)
    set_fact:
      arista_ami: >
        {{ arista_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""arista""'

#### JUNIPER AMI
- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for juniper vsrx (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""junos-vsrx3-x86-64-18.4R1.8--pm*""
        architecture: ""x86_64""
    register: juniper_amis

  - name: save ami for juniper (NETWORKING MODE)
    set_fact:
      juniper_ami: >
        {{ juniper_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""juniper""'",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: fail on purpose now to let user know code server failed
      debug:
        msg: ""VS code integration has failed in provisioner/roles/code_server/tasks/main.yml""
      failed_when: true","---
- name: remove dns entries for each vs code instance
  include_tasks: teardown.yml
  when: teardown|bool

- name: check to see if SSL cert already applied
  become: no
  get_certificate:
    host: ""{{username}}-code.{{ec2_name_prefix|lower}}.{{workshop_dns_zone}}""
    port: 443
  delegate_to: localhost
  run_once: true
  register: check_cert
  ignore_errors: true
  when:
    - not teardown

- name: perform DNS and SSL certs for ansible control node
  block:
    - name: setup vscode for web browser access
      include_tasks: ""codeserver.yml""
  rescue:
    - debug:
        msg: 'VS code integration has failed'

    - name: make sure tower is on
      shell: ansible-tower-service start
      register: install_tower
      until: install_tower is not failed
      retries: 5

    - name: appends
      set_fact:
        coder_information: |
          - VS code integration has failed, please use direct SSH addresses
      run_once: true
      delegate_to: localhost
      delegate_facts: true
  when:
    - not teardown|bool
    - check_cert is failed",VS code integration has failed in provisioner/roles/code_server/tasks/main.yml,No misconfigured_snippet Found,1,0
"command: systemctl daemon-reload
  tags: skip_ansible_lint","systemd:
    daemon-reload: yes",command: systemctl daemon-reload,systemd: daemon-reload: yes,1,1
"copy: src={{ caddy_home }}/caddy dest=/usr/bin/ mode=0755 remote_src=True
  notify: Restart Caddy","unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} copy=no
  unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no

- name: Copy Caddy Binary
  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True",copy: src={{ caddy_home }}/caddy dest=/usr/bin/ mode=0755 remote_src=True,unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} copy=no,1,1
"retries: 3
  delay: 2
  retries: 3
  delay: 2","user:
    name: ""{{ caddy_user }}""
    system: yes
    createhome: yes
    home: ""{{ caddy_home }}""
  get_url:
    url: https://api.github.com/repos/mholt/caddy/git/refs/tags
    dest: ""{{ caddy_home }}/releases.txt""
    force: yes
  copy:
    content: ""{{ caddy_features }}""
    dest: ""{{ caddy_home }}/features.txt""
  get_url:
    url: ""{{ caddy_url | quote }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    force: yes
    timeout: 300
    retries: 3
    delay: 2
  get_url:
    url: ""{{ caddy_url}}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    timeout: 300
    retries: 3
    delay: 2
  command: >
    gpg
      --keyserver-options timeout={{ caddy_pgp_recv_timeout }}
      --keyserver {{ caddy_pgp_key_server }}
      --recv-keys {{ caddy_pgp_key_id }}
  get_url:
    url: ""{{ caddy_sig_url }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz.asc""
    timeout: 60
    force: yes
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
  command: >
    gpg
      --verify {{ caddy_home }}/caddy.tar.gz.asc
      {{ caddy_home }}/caddy.tar.gz
  unarchive:
    src: ""{{ caddy_home }}/caddy.tar.gz""
    dest: ""{{ caddy_home }}""
    copy: no
    owner: ""{{ caddy_user }}""
  unarchive:
   src: ""{{ caddy_home }}/caddy.tar.gz""
   dest: ""{{ caddy_home }}""
   creates: ""{{ caddy_home }}/caddy""
   copy: no
   owner: ""{{ caddy_user }}""
  copy:
    src: ""{{ caddy_home }}/caddy""
    dest: ""{{ caddy_bin }}""
    mode: 0755
    remote_src: true
  file:
    path: ""{{ item }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0770
  file:
    path: ""{{ caddy_log_dir }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0775
  copy:
    content: ""{{ caddy_config }}""
    dest: ""{{ caddy_conf_dir }}/Caddyfile""
    owner: ""{{ caddy_user }}""
  stat:
    path: /run/systemd/system
  template:
    src: ""{{ item }}""
    dest: /etc/init/caddy.conf
    mode: 0644
  template:
    src: caddy.service
    dest: /etc/systemd/system/caddy.service
    mode: 0644
  service:
    name: caddy
    state: started
    enabled: yes",No misconfigured_snippet Found,misconfigured_snippet_1,0,0
"shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'""","when: item.hstore is defined and item.hstore

- name: PostgreSQL | Add uuid-ossp to the database with the requirement
  sudo: yes
  sudo_user: ""{{postgresql_admin_user}}""
  shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;""
  with_items: postgresql_databases
  when: item.uuid_ossp is defined and item.uuid_ossp",psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;',No misconfigured_snippet Found,1,0
"name: ""postgresql-contrib-{{postgresql_version}}""","# file: postgresql/tasks/extensions/contrib.yml

- name: PostgreSQL | Extensions | Make sure the development headers are installed
  apt:
    name: libpq-dev
    state: present
  notify:
    - restart postgresql",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"run_once: True
  run_once: True
- name: 'Create folder {{structured_dir_name}} for structured YAML files'
  run_once: True
  run_once: True

- name: 'Create folder {{structured_cvp_name}} for CVP structured YAML files'
  file:
    path: '{{structured_cvp}}'
    state: directory
    mode: 0755
  delegate_to: localhost
  run_once: True
  run_once: True
  run_once: True
  run_once: True","---
# tasks file for build_directories

- name: 'Cleanup existing folders in {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: absent 
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{structured_dir_name}} for structued YAML files'
  file:
    path: '{{structured_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{eos_config_dir_name}} for EOS Configuration files'
  file:
    path: '{{eos_config_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
- name: Create/invoke script virtualenv for create galaxy admin,"- name: Create/invoke script virtualenv
  pip: name={{ item }} virtualenv={{ galaxy_venv_dir }} virtualenv_command=""{{ pip_virtualenv_command | default( 'virtualenv' ) }}""
  with_items:
    - pyyaml
    - bioblend
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""

- name: Create Galaxy admin user
  command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python /usr/local/bin/create_galaxy_user.py --user {{ galaxy_admin }} --password {{ galaxy_admin_pw }} --key {{ galaxy_admin_api_key }}
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""

#- name: Copy the bootstrap user management script
#  copy: src=manage_bootstrap_user.py dest={{ galaxy_server_dir }}/manage_bootstrap_user.py owner={{ galaxy_user_name }}

#- name: Create Galaxy bootstrap user
#  command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python manage_bootstrap_user.py -c {{ galaxy_config_file }} create -e {{ galaxy_admin }} -p """"

#- name: Remove the bootstrap user management script
#  file: dest={{ galaxy_server_dir }}/manage_bootstrap_user.py state=absent",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /etc/supervisor/conf.d/galaxy.conf""
- debug: var=galaxylogs

- name: get_logs
  shell: ""supervisorctl status""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs","- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs

- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/allowed_images.yml""}","- {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/allowed_images.yml""}",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- export NATIVE_SPEC=""--ntasks=`/usr/bin/nproc` --share""","supervisor_env_vars:
    - export IP_ADDRESS=`curl icanhazip.com`
    - export GALAXY_CONF_FTP_UPLOAD_SITE=""ftp://$IP_ADDRESS""
    - export MASQUERADE_ADDRESS=$IP_ADDRESS
    - export NTASK=""--ntasks=`/usr/bin/nproc` --share""",export NATIVE_SPEC=\,export IP_ADDRESS=`curl icanhazip.com`,1,1
"- name: Add entries for demo into hosts file
  lineinfile: dest=/etc/hosts regexp='^127\.0\.0\.1' line=""127.0.0.1 localhost {{ demo_hostname | default('') }} ala.vagrant.dev ala demo.vagrant1.ala.org.au vagrant1.ala.org.au"" owner=root group=root mode=0644

- name: Ensure data directory exists
    - demo","- include: ../../common/tasks/setfacts.yml

- name: ensure data directory exists
  file: path=/srv/{{ demo_hostname }}/www/html state=directory owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - demo

- name: Copy welcome page (Debian)
  template: src=index.html dest=/srv/{{ demo_hostname }}/www/index.html mode=0666
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/biocache-media"" 
  ignore_errors: yes
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/html/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/html/biocache-media"" 
  ignore_errors: yes
  tags: 
    - demo",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"service-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}
        management-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}/status","authCookieName: {{ auth_cookie_name | default('ALA-Auth') }}
      enabled: {{ oauth_providers_flickr_enabled | default('true') }}
    inaturalist:
      enabled: {{ oauth_providers_inaturalist_enabled | default('false') }}
      key: {{ oauth_providers_inaturalist_key | default('') }}
      secret: {{ oauth_providers_inaturalist_secret | default('') }}
      callback: ${grails.serverURL}/profile/inaturalistCallback
biocache.search.baseUrl: {{ biocache_base_url }}/occurrences/search
headerAndFooter:
  baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3')}}
  version: {{ header_and_footer_version | default('1')}}
{% if bootadmin_enabled %}
        service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}
#        management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status
{% endif %}
{% if spring_session_redis_clustered %}
{% endif %}",service-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}\n        management-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}/status,No misconfigured_snippet Found,1,0
"sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*
# Run again with 8u172 as the basis for servers that were on that version instead
- name: Fix Webupd8 Team failing to update and Oracle removing old download (part 2)
    sed -i 's|JAVA_VERSION=8u172|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_172|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*","sed -i 's|JAVA_VERSION=8u161|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""6dbc56a0e3310b69e91bb64db63a485bd7b6a8083f08e48047276380a0e2021e""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_161|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*
- name: Switch oracle jdk 8 from security (b171) to bug fix+security (b172)
    sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*",sed -i's|JAVA_VERSION=8u171|JAVA_VERSION=8u181|' oracle-java8-installer.*,sed -i's|JAVA_VERSION=8u161|JAVA_VERSION=8u172|' oracle-java8-installer.*,1,1
"- ""{{data_dir}}/ala/runtime/files/""","- include: ../../common/tasks/setfacts.yml
  tags:
    - spatial-hub
    - config    

- include: ../../apache_vhost/tasks/main.yml context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - apache_vhost
    - spatial-hub
  when: not webserver_nginx

- name: add nginx vhost if configured
  include_role:
    name: nginx_vhost
  vars:
    hostname: ""{{ spatial_hub_hostname }}""
    context_path: ""{{ spatial_hub_context_path }}""
  tags:
    - nginx_vhost
    - deploy
    - spatial-hub
  when: webserver_nginx

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ spatial_hub_war_url }}' context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - tomcat_vhost
    - spatial-hub

- name: ensure target directories exist [data subdirectories etc.]
  file: path={{item}} state=directory owner={{tomcat_user}} group={{tomcat_user}}
  with_items:
    - ""{{data_dir}}/ala/data/runtime/files/""
  tags:
    - spatial-hub

- name: copy all config.properties
  template: src=spatial-hub-config.properties dest={{data_dir}}/spatial-hub/config/spatial-hub-config.properties
  tags:
    - spatial-hub 
    - config

- name: copy all log4j.properties
  template: src=log4j.properties dest={{data_dir}}/spatial-hub/config/log4j.properties
  tags:
    - spatial-hub

- name: set data ownership
  file: path={{data_dir}}/ala/data/ owner={{tomcat_user}} group={{tomcat_user}} recurse=true
  notify: 
    - restart tomcat
  tags:
    - spatial-hub",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',"- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
#
# WAR file deployment and Apache/Tomcat virtual host configuration
#
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',"version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
{% if spring_session_redis_clustered is sameas true %},"{% if spring_session_redis_clustered %}
    clustered:
      nodes: {{ spring_session_redis_host }}:{{ spring_session_redis_port | default('6379') }}
    {% endif %}",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
when: elasticsearch_proxy,when: elasticsearch_proxy | bool == True,when: elasticsearch_proxy,when: elasticsearch_proxy | bool == True,1,1
- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass_values }}',"tags:
    - sandbox
#
# WAR file deployment and virtual host configuration
#

- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass }}'
  tags:
    - sandbox
    - deploy
    - apache_vhost

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ sandbox_war_url }}' context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}'
  tags:
    - sandbox
    - deploy
    - tomcat_vhost

- name: Redirect to datacheck 
  template: src=index.html dest=/srv/{{ sandbox_hostname }}/www/index.html owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - sandbox
    - deploy
    - apache_vhost

#
# Properties and data file configuration
#
  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"webapi_war_url: ""{{maven_repo_ws_url}}""","- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
#
# WAR file deployment and Apache/Tomcat virtual host configuration
#
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy",webapi_war_url: \,No misconfigured_snippet Found,1,0
"webapi_war_url: ""{{maven_repo_ws_url}}""","version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}""",webapi_war_url: \,No misconfigured_snippet Found,1,0
"cassandra_user: cassandra
# package variables used for RedHat
datastax: dsc12-1.2.10-1
cassandra: cassandra12-1.2.10-1","# common variables across all roles
cassandra_user: cassandra",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"when: upgrade_check_script.stdout == ""False""","mode: 0644
  import: others/master/postconfigure-upgrade.yaml
  when: upgrade_check_script.stdout == ""False""",when: upgrade_check_script.stdout == \,No misconfigured_snippet Found,1,0
"- name: bootstrap | configure node | kubead show me join command
- name: bootstrap | configure node | compose join command","---
- name: configure others | kubead show me join command
  command: kubeadm token create --print-join-command --ttl 5m
  delegate_to: ""{{ cluster_name }}-kube-master.service.automium.consul""
  register: kubeadm_join_command

- name: configure-bootstrap  | compose join command
  set_fact:
    join_command: ""{{ kubeadm_join_command.stdout }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: Enable/disable services
  service:
    name: ""{{ item }}""
    enabled: ""{{ (enable_services | bool) | ternary('yes','no') }}""
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service

  when: (start_services | bool)
  tags:
    - service","---

- name: Package
  package:
    name: ""{{item}}""
    state: present
  loop: ""{{packages_to_install[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""

- name: Template >> /etc/dhcp/dhcpd.conf
  template:
    src: dhcpd.conf.j2
    dest: /etc/dhcp/dhcpd.conf
    owner: root
    group: root
    mode: 0644
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.networks.conf
  template:
    src: dhcpd.networks.conf.j2
    dest: /etc/dhcp/dhcpd.networks.conf
    owner: root
    group: root
    mode: 0644
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.{{item}}.conf
  template:
    src: dhcpd.subnet.conf.j2
    dest: /etc/dhcp/dhcpd.{{item}}.conf
    owner: root
    group: root
    mode: 0644
  with_items: ""{{networks}}""
  when:
    - j2_current_iceberg_network in item
    - networks[item].is_in_dhcp == true
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Start services
  service:
    name: ""{{item}}""
    state: started
    enabled: yes
  loop: ""{{services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"exporters:
#    node_exporter:
#      port: 9100
    bb_exporter:
      port: 9777
      collectors:
        cpu:
        ram:
        mounted:
          - /scratch
          - /home
        services:
          - slurmd.service
        
  # Define alerts related to selected exporters
    Exporter_down:
#      severity: critical
    bb_exporter_service:
      -  slurmd","monitoring:
  alerts:
    - ExporterDown
    - OutOfDiskSpace",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"loop: ""{{ log_client_services_to_start }}""","---

- name: Restart rsyslog services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service",loop: \,No misconfigured_snippet Found,1,0
advanced_dhcp_server_role_version: 1.0.4,"role_version: 1.0.2
packages_to_install:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcp
    _8:
      - dhcp-server
  centos:
    _7:
      - dhcp
    _8:
      - dhcp-server

services_to_start:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcpd
    _8:
      - dhcpd
  centos:
    _7:
      - dhcpd
    _8:
      - dhcpd",advanced_dhcp_server_role_version: 1.0.4,No misconfigured_snippet Found,1,0
dhcp_server_role_version: 1.0.7,dhcp_server_role_version: 1.0.6,dhcp_server_role_version: 1.0.7,dhcp_server_role_version: 1.0.6,1,1
dhcp_server_role_version: 1.0.4,"---
role_version: 1.0.4",dhcp_server_role_version: 1.0.4,role_version: 1.0.4,1,1
"- name: lineinfile █ Configure root color based on iceberg number
- name: copy █ Add disk usage small script for screenrc
- name: copy █ Add screenrc configuration
- name: copy █ Add vimrc configuration","- name: Configure root color based on iceberg number
  lineinfile:
    path: /root/.bashrc
    line: 'PS1=""\[\e[01;{{31+(j2_current_iceberg_number|int)}}m\]\h:\w#\[\e[00;m\] ""'

- name: Add disk usage small script for screenrc
  copy:
    src: free_root_disk 
    dest: /usr/bin/free_root_disk
    mode: 0700

- name: Add screenrc configuration
  copy:
    src: screenrc
    dest: /root/.screenrc
    mode: 0644",lineinfile █ Configure root color based on iceberg number,lineinfile: path: /root/.bashrc line: 'PS1=\,1,1
- name: service █ Restart dhcp server,"---
- name: Restart dhcp services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service",service █ Restart dhcp server,No misconfigured_snippet Found,1,0
"aws_access_key: ""{{ auth_var['aws_access_key_id'] | default(omit) }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] | default(omit) }}""","aws_access_key: ""{{ auth_var['aws_access_key_id'] }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] }}""",aws_access_key: \,aws_access_key: \,1,1
"when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init"" and cloud_config != {}
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and virt_type == ""cloud-init"" 
  when:  (node_exists['failed'] is defined) and  virt_type == 'virt-customize'
  ignore_errors: yes","- name: set cloud config default
  set_fact:
    cloud_config: ""{{ res_def['cloud_config'] | default({})  }}""

- name: set cloud_config virt_type
  set_fact:
    virt_type: ""{{ cloud_config['virt_type'] | default('cloud-init') }}""

- include_tasks: virt_customize.yml
  when: res_def['cloud_config']['virt_type'] == ""virt-customize""

  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and virt_type == ""cloud-init""
  when:  node_exists['failed'] is defined and (res_def['cloud_config'] is not defined or virt_type == 'virt-customize')",when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == \,No misconfigured_snippet Found,1,0
"dest: ""/tmp/{{ libvirt_resource_name }}{{ definition[4] }}{{ definition[2] }}""
    uri: ""{{ definition[0]['uri'] }}""","uri: ""{{ definition[0] }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"name:
          - python2-dnf 
          - libvirt-devel
          - libguestfs-tools 
          - python-libguestfs","- name: Install dependencies
  block:
    - name: Install package dependencies
      package:
        name: ""{{ libvirt_pkg }}""
        state: latest
      with_items:
      - python2-dnf 
      - libvirt-devel
      - libguestfs-tools 
      - python-libguestfs
      become: true
      loop_control: 
        loop_var: libvirt_pkg
    - name: Install pypi dependencies of libvirt
      pip:
        name: ""{{ libvirt_pypi }}""
      with_items:
      - ""libvirt-python>=3.0.0""
      - ""lxml""
      loop_control: 
        loop_var: libvirt_pypi
  rescue:
    - fail:
        msg: 'Error installing the package dependencies! Please try adding password less priviledged sudo user or with --ask-sudo-pass'",python2-dnf,No misconfigured_snippet Found,1,0
"admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""
    admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""
    admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""
    admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""","---
- name: ""Provisioning Azure VM when not async""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    ssh_public_keys: ""{{ssh_public_keys}}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    name: ""{{ nameOfvm | default(omit) }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    image: ""{{ image | default(omit) }}""
  register: res_def_output
  when: not _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure VM""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    name: ""{{  nameOfvm| default(omit) }}""
    image: ""{{ image | default(omit) }}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""
  when: _async",admin_username: \,No misconfigured_snippet Found,1,0
"topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""","no_log: ""{{ not debug_mode }}""

- name: """"Async::Provisioning Azure Virtual Subnet""
  azure_rm_subnet:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    name: ""{{ res_def['subnet_name'] | default(omit) }}""
    virtual_network_name: ""{{ res_def['virtual_network_name']}}""
    address_prefix: ""{{ res_def['address_prefix']|default('10.1.0.0/24')}}""
  register: res_def_output
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""
  when: _async",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- ""{{ res_grp['resource_definitions'] }}""","---
- name: ""Unset the authvar from previous run""
  set_fact:
    auth_var: """"

- name: ""set cred profile""
  set_fact:
    cred_profile: ""{{ res_grp['credentials']['profile'] | default('default') }}""

- name: ""Get creds from auth driver""
  auth_driver:
    filename: ""{{ res_grp['credentials']['filename']  }}""
    cred_type: ""ovirt""
    cred_path: ""{{ creds_path | default(default_credentials_path) }}""
    driver: ""file""
  register: auth_var
  ignore_errors: true

- name: ""set auth_var""
  set_fact:
    auth_var: ""{{ auth_var['output'][cred_profile] }}""
  ignore_errors: true

- block:
  - name: Obtain SSO token with using username/password credentials
    ovirt_auth:
      url: ""{{ auth_var['ovirt_url'] }}""
      username: ""{{ auth_var['ovirt_username'] }}""
      ca_file: ""{{ auth_var['ovirt_ca_file'] | default(omit) }}""
      password: ""{{ auth_var['ovirt_password'] }}""
      insecure: ""{{ auth_var['ovirt_ca_file'] is not defined }}""

  - name: ""Provisioning resource definitions of current group""
    include: provision_res_defs.yml res_def={{ res_item.0 }} res_grp_name={{ res_item.1 }}
    with_nested:
      - ""{{ res_grp['resource_definitions']) }}""
      - [""{{ res_grp['resource_group_name'] }}""]
    loop_control:
      loop_var: res_item

  always:
    - name: Always revoke the SSO token
      ovirt_auth:
        state: absent
        ovirt_auth: ""{{ ovirt_auth }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"address_prefixes: ""{{ res_def['address_prefixes']|default('10.1.0.0/16')}}""
    address_prefixes: ""{{ res_def['address_prefixes']|default('10.1.0.0/16')}}""","---
- name: ""Provisioning Azure Virtual Network when not async""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: ""{{ res_def['address_prefixes']|default(10.1.0.0/16)}}""
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  register: res_def_output

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure Virtual Network""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: 10.1.0.0/16
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""
  when: _async",address_prefixes: \,No misconfigured_snippet Found,1,0
"copy: src=fetch/{{ fsid }}/etc/ceph/radosgw.gateway.keyring dest=/etc/ceph/radosgw.gateway.keyring owner=root group=root mode=600
  file: path=/etc/ceph/radosgw.gateway.keyring mode=0600 owner=root group=root","- name: Copy RGW bootstrap key
  copy: src=fetch/{{ fsid }}/etc/ceph/keyring.radosgw.gateway dest=/etc/ceph/keyring.radosgw.gateway owner=root group=root mode=600
  when: cephx

- name: Set RGW bootstrap key permissions
  file: path=/etc/ceph/keyring.radosgw.gateway mode=0600 owner=root group=root
  when: cephx",copy: src=fetch/{{ fsid }}/etc/ceph/radosgw.gateway.keyring dest=/etc/ceph/radosgw.gateway.keyring owner=root group=root mode=600,No misconfigured_snippet Found,1,0
copy: >,"---
- name: create red hat storage package directories
  file: >
    path={{ item }}
    state=directory
  with_items:
    - ""{{ ceph_stable_rh_storage_mount_path }}""
    - ""{{ ceph_stable_rh_storage_repository_path }}""

- name: fetch the red hat storage iso from the ansible server
  fetch: >
    src={{ ceph_stable_rh_storage_iso_path }}
    dest={{ ceph_stable_rh_storage_iso_path }}
    flat=yes

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=mounted

- name: copy red hat storage iso content
  shell:
    cp -r {{ ceph_stable_rh_storage_mount_path }}/* {{ ceph_stable_rh_storage_repository_path }}
    creates={{ ceph_stable_rh_storage_repository_path }}/README

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=unmounted",copy: >,No misconfigured_snippet Found,1,0
"with_items: ""{{ groups.get(mon_group_name, []) }}""","---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names","with_items: {{ groups.get(mon_group_name, []) }}",No misconfigured_snippet Found,1,0
"with_items: ""{{ groups.get(mon_group_name, []) }}""","---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names","with_items: {{ groups.get(mon_group_name, []) }}",No misconfigured_snippet Found,1,0
failed_when: false,"---
# NOTE (leseb): the mds container needs the admin key
# so it can create the mds pools for cephfs
- name: set config and keys paths
  set_fact:
    ceph_config_keys:
      - /etc/ceph/ceph.conf
      - /etc/ceph/ceph.client.admin.keyring
      - /var/lib/ceph/bootstrap-mds/ceph.keyring

- name: stat for ceph config and keys
  local_action: stat path={{ item }}
  with_items: ceph_config_keys
  changed_when: false
  sudo: false
  ignore_errors: true
  register: statconfig

- name: try to fetch ceph config and keys
  copy: >
    src=fetch/docker_mon_files/{{ item.0 }}
    dest={{ item.0 }}
    owner=root
    group=root
    mode=644
  with_together:
    - ceph_config_keys
    - statconfig.results
  when: item.1.stat.exists == true",failed_when: false,No misconfigured_snippet Found,1,0
"- name: install nss-tools on redhat
    name: nss-tools
- name: install nss-tools on redhat
    name: nss-tools","- name: install libnss3-tools on redhat
  yum:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""yum""

- name: install libnss3-tools on redhat
  dnf:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""dnf""

- name: install libnss3-tools on debian
  apt:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == 'apt'",name: install nss-tools on redhat,No misconfigured_snippet Found,1,0
"name: '{{ ntp_service_name }}'
    name: '{{ chrony_daemon_name }}'","enabled: yes

- name: disable ntpd
  failed_when: false
  service:
    name: ntpd
    state: stopped
    enabled: no

- name: disable chronyd
  failed_when: false
  service:
    name: chronyd
    enabled: no
    state: stopped

- name: disable timesyncd
  failed_when: false
  service:
    name: timesyncd
    enabled: no
    state: stopped",name: '{{ ntp_service_name }}',misconfigured_snippet_1,1,0
"ceph_authtool_cmd: ""{{ container_binary + ' run --net=host --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""","print(base64.b64encode(header + key).decode())""
  run_once: true # must run on a single mon only
  # add code to read the key or/and try to find it on other nodes
    secret: ""{{ monitor_keyring.stdout }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: copy the initial key in /etc/ceph (for containers)
  command: >
    cp /var/lib/ceph/tmp/{{ cluster }}.mon..keyring /etc/ceph/{{ cluster }}.mon.keyring
  changed_when: false
  when:
    - cephx
    - containerized_deployment
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: set_fact ceph-authtool container command
  set_fact:
    ceph_authtool_cmd: ""{{ container_binary + ' run --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""
  command: >
    {{ ceph_authtool_cmd }}
     /var/lib/ceph/tmp/{{ cluster }}.mon.keyring --import-keyring /etc/ceph/{{ cluster }}.client.admin.keyring
- name: set_fact ceph-mon container command
  set_fact:
    ceph_mon_cmd: ""{{ container_binary + ' run --rm --net=host -v /var/lib/ceph/:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-mon ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' +ceph_client_docker_image_tag if containerized_deployment else 'ceph-mon' }}""

  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    --keyring /var/lib/ceph/tmp/{{ cluster }}.mon..keyring
  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    - not cephx",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"default_release: ""{{ ceph_stable_release_uca | default('') }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else '' }}""","- name: install redhat ceph-mgr package
  package:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
  when:
    - ansible_os_family == 'RedHat'

- name: install ceph mgr for debian
  apt:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
    default_release: ""{{ ceph_stable_release_uca | default(omit) }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else ''}}""
  when:
    - ansible_os_family == 'Debian'",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: include multisite checks
- name: include master multisite tasks
- name: include secondary multisite tasks
- name: add zone to rgw stanza in ceph.conf","- name: Include multisite checks
  include: checks.yml
- name: Include master multisite tasks
  include: master.yml
  when: ""rgw_zonemaster is defined and rgw_zonemaster""
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False
- name: Include secondary multisite tasks
  include: secondary.yml
  when: ""rgw_zonesecondary is defined and rgw_zonesecondary""
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False
# Continue with common tasks
- name: Add zone to RGW stanza in ceph.conf
  lineinfile:
    dest: /etc/ceph/ceph.conf
    regexp: ""{{ ansible_host }}""
    insertafter: ""^[client.rgw.{{ ansible_host }}]""
    line: ""rgw_zone = {{ rgw_zone }}""
    state: present
  notify:
    - restart rgw",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"with_items: ""{{ groups.get(mds_group_name, []) }}""","---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names","with_items: {{ groups.get(mds_group_name, []) }}",No misconfigured_snippet Found,1,0
"with_items: ""{{ groups.get(mds_group_name, []) }}""","---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names","with_items: {{ groups.get(mds_group_name, []) }}",No misconfigured_snippet Found,1,0
ceph_mon_docker_tag: latest,ceph_osd_docker_tag: latest,ceph_mon_docker_tag: latest,ceph_osd_docker_tag: latest,1,1
"template: >
    src=s3gw.fcgi.j2","---
## Deploy RADOS Gateway
#

- name: Add Ceph extra
  apt_repository: >
    repo=""deb http://ceph.com/packages/ceph-extras/debian {{ ansible_lsb.codename }} main""
    state=present

- name: ""Install Apache, fastcgi and Rados Gateway""
  apt: >
    pkg={{ item }}
    state=present
  with_items:
    - apache2
    - libapache2-mod-fastcgi
    - radosgw

## Prepare Apache
#

- name: Install default httpd.conf
  template: src=httpd.conf dest=/etc/apache2/httpd.conf owner=root group=root

- name: Enable some apache mod rewrite and fastcgi
  command: ""{{ item }}""
  with_items:
    - a2enmod rewrite
    - a2enmod fastcgi

- name: Install Rados Gateway vhost
  template: >
    src=rgw.conf
    dest=/etc/apache2/sites-available/rgw.conf
    owner=root
    group=root

## Prepare RGW
#

- name: Create RGW directory
  file: >
    path=/var/lib/ceph/radosgw/{{ ansible_fqdn }}
    state=directory
    owner=root
    group=root
    mode=0644

- name: Enable Rados Gateway vhost and disable default site
  command: ""{{ item }}""
  with_items:
    - a2ensite rgw.conf
    - a2dissite default
  notify:
    - restart apache2

- name: Install s3gw.fcgi script
  copy: >
    src=s3gw.fcgi
    dest=/var/www/s3gw.fcgi
    mode=0555
    owner=root
    group=root

## If we don't perform this check Ansible will start multiple instance of radosgw
- name: Check if RGW is started
  command: /etc/init.d/radosgw status
  register: rgwstatus
  ignore_errors: True

- name: Start RGW
  command: /etc/init.d/radosgw start
  when: rgwstatus.rc != 0",src=s3gw.fcgi.j2,misconfigured_snippet_1,1,0
"- (journal_collocation and raw_multi_journal)
      or (journal_collocation and osd_directory)
      or (journal_collocation and bluestore)
      or (raw_multi_journal and osd_directory)
      or (raw_multi_journal and bluestore)
      or (osd_directory and bluestore)","- (journal_collocation and not raw_multi_journal)
      or (journal_collocation and not osd_directory)
      or (journal_collocation and not bluestore)
      or (raw_multi_journal and not osd_directory)
      or (raw_multi_journal and not bluestore)
      or (osd_directory and not bluestore)
      or bluestore",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
name: python-pip,"- name: install pip on debian
  apt:
    name: pip
    state: present
  when: ansible_os_family == 'Debian'

- name: install pip on redhat
  yum:
    name: python-pip
    state: present
  when: ansible_os_family == 'RedHat'",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- (mon_socket is defined and mon_socket.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)","when:
    # we test for both container and non-container
    - (mon_socket_stat is defined and mon_socket_stat.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)

- name: include configure_ceph_command_aliases.yml
  include_tasks: configure_ceph_command_aliases.yml
  when:
    - containerized_deployment",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- import_role:
        name: ceph-infra
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-engine
      when: containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool","---
# This playbook is used to add a new MON to
# an existing cluster. It can run from any machine. Even if the fetch
# directory is not present it will be created.
#
# Ensure that all monitors are present in the mons
# group in your inventory so that the ceph configuration file
# is created correctly for the new OSD(s).
- hosts: mons
  gather_facts: false
  vars:
    delegate_facts_host: true
  pre_tasks:
    - name: gather facts
      setup:
      when: not delegate_facts_host | bool
    - import_role:
        name: ceph-defaults
    - name: gather and delegate facts
      setup:
      delegate_to: ""{{ item }}""
      delegate_facts: true
      with_items: ""{{ groups[mon_group_name] }}""
      run_once: true
      when: delegate_facts_host | bool
  tasks:
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-validate

- hosts: mons
  gather_facts: false
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool
    - import_role:
        name: ceph-config
    - import_role:
        name: ceph-infra
    - import_role:
        name: ceph-mon

# update config files on OSD nodes
- hosts: osds
  gather_facts: true
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-config",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
dest: /etc/default/ceph,"dest: /etc/ceph/{{ cluster }}.conf

- name: configure cluster name
  lineinfile:
    dest: /etc/sysconfig/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""RedHat""

- name: configure cluster name
  lineinfile:
    dest: /etc/default/ceph/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""Debian""",dest: /etc/default/ceph,misconfigured_snippet_1,1,0
"(ceph_health_raw.stdout | default('{}') | from_json)['state'] in ['leader', 'peon']","(ceph_health_raw.stdout | default({}) | from_json)['state'] in ['leader', 'peon']",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"src: ""{{ grafana_yum_repo_template }}""
    dest: ""/etc/yum.repos.d/{{ grafana_yum_repo_template | basename | regex_replace('\\.j2$', '') }}""","- name: Add Grafana repository file [RHEL/CentOS]
  template:
    src: grafana.yum.repo.j2
    dest: /etc/yum.repos.d/grafana.repo
    force: yes
    backup: yes
- name: Import Grafana GPG signing key [Debian/Ubuntu]
  apt_key:
    url: ""https://packagecloud.io/gpg.key""
    state: present
    validate_certs: false
  environment:
    http_proxy: ""{{ http_proxy | default('') }}""
    https_proxy: ""{{ https_proxy | default('') }}""
  when: ansible_pkg_mgr == ""apt""

- name: Add Grafana repository [Debian/Ubuntu]
  apt_repository:
    repo: deb https://packagecloud.io/grafana/stable/debian/ jessie main
    state: present
    update_cache: yes

- name: Install Grafana
  package:
    name: grafana
    state: present
  notify: restart grafana",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check-config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check-rules %s""","- name: Set validator commands for prometheus 2.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '>=')

- name: Set validator commands for prometheus 1.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '<')",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"author: ""Sebastian Gumprich""
        - 6.5
  categories:
    - security
dependencies: []","---
galaxy_info:
  author: Sebastian Gumprich
  description: 'This Ansible role provides numerous security-related ssh configurations, providing all-round base protection.'
  company: Hardening Framework Team
  license: Apache License 2.0
  min_ansible_version: '1.9'
  platforms:
    - name: EL
      versions:
        - 6.4
	- 6.5
    - name: Oracle Linux
      versions:
        - 6.4
        - 6.5
    - name: Ubuntu
      versions:
        - 12.04
        - 14.04
    - name: Debian
      versions:
        - 6
        - 7
   categories:
    - system",author: \,No misconfigured_snippet Found,1,0
- restart win zabbix agent,"- restart win zabbix-agent
    - restart mac zabbix agent",restart win zabbix agent,restart win zabbix-agent,1,1
,"chain: INPUT
    source: ""{{ zabbix_agent_server }}""",,chain: INPUT\nsource: \,1,1
"dest: ""/etc/zabbix/scripts/""","- name: ""Installing user-defined scripts""
  copy:
    src: ""scripts/{{ item }}""
    dest: ""/etc/zabbix/scripts/{{ item }}""
    owner: zabbix
    group: zabbix
    mode: 0644
  notify: restart zabbix-agent
  become: yes
  with_items: ""{{ zabbix_agent_userparameters }}""
  when: zabbix_agent_custom_scripts",dest: \,No misconfigured_snippet Found,1,0
"when: es_start_service and (es_enable_xpack and ""security"" in es_xpack_features) and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))","#perform security actions here now elasticsearch is started
- include: ./xpack/security/elasticsearch-security-native.yml
  when: es_start_service and (es_enable_xpack and '""security"" in es_xpack_features') and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))

#Templates done after restart - handled by flushing the handlers. e.g. suppose user removes security on a running node and doesn't specify es_api_basic_auth_username and es_api_basic_auth_password.  The templates will subsequently not be removed if we don't wait for the node to restart.
#We also do after the native realm to ensure any changes are applied here first and its denf up.
- include: elasticsearch-template.yml
  when: es_templates
  tags:
      - templates",when: es_start_service and (es_enable_xpack and \,No misconfigured_snippet Found,1,0
"lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^DATA_DIR"" insertafter=""^#DATA_DIR"" line=""DATA_DIR={{ es_data_dir }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_USER"" insertafter=""^#ES_USER"" line=""ES_USER={{ es_user }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_GROUP"" insertafter=""^#ES_GROUP"" line=""ES_GROUP={{ es_group }}""","- name: RedHat - configure memory
  lineinfile: dest=/etc/default/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  when: es_heap_size is defined
  register: elasticsearch_configure",lineinfile: dest=/etc/sysconfig/elasticsearch regexp=\,No misconfigured_snippet Found,1,0
"mode: ""2750""","owner: root
    mode: 2750
  copy: src={{ item }} dest={{ es_conf_dir }}/templates owner=root group={{ es_group }} mode=0660",mode: \,No misconfigured_snippet Found,1,0
#no_log: True,no_log: True,no_log: True,no_log: True,1,1
"author: ""Florian Utz""","author: """"
      - xenial",author: \,author: \,1,1
"set -o pipefail;
      set -o pipefail;","- not ubuntu1804cis_skip_for_travis
      - not ubuntu1804cis_skip_for_travis
  shell: |
      set -o pipefail
      df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type d -perm -0002 2>/dev/null | xargs chmod a+t
  args:
      executable: /bin/bash
      - not ubuntu1804cis_allow_autofs
      - autofs_service_status.stdout == ""loaded""
  shell: |
      set -o pipefail
      dmesg | grep -E ""NX|XD"" | grep "" active""
  args:
      executable: /bin/bash",set -o pipefail;,No misconfigured_snippet Found,1,0
"service:
      name: rsyslog
      enabled: yes
      - ubuntu1804cis_syslog == ""rsyslog""","#4.2.4 is here due to dependencies to 4.2.1.x
  command: ""systemctl enable rsyslog""
      - rsyslog_service_status.stdout != ""enabled""",service: name: rsyslog enabled: yes - ubuntu1804cis_syslog == \,command: \,1,1
"file: 
    path: /etc/nginx/sites-enabled/default
    state: absent
    notify: Restart nginx","- name: disable nginx default vhost
  become: yes
  file: path=/etc/nginx/sites-enabled/default
  state: absent
  notify: Restart nginx",file: path: /etc/nginx/sites-enabled/default state: absent notify: Restart nginx,No misconfigured_snippet Found,1,0
"register: mod_descrs_files
- name: Create mod_descr_list variable to order modules
  with_items: ""{{ mod_descrs_files.stdout_lines }}""
- name: Reset mod_descr_list variable
  set_fact: mod_descr_list=[]
- name: Build mod_descr_list for registration
  set_fact:
    mod_descr_list: ""{{ mod_descr_list }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""","---
- name: Build stripes
  become: yes
  shell: ""yarn install && yarn build -- output""
  args:
    chdir: ""{{ stripes_conf_dir }}""
  listen: ""Rebuild stripes""

- name: Record stripes rebuild variable
  set_fact:
    stripes_rebuild: true
  listen: ""Rebuild stripes""

- name: Get module descriptor filenames
  shell: ls {{ stripes_conf_dir }}/ModuleDescriptors
  register: mod_descrs
  changed_when: false
  listen: ""Register modules""

- name: Slurp module descriptors
  slurp: src={{ stripes_conf_dir }}/ModuleDescriptors/{{ item }}
  with_items: ""{{ mod_descrs.stdout_lines }}""
  register: mod_descrs_raw
  listen: ""Register modules""

- set_fact: mod_descrs=[]
  listen: ""Register modules""

- set_fact:
    mod_descrs: ""{{ mod_descrs }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descrs_raw.results }}""
  listen: ""Register modules""

- name: Check module registration
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: mod_reg_status
  listen: ""Register modules""

- name: Register modules with Okapi
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules""
    method: POST
    body_format: json
    body: ""{{ item.1|to_json }}""
    status_code: 201
  when: mod_reg_status.results[item.0].status == 404
  register: mod_register
  changed_when: mod_register.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

- name: Check tenant-module association
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: tenant_modules
  listen: ""Register modules""

- name: Enable modules for tenant
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""
    method: POST
    body_format: json
    body: '{ ""id"" : ""{{ item.1.id }}"" }'
    status_code: 201
  when: tenant_modules.results[item.0].status == 404
  register: tenant_assoc
  changed_when: tenant_assoc.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- include: bootstrap_user.yml create_user=galaxy_tools_create_bootstrap_user
- include: bootstrap_user.yml delete_user=galaxy_tools_delete_bootstrap_user
  when: galaxy_tools_delete_bootstrap_user","- include: bootstrap_user.yml
  when: galaxy_tools_create_bootstrap_user and not galaxy_tools_api_key

  when: galaxy_tools_install_tools

- include: bootstrap_user.yml
  when: galaxy_tools_delete_bootstrap_user and not galaxy_tools_api_key",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
- hosts: master,"---
- hosts: all
  remote_user: root
  gather_facts: no

  tasks:
  - name: Add devices to heketi nodes
    heketi: action=adddevice sshuser=""{{ ssh_user }}""  userkey=""{{ user_key }}"" server=""{{ servername }}""
            node=""{{ node }}"" devices=""{{ item.devices }}""
    with_items: hdict
    register: result

  - debug: msg=""{{ result.results[0]['msg'] }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"name: ""{{ graylog_mongodb_package_dependencies_python2 }}""
  when: ansible_python_version is version('3.0.0', '<')

- name: ""Package dependencies should be installed""
  yum:
    name: ""{{ graylog_mongodb_package_dependencies_python3 }}""
    state: present
  when: ansible_python_version is version('3.0.0', '>=')","- name: Package dependencies should be installed
  yum: name={{ item }} state=installed
  with_items: ""{{ graylog_mongodb_package_dependencies | default([]) }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
min_ansible_version: 2.3,min_ansible_version: 2.2,min_ansible_version: 2.3,min_ansible_version: 2.2,1,1
"loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:","- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath",No misconfigured_snippet Found,misconfigured_snippet_1,0,0
"loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:","author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch","- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath",No misconfigured_snippet Found,misconfigured_snippet_1,0,0
"description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch","author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshConnectivityHealth.asciidoc","# Custom SOP URLs for alerts
sops:
  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshConnectivityHealth.asciidoc",No misconfigured_snippet Found,sops: BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc,0,1
"name: crud
      name: spring-boot-rest-http-crud","apiVersion: template.openshift.io/v1
    iconClass: icon-node
    tags: nodejs, crud
    openshift.io/display-name: Fruit CRUD Application
    openshift.io/provider-display-name: Red Hat, Inc.
    openshift.io/documentation-url: https://github.com/integr8ly/walkthrough-applications.git
    description: Basic CRUD application for fruit
- kind: DeploymentConfig
  apiVersion: apps.openshift.io/v1
    name: crud-app
      app: crud-app
    revisionHistoryLimit: 10
    test: false
      app: crud-app
          app: crud-app
        - name: crud-app
          image: quay.io/integreatly/fruit-crud-app:1.0.1
          resources: {}
          terminationMessagePath: ""/dev/termination-log""
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
        restartPolicy: Always
        terminationGracePeriodSeconds: 30
        dnsPolicy: ClusterFirst
        securityContext: {}
        schedulerName: default-scheduler
- kind: Service
  apiVersion: v1
    name: crud-app
    ports:
    - protocol: TCP
      port: 8080
      app: crud-app
- kind: Route
  apiVersion: route.openshift.io/v1
    name: spring-boot-rest-http-crud
    to:
      kind: Service
      name: crud-app
    port:
      targetPort: 8080
    tls:
      termination: edge
    wildcardPolicy: None",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-service""
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-service""","---
- name: ""Delete project namespace: {{ msbroker_namespace }}""
  shell: oc delete project {{ msbroker_namespace }}
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Delete CRDs
  shell: ""oc delete crd {{ item }}""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0
  with_items: syndesises.syndesis.io

- name: Clean up clusterservicebroker
  shell: ""oc delete clusterservicebrokers.servicecatalog.k8s.io managed-services-broker""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role
  shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role binding
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0",oc delete clusterroles.rbac.authorization.k8s.io managed-service,No misconfigured_snippet Found,1,0
,"ups_namespace: ""{{ eval_ups_namespace | default('unifiedpush') }}""
ups_app_namespaces: ""{{ eval_mdc_namespace | default('mobile-developer-console') }}""
ups_resources:
  - ""{{ ups_operator_resources }}/service_account.yaml""
  - ""{{ ups_operator_resources }}/role.yaml""
  - ""{{ ups_operator_resources }}/role_binding.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_androidvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_iosvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_pushapplication_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_unifiedpushserver_crd.yaml""
ups_operator_deployment: ""{{ ups_operator_resources }}/operator.yaml""
ups_template_dir: /tmp
ups_server_name: unifiedpush
#backup
ups_backup: ""{{ backup_restore_install | default(false) }}""
ups_backup_name: ups-daily-at-midnight
ups_backup_schedule: ""{{ backup_schedule }}""
ups_backup_secret: ""s3-credentials""
ups_backup_secret_namespace: ""{{ backup_namespace }}""
ups_encryption_secret: ''
ups_encryption_secret_namespace: ""{{ backup_namespace }}""
ups_backup_rbac_template:
  - ""{{ backup_resources_location }}/rbac/role-binding-template.yaml""
ups_backup_rbac_resources:
  - ""{{ backup_resources_location }}/rbac/service-account.yaml""
  - ""{{ backup_resources_location }}/rbac/role.yaml""
#monitor
ups_svc_monitor_resources:
  - ""{{ ups_operator_resources }}/service_monitor.yaml""",,No misconfigured_snippet Found,1,0
shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\},"- name: ""include rhsso vars""
  include_vars: ../../rhsso/defaults/main.yml
    launcher_sso_openshift_idp_client_secret: ""{{ 99999 | random | to_uuid }}""
- name: Retrieve secret for launcher openshift sso client
  shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\} | base64 -d
  failed_when: openshift_client_secret_response.stderr != """"
  until: openshift_client_secret_response.stdout
  retries: 50
  delay: 3
  changed_when: openshift_client_secret_response.stdout",oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\},No misconfigured_snippet Found,1,0
shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10Mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} --param=BACKEND_IMAGE_TAG={{ launcher_backend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10Mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} --param=BACKEND_IMAGE_TAG={{ launcher_backend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,No misconfigured_snippet Found,1,0
,"---
- name: Get RH-SSO secure route
  local_action: command oc get route/secure-sso -o template --template \{\{.spec.host\}\} -n {{ rhsso_namespace }}
  register: rhsso_secure_route

- set_fact:
    rhsso_route: ""{{ rhsso_secure_route.stdout }}""

- name: Retrieve RH-SSO Client Config
  local_action: command cat /tmp/client-config.json
  register: client_config_raw

- set_fact:
    client_config: ""{{ client_config_raw.stdout }}""

- name: Add RH-SSO identity provider to master config
  blockinfile:
    path: ""{{ openshift_master_config }}""
    insertafter: ""identityProviders""
    backup: yes
    block: |2
        - name: rh_sso
          challenge: false
          login: true
          mappingInfo: add
          provider:
            apiVersion: v1
            kind: OpenIDIdentityProvider
            clientID: {{ rhsso_client_id }}
            clientSecret: {{ client_config.json.value }}
            urls:
              authorize: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/auth
              token: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/token
              userInfo: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/userinfo
            claims:
              id:
              - sub
              preferredUsername:
               - preferred_username
              name:
              - name
              email:
              - email
  register: master_config_update
  become: yes

- name: restart openshift master api service
  service:
    name: atomic-openshift-master-api
    state: restarted
  become: yes
  when: master_config_update.changed

- name: restart openshift master controller service
  service:
    name: atomic-openshift-master-controllers
    state: restarted
  become: yes
  when: master_config_update.changed

- name: Delete local client config file
  file: path=/tmp/client-config.json state=absent",,No misconfigured_snippet Found,1,0
"- name: Add labels to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}"", ""integreatly-middleware-service"":""true""}}}'
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0
- name: Add labels to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\"", \""integreatly-middleware-service\"":\""true\""}}}'""
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0","- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """"",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
,"when:
        - user_rhsso | default(true) | bool
        - mdc | default(true) | bool",,when: - user_rhsso | default(true) | bool - mdc | default(true) | bool,1,1
"shell: /usr/local/bin/master-restart controllers

# Delete users and identities created by rhsso
-
  name: ""Get all users created by rhsso""
  shell: oc get users | grep 'rh_sso' | awk '{print $1}'
  register: users
  failed_when: false

-
  name: ""Delete users""
  shell:  ""oc delete users {{ users.stdout | replace('\n', ' ') }}""
  when: users.stdout != ''
  failed_when: false","- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """"",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"loop: ""{{ real_users }}""","# handlers file for robot-pkgs

- name: USB controllers warning
  command: >-
    zenity --warning --text {{ usb_warning_msg }}
  become: yes
  become_user: ""{{ item.user }}""
  with_items: ""{{ real_users }}""
  ignore_errors: yes",loop: \,No misconfigured_snippet Found,1,0
"node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address }}""","---
# node_interface: ""{{ ansible_default_ipv4.interface }}""
node_interface: ""enp0s8""
node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address",node_ip_address: \,No misconfigured_snippet Found,1,0
"- name: Install flannel
- name: Prepare and write flannel configuration to etcd
  include: config.yml
- name: Enable flannel on node
  service: name=flanneld enabled=yes
- name: Start flannel on node
  service: name=flanneld state=started
  register: flannel_started
  notify:
     - Restart docker engine","---
- name: Install flannel service (RHEL/CentOS)
  when: ansible_os_family == ""RedHat""
  yum:
    name: flannel
    state: latest
    update_cache: yes

- name: Update flannel config
  template: src=""flanneld.j2"" dest={{ flannel_dir }}/flanneld
  register: change_flannel

- name: Set facts about etcdctl command
  set_fact:
    peers: ""{% for hostname in groups['etcd'] %}http://{{ hostname }}:2379{% if not loop.last %},{% endif %}{% endfor %}""
    conf_file: ""/tmp/config.json""
    conf_loc: ""/{{ flannel_key }}/config""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Create flannel config file to go in etcd
  template: src=flannel-config.json dest={{ conf_file }}
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Load the flannel config file into etcd
  when: etcd_peer_url_scheme == 'http'
  shell: ""/usr/bin/etcdctl --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Load the flannel config file into secure etcd
  when: etcd_peer_url_scheme == 'https'
  shell: ""/usr/bin/etcdctl -cert-file={{ etcd_peer_cert_file }} --ca-file={{ etcd_peer_ca_file }} --key-file={ etcd_peer_key_file }} --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Start and enable flannel on node
  when: change_flannel|succeeded
  service: name=flanneld enabled=no state=started

- name: Reload flanneld
  when: change_flannel|changed or etcd_cert|changed
  service: name=flanneld state=restarted",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
,"directories:
    directories:
      - kubedns/kubedns-autoscale-dp.yml
    directories:
    directories:
    directories:
        kind: namespace
    directories:
      - logging/elasticsearch
      - logging/elasticsearch/elasticsearch-sa.yml
      - logging/elasticsearch/elasticsearch-rbac.yml
      - logging/elasticsearch/elasticsearch-svc.yml
      - logging/elasticsearch/elasticsearch-sts.yml
      - logging/kibana/kibana-anonymous-rbac.yml
    directories:
      - monitoring/gpu-exporter
      - monitoring/prometheus-adapter
        kind: namespace
        namespace: """"
      - monitoring/grafana/grafana-res-definitions.yml
      - monitoring/grafana/grafana-gpu-cluster-definitions.yml
      - monitoring/grafana/grafana-gpu-node-definitions.yml
      - monitoring/grafana/grafana-gpu-pod-definitions.yml
      - monitoring/grafana/grafana-cluster-definitions.yml
      - monitoring/gpu-exporter/gpu-exporter-svc.yml
      - monitoring/gpu-exporter/gpu-exporter-ds.yml
      - monitoring/prometheus-adapter/prometheus-adapter-sa.yml
      - monitoring/prometheus-adapter/prometheus-adapter-rbac.yml
      - monitoring/prometheus-adapter/prometheus-adapter-svc.yml
      - monitoring/prometheus-adapter/prometheus-adapter-cm.yml
      - monitoring/prometheus-adapter/prometheus-adapter-apiservice.yml
      - monitoring/prometheus-adapter/prometheus-adapter-dp.yml
      - monitoring/servicemonitor/kube-state-metrics-sm.yml
      - monitoring/servicemonitor/gpu-exporter-sm.yml
      - monitoring/servicemonitor/grafana-sm.yml",,misconfigured_snippet_1,1,0
"when: ""'pve-no-subscription' in proxmox_repository_line""","- block:

  - name: Remove automatically installed PVE Enterprise repo configuration
    apt_repository:
      repo: ""deb https://enterprise.proxmox.com/debian jessie pve-enterprise""
      filename: pve-enterprise
      state: absent

  - name: Remove subscription popup dialog in web UI
    replace:
      dest: /usr/share/pve-manager/ext6/pvemanagerlib.js
      regexp: ""^          if .data.status !== 'Active'. {""
      replace: ""          if (false) {""
      backup: yes

  when: 'pve-no-subscription' in proxmox_repository_line
  when: proxmox_check_for_kernel_update

  when:
    - proxmox_reboot_on_kernel_update
    - __proxmox_kernel | changed

- name: Remove old Debian kernels
  apt:
    name: ""{{ item }}""
    state: absent
  with_items:
    - linux-image-amd64
    - linux-image-3.16.0-4-amd64
  when: proxmox_remove_old_kernels

- name: LDAP fix for authenticated search
  lineinfile:
    line: ""    $ldap->bind('{{ proxmox_ldap_bind_user }}', password => '{{ proxmox_ldap_bind_password }}');""
    insertbefore: ""ldap->search\\(""
    dest: /usr/share/perl5/PVE/Auth/LDAP.pm
  notify:
    - restart pvedaemon
  when:
    - proxmox_ldap_bind_user is defined
    - proxmox_ldap_bind_password is defined",when: 'pve-no-subscription' in proxmox_repository_line,No misconfigured_snippet Found,1,0
"- name: Mkdir for java installation
      win_file:
        path: '{{ java_path }}\{{ java_folder }}'
        state: directory
    - name: Create temporary directory
      win_tempfile:
        state: directory
      register: temp_dir_path
    - name: Unarchive to temporary directory
      win_unzip:
        src: '{{ java_artifact }}'
        dest: '{{ temp_dir_path }}'
    - name: Find java_folder in temp
      win_find:
        paths: '{{ temp_dir_path }}'
        recurse: false
        file_type: directory
      register: java_temp_folder
    - name: Copy from temporary directory
      win_copy:
        src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
        dest: '{{ java_path }}\{{ java_folder }}'
        remote_src: true

    - name: Check choco
      win_chocolatey:
        name: chocolatey
        state: present

    # https://help.sap.com/viewer/65de2977205c403bbc107264b8eccf4b/Cloud/en-US/76137f42711e1014839a8273b0e91070.html
    - name: 'Install vcredist package prior to using SAP JVM'
      win_chocolatey:
        name: vcredist2013
      register: choco_install
      retries: 15
      delay: 5
      until: choco_install is succeeded","---
- name: Check that the java_folder exists
  win_stat:
    path: '{{ java_path }}\{{ java_folder }}/bin'
  register: java_folder_bin

- name: Install java from tarball
  block:
  - name: Mkdir for java installation
    win_file:
      path: '{{ java_path }}\{{ java_folder }}'
      state: directory

  - name: Create temporary directory
    win_tempfile:
      state: directory
    register: temp_dir_path

  - name: Unarchive to temporary directory
    win_unzip:
      src: '{{ java_artifact }}'
      dest: '{{ temp_dir_path }}'

  - name: Find java_folder in temp
    win_find:
      paths: '{{ temp_dir_path }}'
      recurse: false
      file_type: directory
    register: java_temp_folder

  - name: Copy from temporary directory
    win_copy:
      src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
      dest: '{{ java_path }}\{{ java_folder }}'
      remote_src: true
  when: not java_folder_bin.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"| regex_findall('(https://download[\.\w]+/java/GA/jdk'
 java_major_version|string + '[.\d]+/[\d\w]+/'
 java_major_version|string + '/GPL/openjdk-'
 java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')","---
- name: Set java minor version
  set_fact:
    minor: ""{{ java_minor_version | default('.*', True) }}""

- name: 'Fetch root page {{ openjdk_root_page }}'
  uri:
    url: '{{ openjdk_root_page }}/{{ java_major_version }}/'
    return_content: True
  register: root_page

- name: Find release url
  set_fact:
    release_url: >-
      {{ root_page['content']
        | regex_findall('(https://download\.oracle\.com/java/GA/jdk'
          + java_major_version|string + '[.\d]+/[\d\w]+/'
          + java_major_version|string + '/GPL/openjdk-'
          + java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')
      }}

- name: Exit if OpenJDK version is not General-Availability Release
  fail:
    msg: 'OpenJDK version {{ java_major_version }} not GA Release'
  when: release_url[1] is not defined

- name: 'Get artifact checksum {{ release_url[1] }}'
  uri:
    url: '{{ release_url[1] }}'
    return_content: True
  register: artifact_checksum

- name: Show artifact checksum
  debug:
    var: artifact_checksum.content

- name: 'Download artifact from {{ release_url[0] }}'
  get_url:
    url: '{{ release_url[0] }}'
    dest: '{{ download_path }}'
    checksum: 'sha256:{{ artifact_checksum.content }}'
  register: file_downloaded
  retries: 20
  delay: 5
  until: file_downloaded is succeeded

- name: Set downloaded artifact variable
  set_fact:
    java_artifact: '{{ file_downloaded.dest }}'

- name: Split artifact name
  set_fact:
    parts: >-
      {{ java_artifact
        | regex_findall('^(.*j[dkre]{2})-([0-9]+)[u.]([0-9.]+)[-_]([a-z]+)-(x64|i586)')
        | first | list }}

- name: Set variables based on split
  set_fact:
    java_package: '{{ parts[0][-3:] }}'
    java_major_version: '{{ parts[1] }}'
    java_minor_version: '{{ parts[2] }}'
    java_os: '{{ parts[3] }}'
    java_arch: '{{ parts[4] }}'",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- { src: php.ini.j2, dest: /etc/php5/conf.d/90-php-docker.ini }
  - { src: php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}","- name: install PHP5 packages

- name: Place PHP configuration files in place.
  template: src={{ item.src }} dest={{ item.dest }} owner=root group=root mode=644
  with_items:
  - { src: php/php.ini.j2, dest: /etc/php5/apache2/90-php-docker.ini }
  - { src: php/php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: php/xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}
  notify: restart apache",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"become: yes
  become: yes
  become: yes
  become: yes","- name: ensure rabbitmq is installed
  apt: pkg=rabbitmq-server state=installed
  sudo: yes

- name: activate rabbitmq_management plugin
  shell: ""/usr/sbin/rabbitmq-plugins enable rabbitmq_management""
  sudo: yes

- name: restart rabbitmq
  service: name=rabbitmq-server state=restarted
  sudo: yes

- name: get rabbitmqadmin script
  get_url: url=http://localhost:15672/cli/rabbitmqadmin dest=/usr/local/bin/rabbitmqadmin mode=755
  sudo: yes",become: yes,misconfigured_snippet_1,1,0
"pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int < 16 else 'python3-venv' }}""","when: ansible_lsb.major_release|int >= 8

# The Python 3 virtualenv package is named ""python3.4-venv"" on Ubuntu before
# Xenial
- name: install python 3 virtualenv package
  apt:
    pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int >= 16 else 'python3-venv' }}""
    state: latest
  become: yes",pkg: \,when: ansible_lsb.major_release|int >= 8,1,1
"file: dest=""{{ solr_config_dir }}"" state=directory group=solr mode=""g+rwX"" recurse=yes","- name: create solr group
  group: name=solr state=present
  become: yes

  user: name=solr group=solr groups=""www-data"" comment=""Solr Daemon"" home=""{{ solr_install_dir }}""
- name: solr config directory permission
  file: dest=""{{ solr_config_dir }}"" state=directory owner=vagrant group=solr mode=""g+rwX"" recurse=yes
  become: yes

- name: solr install directory permission",file: dest={{ solr_config_dir }} state=directory group=solr mode=\,No misconfigured_snippet Found,1,0
"shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20 ] ; then exit 1 ; fi ; exit 0""
  args:
    executable: /bin/bash","- name: ""Wait for the running state of ToscaSubmitter""
  shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20] ; then echo '20 seconds expired, unsuccessfully. An error occurred.' ; exit 1 ; fi""
  register: output
  changed_when: output.stdout != """"
  when: f.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"service:
    name: iptables
    state: restarted
    enabled: yes
  service:
    name: micado
    state: restarted
    enabled: yes","- name: Enable packet filtering
  shell: systemctl enable --now iptables",service: name: iptables state: restarted enabled: yes,shell: systemctl enable --now iptables,1,1
"# TODO: Find out why this doesn't work
  command: ""sudo /opt/influxdb/influxd config -config {{influxdb_generated_config}}""
  register: influxdb_merged_config
  tags:
    - influxdb

- name: Write merged config
  copy:
    content: ""{{influxdb_merged_config.stdout}}""
    dest: ""{{influxdb_config_file}}""
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
  tags:
    - influxdb

- name: Ensure directories have correct permissions
  command: ""sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{item}}""
  with_items:
    - ""{{influxdb_meta_dir}}""
    - ""{{influxdb_data_dir}}""
    - ""{{influxdb_hh_dir}}""
    - ""{{influxdb_wal_dir}}""","when: ansible_distribution in ['Ubuntu', 'Debian']
- name: Create data dir
    path: ""{{influxdb_data_dir}}""
    state: directory
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
- name: Create meta dir
    path: ""{{influxdb_meta_dir}}""
- name: Create hh dir
    path: ""{{influxdb_hh_dir}}""
- name: Create config directory
    dest: ""{{influxdb_generated_config}}""
- name: Run config update
  command: ""sudo su -c \""{{influxdb_opt_dir}}/influxd config -config {{influxdb_generated_config}} > {{influxdb_config_file}}\"" {{influxdb_user}}""",No misconfigured_snippet Found,"when: ansible_distribution in ['Ubuntu', 'Debian']",0,1
"file:
    path: ""{{ degoss_test_dir }}""
    state: directory
  loop: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
    clean: ""{{ degoss_clean | bool }}""
    clean_on_failure: ""{{ degoss_clean_on_failure | bool }}""
    debug: ""{{ degoss_debug | bool }}""","- include: versions/latest.yml
  when: goss_version == ""latest""

- include: versions/pinned.yml
  when: goss_version != ""latest""

# set play facts
- name: establish download url
  set_fact:
    goss_download_url: ""{{ goss_github_repo_url }}/releases/download/v{{ goss_real_version }}/goss-linux-amd64""

# create goss directories
- name: create goss directories
  file: path={{ item }} state=directory
  with_items:
    - ""{{ degoss_tmp_root }}""
    - ""{{ degoss_test_root }}""
    - ""{{ degoss_goss_install_dir }}""
  changed_when: degoss_changed_when

# download goss
- name: install
  get_url:
    url: ""{{ goss_download_url }}""
    dest: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    mode: 0755
  changed_when: degoss_changed_when

# symlink
- name: link
  file:
    state: link
    src: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    dest: ""{{ degoss_goss_bin }}""
    force: true
  changed_when: degoss_changed_when

# deploy test files including the main and additional test files
- name: deploy test files
  copy: src={{ goss_file }} dest={{ degoss_test_root }}
  with_items: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
  changed_when: degoss_changed_when

# run the tests
- name: run tests
  goss: executable={{ degoss_goss_bin }} path=""{{ goss_file }}"" format=""{{ goss_output_format }}""
  # never report failure, allowing us to clean up and then report failure later
  failed_when: false
  register: goss_output

# clean everything up
- name: clean
  file: path={{ degoss_tmp_root }} state=absent
  when: degoss_no_clean is undefined and not degoss_no_clean
  changed_when: degoss_changed_when

# our output callback plugin will catch the tag of this and format output accordingly
- name: report errors
  fail: msg=""Goss Tests Failed.""
  when: goss_output.goss_failed
  tags: [format_goss_output]",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
-signer {{ codesign_cert_filename }} -inkey {{ codesign_key_filename }} -certfile {{ cert_file_filename }} -outform DER \,"---
  - name: Gather list of source files
    command: ls {{ netbootxyz_root }}
    register: source_files

  - name: Create directories for signatures
    file:
      path: ""{{ item }}""
      state: directory
    with_items:
      - ""{{ sigs_dir }}""

  - name: Generate signatures for source files
    shell: |
      openssl cms -sign -binary -noattr -in {{ netbootxyz_root }}/{{ item }} \ 
      -signer {{ codesign_cert_location }} -inkey {{ codesign_key_location }} -certfile {{ cert_file_location }} -outform DER \
      -out {{ sigs_dir }}/{{ item }}.sig
    args:
      chdir: ""{{ cert_dir }}""
      warn: false
    with_items:
      - ""{{ source_files.stdout_lines }}""
    tags:
    - skip_ansible_lint",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- ""{{ ansible_os_family|lower }}.yml""","- name: gather os specific variables
  include_vars: ""{{ item }}""
  with_first_found:
    - files:
      - ""{{ ansible_distribution|lower }}-{{ ansible_distribution_major_version|lower }}.yml""
      - ""{{ ansible_distribution|lower }}.yml""
      paths:
      - ../vars
      skip: true
  tags: vars",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}'""","- name: add options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Added' in check_installation_options.stdout""
  when: item.1.command == 'add'
  tags: [configuration, wordpress, wordpress-options]

- name: update options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  changed_when: ""'unchanged' not in check_installation_options.stdout""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'update'
  tags: [configuration, wordpress, wordpress-options]

- name: delete options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Could not delete' not in check_installation_options.stderr""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'delete'",command: \,No misconfigured_snippet Found,1,0
"shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1.name }} --activate""
  when: check_installation_plugins is defined and item.item.1.name and item.rc != 0
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  when: item.1.name

- name: activate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin activate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-activate-plugin]

- name: deactivate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin deactivate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and not item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-deactivate-plugin]","---
# tasks file for wordpress, plugins
- name: identify installation (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  register: check_installation_plugins
  failed_when: False
  changed_when: False
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-is-installed-plugin]

- name: install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1 }} --activate""
  with_items: check_installation_plugins.results
  when: check_installation_plugins is defined and item.item.1 and item.rc != 0
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin]

- name: check install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin-check]",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",springapp_min_heapsize: \,No misconfigured_snippet Found,1,0
"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_heapsize: ""128m""",springapp_min_heapsize: \,springapp_heapsize: \,1,1
"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",springapp_min_heapsize: \,No misconfigured_snippet Found,1,0
"user: name=authz-server home={{ authz_server_dir }} shell=/sbin/nologin
- include: install-release.yml
- include: install-snapshot.yml
- include: install-local.yml","---
- name: Create user
  user: name=authz-server home={{ authz_server_dir }}
  tags: authz-server

- name: Create logging directory
  file: path=/var/log/{{ java_app.name }} state=directory owner=authz-server group=authz-server mode=0755
  tags: authz-server

- name: Get the md5 sum of current jar
  stat: path={{ authz_server_dir }}/{{ authz_server_jar }}
  register: current_jar_stat
  tags: authz-server

- name: Create directory where we download application packages (e.g. jar/war files)
  file: path=~/app-downloads state=directory
  tags: authz-server

- name: Deploy | Download release
  get_url:
    url: ""{{ maven_repo }}/org/openconext/authz-server/{{ authz_server_version }}/authz-server-{{ authz_server_version }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp == '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Download snapshot
  get_url:
    url: ""{{ maven_snapshot_repo }}/org/openconext/authz-server/{{ authz_server_version }}-SNAPSHOT/authz-server-{{ authz_server_version }}-{{ authz_server_snapshot_timestamp }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp != '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Upload from local
  copy: src={{ authz_server_local_jar }} dest=~/app-downloads/{{ authz_server_jar }}
  when: authz_server_local_jar != ''
  tags: authz-server

- name: Get the md5 sum of the candidate
  stat: path=~/app-downloads/{{ authz_server_jar }}
  register: candidate_jar_stat
  tags: authz-server

- name: Replace the jar if it has changed
  shell: cp ~/app-downloads/{{ authz_server_jar }} {{ authz_server_dir }}/{{ authz_server_jar }}
  when: current_jar_stat.stat.exists == false or candidate_jar_stat.stat.md5 != current_jar_stat.stat.md5
  notify: restart authz-server
  tags: authz-server

- name: Copy logging config
  template: src=logback.xml.j2 dest={{ authz_server_dir }}/logback.xml owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy application config
  template: src=application.properties.j2 dest={{ authz_server_dir }}/application.properties owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy start script
  template: src=templates/spring-boot.j2 dest=/etc/init.d/{{ java_app.name }} mode=0755
  notify: restart authz-server
  tags: authz-server",user: name=authz-server home={{ authz_server_dir }} shell=/sbin/nologin,No misconfigured_snippet Found,1,0
"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",springapp_min_heapsize: \,No misconfigured_snippet Found,1,0
"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_heapsize: ""128m""",springapp_min_heapsize: \,springapp_heapsize: \,1,1
"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",springapp_min_heapsize: \,No misconfigured_snippet Found,1,0
service: name=authz-admin state=restarted sleep=45,service: name=authz-admin state=restarted sleep=45`,service: name=authz-admin state=restarted sleep=45,service: name=authz-admin state=restarted sleep=45,1,1
"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",springapp_min_heapsize: \,No misconfigured_snippet Found,1,0
"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_heapsize: ""128m""",springapp_min_heapsize: \,springapp_heapsize: \,1,1
"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",springapp_min_heapsize: \,No misconfigured_snippet Found,1,0
"- name: copy load engineblock sql
  template: src=files/{{ env_name }}/{{ engine_initial_sql }}.j2 dest=/tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''

- name: run load engineblock sql
  shell: mysql -u {{ engine_database_user }} -p{{ engine_database_password | vault }} -h localhost -D {{ engine_database_name }} < /tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''
  register: mysql_output
  changed_when: False # script should be idempotent

  register: migrate_output
  changed_when: ""'no update needed' not in migrate_output.stderr""","- name: Run EngineBlock migrations
  command: ./bin/migrate
  args:
    chdir: ""{{ engine_release_dir }}""
  changed_when: False # TODO How to check when migrate is up to date?",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ shared_path }}/""                  # Dest on host specified in {{ rsync_to }}
    set_remote_user: false
    ansible_ssh_extra_args: ""-l {{ unicorn_user }} -o ControlMaster=auto -o ControlPersist=60s -o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout=600""","---

- name: rsync asset files with ssh forwarding
  synchronize:
    src: ""{{ shared_path }}/{{ migrate_dir }}""  # Source on target host specified in --limit
    dest: ""{{ shared_path }}/{{ migrate_dir }}"" # Dest on host specified in {{ rsync_to }}
    mode: pull
    rsync_opts:
      - ""--chown={{ unicorn_user }}:{{ unicorn_user }}""
  vars:
    ansible_ssh_extra_args: '-o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout 600'
  loop:
    - assets
    - spree
    - system
  loop_control:
    loop_var: migrate_dir
  delegate_to: ""{{ groups[rsync_to][0] }}""",misconfigured_snippet_1,No misconfigured_snippet Found,0,0
"# Remove empty rendered volume templates (e.g. inactivated for an env_type)
- name: Ignore empty rendered volume templates
  set_fact:
    path: ""{{ item }}""
  register: filtered_volumes
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined and lookup('template', item) | length > 1
  tags: deploy

- name: Update volume templates list for this app
  set_fact:
    volumes: ""{{ filtered_volumes | json_query('results[*].ansible_facts.path') | list }}""
  when: app.volumes is defined
  
  with_items: ""{{ volumes }}""","---
# Create volumes for an app

- name: Print app name
  debug: msg=""App name {{ app.name }}""
  tags: route

- name: Make sure application volumes exist
  openshift_raw:
    force: true
    definition: ""{{ lookup('template', item) | from_yaml }}""
    state: present
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined
  tags: volume",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
when: not registries_vault.stat.exists,"---
# Create secrets to login to private docker registries

- name: Retrieve registries vault file
  stat:
    path: ""group_vars/customer/{{ customer }}/{{ env_type }}/secrets/registries.vault.yml""
  register: registries_vault

- block:
    - name: Check if registries vault exists
      debug:
        msg: ""No registries vault is associated with the project""
    - meta: end_play
  when: registries_vault.stat.exists

- name: Import registries variable
  include_vars:
    file: ""{{ registries_vault.stat.path }}""

- include_tasks: tasks/create_docker_registry_secret.yml
  loop: ""{{ registries }}""
  loop_control:
    loop_var: registry",when: not registries_vault.stat.exists,No misconfigured_snippet Found,1,0
"line: ""OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""","---
- name: ""Setting Docker Facts""
  set_fact:
    docker_storage_block_device: ""{{ docker_storage_block_device | default(default_docker_storage_block_device) }}""
    docker_storage_volume_group: ""{{ docker_storage_volume_group | default(default_docker_storage_volume_group) }}""

- name: ""Install Docker""
  yum: 
    name: docker
    state: latest
  notify:
    - enable docker

- name: ""Confige Docker""
  lineinfile: 
    dest: /etc/sysconfig/docker 
    regexp: '^OPTIONS=.*$' 
    line: ""^OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""

- name: ""Check for existing Docker Storage device""
  command: pvs
  register: pvs

- name: ""Set Docker Storage fact if already configured""
  set_fact:
    docker_storage_setup: true
  when: pvs.stdout | search('{{ docker_storage_block_device }}.*{{ docker_storage_volume_group }}')

- name: ""Configure Docker Storage Setup""
  template:
    src: docker-storage-setup.j2
    dest: /etc/sysconfig/docker-storage-setup
  when: docker_storage_setup is undefined
  
- name: ""Run Docker Storage Setup""
  command: docker-storage-setup
  when: docker_storage_setup is undefined
  notify:
  - restart docker

- name: ""Extend the Volume Group for Docker Storage""
  command: lvextend -l 90%VG /dev/{{ docker_storage_volume_group }}/docker-pool
  when: docker_storage_setup is undefined
  notify:
  - restart docker",OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16',No misconfigured_snippet Found,1,0
"| intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host_type_master'])) }}""
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host_type_node'])) }}""","vars:
    env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersection(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersection(groups['tag_clusterid_' ~ cluster_id]
                                     | intersection( groups['tag_host_type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersection(groups['tag_clusterid_' ~ cluster_id]
                                   | intersection(groups['tag_host_type_node'])) }}""
    with_items: ""{{ env_cluster_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ master_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ node_hosts }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host-type_node'])) }}""","env_cluster_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                           | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                      | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups.tags.['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                    | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                | intersect(groups.tags.['tag_host-type_node'])) }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",misconfigured_snippet_1,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",misconfigured_snippet_1,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",misconfigured_snippet_1,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",misconfigured_snippet_1,No misconfigured_snippet Found,0,0
"- { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - [{ name: tempest, group: tempest }]
    - [{ name: tempest, group: tempest }]","---
- name: Ensuring the containers up
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- include: config.yml

- name: Check the configs
  command: docker exec {{ item.name }} /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

# NOTE(jeffrey4l): when config_strategy == 'COPY_ALWAYS'
# and container env['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE',
# just remove the container and start again
- name: Containers config strategy
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_env""
  register: container_envs
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- name: Remove the containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE"" or item[1]['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'
    - item[1]['KOLLA_CONFIG_STRATEGY'] != 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"when: inventory_hostname in groups['glance-api']
  when: inventory_hostname in groups['glance-api']","- include: ceph.yml
  when: enable_ceph | bool

  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']",when: inventory_hostname in groups['glance-api'],No misconfigured_snippet Found,1,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
- include: deploy.yml,"- name: Ensuring the containers up
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false

- include: config.yml

- name: Check the configs
  command: docker exec telegraf /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results

# NOTE(jeffrey4l): when config_strategy == 'COPY_ALWAYS'
# and container env['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE',
# just remove the container and start again
- name: Containers config strategy
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_env""
  register: container_envs

- name: Remove the containers
  kolla_docker:
    name: ""telegraf""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE""

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""telegraf""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"name: ""{{ watcher_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"name: ""{{ watcher_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"name: ""{{ watcher_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"name: ""{{ trove_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"name: ""{{ trove_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"name: ""{{ trove_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"update_cache: yes
    update_cache: yes","- name: Update yum cache
  yum:
    update_cache: yes
  become: True
  when: ansible_os_family == 'RedHat'

# Upgrading docker engine may cause containers to stop. Take a snapshot of the
# running containers prior to a potential upgrade of Docker.

- name: Check which containers are running
  command: docker ps -f 'status=running' -q
  become: true
  # If Docker is not installed this command may exit non-zero.
  failed_when: false
  changed_when: false
  register: running_containers

  register: apt_install_result
  register: yum_install_result

# If any packages were updated, and any containers were running, wait for the
# daemon to come up and start all previously running containers.

- block:
    - name: Wait for Docker to start
      command: docker info
      become: true
      changed_when: false
      register: result
      until: result is success
      retries: 6
      delay: 10

    - name: Ensure containers are running after Docker upgrade
      command: ""docker start {{ running_containers.stdout }}""
      become: true
  when:
    - install_result is changed
    - running_containers.rc == 0
    - running_containers.stdout != ''
  vars:
    install_result: ""{{ yum_install_result if ansible_os_family == 'RedHat' else apt_install_result }}""
  when:
    - ansible_distribution|lower == ""ubuntu""
    - item != """"
  when:
    - ansible_os_family == 'RedHat'
    - item != """"",update_cache: yes,No misconfigured_snippet Found,1,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"name: ""{{ octavia_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"name: ""{{ octavia_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
"name: ""{{ octavia_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",name: \,misconfigured_snippet_1,1,0
,"---
- name: Restart rabbitmq container
  vars:
    service_name: ""rabbitmq""
    service: ""{{ rabbitmq_services[service_name] }}""
    config_json: ""{{ rabbitmq_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    rabbitmq_container: ""{{ check_rabbitmq_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes }}""
  when:
    - action != ""config""
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or rabbitmq_confs.changed | bool
      or rabbitmq_container.changed | bool",,No misconfigured_snippet Found,1,0
"# NOTE(mgoddard): Currently (just prior to Stein release), sending SIGHUP to
# nova compute services leaves them in a broken state in which they cannot
# start new instances. The following error is seen in the logs:
# ""In shutdown, no new events can be scheduled""
# To work around this we restart the nova-compute services.
# Speaking to the nova team, this seems to be an issue in oslo.service,
# with a fix proposed here: https://review.openstack.org/#/c/641907.
# This issue also seems to affect the proxy services, which exit non-zero in
# reponse to a SIGHUP, so restart those too.
# TODO(mgoddard): Remove this workaround when this bug has been fixed.
- name: Send SIGHUP to nova services
  become: true
  command: docker exec -t {{ item.value.container_name }} kill -1 1
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - not item.key.startswith('nova-compute')
    - not item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""
- name: Restart nova compute and proxy services
  become: true
  kolla_docker:
    action: restart_container
    common_options: ""{{ docker_common_options }}""
    name: ""{{ item.value.container_name }}""
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - item.key.startswith('nova-compute')
      or item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""","---
# This play calls sighup on every service to refresh upgrade levels
- name: Sighup nova-api
  command: docker exec -t nova_api kill -1 1
  when: inventory_hostname in groups['nova-api']

- name: Sighup nova-conductor
  command: docker exec -t nova_conductor kill -1 1
  when: inventory_hostname in groups['nova-conductor']

- name: Sighup nova-consoleauth
  command: docker exec -t nova_consoleauth kill -1 1
  when: inventory_hostname in groups['nova-consoleauth']

- name: Sighup nova-novncproxy
  command: docker exec -t nova_novncproxy kill -1 1
  when:
    - inventory_hostname in groups['nova-novncproxy']
    - nova_console == 'novnc'

- name: Sighup nova-scheduler
  command: docker exec -t nova_scheduler kill -1 1
  when: inventory_hostname in groups['nova-scheduler']

- name: Sighup nova-spicehtml5proxy
  command: docker exec -t nova_spicehtml5proxy kill -1 1
  when:
    - inventory_hostname in groups['nova-spicehtml5proxy']
    - nova_console == 'spice'

- name: Sighup nova-compute
  command: docker exec -t nova_compute kill -1 1
  when: inventory_hostname in groups['compute']",No misconfigured_snippet Found,misconfigured_snippet_1,0,0
restart_policy: no,"---
- name: Running trove bootstrap container
  kolla_docker:
    action: ""start_container""
    common_options: ""{{ docker_common_options }}""
    detach: False
    environment:
      KOLLA_BOOTSTRAP:
      KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""
    image: ""{{ trove_api_image_full }}""
    labels:
      BOOTSTRAP:
    name: ""bootstrap_trove""
    restart_policy: ""never""
    volumes:
      - ""{{ node_config_directory }}/trove-api/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""kolla_logs:/var/log/kolla/""
      - ""trove:/var/lib/trove/""
  run_once: True
  delegate_to: ""{{ groups['trove-api'][0] }}""",restart_policy: no,No misconfigured_snippet Found,1,0
"- ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/aodh.conf""","---
- name: Ensuring config directories exist
  file:
    path: ""{{ node_config_directory }}/{{ item }}""
    state: ""directory""
    recurse: yes
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over config.json files for services
  template:
    src: ""{{ item }}.json.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/config.json""
  with_items:
    - ""aodh-api""
    - ""aodh-listener""
    - ""aodh-evaluator""
    - ""aodh-notifier""

- name: Copying over aodh.conf
  merge_configs:
    vars:
      service_name: ""{{ item }}""
    sources:
      - ""{{ role_path }}/templates/aodh.conf.j2""
      - ""{{ node_custom_config }}/global.conf""
      - ""{{ node_custom_config }}/database.conf""
      - ""{{ node_custom_config }}/messaging.conf""
      - ""{{ node_custom_config }}/aodh.conf""
      - ""{{ node_custom_config }}/aodh/{{ item }}.conf""
      - ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/{{ item }}.conf""
    dest: ""{{ node_config_directory }}/{{ item }}/aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over wsgi-aodh files for services
  template:
    src: ""wsgi-aodh.conf.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/wsgi-aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"cloudkitty_processor_dimensions: ""{{ default_container_dimensions }}""","dimensions: ""{{ cloudkitty_api_dimensions }}""
    dimensions: ""{{ cloudkitty_processor_dimensions }}""
cloudkitty_processor_diensions: ""{{ default_container_dimensions }}""
cloudkitty_api_dimensions: ""{{ default_container_dimensions }}""",cloudkitty_processor_dimensions: \,cloudkitty_processor_diensions: {{ default_container_dimensions }},1,1
"command: docker exec -t kolla_toolbox /usr/bin/ansible localhost
  command: docker exec -t kolla_toolbox /usr/bin/ansible localhost","- name: Creating Nova-api database
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_db
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'""
  register: database_api
  changed_when: ""{{ database_api.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""

- name: Reading json from variable
  set_fact:
    database_api_created: ""{{ (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""

- name: Creating Nova-api database user and setting permissions
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_user
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'
        password='{{ nova_api_database_password }}'
        host='%'
        priv='{{ nova_api_database_name }}.*:ALL'
        append_privs='yes'""
  register: database_api_user_create
  changed_when: ""{{ database_api_user_create.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api_user_create.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api_user_create.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""",command: docker exec -t kolla_toolbox /usr/bin/ansible localhost,No misconfigured_snippet Found,1,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"vhost: ""{{ notify_vhost }}""","---
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Usage:
#  To use this common task to create to create the user and vhost if
#  needed for the messaging backend configured for Notify communications.
#  To used this common task, the variables ""notify_user"", ""notify_password""
#  and ""notify_vhost"" must be set.

- name: Ensure Notify Rabbitmq vhost
  rabbitmq_vhost:
    name: ""{{ notify_vhost }}""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  tags:
    - common-rabbitmq
  when:
    - oslomsg_notify_transport == ""rabbit""

- name: Ensure Notify Rabbitmq user
  rabbitmq_user:
    user: ""{{ notify_user }}""
    password: ""{{ notify_password }}""
    vhost: ""{{ vhost }}""
    configure_priv: "".*""
    read_priv: "".*""
    write_priv: "".*""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  no_log: true
  tags:
    - common-rabbitmq
  when:
    - oslomsge_notify_transport == ""rabbit""",vhost: \,No misconfigured_snippet Found,1,0
"domain create {{ stack_user_domain_name }} --description ""Owns users and projects created by heat""","- name: Create heat domain
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              domain create {{ stack_domain }} --description ""Owns users and projects created by heat""
  ignore_errors: true

- name: Create heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              user create --domain {{ stack_user_domain_name }} --password {{ stack_domain_admin_password }} {{ stack_domain_admin }}
  ignore_errors: true

- name: Retrieve heat domain id
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
                    domain show {{ stack_user_domain_name }} | grep -oE -m 1 ""[0-9a-f]{32}""

- name: Assign admin role to heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin",domain create {{ stack_user_domain_name }} --description \,No misconfigured_snippet Found,1,0
"gnocchi_git_install_branch: b3b49c87e866475c149343fd77408bef259c5534 # HEAD of ""master"" as of 02.02.2017","gnocchi_git_install_branch: 908ca555f6a14547082e38e4f114926ec384a1bd # HEAD of ""master"" as of 24.01.2017",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"pip_install_options_fact: ""{{ pip_install_options|default('') }} --constraint /opt/developer-pip-constraints.txt --constraint /opt/requirements/upper-constraints.txt""
  tags:
    - neutron-install
    - neutron-pip-packages

- name: Set pip_install_options_fact when not in developer mode
  set_fact:
    pip_install_options_fact: ""{{ pip_install_options|default('') }}""
  when:
    - not neutron_developer_mode | bool
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""","extra_args: ""{{ pip_install_options|default('') }}""",No misconfigured_snippet Found,extra_args: \,0,1
"- name: Create tmpfiles.d entry
    src: ""neutron-systemd-tmpfiles.j2""
    dest: ""/etc/tmpfiles.d/{{ program_name }}.conf""","---
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create neutron TEMP dirs
  file:
    path: ""{{ item.path }}/{{ program_name }}""
    state: directory
    owner: ""{{ system_user }}""
    group: ""{{ system_group }}""
    mode: ""2755""
  with_items:
    - { path: ""/var/run"" }
    - { path: ""/var/lock"" }

- name: Create tempfile.d entry
  template:
    src: ""neutron-systemd-tempfiles.j2""
    dest: ""/etc/tmpfiles.d/neutron.conf""
    mode: ""0644""
    owner: ""root""
    group: ""root""

- name: Place the systemd init script
  template:
    src: ""neutron-systemd-init.j2""
    dest: ""/etc/systemd/system/{{ program_name }}.service""
    mode: ""0644""
    owner: ""root""
    group: ""root""
  register: systemd_init

- name: Reload the systemd daemon
  command: ""systemctl daemon-reload""
  when: systemd_init | changed
  notify:
    - Restart neutron services",misconfigured_snippet_1,No misconfigured_snippet Found,0,0
"until: _stop  is success
  until: _start  is success","service:
    enabled: yes
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
- name: Stop services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""stopped""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _stop
  until: _stop | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
# Note (odyssey4me):
# The policy.json file is currently read continually by the services
# and is not only read on service start. We therefore cannot template
# directly to the file read by the service because the new policies
# may not be valid until the service restarts. This is particularly
# important during a major upgrade. We therefore only put the policy
# file in place after the service has been stopped.
#
- name: Copy new policy file into place
  copy:
    src: ""/etc/nova/policy.json-{{ nova_venv_tag }}""
    dest: ""/etc/nova/policy.json""
    owner: ""root""
    group: ""{{ nova_system_group_name }}""
    mode: ""0640""
    remote_src: yes
  listen: ""Restart nova services""
- name: Start services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""started""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _start
  until: _start | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
- name: Wait for the nova-compute service to initialize
  command: ""openstack --os-cloud default compute service list --service nova-compute --format value --column Host""
  register: _compute_host_list
  retries: 10
  delay: 5
  until: ""ansible_nodename in _compute_host_list.stdout_lines""
  when:
    - ""'nova_compute' in group_names""
    - ""nova_discover_hosts_in_cells_interval | int < 1""
  listen: ""Restart nova services""
  service:
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  register: _restart
  until: _restart | success
  when:
    - inventory_hostname in groups['nova_api_placement']",until: _stop  is success,No misconfigured_snippet Found,1,0
-mtime +{{ image_cache_expire_days|int - 1 }} {% endif %}|,"- name: Clean image cache directory
  shell: >-
    find {{ image_cache_dir }} -type f
    {% if not image_cache_dir_cleanup|bool %}
    -mtime +{{ image_cache_expire_days - 1 }} {% endif %}|
    xargs --no-run-if-empty -t rm -rf",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- teardown-environment
    - teardown-provision","# This teardown role will destroy all vms defined in the overcloud_nodes
# key, and the undercloud
- name:  Teardown undercloud and overcloud vms
  hosts: virthost
  gather_facts: yes
  roles:
    - libvirt/teardown
  tags:
    - teardown-all
    - teardown-virthost
    - teardown-nodes

# This teardown role will destroy libvirt networks
- name: Tear down environment
  hosts: virthost
  roles:
    - environment/teardown
  tags:
    - teardown-all
    - teardown-virthost

# Finally, we conditionally remove basic setup (users,
# groups, directories)to start from scratch
- name: Teardown user setup on virt host
  hosts: virthost
  roles:
    - provision/teardown
  tags:
    - teardown-all",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- when: networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length > 0","# If virtualport_type is defined for any networks, include OVS dependencies
- when: ""{{ networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length is greaterthan 0}}""
  block:

  # Install OVS dependencies
  - name: Install OVS dependencies
    include_role:
      name: 'parts/ovs'

  # Create any OVS Bridges that have been defined
  - name: Create OVS Bridges
    openvswitch_bridge:
      bridge: ""{{ item.bridge }}""
      state: present
    when: item.virtualport_type is defined and item.virtualport_type == ""openvswitch""
    with_items: ""{{ networks }}""
    become: true","when: networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length > 0",No misconfigured_snippet Found,1,0
"when: undercloud_install_cli_options == '--use-heat' or containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_t_h_t_undercloud }}""
        dest: ""./hieradata-overrides-t-h-t-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-t-h-t-undercloud.yaml""
  when:
    - not undercloud_install_cli_options
    - not containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_classic_undercloud }}""
        dest: ""./hieradata-overrides-classic-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-classic-undercloud.yaml""

- name: Create undercloud configuration
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""","# Creat the scripts that will be used to deploy the undercloud
# environment.
- name: Create undercloud configuration
  template:
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""
    mode: 0600

- name: Create undercloud hieradata overrides
  template:
    src: ""{{ undercloud_hieradata_override_file }}""
    dest: ""./quickstart-hieradata-overrides.yaml""
    mode: 0600

- name: Create undercloud install script
  template:
    src: ""{{ undercloud_install_script }}""
    dest: ""{{ working_dir }}/undercloud-install.sh""
    mode: 0755",when: undercloud_install_cli_options == '--use-heat' or containerized_undercloud|bool,No misconfigured_snippet Found,1,0
nodepool_cirros_checksum: md5:d56d54f110654dfd29b0e8ed56e6cda8,"nodepool_cirros_url: http://download.cirros-cloud.net/0.3.6/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_dest: /opt/cache/files/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_checksum: md5:a30f156a83471adf89ebb70dc96d0c82",md5:d56d54f110654dfd29b0e8ed56e6cda8,nodepool_cirros_url: http://download.cirros-cloud.net/0.3.6/cirros-0.3.6-x86_64-disk.img,1,1
"tempest_init: ""tempest init {{ tempest_dir }}""
     tempestconf: ""/usr/bin/discover-tempest-config""

- name: Create /var/log/containers/tempest
  file:
     path: /var/log/containers/tempest
     state: directory
  become: true

- name: Create /var/lib/tempestdata
  file:
     path: /var/lib/tempestdata
     state: directory
  become: true","tempest_init: ""tempest init {{ tempest_dir }}""
      tempestconf: ""/usr/bin/discover-tempest-config""",tempest init {{ tempest_dir }},tempest init {{ tempest_dir }},1,1
"undercloud_key: ""{{ local_working_dir }}/id_rsa_undercloud""","---
# defaults for all ovb-stack related tasks
local_working_dir: ""{{ lookup('env', 'HOME') }}/.quickstart""
working_dir: /home/stack

release: mitaka

node:
    prefix:
        - ""{{ 1000 |random }}""
        - ""{{ lookup('env', 'USER') }}""
        - ""{{ lookup('env', 'BUILD_NUMBER') }}""
tmp:
    node_prefix: '{{ node.prefix | reject(""none"") | join(""-"") }}-'

os_username: admin
os_password: password
os_tenant_name: admin
os_auth_url: 'http://10.0.1.10:5000/v2.0'
cloud_name: qeos7

stack_name: 'oooq-{{ prefix }}stack'
rc_file: /home/stack/overcloudrc
node_name: 'undercloud'
ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'
undercoud_key: ""{{ local_working_dir }}/id_rsa_undercloud""
node_groups:
    - 'undercloud'
    - 'tester'
templates_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal/templates""
ovb_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal""
heat_template: ""{{ templates_dir }}/quintupleo.yaml""
environment_list:
    - ""{{ templates_dir }}/resource-registry.yaml""
    - ""{{ local_working_dir }}/{{ prefix }}env.yaml""

existing_key_location: '{{ local_working_dir }}'
remove_image_from_host_cloud: false

bmc_flavor: m1.medium
bmc_image: 'bmc-base'
bmc_prefix: '{{ prefix }}bmc'

baremetal_flavor: m1.large
baremetal_image: 'ipxe-boot'
baremetal_prefix: '{{ prefix }}baremetal'

key_name: '{{ prefix }}key'
private_net: '{{ prefix }}private'
node_count: 2
public_net: '{{ prefix }}public'
provision_net: '{{ prefix }}provision'

# QuintupleO-specific params ignored by virtual-baremetal.yaml
undercloud_name: '{{ prefix }}undercloud'
undercloud_image: '{{ prefix }}undercloud.qcow2'
undercloud_flavor: m1.xlarge
external_net:  '10.2.1.0/22'

network_isolation_type: multi-nic

setup_undercloud_connectivity_log: ""{{ working_dir }}/setup_undercloud_connectivity.log""

mtu: 1350
mtu_interface:
  - eth1
pvt_nameserver: 8.8.8.8

external_interface: eth2
external_interface_ip: 10.0.0.1
external_interface_netmask: 255.255.255.0

registered_releases:
  - mitaka
  - newton
  - master
  - rhos-9",undercloud_key: \,No misconfigured_snippet Found,1,0
"fail: msg=""Overcloud nova list does not show expected number of {{ node_to_scale }} services""
  when: post_scale_node_count.stdout|int != {{ final_scale_value|int }}","source {{ working_dir }}/stackrc;
    {{ working_dir }}/scale-deployment.sh &> overcloud_deployment_scale_console.log;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    nova list | grep {{ node_to_scale }} | cut -f2- -d':' | wc -l
  fail: msg=Overcloud nova list does not show expected number of {{ node_to_scale }} services
  when: post_scale_node_count.stdout != {{ final_scale_value }}",Overcloud nova list does not show expected number of 2 services,No misconfigured_snippet Found,1,0
"pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8",pkg_extras: python-setuptools haproxy PyYAML,No misconfigured_snippet Found,1,0
"pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8",pkg_extras: python-setuptools haproxy PyYAML,No misconfigured_snippet Found,1,0
"pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8",pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML,No misconfigured_snippet Found,1,0
"pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8",pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML,No misconfigured_snippet Found,1,0
"name: ""postgresql@{{ postgres_version }}-main.service""","- name: enable postgres
  become: yes
  service:
    name: ""postgresql@{{ postgresql_version }}-main.service""
    state: started
    enabled: yes
  tags:
    - postgres-enable",name: \,misconfigured_snippet_1,1,0
"grants={{ item.1.grants |default(omit)  }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, schema: {{ item.1.schema }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants | default (omit) }}
          object_privs={{ item.1.object_privs |default (omit)}}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, schema: {{ item.1.schema | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""","---
# tasks file for manage-db-users
- name: Manage grants (cdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.oracle_db_name }}
          user={{ db_user }}
          password={{ db_password_cdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_databases }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants

- name: Manage grants (pdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.pdb_name }}
          user={{ db_user }}
          password={{ db_password_pdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_pdbs }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0 is defined and item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"args:
      warn: false
    command: ""engine-setup --accept-defaults --config-append={{ answer_file_path }} {{ offline }}""
    until: health_page is success","---
- block:
  - name: Set answer file path
    set_fact:
      answer_file_path: ""/tmp/answerfile-{{ lookup('pipe', 'date +%Y%m%d%H%M%SZ') }}.txt""


  - name: Use the default answerfile
    template:
      src: answerfile_{{ ovirt_engine_setup_version }}_basic.txt.j2
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is undefined

  - name: Copy custom answer file
    template:
      src: ""{{ ovirt_engine_setup_answer_file_path }}""
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is defined

  - name: Update setup packages
    package:
      name: ""ovirt*setup*""
      state: latest
    when: ovirt_engine_setup_update_setup_packages

  - name: Update all packages
    package:
      name: ""*""
      state: latest
    when: ovirt_engine_setup_update_all_packages

  - name: Set accept defaults parameter if variable is set
    set_fact:
      accept_defaults: ""{{ '--accept-defaults' if ovirt_engine_setup_accept_defaults else '' }}""

  - name: Run engine-setup with answerfile
    command: ""engine-setup --config-append={{ answer_file_path }} {{ accept_defaults }}""
    tags:
      - skip_ansible_lint

  - name: Make sure `ovirt-engine` service is running
    service:
      name: ovirt-engine
      state: started

  - name: Check if Engine health page is up
    uri:
      url: ""http://{{ ansible_fqdn }}/ovirt-engine/services/health""
      status_code: 200
    register: health_page
    retries: 12
    delay: 10
    until: health_page|success

  always:
    - name: Clean temporary files
      file:
        path: ""{{ answer_file_path }}""
        state: 'absent'",command: engine-setup --accept-defaults --config-append={{ answer_file_path }} {{ offline }},No misconfigured_snippet Found,1,0
,- /opt/appdata/themes/nzbget:/app/nzbget/webui:shared,,misconfigured_snippet_1,1,0
"regexp: nzb_backup_dir = """"","- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""nzb_backup_dir =""
    replace: ""nzb_backup_dir = /nzb""
  when: sabnzbd_ini.stat.exists == False

- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""admin_dir = admin""
    replace: ""admin_dir = /admin""
  when: sabnzbd_ini.stat.exists == False",nzb_backup_dir = \,No misconfigured_snippet Found,1,0
"#- debug: msg=""Using following password {{password_input.user_input}}""
#- name: Replace password with user input
#  replace:
#    path: /opt/appdata/plexguide/var.yml
#    regexp: defaultpassword
#    replace: ""{{password_input.user_input}}""","- name: password
  pause:
    prompt: ""Please create a Universal Password (.htaccess / Wordpress & etc)""
  register: pw

- debug: msg=""Using following pw {{pw_input.user_input}}""

- name: Replace password with user input
  replace:
    path: /opt/appdata/plexguide/var.yml
    regexp: defaultpassword
    replace: ""{{pw_input.user_input}}""",No misconfigured_snippet Found,misconfigured_snippet_1,0,0
"traefik.frontend.rule: ""Host:heimdall.{{domain.stdout}}""","############################ Replace Variables
- name: Register Domain
  shell: ""cat /var/plexguide/server.domain""
  register: domain
  ignore_errors: True
############################  Replace Variables
      traefik.frontend.rule: ""Host:heimdall.{domain.stdout}}""",traefik.frontend.rule: \,No misconfigured_snippet Found,1,0
,"---
########### Move Service
  - name: Check MOVE Service
    stat:
      path: ""/etc/systemd/system/move.service""
    register: move

  - name: Stop If Move Service Running
    systemd: state=stopped name=move
    when: move.stat.exists
    
  - name: Install Move Service
    template:
      src: move.js2
      dest: /etc/systemd/system/move.service 
      force: yes
    when: move.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=move daemon_reload=yes enabled=no

  - name: Start Move
    systemd: state=started name=move enabled=yes
    when: move.stat.exists
    
########### rclone
  - name: Check RCLONE Service
    stat:
      path: ""/etc/systemd/system/rclone.service""
    register: rclone

  - name: Stop If RClone Service Running
    systemd: state=stopped name=rclone
    when: rclone.stat.exists
    
  - name: Install RCLONE Service
    template:
      src: rclone.js2
      dest: /etc/systemd/system/rclone.service 
      force: yes
    when: rclone.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=rclone daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=rclone enabled=yes
    when: rclone.stat.exists

########### UNIONFS
  - name: Check UNIONFS Service
    stat:
      path: ""/etc/systemd/system/unionfs.service""
    register: unionfs

  - name: Stop If UNIONFS Service Running
    systemd: state=stopped name=unionfs
    when: unionfs.stat.exists
    
  - name: Install UNIONFS Service
    template:
      src: unionfs.js2
      dest: /etc/systemd/system/unionfs.service 
      force: yes
    when: unionfs.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=unionfs daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=unionfs enabled=yes
    when: unionfs.stat.exists",,misconfigured_snippet_1,1,0
"extport: ""7997""","#!/bin/bash
#
# Version:  Ansible-1
# GitHub:   https://github.com/Admin9705/PlexGuide.com-The-Awesome-Plex-Server
# Author:   Admin9705 & Deiteq
# URL:      https://plexguide.com
#
# PlexGuide Copyright (C) 2018 PlexGuide.com
# Licensed under GNU General Public License v3.0 GPL-3 (in short)
#
#   You may copy, distribute and modify the software as long as you track
#   changes/dates in source files. Any modifications to our software
#   including (via compiler) GPL-licensed code must also be made available
#   under the GPL along with build & install instructions.
#
############################################################# (KEY START)
---
- name: ""Establish Key Variables""
  set_fact:
    intport: ""8000""
    extport: ""7999""
    pgrole: ""{{role_name}}""
    image: ""coderaiser/cloudcmd""

- name: ""Key Variables Recall""
  include_role:
    name: ""pgmstart""
    tasks_from: ""keyvar.yml""
############################################################# (KEY END)
#
############################################################# (BASICS START)

- name: Create Directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - ""/opt/appdata/cloudblitz/""

- name: Check config file exists
  stat:
    path: ""/opt/appdata/cloudblitz/.cloudcmd.json""
  register: cloud_json

- name: Install configblitz.json
  template:
    src: configblitz.json
    dest: /opt/appdata/cloudblitz/.cloudcmd.json
    force: yes
  when: cloud_json.stat.exists == False

############################################################# (BASICS END)
- name: ""Set Default Volume - {{pgrole}}""
  set_fact:
    default_volumes:
      - /:/SERVER
      - /opt/appdata/cloudblitz:/root/

- name: ""Establish Key Variables - {{pgrole}}""
  set_fact:
    default_env:
      PUID: 1000
      PGID: 1000

- name: ""Set Default Labels - {{pgrole}}""
  set_fact:
    default_labels:
      traefik.enable: ""true""
      traefik.frontend.redirect.entryPoint: ""https""
      traefik.frontend.rule: ""Host:{{pgrole}}.{{domain.stdout}}""
      traefik.port: ""{{intport}}""

######################## Deploy PGMSTART
- include_role:
    name: ""pgmstart""",extport: \,No misconfigured_snippet Found,1,0
"- ""443:443""","- name: Create nginx-proxy directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - /opt/nginx-proxy

      - ""443:443""""",443:443,No misconfigured_snippet Found,1,0
enable-telemetry: true,"dashboard:
  public-path-prefix: ""/dashboard""
  internal-proxy: false
  disable-telemetry: false",enable-telemetry: true,dashboard: public-path-prefix: /dashboard internal-proxy: false disable-telemetry: false,1,1
,"pulp_webserver_trusted_root_certificates_path: /usr/local/share/ca-certificates/
pulp_webserver_trusted_root_certificates_update_bin: update-ca-certificates
pulp_webserver_python_cryptography: python3-cryptography",,pulp_webserver_trusted_root_certificates_path: /usr/local/share/ca-certificates/,1,1
"- name: Delegate a master control plane node
  block:
    - name: Lookup control node from file
      command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
      changed_when: false
      register: k3s_control_delegate_raw
    - name: Ensure control node is delegated to for obtaining a token
      set_fact:
        k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""
    - name: Ensure the control node address is registered in Ansible
      set_fact:
        k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""","when: hostvars[item].k3s_control_node is defined
        and hostvars[item].k3s_control_node
  when: k3s_controller_count is defined
        and k3s_controller_count | length > 1

- name: Ensure ansible_host is mapped to inventory_hostname
  lineinfile:
    path: /tmp/inventory.txt
    line: >-
      {{ item }}
      @@@
      {{ hostvars[item].ansible_host | default(hostvars[item].ansible_fqdn) }}
      @@@
      C_{{ hostvars[item].k3s_control_node }}
      @@@
      P_{{ hostvars[item].k3s_primary_control_node | default(False) }}
    create: true
  loop: ""{{ play_hosts }}""
  when: hostvars[item].k3s_control_node is defined

- name: Lookup control node from file
  command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
  changed_when: false
  register: k3s_control_delegate_raw

- name: Ensure control node is delegated to for obtaining a token
  set_fact:
    k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""

- name: Ensure the control node address is registered in Ansible
  set_fact:
    k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""
  when: k3s_control_node_address is not defined",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"zone: ""{{ dns_domain }}""
    zone: ""{{ dns_domain }}.""
    record: ""master-0.{{ env_id }}.{{ dns_domain }}.""
    zone: ""{{ dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ dns_domain }}.""","# Create/update Route 53 zone with new records fro Infra and Master
---
- name: Ensure Route53 zone is present
  route53_zone:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}""
    state: present

- name: Update Route53 with OCP master record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""master.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ master_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode

- name: Update Route53 with OCP infra wildcard record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ infra_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode",zone: \,No misconfigured_snippet Found,1,0
"- role: rhsm
    - configure_rhsm
    - role: config-bonding
    - role: config-vlans
    - role: config-routes
  tags:
    - configure_infra_hosts_networking

    - role: update-host
  tags:
    - update_host
    - role: config-iscsi-client
    - configure_iscsi_client","- hosts: aws-provisioner
  roles:
  - aws/manage-networks",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"body_format: json
    body:
      name: ""{{ atlassian.jira.permission_scheme.name }}""
      description: ""{{ atlassian.jira.permission_scheme.description }}""
      permissions: ""{{ permission_list }}""
  register: permission_scheme_output
    default_permission_scheme_id: ""{{ permission_scheme_output.json.id }}""","---
- name: Create Jira Permission Scheme
  uri:
    url: ""{{ atlassian.jira.url }}/rest/api/2/permissionscheme""
    method: POST
    user: ""{{ atlassian.jira.username }}""
    password: ""{{ atlassian.jira.password }}""
    return_content: yes
    force_basic_auth: yes
    body_format: json
    header:
      - Accept: 'application/json'
      - Content-Type: 'application/json'
    body: ""{{ lookup('template','permissionScheme.json.j2') }}""
    status_code: 201
  register: permissionScheme

- name: Set fact for Permission Scheme ID
  set_fact:
    PermissionScheme: ""{{ permissionScheme.json.id }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"---

    - aws/manage-networks","- hosts: aws-provisioner
  roles:
  - aws/manage-networks",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: ""Reset facts to ensure we start over""
  set_fact:
    inventory_id: """"
    project_id: """"
    job_template_id: """"

  - job_template_id is not defined or job_template_id == """"","- name: ""Get the job template id based on job template name""
    job_template_id: ""{{ item.id }}""
  - item.name|trim == job_template.name|trim
  - ""{{ existing_job_templates_output.rest_output }}""
  block:
  - name: ""Create job template {{ job_template.name }}""
    uri:
      url: ""{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/job_templates/""
      user: ""{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}""
      password: ""{{ ansible_tower.admin_password }}""
      force_basic_auth: yes
      method: POST
      body: ""{{ lookup('template', 'job-template.j2') }}""
      body_format: 'json'
      headers:
        Content-Type: ""application/json""
        Accept: ""application/json""
      validate_certs: no
      status_code: 200,201,400
    register: job_template_creation_output

  - name: ""Get the created job template id""
    set_fact:
      job_template_id: ""{{ job_template_creation_output.json.id }}""
  when:
  - job_template_id is not defined

- name: ""Add credentials to job template: {{ job_template.name }}""
  include_tasks: process-job-template-credentials.yml
  loop: ""{{ job_template.credentials | default([ job_template.credential|trim ]) }}""
  loop_control:
    loop_var: job_template_credential
  when: (job_template.credentials is defined) or (job_template.credential is defined)",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
- docker_install|default(False),"---

- name: ""Install, configure and enable Docker""
  include: docker.yml
  when: 
  - docker_install|default('no') == ""yes""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"fp_ip_address_from: ""{{ opstools_external_ip.stdout }}""","- name: run port forwarding role
  include_role:
      name: network/forward-port
  vars:
      fp_ip_address_from: ""{{ opstools_management_ip.stdout }}""
      fp_destination_port: 8081
  delegate_to: hypervisor
  when:
      - ""'hypervisor' in groups""
      - install.overcloud.opstools_forward|default(True)",fp_ip_address_from: \,No misconfigured_snippet Found,1,0
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: get the vlan number where external network should be served
    - name: create new vlan interface in ovs system
    - name: get the IP address for the external network interface
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalInterfaceDefaultRoute | awk -F' ' '{print $2}'""
      register: stat_ip_result
    - name: configure external gateway's IP for this interface
      shell: ""sudo ip addr add {{ stat_ip_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""

    - debug: var={{ iface_ip_result }}

#      failed_when: ""iface_ip_result.stderr != '' and 'RTNETLINK answers: File exists' not in iface_ip_result.stderr""

    - name: get cidr of the external network
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalNetCidr | awk -F' ' '{print $2}'""
      register: route_result

    - name: add new static route for external network
      shell: ""sudo ip route add {{ route_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""","---
- set_fact:
      isolation_file: ""network-isolation{{ (installer.network.protocol == 'ipv6') | ternary('-v6','') }}.yaml""

- name: append the network environment template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/network/network-environment.yaml \'

- name: append the network isolation template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/{{ isolation_file }} \'

- block:
    - name: get the IP address on the external network default route
      shell: ""cat {{ template_base }}/network/network-environment.yaml | grep ExternalNetworkVlanID | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(\""'\"",'') }} tag={{ result.stdout | replace(\""'\"",'') }} -- set interface vlan{{ result.stdout | replace(\""'\"",'') }} type=internal;""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""

    - name: get the IP address on the external network default route
      shell: ""cat {{ installer.overcloud.template_base }}/network-environment.yaml | grep ExternalNetCidr | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ip addr add {{ result.stdout | replace(\""'\"",'') }}/{{ (installer.network.protocol == 'ipv6') | ternary('64','24') }} dev {{ ansible_interfaces | first }}""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""
  when: installer.network.backend == 'vlan'",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"command: ""sosreport --batch -e openstack_ironic --batch -p openstack,openstack_undercloud,openstack_controller --all-logs --experimental -k docker.all=on --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""","---
- name: Define sosrep folder
  set_fact:
      tmp_sos_dir: '/var/sosrep'

- name: Make sure that sosreport is installed on the host
  yum:
      name: sos
      state: latest

- name: Cleanup sosreport dir if exists
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent

- name: Create dir for sosreport
  file:
      name: ""{{ tmp_sos_dir }}""
      state: directory
      mode: 0644

- name: Collect sosreport logs
  command: ""sosreport --batch --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""
  register: sosreport_result
  failed_when: ""'sosreport has been generated' not in sosreport_result.stdout""

- name: get name of the generated file
  find:
      paths: ""{{ tmp_sos_dir }}""
      patterns: ""sosreport-{{ inventory_hostname }}-*.tar.gz""
      recurse: no
  register: sosreport_file

- name: fetch sosreport archive
  fetch:
      flat: yes
      src: ""{{ item.path }}""
      dest: ""{{ dest_dir }}/""
      validate_checksum: no
  ignore_errors: true
  with_items: ""{{ sosreport_file.files }}""

- name: Cleanup sosreport dir after run
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
,"- include_tasks: pre.yml
        delegate_to: ""{{  vbmc_inventory_host }}""

- block:
      - include_tasks: check.yml
        when: action == 'check'
      - include_tasks: cleanup.yml
        when: action == 'cleanup'
      - include_tasks: remove.yml
        when: action == 'remove'
  delegate_to: ""{{  vbmc_inventory_host }}""",,No misconfigured_snippet Found,1,0
"- name: append the ceph storage flavor name to the base overcloud deploy script
  when:
    - ""not install.storage.external""
    - ""not install.splitstack""
  when:
    - install.storage.config == 'internal'

- name: append the storage ceph template in splitstack deployment
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'
  when:
    - ""install.splitstack""
    - install.version|default(undercloud_version)|openstack_release < 12","- block:
    - name: import ceph facts
      set_fact:
          ceph_facts: ""{{ lookup('file', '{{ inventory_dir }}/{{ hostvars[(groups.ceph|first)].inventory_hostname }}') }}""

    # If you reached this stage, it means a user explicitely entered the storage flag therefore he wants at least 1 node
    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-scale {{ storage_nodes }} \'
      when: ""templates.storage_add_scale | default(True)""

    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-flavor {% if groups[""ceph""] is defined %}ceph{% else %}baremetal{% endif %} \'
      when: ""templates.storage_add_scale | default(True)""

  when: ""not install.storage.external""
  vars:
      storage_nodes: ""{{ (install.storage.nodes|default(0)) or (groups['ceph']|default([])|length) or 1 }}""

- name: append the storage template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'

- name: prepare ceph storage template
  vars:
      # we have different keyrings mapped to each ospd version capabilities
      ceph_compt_version: ""
          {%- if install.version|openstack_release < 8 -%}8
          {%- elif install.version|openstack_release >= 10 -%}10
          {%- else -%}{{ install.version|openstack_release }}{%- endif -%}""
  template:
      src: ""storage/ceph.yml.j2""
      dest: ""{{ template_base }}/ceph.yaml""
      mode: 0755

- name: append the storage ceph custom template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/ceph.yaml \'",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"state: ""{{ ir_default_pip_versions[item] is defined | ternary('present', 'latest') }}""","- name: Include configuration vars
  include_vars: ""vars/config/{{ test.setup }}.yml""

- name: Clone tempest_conf
  git:
      repo: ""{{ tempest_conf.repo }}""
      version: ""{{ tempest_conf.revision | default(omit) }}""
      dest: ""{{ tempest_conf.dir }}""
  register: tempest_conf_repo

- name: Create tempest conf venv with latest pip, setuptools and pbr
  pip:
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      name: ""{{ item }}""
      state: latest
  with_items:
      - pip
      - setuptools
      - pbr

- name: Install tempest_conf
  pip:
      name: "".""
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      chdir: ""{{ tempest_conf.dir }}""

- name: Init tempest
  shell: |
      source .venv/bin/activate
      tempest init ""~/{{ test.dir }}""
  args:
      chdir: ""{{ tempest_conf.dir }}""
      creates: ""~/{{ test.dir }}/etc""

- name: Set facts for configuration run
  set_fact:
      config_command: ""discover-tempest-config""
      config_dir: ""{{ tempest_conf.dir }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"OS::TripleO::Galera::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/galera_internal.yaml""","galera_role:
    name: Galera
        OS::TripleO::Galera::Net::SoftwareConfig: ${deployment_dir}/network/nic-configs/galera_internal.yaml
        OS::TripleO::Galera::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api${ipv6_postfix_underscore}.yaml
    flavor: galera
    host_name_format: 'galera-%index%'",OS::TripleO::Galera::Net::SoftwareConfig: \,No misconfigured_snippet Found,1,0
cp {{ base_image_path }}/{{ base_image }} {{ base_image_path }}/{{ node_image }},"---
# For disks usually it will be only 1 disk so the async will run on nodes rather than disks creation
- name: create disk(s) from vm base image
  shell: |
      {% for num in range(1, item.value.amount + 1, 1) %}
      {% for disk_name, disk_values in item.value.disks.iteritems() %}
      {% set template_image = '{0}-{1}.qcow2'.format(item.value.name, disk_name) %}
      {% set node_image = '{0}-{1}-{2}.qcow2'.format(item.value.name, num - 1, disk_name) %}
      {% if not disk_values.import_url %}

      cp {{ base_image_path }}/{{ template_image }} {{ base_image_path }}/{{ node_image }}
      qemu-img create -f qcow2 -o preallocation={{ disk_values.preallocation }} {{ disk_values.path  }}/{{ node_image }} {{ disk_values.size }}
      {% if disk_name == 'disk1' -%}
      virt-resize --expand /dev/sda1 {{ base_image_path }}/{{ base_image }} {{ disk_values.path }}/{{ node_image }}
      virt-customize -a {{ disk_values.path }}/{{ node_image }} \
      {%- for index in range(item.value.interfaces | length - 1) %}
          --run-command 'cp /etc/sysconfig/network-scripts/ifcfg-eth{0,{{ index + 1 }}} && sed -i s/DEVICE=.*/DEVICE=eth{{ index + 1 }}/g /etc/sysconfig/network-scripts/ifcfg-eth{{ index +1 }}' {% if not loop.last %}\
          {% endif %}
      {% endfor %}
      {% endif %}
      {% endif %}
      {% endfor %}
      {% endfor %}
  with_dict: ""{{ provisioner.topology.nodes }}""
  register: ""vm_disks""
  async: 7200
  poll: 0

- name: Wait for our disks to be created
  async_status:
      jid: ""{{ item.ansible_job_id }}""
  register: disk_tasks
  until: disk_tasks.finished
  retries: 300
  with_items: ""{{ vm_disks.results }}""",cp {{ base_image_path }}/{{ base_image }} {{ base_image_path }}/{{ node_image }},No misconfigured_snippet Found,1,0
"- {role: ""installer/ospd/loadbalancer/"", when: 'loadbalancer' in groups}","- {role: ""installer/ospd/loadbalancer/"", when: provisioner.topology.nodes.loadbalancer is defined }",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"stdout: ""{{ (network_template|selectattr('name_lower','equalto','external')|first).ip_subnet }}""","when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the external vlan id from the network_environment_file
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      vlan_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'equalto', 'external')|first).vlan }}""
  when: use_network_data|bool
  when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the cidr of the external network from network_data
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      route_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'external')|first).ip_subnet }}""
  when: use_network_data|bool
  when: ansible_os_family == ""RedHat""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"when: ""'virthost' in groups""","when: ""groups.virthost is defined""",when: 'virthost' in groups,when: \,1,1
"shell: |
            test -e .venv/bin/activate && source .venv/bin/activate
            subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html
            test -e .venv/bin/activate && source .venv/bin/activate","test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ inventory_dir }}/tempest_results/tempest-results-{{ test_suite }}.*.subunit""
      test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ test_output_filename }}.subunit""
      - ""{{ test_output_filename }}.xml""
      - ""{{ test_output_filename }}.html""
            > {{ test_output_filename }}.subunit;
            testr run {{ ('--parallel --concurrency=' + test.threads|string) if test.threads|default('') else '' }} --subunit '{{ '|'.join(parts) }}' > {{ test_output_filename }}.subunit
      - name: generate results report in HTML format
        command: ""subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: generate results report in JunitXML format
        shell: |
            subunit2junitxml --output-to={{ test_output_filename }}.xml \
                < {{ test_output_filename }}.subunit | subunit2pyunit
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.junitxml is defined

        command: ""sed '/^<testsuite/s/name=\""\""/name=\""{{ test_suite }}\""/' -i {{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined
      - name: add the test suite name in the HTML report title
        command: ""sed 's#<title>\\(.*Test Report\\)</title>#<title>\\1 - {{ test_suite }}</title>#' \
                  -i {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: Fetch subunit results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.subunit""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.subunit""
            flat: yes
            fail_on_missing: yes

      - name: Fetch JUnit XML results file
            src: ""{{ test.dir }}/{{ test_output_filename }}.xml""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined

      - name: Fetch HTML results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.html""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.html""
            flat: yes
            fail_on_missing: no
        when:
            results_formats.html is defined",test -e.venv/bin/activate && source.venv/bin/activate,No misconfigured_snippet Found,1,0
"path: ""{{ overcloud_deploy_script|default('~/overcloud_deploy.sh') }}""","---
- name: Unsupported OS version on undercloud
  fail:
      msg: ""InfraRed supports updates for OpenStack version 11""
  when:
      - not (undercloud_version == ""11"")

- name: Checking overcloud_deploy_file
  stat:
      path: ""~/overcloud_deploy.sh""
  register: overcloud_deploy_file

- name: Deployment script not found
  fail:
      msg: ""Overcloud deployment script not found. Expected path: ~/overcloud_deploy.sh ""
  when: not overcloud_deploy_file.stat.exists or not overcloud_deploy_file.stat.executable

- name: Checking file with overcloud credentials
  stat:
      path: ""~/overcloudrc.v3""
  register: oc_credentials_file

- name: Overcloud RC not found
  fail:
      msg: ""File with OC credentials not found. Expected path: ~/overcloudrc.v3""
  when: not oc_credentials_file.stat.exists or not oc_credentials_file.stat.readable",path: \,No misconfigured_snippet Found,1,0
"""OS::TripleO::CephStorage::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""","---
ceph_role:
    name: CephStorage

    resource_registry:
        ""OS::TripleO::Compute::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""

    flavor: ceph
    host_name_format: 'ceph-%index%'

    services:
        - OS::TripleO::Services::CACerts
        - OS::TripleO::Services::CephOSD
        - OS::TripleO::Services::Kernel
        - OS::TripleO::Services::Ntp
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoPackages
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::SensuClient
        - OS::TripleO::Services::FluentdClient
        - OS::TripleO::Services::VipHosts",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""","networks:
        - InternalApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: get controller flavor
  shell: ""source ~/stackrc; openstack flavor list -c Name -f value | grep 'controller-'""
  register: controller_flavor

  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local'""","- name: set additional propertiesx
  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:baremetal,node:controller-{{ item.0 }},boot_option:local'""
  with_indexed_items: ""{{ groups['controller'] }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: Override postgres params for CentOs or RedHat when ovirt >= 4.2
    - ansible_distribution in ('CentOS', 'RedHat')
    - ansible_distribution in ('CentOS', 'RedHat')","- name: Include postgres params
  include_vars: default.yml

- name: Override postgres params for CentOs or Red Hat when ovirt >= 4.2
  include_vars: postgres95.yml
  when:
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')

- name: install psycopg2 requirements to run ansible modules managing postgres.
  yum:
    name: ""python-psycopg2""
    state: ""present""

    name: ""{{ postgres_service_name }}""
    name: ""{{ postgres_server }}""
- name: scl enable
  shell: 'scl enable rh-postgresql95 bash'
  when:
    - postgresql_status|failed
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version < '4.2'
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
  args:
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version >= '4.2'
    name: ""{{ postgres_service_name }}""
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: ""{{ postgres_config_file }}""
    dest: ""{{ postgres_config_file }}""
    dest: '/usr/lib/systemd/system/{{ postgres_service_name }}.service'
    path: ""{{ postgres_config_file }}""
    name: ""{{ postgres_service_name }}""
- name: create DWH DB user
  become: true
  postgresql_user:
    name: ""{{ item.user }}""
    password: ""{{ item.password }}""
    - user: ""{{ ovirt_engine_db_user }}""
      password: ""{{ ovirt_engine_db_password }}""
    - user: ""{{ ovirt_engine_dwh_db_user }}""
      password: ""{{ ovirt_engine_dwh_db_password }}""
  when: ovirt_engine_dwh_remote_db == True
- name: create engine & DWH DBs
  become: true
  postgresql_db:
    name: ""{{ item.db_name }}""
    owner: ""{{ item.user }}""
    encoding: UTF-8
    lc_collate: en_US.UTF-8
    lc_ctype: en_US.UTF-8
    template: template0
    - db_name: ""{{ ovirt_engine_db_name }}""
      user: ""{{ ovirt_engine_db_user }}""
    - db_name: ""{{ ovirt_engine_dwh_db_name }}""
      user: ""{{ ovirt_engine_dwh_db_user }}""
    name: ""{{ postgres_service_name }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
name: postgresql,"- name: check state of database
  service:
    name: ovirt-engine
    state: running

- name: check state of engine
  service:
    name: ovirt-engine
    state: running

- name: restart of ovirt-engine service
  service:
    name: ovirt-engine
    state: restarted

- name: check health status of page
  uri:
    url: ""http://{{ovirt_engine_hostname}}/ovirt-engine/services/health""
    status_code: 200
  register: health_page
  retries: 12
  delay: 10
  until: health_page|success",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
when: gitlab_runner_registration_token | length > 0  # Ensure value is set,"- name: Register GitLab Runner
  include: register-runner.yml
  when: gitlab_runner_registration_token != ''",when: gitlab_runner_registration_token | length > 0  # Ensure value is set,No misconfigured_snippet Found,1,0
--cache-s3-secret-key '{{ gitlab_runner_cache_s3_secret_key }}',"{% if gitlab_runner_cache_type is defined %}
    --cache-type '{{ gitlab_runner_cache_type }}'
    --cache-s3-server-address '{{ gitlab_runner_cache_s3_server_address }}'
    --cache-s3-access-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-secret-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-bucket-name '{{ gitlab_runner_cache_s3_bucket_name }}'
    --cache-s3-insecure '{{ gitlab_runner_cache_s3_insecure }}'
    --cache-cache-shared '{{ gitlab_runner_cache_cache_shared }}'
    {% endif %}",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
insertafter: '^\s*url =',"- name: Set environment option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^\s*environment ='
    line: '  environment = {{ gitlab_runner.env_vars|default([]) | to_json }}'
    state: present
    insertafter: '^\s*url='
    backrefs: no
  check_mode: no
  notify: restart_gitlab_runner",insertafter: '^\s*url =',No misconfigured_snippet Found,1,0
"dest: ""{{ gitlab_runner_config_file }}""","---
- name: (Windows) Create .gitlab-runner dir
  win_file:
    path: ""{{ gitlab_runner_config_file_location }}""
    state: directory

- name: (Windows) Ensure config.toml exists
  win_file:
    path: ""{{ gitlab_runner_config_file }}""
    state: touch
    modification_time: preserve
    access_time: preserve

- name: (Windows) Set concurrent option
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^(\s*)concurrent =.*'
    line: '$1concurrent = {{ gitlab_runner_concurrent }}'
    state: present
    backrefs: yes
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows

- name: (Windows) Add listen_address to config
  win_lineinfile:
    dest: /etc/gitlab-runner/config.toml
    regexp: '^listen_address =.*'
    line: 'listen_address = ""{{ gitlab_runner_listen_address }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_listen_address | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_windows

- name: (Windows) Add sentry dsn to config
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^sentry_dsn =.*'
    line: 'sentry_dsn = ""{{ gitlab_runner_sentry_dsn }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_sentry_dsn | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows",dest: \,No misconfigured_snippet Found,1,0
"name: ""{{ item }}""","name: ""{{ item }}""
    with_items: ""{{ rocket_chat_dep_packages }}""
      name: 
    with_items: ""{{ rocket_chat_dep_packages }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
file:  CentOS-Base,"- name: Configure default CentOS online repos
    yum_repository:
      name: {{ item }}
      enabled: {{ rock_online_install }}
      file:  CentOS-Base.repo
    with_items:
      - base
      - updates
      - extras

    when: with_elasticsearch
    when: (with_elasticsearch and with_bro)
    when: (with_elasticsearch and with_bro) and bro_mapping.status == 404
  - name: Add broctl wrapper for admin use
    copy:
      src: broctl.sh
      dest: /usr/sbin/broctl
      mode: 0754
      owner: root
      group: root
    when: with_bro

    when: with_pulledpork and not rules_file.stat.exists
    when: with_kibana and rock_online_install
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana and (kibana_config.rock_config is undefined or kibana_config.rock_config != rock_dashboards_version)
    when: with_kibana and with_bro
    when: with_kibana and with_suricata and not with_bro
    when: with_kibana
    when: with_kibana
    seboolean: 
      name: httpd_can_network_connect
      state: yes 
      persistent: yes
        for intf in {{ rock_monifs | join(' ') }}; do
    - name: create kafka suricata topic
           --topic suricata-raw",file:  CentOS-Base,No misconfigured_snippet Found,1,0
"state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""","state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"apt:
    pkg: fail2ban
    state: latest
    update_cache: true
    cache_valid_time: ""{{ apt_cache_valid_time }}""
  template:
    src: ""{{ item }}.j2""
    dest: /etc/fail2ban/{{ item }}
  service:
    name: fail2ban
    state: started
    enabled: yes","---
- name: ensure fail2ban is installed
  apt: pkg=fail2ban state=latest update_cache=true cache_valid_time={{ apt_cache_valid_time }}
  notify:
    - restart fail2ban

- name: ensure fail2ban is configured
  template: src={{ item }}.j2 dest=/etc/fail2ban/{{ item }}
  with_items:
    - jail.local
    - fail2ban.local
  notify:
    - restart fail2ban

- name: ensure fail2ban starts on a fresh reboot
  service: name=fail2ban state=started enabled=yes",misconfigured_snippet_1,No misconfigured_snippet Found,0,0
"local_action: command ansible {{ inventory_hostname }} -m ping{{ (inventory_file == None) | ternary('', ' -i ' + inventory_file | string) }} -u root","local_action: command ansible {{ inventory_hostname }} -m ping -i {{ inventory_file }} -u root
    ansible_ssh_user: ""{{ (root_status.rc == 0) | ternary('root', admin_user) }}""

- name: Announce which user was selected
  debug:
    msg: ""Note: Ansible will attempt connections as user = {{ ansible_ssh_user }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"project_subtree: ""{{ project.subtree }}""","project_subtree: ""{{ project.subtree | default(False) }}""",project_subtree: \,project_subtree: \,1,1
"when: sensu_redis_server
  when: sensu_rabbitmq_server","---

  - name: Ensure the Sensu group is present
    group: name={{ sensu_group_name }}
             state=present
             
  - name: Ensure the Sensu user is present
    user: name={{ sensu_user_name }}
          group={{ sensu_group_name }}
          shell=/bin/false
          home={{ sensu_config_path }}
          createhome=yes
          state=present

  - name: Ensure the Sensu config directory is present
    file: dest={{ sensu_config_path }}/conf.d state=directory recurse=yes
          owner={{ sensu_user_name }} group={{ sensu_group_name }}

  - name: Ensure Sensu dependencies are installed
    pkgin: name=build-essential,ruby21-base state=present

  - name: Ensure Uchiwa (dashboard) dependencies are installed
    pkgin: name=go state=present
    when: sensu_include_dashboard

  - name: Ensure Sensu is installed
    gem: name=sensu state={{ sensu_gem_state }} user_install=no
    notify:
      - restart sensu-client service
    
  - name: Ensure Sensu 'plugins' gem is installed
    gem: name=sensu-plugin state={{ sensu_plugin_gem_state }} user_install=no

  - include: ssl.yml tags=ssl

  - include: rabbit.yml tags=rabbitmq
    when: rabbitmq_server

  - include: redis.yml tags=redis
    when: redis_server

  - include: server.yml tags=server
    when: sensu_master

  - include: dashboard.yml tags=dashboard
    when: sensu_include_dashboard
    
  - include: client.yml tags=client

  - include: plugins.yml tags=plugins
    when: sensu_include_plugins",when: sensu_redis_server\nwhen: sensu_rabbitmq_server,No misconfigured_snippet Found,1,0
enabled: yes,"- name: Deploy Tessen server configuratiuon
  template:
    dest: ""{{ sensu_config_path }}/conf.d/tessen.json""
    owner: ""{{ sensu_user_name }}""
    group: ""{{ sensu_group_name }}""
    src: sensu-tessen.json.j2
  notify: restart sensu-server service

  service:
    name: ""{{ sensu_server_service_name if not se_enterprise else sensu_enterprise_service_name }}""
    state: started
  enabled: yes
  service:
    name: sensu-api
    state: started
    enabled: yes",enabled: yes,No misconfigured_snippet Found,1,0
istio_git_repo: https://github.com/istio/istio.git,"istio_git_repo: https://github.com/istio/istio.git
istio_git_branch: 0.6.0

istio_playbook_release_tag: 0.6.0
istio_playbook_auth: false
istio_playbook_jaeger: false
istio_playbook_delete_resources: false
istio_playbook_cluster_flavour: ocp
istio_playbook_dest: /home/istio
istio_playbook_namespace: istio-system
istio_playbook_addon: grafana,prometheus,servicegraph
istio_playbook_samples:",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
value: sb-2.1.x,"apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: s2i-springboot-example
spec:
  taskRef:
    name: s2i-jdk8
  inputs:
    resources:
      - name: git-repo
        resourceSpec:
          type: git
          params:
            - name: revision
              value: master
            - name: url
              value: https://github.com/snowdrop/rest-http-example
  outputs:
    resources:
      - name: image
        resourceSpec:
          type: image
          params:
            - name: url
              value: quay.io/snowdrop/spring-boot-example",sb-2.1.x,No misconfigured_snippet Found,1,0
"- name: ""Setting service_name fact from config""
    splunk_service_name: ""{{ splunk.service_name }}""
    - ""'service_name' in splunk""
- name: Set Splunk service name
  block:
    - name: Setting SplunkForwarder service
      set_fact:
        splunk_service_name: ""SplunkForwarder.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role == ""splunk_universal_forwarder""
    - name: Setting Splunkd service
      set_fact:
        splunk_service_name: ""Splunkd.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role != ""splunk_universal_forwarder""
    - name: Setting splunk service
      set_fact:
        splunk_service_name: ""splunk""
      when:
        - ansible_system is match(""Linux"")
        - not splunk_systemd

    - name: Setting splunkforwarder Windows service
      set_fact:
        splunk_service_name: ""splunkforwarder""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role == ""splunk_universal_forwarder""

    - name: Setting splunkd Windows service
      set_fact:
        splunk_service_name: ""splunkd""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role != ""splunk_universal_forwarder""
    - splunk_service_name is not defined or not splunk_service_name
    - splunk.enable_service","---
- name: ""Setting service_name to SplunkForwarder.service""
  set_fact:
    splunk_service_name: ""SplunkForwarder.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd
    - splunk.build_location is search('forwarder')

- name: ""Setting service_name to Splunkd.service""
  set_fact:
    splunk_service_name: ""Splunkd.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd is False

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and not ansible_system is match(""Linux"")",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"changed_when:
    - splunk_cluster_bundle_result.stdout.find('Applying') != -1
    - splunk_cluster_bundle_result.stdout.find('bundle') != -1","---
- name: Get indexer count
  set_fact:
    num_indexer_hosts: ""{{ groups['splunk_indexer'] | length }}""

- name: Get default replication factor
  set_fact:
    idxc_search_factor: ""{{ splunk.idxc.search_factor }}""
    idxc_replication_factor: ""{{ splunk.idxc.replication_factor }}""

- name: Lower indexer search/replication factor
  set_fact:
    idxc_search_factor: 1
    idxc_replication_factor: 1
  when: num_indexer_hosts|int < 3

- name: Set indexer discovery
  uri:
    url: ""https://127.0.0.1:{{ splunk.svc_port }}/servicesNS/nobody/system/configs/conf-server""
    method: POST
    user: admin
    password: ""{{ splunk.password }}""
    validate_certs: False
    body: ""name=indexer_discovery&pass4SymmKey={{ splunk.shc.secret }}""
    body_format: json
    headers:
      Content-Type: ""application/x-www-form-urlencoded""
    status_code: 201,409
    timeout: 10
  register: set_indexer_discovery
  changed_when: set_indexer_discovery.status == 201

- name: Set the current node as a Splunk indexer cluster master
  command: ""{{ splunk.exec }} edit cluster-config -mode master -replication_factor {{ splunk.idxc.replication_factor }} -search_factor {{ splunk.idxc.search_factor }} -secret '{{ splunk.idxc.secret }}' -cluster_label '{{ splunk.idxc.label }}' -auth 'admin:{{ splunk.password }}'""
  register: task_result
  until: task_result.rc == 0
  retries: ""{{ retry_num }}""
  delay: 3
  notify:
    - Restart the splunkd service

- name: Flush restart handlers
  meta: flush_handlers

- name: Apply the cluster bundle to the Splunk cluster master
  command: ""{{ splunk.exec }} apply cluster-bundle -auth admin:{{ splunk.password }} --skip-validation --answer-yes""
  register: splunk_cluster_bundle_result
  failed_when: >
    (""No new bundle will be pushed"" not in splunk_cluster_bundle_result.stderr)
    and (""Rolling restart of the peers"" not in splunk_cluster_bundle_result.stderr)
    and (""Applying bundle"" not in splunk_cluster_bundle_result.stdout)
  changed_when: splunk_cluster_bundle_result.stdout.find('Applying bundle') != -1

- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml",changed_when: - splunk_cluster_bundle_result.stdout.find('Applying')!= -1 - splunk_cluster_bundle_result.stdout.find('bundle')!= -1,No misconfigured_snippet Found,1,0
"register: set_symmkey

- include_tasks: trigger_restart.yml
  when: set_symmkey is changed","- name: Set general pass4SymmKey
  ini_file: 
    dest: ""{{ splunk.home }}/etc/system/local/server.conf""
    section: ""general""
    option: ""pass4SymmKey""
    value: ""{{ splunk.pass4SymmKey }}""
  notify:
    - Restart the splunkd service",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- include_tasks: prepare_apps_bundle.yml
- include_tasks: initialize_cluster_master.yml","- include_tasks: initialize_cluster_master.yml
- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml
- include_tasks: ../../../roles/splunk_common/tasks/provision_apps.yml
  when:
    - splunk.apps_location
- include_tasks: push_apps_to_indexers.yml
  when:
    - splunk.apps_location",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"path: ""{{ splunk.app_paths.deployment }}""","- name: Gather all deployment server apps
  find:
    path: ""{{ splunk.home }}/etc/deployment_apps""
    recurse: no
    file_type: directory
  register: deployment_apps

    section: ""serverClass:all:app:{{ item.path | basename }}""
  with_items: ""{{ deployment_apps.files }}""",path: \,No misconfigured_snippet Found,1,0
- include_tasks: ../../../roles/splunk_common/tasks/check_for_required_restarts.yml,- include_tasks: check_for_required_restarts.yml,No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- name: ""Wait for port {{ splunk.svc_port }} to become open""
    port: ""{{ splunk.svc_port }}""","- name: ""Wait for port 8089 to become open""
    port: 8089",port: \,port: 8089,1,1
"license: MIT
  min_ansible_version: 2.0.1","---
galaxy_info:
  author: Justin Leitgeb
  description: Base image and common roles
  company: Stack Builders
  license: Proprietary
  min_ansible_version: 1.2
  platforms:
    - name: Debian
      versions:
        - all

  galaxy_tags:
    - sb-base

dependencies:
  - role: kamaln7.swapfile
    become: yes
    become_method: sudo
    remote_user: administrator
    swapfile_size: ""{{ swap_file_size }}""
    tags:
      - bootstrap

  - role: ansible-role-unattended-upgrades
    become: yes
    become_method: sudo
    remote_user: administrator
    unattended_origins_patterns:
      - 'origin=Debian,archive=${distro_codename},label=Debian-Security'
    unattended_mail: '{{ uu_email_alerts }}'
    unattended_automatic_reboot: false
    tags:
      - bootstrap

  - role: nickjj.fail2ban
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - bootstrap

  - role: geerlingguy.ntp
    become: yes
    become_method: sudo
    remote_user: administrator
    ntp_timezone: UTC
    tags:
      - bootstrap

  - role: jdauphant.ssl-certs
    become: yes
    become_method: sudo
    remote_user: administrator
    ssl_certs_common_name: ""{{ inventory_hostname }}""

    tags:
      - nginx-http
      - nginx-https

  - role: jdauphant.nginx
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - nginx

  - role: ANXS.postgresql
    tags:
      - basic-postgres",license: MIT,No misconfigured_snippet Found,1,0
service: name={{openvpn_service}} state=restarted,"---

- name: openvpn restart
  service: name=openvpn state=restarted",service: name={{openvpn_service}} state=restarted,No misconfigured_snippet Found,1,0
"- include_tasks: system/firewall-deps.yml
  when:
    openvpn_open_firewall | bool
    or openvpn_route_traffic | bool
    or openvpn_client_to_client_via_ip | bool

- include_tasks: system/open-firewall.yml
  when: openvpn_open_firewall | bool","- include_tasks: system/forwarding.yml

- include_tasks: system/firewall.yml

- include_tasks: system/routing.yml
  when: openvpn_route_traffic | bool",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
when: threatstack_hostname and threatstack_policy,"- name: Cloudsight setup default
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }}
  when: not threatstack_hostname and not threatstack_policy
- name: Cloudsight setup policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --policy=""{{ threatstack_policy}}""
  register: setup_result
  when: threatstack_policy and not threatstack_hostname
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and not threatstack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname/policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and threastack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret",when: threatstack_hostname and threatstack_policy,No misconfigured_snippet Found,1,0
"secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""","---
- set_fact:
    resource_group: ""Algo_{{ region }}""

- name: Create a resource group
  azure_rm_resourcegroup:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    name: ""{{ resource_group }}""
    location: ""{{ region }}""
    tags:
        service: algo

- name: Create a virtual network
  azure_rm_virtualnetwork:
    resource_group: ""{{ resource_group }}""
    name: algo_net
    address_prefixes: ""10.10.0.0/16""
    tags:
        service: algo

- name: Create a subnet
  azure_rm_subnet:
    resource_group: ""{{ resource_group }}""
    name: algo_subnet
    address_prefix: ""10.10.0.0/24""
    virtual_network: algo_net
    tags:
        service: algo

- name: Create an instance
  azure_rm_virtualmachine:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    resource_group: ""{{ resource_group }}""
    admin_username: ubuntu
    virtual_network: algo_net
    name: ""{{ azure_server_name }}""
    ssh_password_enabled: false
    vm_size: Standard_D1
    tags:
      service: algo
    ssh_public_keys:
      - { path: ""/home/ubuntu/.ssh/authorized_keys"", key_data: ""{{ lookup('file', '{{ ssh_public_key }}') }}"" }
    image:
      offer: UbuntuServer
      publisher: Canonical
      sku: '16.04-LTS'
      version: latest
  register: azure_rm_virtualmachine

- set_fact:
    ip_address: ""{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].properties.ipConfigurations[0].properties.publicIPAddress.properties.ipAddress }}""

- name: Add the instance to an inventory group
  add_host:
    name: ""{{ ip_address }}""
    groups: vpn-host
    ansible_ssh_user: ubuntu
    ansible_python_interpreter: ""/usr/bin/python2.7""
    easyrsa_p12_export_password: ""{{ easyrsa_p12_export_password }}""
    cloud_provider: azure
    ipv6_support: no

- name: Wait for SSH to become available
  local_action: ""wait_for port=22 host={{ ip_address }} timeout=320""",secret: \,No misconfigured_snippet Found,1,0
"- name: Build python virtual environment
  import_tasks: venv.yml
- name: Include prompts
  import_tasks: prompts.yml
- block:
    - set_fact:
        algo_region: >-
          {% if region is defined %}{{ region }}
          {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
          {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

    - name: Security group created
      cs_securitygroup:
        name: ""{{ algo_server_name }}-security_group""
        description: AlgoVPN security group
      register: cs_security_group

    - name: Security rules created
      cs_securitygroup_rule:
        security_group: ""{{ cs_security_group.name }}""
        protocol: ""{{ item.proto }}""
        start_port: ""{{ item.start_port }}""
        end_port: ""{{ item.end_port }}""
        cidr: ""{{ item.range }}""
      with_items:
        - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

    - name: Keypair created
      cs_sshkeypair:
        name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
        public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
      register: cs_keypair

    - name: Set facts
      set_fact:
        image_id: ""{{ cloud_providers.cloudstack.image }}""
        size: ""{{ cloud_providers.cloudstack.size }}""
        disk: ""{{ cloud_providers.cloudstack.disk }}""
        keypair_name: ""{{ cs_keypair.name }}""

    - name: Server created
      cs_instance:
        name: ""{{ algo_server_name }}""
        root_disk_size: ""{{ disk }}""
        template: ""{{ image_id }}""
        ssh_key: ""{{ keypair_name }}""
        security_groups: ""{{ cs_security_group.name }}""
        zone: ""{{ algo_region }}""
        service_offering: ""{{ size }}""
      register: cs_server

    - set_fact:
        cloud_instance_ip: ""{{ cs_server.default_ip }}""
        ansible_ssh_user: ubuntu
  environment:
    CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
    CLOUDSTACK_REGION: ""{{ algo_cs_region }}""","---
- block:
    - name: Build python virtual environment
      import_tasks: venv.yml

    - name: Include prompts
      import_tasks: prompts.yml

    - block:
      - set_fact:
          algo_region: >-
            {% if region is defined %}{{ region }}
            {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
            {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

      - name: Security group created
        cs_securitygroup:
          name: ""{{ algo_server_name }}-security_group""
          description: AlgoVPN security group
        register: cs_security_group

      - name: Security rules created
        cs_securitygroup_rule:
          security_group: ""{{ cs_security_group.name }}""
          protocol: ""{{ item.proto }}""
          start_port: ""{{ item.start_port }}""
          end_port: ""{{ item.end_port }}""
          cidr: ""{{ item.range }}""
        with_items:
          - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

      - name: Keypair created
        cs_sshkeypair:
          name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
          public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
        register: cs_keypair

      - name: Set facts
        set_fact:
          image_id: ""{{ cloud_providers.cloudstack.image }}""
          size: ""{{ cloud_providers.cloudstack.size }}""
          disk: ""{{ cloud_providers.cloudstack.disk }}""
          keypair_name: ""{{ cs_keypair.name }}""

      - name: Server created
        cs_instance:
          name: ""{{ algo_server_name }}""
          root_disk_size: ""{{ disk }}""
          template: ""{{ image_id }}""
          ssh_key: ""{{ keypair_name }}""
          security_groups: ""{{ cs_security_group.name }}""
          zone: ""{{ algo_region }}""
          service_offering: ""{{ size }}""
        register: cs_server

      - set_fact:
          cloud_instance_ip: ""{{ cs_server.default_ip }}""
          ansible_ssh_user: ubuntu
      environment:
        PYTHONPATH: ""{{ cloudstack_venv }}/lib/python2.7/site-packages/""
        CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
        CLOUDSTACK_REGION: ""{{ algo_cs_region }}""

      rescue:
      - debug: var=fail_hint
        tags: always
      - fail:
        tags: always",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"-out crl/{{ item }}.crt
      creates: crl/{{ item }}.crt","- name: Get active users
  local_action: >
    shell grep ^V index.txt | grep -v ""{{ IP_subject_alt_name }}"" | awk '{print $5}' | sed 's/\/CN=//g'
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
  register: valid_certs

- name: Revoke non-existing users
  local_action: >
    shell openssl ca -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt &&
      openssl ca -gencrl -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt -out crl/{{ item }}.crt
      touch crl/{{ item }}_revoked
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
    creates: crl/{{ item }}_revoked
  environment:
    subjectAltName: ""DNS:{{ item }}""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""

- name: Copy the revoked certificates to the vpn server
  copy:
    src: configs/{{ IP_subject_alt_name }}/pki/crl/{{ item }}.crt
    dest: ""{{ config_prefix|default('/') }}etc/ipsec.d/crls/{{ item }}.crt""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""
  notify:
    - rereadcrls",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"name: ""{{ opensmtpd_extra_packages }}""","---

- name: Enable opensmtpd_service
  service:
    name: ""{{ opensmtpd_service }}""
    arguments: ""{{ opensmtpd_flags }}""
    enabled: yes

- name: Install opensmtpd_extra_packages
  openbsd_pkg:
    name: ""{{ item }}""
  with_items: ""{{ opensmtpd_extra_packages }}""",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"mode: 0644
    group: 'root'
    owner: 'root'",mode: 0600,mode: 0644\ngroup: 'root'\nowner: 'root',mode: 0600,1,1
- irods-rule-engine-plugin-python-4.2.8.0-1,"- name: Ensure iRODS 4.2.7 packages are absent
      - irods-uu-microservices-4.2.7_0.8.1-1
      - irods-sudo-microservices-4.2.7_1.0.0-1
      - davrods-4.2.7_1.4.2-1
      - irods-server-4.2.8-1
      - irods-runtime-4.2.8-1
      - irods-database-plugin-postgres-4.2.8-1
      - irods-rule-engine-plugin-python-4.2.8-1",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
when: ruleset.install_scripts and install_rulesets,"when: ruleset.install_scripts == ""yes"" and install_rulesets == ""yes""",when: ruleset.install_scripts and install_rulesets,when: ruleset.install_scripts == \,1,1
description: Install and configure viasite/zsh-config and oh-my-zsh,"---
galaxy_info:
  author: Stanislav Popov
  company: Viasite
  description: Install and configure popstas/zsh-config and oh-my-zsh
  license: MIT
  min_ansible_version: 1.8
  platforms:
    - name: Ubuntu
      versions:
        - trusty
        - xenial
  categories:
    - system
dependencies: []",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"- wazuh_manager_config.cluster.node_type == ""master"" or wazuh_manager_config.cluster.node_type == ""worker""","when:       
      - wazuh_manager_config.cluster.node_type == ""master""
      - wazuh_manager_config.cluster.node_type == ""master""",wazuh_manager_config.cluster.node_type == \,when:        - wazuh_manager_config.cluster.node_type == \,1,1
"baseurl: ""{{ wazuh_manager_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}-5""
    baseurl: ""{{ wazuh_manager_config.repo.yum }}""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}""","- name: RedHat/CentOS 5 | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}-5""
    - (ansible_facts['os_family']|lower == 'redhat')
    - (ansible_os_family = ansible_distribution_major_version|int <= 5)
  register: repo_v5_manager_installed
- name: RedHat/CentOS/Fedora | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}""
  changed_when: false
    - repo_v5_manager_installed is undefined",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
"od_node_name: ""{{ elasticsearch_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""","- name: Configure node name
    block:
      - name: Setting node name (Elasticsearch)
        set_fact:
          od_node_name: elasticsearch_node_name
        when:
          elasticsearch_node_name is defined and kibana_node_name is not defined

      - name: Setting node name (Kibana)
        set_fact:
          od_node_name: kibana_node_name
        when:
          kibana_node_name is defined

      - name: Setting node name (Filebeat)
        set_fact:
          od_node_name: filebeat_node_name
        when:
          filebeat_node_name is defined


      - ""{{ od_node_name }}.key""
      - ""{{ od_node_name }}.pem""
      - ""{{ od_node_name }}_http.key""
      - ""{{ od_node_name }}_http.pem""
      - ""{{ od_node_name }}_elasticsearch_config_snippet.yml""
      block: ""{{ lookup('file', '{{ local_certs_path }}/certs/{{ od_node_name }}_elasticsearch_config_snippet.yml') }}""
      -h {{ hostvars[od_node_name]['ip'] }}",od_node_name: \,No misconfigured_snippet Found,1,0
"- {
      role: geerlingguy.repo-epel,
      version: 1.2.3,
      tags: [
        ""dependency"",
        ""dependency.epel""
      ],
      when: ansible_os_family == 'RedHat'
  }
  - {
      role: srsp.oracle-java,
      version: 2.19.1,
      tags: [
        ""dependency"",
        ""dependency.java""
      ]
  }
  - {
      role: gantsign.maven,
      version: 4.0.0,
      tags: [
        ""dependency,"",
        ""dependency.maven""
      ]
  }
  - { role: andrewrothstein.terraform,
      version: v2.2.10,
      tags: [
        ""dependency"",
        ""dependency.terraform""
      ]
  }","dependencies:
  - { role: geerlingguy.repo-epel, tags: [""dependency""] }
  - { role: srsp.oracle-java, tags: [""dependency""] }
  - { role: gantsign.maven, tags: [""dependency""] }
  - { role: andrewrothstein.terraform, tags: [""dependency""] }",No misconfigured_snippet Found,No misconfigured_snippet Found,0,0
