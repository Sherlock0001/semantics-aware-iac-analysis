Row_ID,Added_lines_fixing_commit,Added_lines_bug_inducing_commit,number,Fixing_Script_Misconfiguration,Bug_Inducing_Script_Misconfiguration,Fixing_LLM_Found_Label,verification_Fixing,Bug_Inducing_LLM_Found_Label,verification_Fixing_Bug_Inducing
1,"command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""","- name: ""[NC] - Set Nextcloud settings in config.php""
  shell: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_config_settings }}""
- name: ""[NC] - Set Redis Server""
  command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_redis_settings }}""
  when: nextcloud_install_redis_server == True

    mode: 0750",1,"command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""","shell: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""; command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""; mode: 0750",1,1,1,1
2,when: (not _nextcloud_conf.stat.exists) or (_nextcloud_configured.rc is defined and _nextcloud_configured.rc != 0),"check_mode: no
  when: _nextcloud_configured.rc != 0",2,No issue found,No issue found,0,0,0,0
3,when: (nc_sudo_installed_result.rc is defined) and (nc_sudo_installed_result.rc != 0),"register: nc_sudo_installed_result
  when: nc_sudo_installed_result is defined and nc_sudo_installed_result.rc != 0",3,No issue found,No issue found,0,0,0,0
4,"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
","action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""",4,ansible_pkg_mgr is used without validation; util_pkg_state is used without validation; item is used without validation; update_cache is set to yes without validation,"ansible_pkg_mgr is not sanitized, which can lead to command injection; util_pkg_state is not sanitized, which can lead to command injection; item is not sanitized, which can lead to command injection; update_cache is always set to yes, which can lead to unnecessary network traffic and potential exposure to malicious updates",1,1,1,1
5,"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""","- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes",5,ansible_pkg_mgr is used without validation; item is used without validation; util_pkg_state is used without default filter; update_cache is always set to yes,"action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""; ignore_errors: true",1,1,1,0
6,"action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""","action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""",6,"ansible_pkg_mgr is used without validation; util_pkg_state is used without validation; item is used without validation; Potential command injection via ansible_pkg_mgr, util_pkg_state, or item","Potential command injection via ansible_pkg_mgr; Potential command injection via item; Potential command injection via util_pkg_state; Lack of input validation for ansible_pkg_mgr, item, and util_pkg_state; Using default('present') may lead to unintended package installations",1,1,1,0
7,"action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""","- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes",7,"ansible_pkg_mgr is used without validation; util_pkg_state is used without validation; item is used without validation; Potential command injection via ansible_pkg_mgr, util_pkg_state, or item","action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""; ignore_errors: true",1,1,1,0
8,"- (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare(8, '<'))","- (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare
(8, '<'))",8,No issue found,No issue found,0,0,0,0
9,"owner: ""{{ consul_user }}""
    group: ""{{ consul_group }}""","file:
    dest: ""{{ consul_tls_dir }}""
    state: directory
    owner: root
    group: root
    mode: 0755
    copy:
      src: ""{{ consul_src_files }}/{{ consul_ca_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_ca_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_key }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_key }}""
  template:
    src: config_server_tls.json.j2
    dest: ""{{ consul_config_path }}/server/config_server_tls.json""",9,No issue found,No issue found,0,0,0,0
10,"url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ item }}""
    url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script""","url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}/service/siesta/rest/v1/script/{{ item }}""",10,http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ item }}; http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script,http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}/service/siesta/rest/v1/script/{{ item }},1,0,1,0
11,"- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: ""modify auth user admin password {{admin_password}}""
  register: change_password
  until: change_password is not failed
  retries: 5","- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: modify auth user admin password admin",11,"ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""; commands: ""modify auth user admin password {{admin_password}}""",Hardcoded password; Private key file path is predictable; Usage of admin user,1,1,1,1
12,"- name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""{{ec2_info.arista.filter}}""
        architecture: ""x86_64""
    register: arista_amis","- name: BLOCK FOR CISCO AMI
  block:
  - name: find ami for cisco (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""cisco-CSR*BYOL*""
        architecture: ""x86_64""
    register: cisco_ami_list

  - name: save ami for cisco (NETWORKING MODE)
    set_fact:
      cisco_ami: >
        {{ cisco_ami_list.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""cisco""'

- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""*EOS*""
        architecture: ""x86_64""
    register: arista_amis

  - name: save ami for arista eos (NETWORKING MODE)
    set_fact:
      arista_ami: >
        {{ arista_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""arista""'

- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for juniper vsrx (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""junos-vsrx3-x86-64-18.4R1.8--pm*""
        architecture: ""x86_64""
    register: juniper_amis

  - name: save ami for juniper (NETWORKING MODE)
    set_fact:
      juniper_ami: >
        {{ juniper_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""juniper""'",12,No issue found,"region: ""{{ ec2_region }}""; owners: ""679593333241""; when: 'network_type == ""multivendor"" or network_type == ""cisco""'; when: 'network_type == ""multivendor"" or network_type == ""arista""'; when: 'network_type == ""multivendor"" or network_type == ""juniper""'",0,0,1,0
13,"- name: fail on purpose now to let user know code server failed
      debug:
        msg: ""VS code integration has failed in provisioner/roles/code_server/tasks/main.yml""
      failed_when: true","---
- name: remove dns entries for each vs code instance
  include_tasks: teardown.yml
  when: teardown|bool

- name: check to see if SSL cert already applied
  become: no
  get_certificate:
    host: ""{{username}}-code.{{ec2_name_prefix|lower}}.{{workshop_dns_zone}}""
    port: 443
  delegate_to: localhost
  run_once: true
  register: check_cert
  ignore_errors: true
  when:
    - not teardown

- name: perform DNS and SSL certs for ansible control node
  block:
    - name: setup vscode for web browser access
      include_tasks: ""codeserver.yml""
  rescue:
    - debug:
        msg: 'VS code integration has failed'

    - name: make sure tower is on
      shell: ansible-tower-service start
      register: install_tower
      until: install_tower is not failed
      retries: 5

    - name: appends
      set_fact:
        coder_information: |
          - VS code integration has failed, please use direct SSH addresses
      run_once: true
      delegate_to: localhost
      delegate_facts: true
  when:
    - not teardown|bool
    - check_cert is failed",13,No issue found,"get_certificate:
    host: ""{{username}}-code.{{ec2_name_prefix|lower}}.{{workshop_dns_zone}}""
    port: 443; shell: ansible-tower-service start; ignore_errors: true",0,0,1,1
14,"command: systemctl daemon-reload
  tags: skip_ansible_lint","systemd:
    daemon-reload: yes",14,command: systemctl daemon-reload,No issue found,1,0,0,0
15,"copy: src={{ caddy_home }}/caddy dest=/usr/bin/ mode=0755 remote_src=True
  notify: Restart Caddy","unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} copy=no
  unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no

- name: Copy Caddy Binary
  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True",15,copy: src={{ caddy_home }}/caddy dest=/usr/bin/ mode=0755 remote_src=True,unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} copy=no; unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no; copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True,1,0,1,1
16,"retries: 3
  delay: 2
  retries: 3
  delay: 2","user:
    name: ""{{ caddy_user }}""
    system: yes
    createhome: yes
    home: ""{{ caddy_home }}""
  get_url:
    url: https://api.github.com/repos/mholt/caddy/git/refs/tags
    dest: ""{{ caddy_home }}/releases.txt""
    force: yes
  copy:
    content: ""{{ caddy_features }}""
    dest: ""{{ caddy_home }}/features.txt""
  get_url:
    url: ""{{ caddy_url | quote }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    force: yes
    timeout: 300
    retries: 3
    delay: 2
  get_url:
    url: ""{{ caddy_url}}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    timeout: 300
    retries: 3
    delay: 2
  command: >
    gpg
      --keyserver-options timeout={{ caddy_pgp_recv_timeout }}
      --keyserver {{ caddy_pgp_key_server }}
      --recv-keys {{ caddy_pgp_key_id }}
  get_url:
    url: ""{{ caddy_sig_url }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz.asc""
    timeout: 60
    force: yes
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
  command: >
    gpg
      --verify {{ caddy_home }}/caddy.tar.gz.asc
      {{ caddy_home }}/caddy.tar.gz
  unarchive:
    src: ""{{ caddy_home }}/caddy.tar.gz""
    dest: ""{{ caddy_home }}""
    copy: no
    owner: ""{{ caddy_user }}""
  unarchive:
   src: ""{{ caddy_home }}/caddy.tar.gz""
   dest: ""{{ caddy_home }}""
   creates: ""{{ caddy_home }}/caddy""
   copy: no
   owner: ""{{ caddy_user }}""
  copy:
    src: ""{{ caddy_home }}/caddy""
    dest: ""{{ caddy_bin }}""
    mode: 0755
    remote_src: true
  file:
    path: ""{{ item }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0770
  file:
    path: ""{{ caddy_log_dir }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0775
  copy:
    content: ""{{ caddy_config }}""
    dest: ""{{ caddy_conf_dir }}/Caddyfile""
    owner: ""{{ caddy_user }}""
  stat:
    path: /run/systemd/system
  template:
    src: ""{{ item }}""
    dest: /etc/init/caddy.conf
    mode: 0644
  template:
    src: caddy.service
    dest: /etc/systemd/system/caddy.service
    mode: 0644
  service:
    name: caddy
    state: started
    enabled: yes",16,No issue found,"get_url: url: ""{{ caddy_url | quote }}"" dest: ""{{ caddy_home }}/caddy.tar.gz"" force_basic_auth: ""{{ caddy_license != 'personal' }}"" force: yes timeout: 300 retries: 3 delay: 2; get_url: url: ""{{ caddy_url}}"" dest: ""{{ caddy_home }}/caddy.tar.gz"" force_basic_auth: ""{{ caddy_license != 'personal' }}"" timeout: 300 retries: 3 delay: 2; get_url: url: ""{{ caddy_sig_url }}"" dest: ""{{ caddy_home }}/caddy.tar.gz.asc"" timeout: 60 force: yes force_basic_auth: ""{{ caddy_license != 'personal' }}""; command: > gpg --keyserver-options timeout={{ caddy_pgp_recv_timeout }} --keyserver {{ caddy_pgp_key_server }} --recv-keys {{ caddy_pgp_key_id }}; command: > gpg --verify {{ caddy_home }}/caddy.tar.gz.asc {{ caddy_home }}/caddy.tar.gz; copy: src: ""{{ caddy_home }}/caddy"" dest: ""{{ caddy_bin }}"" mode: 0755 remote_src: true; template: src: ""{{ item }}"" dest: /etc/init/caddy.conf mode: 0644; template: src: caddy.service dest: /etc/systemd/system/caddy.service mode: 0644",0,0,1,1
17,"shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'""","when: item.hstore is defined and item.hstore

- name: PostgreSQL | Add uuid-ossp to the database with the requirement
  sudo: yes
  sudo_user: ""{{postgresql_admin_user}}""
  shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;""
  with_items: postgresql_databases
  when: item.uuid_ossp is defined and item.uuid_ossp",17,Shell module usage; Potential SQL injection,"sudo: yes; sudo_user: ""{{postgresql_admin_user}}""; shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;""",1,1,1,1
18,"name: ""postgresql-contrib-{{postgresql_version}}""","- name: PostgreSQL | Extensions | Make sure the development headers are installed
  apt:
    name: libpq-dev
    state: present
  notify:
    - restart postgresql",18,No issue found,No issue found,0,0,0,0
19,"run_once: True
  run_once: True
- name: 'Create folder {{structured_dir_name}} for structured YAML files'
  run_once: True
  run_once: True

- name: 'Create folder {{structured_cvp_name}} for CVP structured YAML files'
  file:
    path: '{{structured_cvp}}'
    state: directory
    mode: 0755
  delegate_to: localhost
  run_once: True
  run_once: True
  run_once: True
  run_once: True","---

- name: 'Cleanup existing folders in {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: absent 
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{structured_dir_name}} for structued YAML files'
  file:
    path: '{{structured_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{eos_config_dir_name}} for EOS Configuration files'
  file:
    path: '{{eos_config_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost",19,run_once: True is repeated multiple times; The task 'Create folder {{structured_dir_name}} for structured YAML files' does not have any action; The variable structured_cvp is not defined before use,mode: 0755; delegate_to: localhost,1,1,1,0
20,- name: Create/invoke script virtualenv for create galaxy admin,"- name: Create/invoke script virtualenv
  pip: name={{ item }} virtualenv={{ galaxy_venv_dir }} virtualenv_command=""{{ pip_virtualenv_command | default( 'virtualenv' ) }}""
  with_items:
    - pyyaml
    - bioblend
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""

- name: Create Galaxy admin user
  command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python /usr/local/bin/create_galaxy_user.py --user {{ galaxy_admin }} --password {{ galaxy_admin_pw }} --key {{ galaxy_admin_api_key }}
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""",20,No issue found,"pip: name={{ item }} virtualenv={{ galaxy_venv_dir }} virtualenv_command=""{{ pip_virtualenv_command | default( 'virtualenv' ) }}""; command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python /usr/local/bin/create_galaxy_user.py --user {{ galaxy_admin }} --password {{ galaxy_admin_pw }} --key {{ galaxy_admin_api_key }}",0,0,1,1
21,"shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /etc/supervisor/conf.d/galaxy.conf""
- debug: var=galaxylogs

- name: get_logs
  shell: ""supervisorctl status""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs","- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs

- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs",21,"shell: ""cat /var/log/supervisor/galaxy*""; shell: ""cat /home/galaxy/galaxy/*.log""; shell: ""cat /etc/supervisor/conf.d/galaxy.conf""; shell: ""supervisorctl status""; ignore_errors: yes; debug: var=galaxylogs",shell: cat /var/log/supervior/galaxy*; ignore_errors: yes; debug var=galaxy_logs,1,0,1,0
22,"- {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/allowed_images.yml""}","- {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/allowed_images.yml""}",22,No issue found,No issue found,0,0,0,0
23,"- export NATIVE_SPEC=""--ntasks=`/usr/bin/nproc` --share""","supervisor_env_vars:
    - export IP_ADDRESS=`curl icanhazip.com`
    - export GALAXY_CONF_FTP_UPLOAD_SITE=""ftp://$IP_ADDRESS""
    - export MASQUERADE_ADDRESS=$IP_ADDRESS
    - export NTASK=""--ntasks=`/usr/bin/nproc` --share""",23,"export NATIVE_SPEC=""--ntasks=`/usr/bin/nproc` --share""","export IP_ADDRESS=`curl icanhazip.com`; export GALAXY_CONF_FTP_UPLOAD_SITE=""ftp://$IP_ADDRESS""; export MASQUERADE_ADDRESS=$IP_ADDRESS; export NTASK=""--ntasks=`/usr/bin/nproc` --share""",1,0,1,1
24,"- name: create data directory for cassandra on mounted storage
  file: path={{data_dir}}/cassandra state=directory owner={{cassandra_user}}  group={{cassandra_user}}

- name: create the cassandra yaml from template
  template: src=cassandra/cassandra.yaml dest={{cassandra_yaml}} owner={{cassandra_user}}  group={{cassandra_user}}
    - restart cassandra","- name: setup yum repo
  copy: src=yum/datastax.repo dest=/etc/yum.repos.d/datastax.repo
  when: ansible_os_family == ""RedHat""   

- name: install packages (Redhat)
  yum: name={{item}} state=present
  with_items:
    - $datastax
    - $cassandra
  tags:
    - packages 
  notify: 
    - restart cassandra
    - configure cassandra
  when: ansible_os_family == ""RedHat"" 

- name: set cassandra to use the ByteOrderedPartitioner (RedHat)
  lineinfile: "" 
    dest=/etc/cassandra/conf/cassandra.yaml  
    regexp=^partitioner 
    line='partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner'""
  notify:
    - restart cassandra
  when: ansible_os_family == ""RedHat""     
 


- name: install packages (Debian)
  apt: pkg=wget  state=present
  when: ansible_os_family == ""Debian"" 

- name: install packages (Debian)
  apt: pkg=software-properties-common  state=present
  when: ansible_os_family == ""Debian"" 

- name: install packages 2 (Debian)
  apt: pkg=python-software-properties  state=present
  when: ansible_os_family == ""Debian"" 

- name: apt-add-repository (Debian)
  command: apt-add-repository 'deb http://debian.datastax.com/community stable main'
  when: ansible_os_family == ""Debian"" 

- name: get datastax repo key (Debian)
  command: wget http://debian.datastax.com/debian/repo_key --output-document=/tmp/datastax.key
  when: ansible_os_family == ""Debian""   

- name: add datastax repo key  (Debian)
  command: apt-key add /tmp/datastax.key
  when: ansible_os_family == ""Debian""   

- name: install Cassandra (Debian)
  apt: pkg=cassandra=1.2.10 state=present update_cache=yes
  with_items:
    - $cassandra
  tags:
    - packages 
  notify: 
    - restart cassandra
    - configure cassandra
  when: ansible_os_family == ""Debian"" 


- name: set cassandra to use the ByteOrderedPartitioner  (Debian)
  lineinfile: "" 
    dest=/etc/cassandra/cassandra.yaml  
    regexp=^partitioner 
    line='partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner'""
  notify:
    - restart cassandra
  when: ansible_os_family == ""Debian""


- name: increase stack size (Debian only)
  shell: ""sed -e s/Xss180k/Xss256k/g /etc/cassandra/cassandra-env.sh --in-place""
  when: ansible_os_family == ""Debian""

- name: setup production settings for Cassandra (not overwrites files)
  copy: src={{item.src}} dest={{item.dest}}
  with_items:
    - { src: cassandra/90-nproc.conf, dest: /etc/security/limits.d/90-nproc.conf }
    - { src: cassandra/limits.conf, dest: /etc/security/limits.conf }
  notify: 
    - restart cassandra",24,file: path={{data_dir}}/cassandra state=directory owner={{cassandra_user}}  group={{cassandra_user}}; template: src=cassandra/cassandra.yaml dest={{cassandra_yaml}} owner={{cassandra_user}}  group={{cassandra_user}},yum: name={{item}} state=present; apt: pkg=cassandra=1.2.10 state=present update_cache=yes; copy: src={{item.src}} dest={{item.dest}},1,1,1,1
25,"dir: '{{ data_dir }}/spatial-data'
  url: '{{ geoserver_url }}'
  geoserver_data_dir: ""{{ geoserver_data_dir | default('/data/spatial-data/geoserver_data_dir') }}""
biocacheServiceUrl: ""{{ biocache_service_url | default('https://biocache.ala.org.au/ws') }}""
biocacheUrl: ""{{ biocache_url | default('https://biocache.ala.org.au') }}""
spatialService.url: ""{{ spatial_service_url }}""
grails.serverURL: {{spatial_service_url}}","data:
  dir: '{{data_dir}}/spatial-data'
geoserver:
  url: '{{geoserver_url}}'
  username: '{{ geoserver_username | default('admin') }}'
  password: '{{ geoserver_password | default('geoserver') }}'
  canDeploy: {{ can_deploy_to_geoserver | default('true') }}
  geoserver_data_dir: ""{{ geoserver_data_dir | default('{{data_dir}}/geoserver_data_dir') }}""

shpResolutions: {{ shp_resolutions | default([0.5, 0.25, 0.1, 0.05]) }}
grdResolutions: {{ grd_resolutions | default([0.5, 0.25, 0.1, 0.05, 0.01]) }}

biocacheServiceUrl: ""{{biocache_service_url | default('https://biocache.ala.org.au/ws')}}""
biocacheUrl: ""{{biocache_url | default('https://biocache.ala.org.au')}}""
openstreetmap:
  url: ""{{ openstreetmap_tile_url | default('https://tile.openstreetmap.org') }}""

slave.enable: {{ slave_enable | default(true) }}
service.enable: {{ service_enable | default(true) }}

serviceKey: {{ spatial_service_service_key  | default('') }}
batch_sampling_passwords: ""{{ batch_sampling_passwords | default('') }}""
batch_sampling_points_limit: {{ batch_sampling_points_limit | default(1000000) }}
batch_sampling_fields_limit: {{ batch_sampling_fields_limit | default(1000) }}

---
spatialService.url: ""{{spatial_service_url}}""
data.dir: ""{{ data_dir }}/spatial-data""
shp2pgsql.path: ""{{ shp2pgsql_path | default('/usr/bin/shp2pgsql') }}""
gdal.dir: ""{{ gdal_dir | default('/usr/bin/') }}""
gdm.dir: ""{{ gdm_dir | default('/data/spatial-data/modelling/gdm/DoGdm') }}""

aloc.xmx: ""{{ aloc_xmx | default('6G') }}""
aloc.threads: ""{{ aloc_threads | default(4) }}""
maxent.mx: ""{{ maxent_mx | default('1G') }}""
maxent.threads: ""{{ maxent_threads | default(4) }}""

sampling.threads: ""{{ sampling_threads | default(4) }}""

slaveKey: ""{{ spatial_service_slave_key | default('') }}""
serviceKey: ""{{ spatial_service_service_key | default('') }}""

statusTime: ""{{ status_time | default(3000) }}""
retryCount: ""{{ retry_count | default(10) }}""
retryTime: ""{{ retry_time | default(30000) }}""

security:
  cas:
    casServerName: {{ auth_base_url }}
    uriFilterPattern: {{ uri_filter_pattern | default('/manageLayers,/manageLayers/.*,/admin,/admin/.*,/alaAdmin.*') }}
    uriExclusionFilterPattern: {{ uri_exclusion_filter_pattern | default('/assets.*,/images.*,/css.*,/js.*,/less.*,/tasks/status/.*') }}
    authenticateOnlyIfLoggedInFilterPattern: {{ authenticate_only_if_logged_in_filter_pattern | default('/master,/master/.*,/tasks,/tasks/.*') }}
    appServerName: {{ spatial_service_base_url }}
    casServerUrlPrefix: {{ auth_cas_url }}
    loginUrl: {{ auth_cas_url }}/login
    logoutUrl: {{ auth_cas_url }}/logout
    contextPath: {{ spatial_service_context_path }}
    bypass: {{ bypass_cas | default(true) }}
    disableCAS: {{ bypass_cas | default(true) }}
    gateway: {{ gateway_cas | default(false) }}

auth.admin_role: {{ auth_admin_role | default('ROLE_ADMIN') }}
app.http.header.userId: {{ app_http_header_userid | default('X-ALA-userId') }}

headerAndFooter.baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3') }}
ala.baseURL: {{ ala_base_url | default('https://www.ala.org.au') }}
bie.baseURL: {{ bie_base_url | default('https://bie.ala.org.au') }}
bie.searchPath: '/search'

records.url: {{ records_url | default('https://archives.ala.org.au/archives/exports/lat_lon_taxon.zip') }}

api_key: {{ spatial_service_api_key  | default('') }}
lists.url: {{ lists_url | default('https://lists.ala.org.au') }}
collections.url: {{ collections_url | default('https://collections.ala.org.au') }}
sandboxHubUrl: {{ sandbox_url | default('http://sandbox.ala.org.au/ala-hub') }}
sandboxBiocacheServiceUrl: {{sandbox_biocache_service_url | default('http://sandbox.ala.org.au/biocache-service') }}
phyloServiceUrl: {{ phylolink_url | default('https://phylolink.ala.org.au') }}

spatialHubUrl: {{ spatial_hub_url }}

gazField: {{ gaz_field | default('cl915') }}
userObjectsField: {{ user_objects_field | default('cl1083') }}

apiKeyCheckUrlTemplate: ""{{api_key_check_url_template | default('https://auth.ala.org.au/apikey/ws/check?apikey={0}') }}""
spatialService.remote: ""{{spatial_service_remote_url}}""

journalmap.api_key: {{ journalmap_api_key | default('') }}
journalmap.url: {{ journalmap_url | default('https://www.journalmap.org/') }}





grails.plugin.elfinder.rootDir: '{{ data_dir }}/spatial-service'

i18n.override.dir: '{{ data_dir }}/spatial-service/config/i81n/'












layers_store.GEONETWORK_URL: '{{ geonetwork_url | default('') }}'

distributions.cache.dir: ""{{ data_dir }}/${appName}/mapCache/""
distributions.geoserver.image.url: ""/ALA/wms?service=WMS&version=1.1.0&request=GetMap&sld={{ distribution_image_sld_url | default('https://fish.ala.org.au/data/dist.sld')}}&layers=ALA:aus1,ALA:Distributions&styles=&bbox=109,-47,157,-7&srs=EPSG:4326&format=image/png&width=400&height=400&viewparams=s:""

dataSource:
    url: 'jdbc:postgresql://{{layers_db_host}}/{{layers_db_name}}'
    username: {{layers_db_username}}
    password: {{layers_db_password}}

grails.serverURL: {{spatial_service_base_url}}
grails.app.context: {{spatial_service_context_path}}

skin.orgNameLong: {{ orgNameLong | default('Atlas of Living Australia') }}
skin.orgNameShort: {{ orgNameShort | default('ALA') }}

grails.controllers.upload.maxFileSize: {{ max_request_size | default(524288000) }}
grails.controllers.upload.maxRequestSize: {{ max_request_size | default(524288000) }}",25,No issue found,"username: '{{ geoserver_username | default('admin') }}'; password: '{{ geoserver_password | default('geoserver') }}'; serviceKey: {{ spatial_service_service_key  | default('') }}; batch_sampling_passwords: ""{{ batch_sampling_passwords | default('') }}""; slaveKey: ""{{ spatial_service_slave_key | default('') }}""; serviceKey: ""{{ spatial_service_service_key | default('') }}""; security:
  cas:
    casServerName: {{ auth_base_url }}; api_key: {{ spatial_service_api_key  | default('') }}; journalmap.api_key: {{ journalmap_api_key | default('') }}; dataSource:
    url: 'jdbc:postgresql://{{layers_db_host}}/{{layers_db_name}}'
    username: {{layers_db_username}}
    password: {{layers_db_password}}",0,0,1,1
26,"- name: Add entries for demo into hosts file
  lineinfile: dest=/etc/hosts regexp='^127\.0\.0\.1' line=""127.0.0.1 localhost {{ demo_hostname | default('') }} ala.vagrant.dev ala demo.vagrant1.ala.org.au vagrant1.ala.org.au"" owner=root group=root mode=0644

- name: Ensure data directory exists
    - demo","- include: ../../common/tasks/setfacts.yml

- name: ensure data directory exists
  file: path=/srv/{{ demo_hostname }}/www/html state=directory owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - demo

- name: Copy welcome page (Debian)
  template: src=index.html dest=/srv/{{ demo_hostname }}/www/index.html mode=0666
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/biocache-media"" 
  ignore_errors: yes
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/html/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/html/biocache-media"" 
  ignore_errors: yes
  tags: 
    - demo",26,"lineinfile: dest=/etc/hosts regexp='^127\.0\.0\.1' line=""127.0.0.1 localhost {{ demo_hostname | default('') }} ala.vagrant.dev ala demo.vagrant1.ala.org.au vagrant1.ala.org.au"" owner=root group=root mode=0644; - demo","file: path=/srv/{{ demo_hostname }}/www/html state=directory owner={{tomcat_user}} group={{tomcat_user}}; template: src=index.html dest=/srv/{{ demo_hostname }}/www/index.html mode=0666; command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/biocache-media""; command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/html/biocache-media""",1,1,1,1
27,"service-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}
        management-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}/status","authCookieName: {{ auth_cookie_name | default('ALA-Auth') }}
      enabled: {{ oauth_providers_flickr_enabled | default('true') }}
    inaturalist:
      enabled: {{ oauth_providers_inaturalist_enabled | default('false') }}
      key: {{ oauth_providers_inaturalist_key | default('') }}
      secret: {{ oauth_providers_inaturalist_secret | default('') }}
      callback: ${grails.serverURL}/profile/inaturalistCallback
biocache.search.baseUrl: {{ biocache_base_url }}/occurrences/search
headerAndFooter:
  baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3')}}
  version: {{ header_and_footer_version | default('1')}}
{% if bootadmin_enabled %}
        service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}
{% endif %}
{% if spring_session_redis_clustered %}
{% endif %}",27,No issue found,key: {{ oauth_providers_inaturalist_key | default('') }}; secret: {{ oauth_providers_inaturalist_secret | default('') }}; callback: ${grails.serverURL}/profile/inaturalistCallback; service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }},0,0,1,1
28,"sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*
- name: Fix Webupd8 Team failing to update and Oracle removing old download (part 2)
    sed -i 's|JAVA_VERSION=8u172|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_172|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*","sed -i 's|JAVA_VERSION=8u161|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""6dbc56a0e3310b69e91bb64db63a485bd7b6a8083f08e48047276380a0e2021e""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_161|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*
- name: Switch oracle jdk 8 from security (b171) to bug fix+security (b172)
    sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*",28,No issue found,Hardcoded URLs; Hardcoded SHA256SUM_TGZ; Hardcoded JAVA_VERSION; Hardcoded J_DIR,0,0,1,1
29,"- ""{{data_dir}}/ala/runtime/files/""","- include: ../../common/tasks/setfacts.yml
  tags:
    - spatial-hub
    - config    

- include: ../../apache_vhost/tasks/main.yml context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - apache_vhost
    - spatial-hub
  when: not webserver_nginx

- name: add nginx vhost if configured
  include_role:
    name: nginx_vhost
  vars:
    hostname: ""{{ spatial_hub_hostname }}""
    context_path: ""{{ spatial_hub_context_path }}""
  tags:
    - nginx_vhost
    - deploy
    - spatial-hub
  when: webserver_nginx

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ spatial_hub_war_url }}' context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - tomcat_vhost
    - spatial-hub

- name: ensure target directories exist [data subdirectories etc.]
  file: path={{item}} state=directory owner={{tomcat_user}} group={{tomcat_user}}
  with_items:
    - ""{{data_dir}}/ala/data/runtime/files/""
  tags:
    - spatial-hub

- name: copy all config.properties
  template: src=spatial-hub-config.properties dest={{data_dir}}/spatial-hub/config/spatial-hub-config.properties
  tags:
    - spatial-hub 
    - config

- name: copy all log4j.properties
  template: src=log4j.properties dest={{data_dir}}/spatial-hub/config/log4j.properties
  tags:
    - spatial-hub

- name: set data ownership
  file: path={{data_dir}}/ala/data/ owner={{tomcat_user}} group={{tomcat_user}} recurse=true
  notify: 
    - restart tomcat
  tags:
    - spatial-hub",29,{{data_dir}}/ala/runtime/files/,- include: ../../common/tasks/setfacts.yml; - include: ../../apache_vhost/tasks/main.yml context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'; - include: ../../tomcat_deploy/tasks/main.yml war_url='{{ spatial_hub_war_url }}' context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'; file: path={{item}} state=directory owner={{tomcat_user}} group={{tomcat_user}}; template: src=spatial-hub-config.properties dest={{data_dir}}/spatial-hub/config/spatial-hub-config.properties; template: src=log4j.properties dest={{data_dir}}/spatial-hub/config/log4j.properties; file: path={{data_dir}}/ala/data/ owner={{tomcat_user}} group={{tomcat_user}} recurse=true,1,0,1,1
30,- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',"- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy",30,include: ../../tomcat_deploy/tasks/main.yml; war_url='{{ webapi_war_url }}'; context_path='{{ webapi_context_path }}'; hostname='{{ webapi_hostname }}',mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present; template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties,1,1,1,1
31,- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',"version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}""",31,Relative path in include statement; Use of variable in include statement without validation,"webapi_url: ""{{maven_repo_ws_url}}""",1,1,1,0
32,{% if spring_session_redis_clustered is sameas true %},"{% if spring_session_redis_clustered %}
    clustered:
      nodes: {{ spring_session_redis_host }}:{{ spring_session_redis_port | default('6379') }}
    {% endif %}",32,No issue found,spring_session_redis_host; spring_session_redis_port,0,0,1,0
33,when: elasticsearch_proxy,when: elasticsearch_proxy | bool == True,33,No issue found,No issue found,0,0,0,0
34,- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass_values }}',"tags:
    - sandbox

- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass }}'
  tags:
    - sandbox
    - deploy
    - apache_vhost

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ sandbox_war_url }}' context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}'
  tags:
    - sandbox
    - deploy
    - tomcat_vhost

- name: Redirect to datacheck 
  template: src=index.html dest=/srv/{{ sandbox_hostname }}/www/index.html owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - sandbox
    - deploy
    - apache_vhost

  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties",34,include: ../../apache_vhost/tasks/main.yml; context_path='{{ sandbox_context_path }}'; hostname='{{ sandbox_hostname }}'; additional_proxy_pass='{{ additional_proxy_pass_values }}',No issue found,1,1,0,0
35,"webapi_war_url: ""{{maven_repo_ws_url}}""","- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy",35,No issue found,mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present; template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties,0,0,1,1
36,"webapi_war_url: ""{{maven_repo_ws_url}}""","version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}""",36,No issue found,"webapi_url: ""{{maven_repo_ws_url}}""",0,0,1,0
37,"cassandra_user: cassandra
datastax: dsc12-1.2.10-1
cassandra: cassandra12-1.2.10-1",cassandra_user: cassandra,37,No issue found,No issue found,0,0,0,0
38,"when: upgrade_check_script.stdout == ""False""","mode: 0644
  import: others/master/postconfigure-upgrade.yaml
  when: upgrade_check_script.stdout == ""False""",38,No issue found,mode: 0644; import: others/master/postconfigure-upgrade.yaml,0,0,1,0
39,"- name: bootstrap | configure node | kubead show me join command
- name: bootstrap | configure node | compose join command","---
- name: configure others | kubead show me join command
  command: kubeadm token create --print-join-command --ttl 5m
  delegate_to: ""{{ cluster_name }}-kube-master.service.automium.consul""
  register: kubeadm_join_command

- name: configure-bootstrap  | compose join command
  set_fact:
    join_command: ""{{ kubeadm_join_command.stdout }}""",39,No issue found,"command: kubeadm token create --print-join-command --ttl 5m; delegate_to: ""{{ cluster_name }}-kube-master.service.automium.consul""",0,0,1,1
40,"- name: Enable/disable services
  service:
    name: ""{{ item }}""
    enabled: ""{{ (enable_services | bool) | ternary('yes','no') }}""
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service

  when: (start_services | bool)
  tags:
    - service","---

- name: Package
  package:
    name: ""{{item}}""
    state: present
  loop: ""{{packages_to_install[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""

- name: Template >> /etc/dhcp/dhcpd.conf
  template:
    src: dhcpd.conf.j2
    dest: /etc/dhcp/dhcpd.conf
    owner: root
    group: root
    mode: 0644
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.networks.conf
  template:
    src: dhcpd.networks.conf.j2
    dest: /etc/dhcp/dhcpd.networks.conf
    owner: root
    group: root
    mode: 0644
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.{{item}}.conf
  template:
    src: dhcpd.subnet.conf.j2
    dest: /etc/dhcp/dhcpd.{{item}}.conf
    owner: root
    group: root
    mode: 0644
  with_items: ""{{networks}}""
  when:
    - j2_current_iceberg_network in item
    - networks[item].is_in_dhcp == true
  tags:
    - templates

- name: Start services
  service:
    name: ""{{item}}""
    state: started
    enabled: yes
  loop: ""{{services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""",40,"name: ""{{ item }}""; loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""; when: (start_services | bool)","Package installation: The package names are being taken from a variable. If an attacker can control the 'packages_to_install' variable, they could potentially install unwanted packages.; Service start: The service names are being taken from a variable. If an attacker can control the 'services_to_start' variable, they could potentially start unwanted services.; Template task: The destination file name is being taken from a variable. If an attacker can control the 'item' variable, they could potentially overwrite any file.; Template task: The condition 'j2_current_iceberg_network in item' and 'networks[item].is_in_dhcp == true' are not properly validated. If an attacker can control these variables, they could potentially execute the task under unwanted conditions.",1,1,1,1
41,"exporters:
    bb_exporter:
      port: 9777
      collectors:
        cpu:
        ram:
        mounted:
          - /scratch
          - /home
        services:
          - slurmd.service
        
    Exporter_down:
    bb_exporter_service:
      -  slurmd","monitoring:
  alerts:
    - ExporterDown
    - OutOfDiskSpace",41,No issue found,No issue found,0,0,0,0
42,"loop: ""{{ log_client_services_to_start }}""","---

- name: Restart rsyslog services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service",42,No issue found,"name: ""{{ item }}""; loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""",0,0,1,1
43,advanced_dhcp_server_role_version: 1.0.4,"role_version: 1.0.2
packages_to_install:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcp
    _8:
      - dhcp-server
  centos:
    _7:
      - dhcp
    _8:
      - dhcp-server

services_to_start:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcpd
    _8:
      - dhcpd
  centos:
    _7:
      - dhcpd
    _8:
      - dhcpd",43,No issue found,No issue found,0,0,0,0
44,dhcp_server_role_version: 1.0.7,dhcp_server_role_version: 1.0.6,44,No issue found,No issue found,0,0,0,0
45,dhcp_server_role_version: 1.0.4,"---
role_version: 1.0.4",45,No issue found,No issue found,0,0,0,0
46,"- name: lineinfile █ Configure root color based on iceberg number
- name: copy █ Add disk usage small script for screenrc
- name: copy █ Add screenrc configuration
- name: copy █ Add vimrc configuration","- name: Configure root color based on iceberg number
  lineinfile:
    path: /root/.bashrc
    line: 'PS1=""\[\e[01;{{31+(j2_current_iceberg_number|int)}}m\]\h:\w#\[\e[00;m\] ""'

- name: Add disk usage small script for screenrc
  copy:
    src: free_root_disk 
    dest: /usr/bin/free_root_disk
    mode: 0700

- name: Add screenrc configuration
  copy:
    src: screenrc
    dest: /root/.screenrc
    mode: 0644",46,No issue found,No issue found,0,0,0,0
47,- name: service █ Restart dhcp server,"---
- name: Restart dhcp services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service",47,No issue found,No issue found,0,1,0,0
48,"aws_access_key: ""{{ auth_var['aws_access_key_id'] | default(omit) }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] | default(omit) }}""","aws_access_key: ""{{ auth_var['aws_access_key_id'] }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] }}""",48,"aws_access_key: ""{{ auth_var['aws_access_key_id'] | default(omit) }}""; aws_secret_key: ""{{ auth_var['aws_secret_access_key'] | default(omit) }}""","aws_access_key: ""{{ auth_var['aws_access_key_id'] }}""; aws_secret_key: ""{{ auth_var['aws_secret_access_key'] }}""",1,0,1,
49,no_log: true,"- name: ""Provisioning resource group {{ res_grp }}""
  debug:
    msg: ""The current server obj is {{ res_grp }} \n groups vars are {{ r_grp_vars }} ""

- name: ""Including credentials of current resource {{ res_grp['resource_group_name'] }} ""
  include_vars: ""../vars/{{ res_grp['assoc_creds'] }}.yml""
  no_log: false 

- name: ""Checking res_grp ""
  debug:
    msg: "" res_grp {{ res_grp }}""

- name: ""Provision resource definitions""
  include: provision_res_defs.yml res_def={{ outer_item.0 }} res_grp_name={{ outer_item.1 }}
  with_nested:
    - ""{{ res_grp['res_defs'] }}""
    - [""{{ res_grp['resource_group_name'] }}""]
  loop_control:
    loop_var: outer_item",49,No issue found,"debug: msg: ""The current server obj is {{ res_grp }} \n groups vars are {{ r_grp_vars }} ""; include_vars: ""../vars/{{ res_grp['assoc_creds'] }}.yml""; debug: msg: "" res_grp {{ res_grp }}""; include: provision_res_defs.yml res_def={{ outer_item.0 }} res_grp_name={{ outer_item.1 }}",0,1,1,0
50,"when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init"" and cloud_config != {}
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and virt_type == ""cloud-init"" 
  when:  (node_exists['failed'] is defined) and  virt_type == 'virt-customize'
  ignore_errors: yes","- name: set cloud config default
  set_fact:
    cloud_config: ""{{ res_def['cloud_config'] | default({})  }}""

- name: set cloud_config virt_type
  set_fact:
    virt_type: ""{{ cloud_config['virt_type'] | default('cloud-init') }}""

- include_tasks: virt_customize.yml
  when: res_def['cloud_config']['virt_type'] == ""virt-customize""

  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and virt_type == ""cloud-init""
  when:  node_exists['failed'] is defined and (res_def['cloud_config'] is not defined or virt_type == 'virt-customize')",50,ignore_errors: yes,No issue found,1,0,0,0
51,"dest: ""/tmp/{{ libvirt_resource_name }}{{ definition[4] }}{{ definition[2] }}""
    uri: ""{{ definition[0]['uri'] }}""","uri: ""{{ definition[0] }}""",51,"dest: ""/tmp/{{ libvirt_resource_name }}{{ definition[4] }}{{ definition[2] }}""; uri: ""{{ definition[0]['uri'] }}""","uri: ""{{ definition[0] }}""",1,0,1,0
52,"name:
          - python2-dnf 
          - libvirt-devel
          - libguestfs-tools 
          - python-libguestfs","- name: Install dependencies
  block:
    - name: Install package dependencies
      package:
        name: ""{{ libvirt_pkg }}""
        state: latest
      with_items:
      - python2-dnf 
      - libvirt-devel
      - libguestfs-tools 
      - python-libguestfs
      become: true
      loop_control: 
        loop_var: libvirt_pkg
    - name: Install pypi dependencies of libvirt
      pip:
        name: ""{{ libvirt_pypi }}""
      with_items:
      - ""libvirt-python>=3.0.0""
      - ""lxml""
      loop_control: 
        loop_var: libvirt_pypi
  rescue:
    - fail:
        msg: 'Error installing the package dependencies! Please try adding password less priviledged sudo user or with --ask-sudo-pass'",52,No issue found,"name: ""{{ libvirt_pkg }}""
state: latest; name: ""{{ libvirt_pypi }}""",0,0,1,1
53,nan,"---
- name: ""provision/deprovision os_subnet""
  os_subnet:
    allocation_pool_end:  ""{{ res_def['allocation_pool_end'] | default(omit) }}""
    allocation_pool_start:  ""{{ res_def['allocation_pool_start'] | default(omit) }}""
    api_timeout: ""{{ res_def['api_timeout'] | default(omit) }}""
    auth: ""{{ auth_var }}""
    cidr:  ""{{ res_def['cidr'] | default(omit) }}""
    dns_nameservers:  ""{{ res_def['dns_nameservers'] | default(omit) }}""
    enable_dhcp: ""{{ res_def['enable_dhcp'] | default(omit) }}""
    extra_specs: ""{{ res_def['extra_specs'] | default(omit) }}""
    gateway_ip:  ""{{ res_def['gateway_ip'] | default(omit) }}""
    host_routes:  ""{{ res_def['host_routes'] | default(omit) }}""
    ip_version:  ""{{ res_def['ip_version'] | default(omit) }}""
    ipv6_address_mode:  ""{{ res_def['ipv6_address_mode'] | default(omit) }}""
    ipv6_ra_mode:  ""{{ res_def['ipv6_ra_mode'] | default(omit) }}""
    network_name:  ""{{ res_def['network_name'] | default(omit) }}""
    no_gateway_ip:  ""{{ res_def['no_gateway_ip'] | default(omit) }}""
    use_default_subnetpool:  ""{{ res_def['use_default_subnetpool'] | default(omit) }}""
    interface: ""{{ res_def['interface'] | default(omit) }}""
    name: ""{{ os_resource_name }}""
    key: ""{{ res_def['key'] | default(omit) }}""
    project: ""{{ res_def['project'] | default(omit) }}""
    region_name: ""{{ res_def['region_name'] | default(omit) }}""
    state: ""{{ state }}""
    timeout: ""{{ res_def['timeout'] | default(600) }}""
    verify: no
    wait: yes
  register: res_def_output_auth
  no_log: ""{{ not debug_mode }}""
  when: 
   - auth_var != """"

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_os_network: ""{{ topology_outputs_os_network + [ res_def_output_auth ] }}""
  when:
    - auth_var != """"
      
- name: ""provision/deprovision os_subnet""
  os_subnet:
    allocation_pool_end:  ""{{ res_def['allocation_pool_end'] | default(omit) }}""
    allocation_pool_start:  ""{{ res_def['allocation_pool_start'] | default(omit) }}""
    api_timeout: ""{{ res_def['api_timeout'] | default(omit) }}""
    auth: ""{{ auth_var }}""
    cidr:  ""{{ res_def['cidr'] | default(omit) }}""
    dns_nameservers:  ""{{ res_def['dns_nameservers'] | default(omit) }}""
    enable_dhcp: ""{{ res_def['enable_dhcp'] | default(omit) }}""
    extra_specs: ""{{ res_def['extra_specs'] | default(omit) }}""
    gateway_ip:  ""{{ res_def['gateway_ip'] | default(omit) }}""
    host_routes:  ""{{ res_def['host_routes'] | default(omit) }}""
    ip_version:  ""{{ res_def['ip_version'] | default(omit) }}""
    ipv6_address_mode:  ""{{ res_def['ipv6_address_mode'] | default(omit) }}""
    ipv6_ra_mode:  ""{{ res_def['ipv6_ra_mode'] | default(omit) }}""
    network_name:  ""{{ res_def['network_name'] | default(omit) }}""
    no_gateway_ip:  ""{{ res_def['no_gateway_ip'] | default(omit) }}""
    use_default_subnetpool:  ""{{ res_def['use_default_subnetpool'] | default(omit) }}""
    interface: ""{{ res_def['interface'] | default(omit) }}""
    name: ""{{ os_resource_name }}""
    key: ""{{ res_def['key'] | default(omit) }}""
    project: ""{{ res_def['project'] | default(omit) }}""
    region_name: ""{{ res_def['region_name'] | default(omit) }}""
    state: ""{{ state }}""
    timeout: ""{{ res_def['timeout'] | default(600) }}""
    verify: no
    wait: yes
  register: res_def_output_auth
  no_log: ""{{ not debug_mode }}""
  when:
   - auth_var == """"

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_os_network: ""{{ topology_outputs_os_network + [ res_def_output_no_auth ] }}""
  when:
    - auth_var == """"",53,No issue found,"verify: no; auth: ""{{ auth_var }}""; no_log: ""{{ not debug_mode }}""",0,1,1,1
54,"admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""
    admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""
    admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""
    admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""","---
- name: ""Provisioning Azure VM when not async""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    ssh_public_keys: ""{{ssh_public_keys}}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    name: ""{{ nameOfvm | default(omit) }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    image: ""{{ image | default(omit) }}""
  register: res_def_output
  when: not _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure VM""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    name: ""{{  nameOfvm| default(omit) }}""
    image: ""{{ image | default(omit) }}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""
  when: _async",54,"admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""; admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""; admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""; admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""","admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""; secret: ""{{ auth_var['secret'] | default(omit) }}""; no_log: ""{{ not debug_mode }}""",1,0,1,1
55,"cloudconfig_users: ""{{ cloud_config['users'] | default([]) }}""","register: pubkey_local
  register: pubkey_remote
- name: ""Create directories""
  file:
    path: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}""
    state: ""directory""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ res_def['name'] | default(res_def['res_name']) }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  when: node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-local""
  template:
    src: ""templates/cloud-config/user-data-fixed-local""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  when: res_def['cloud_config'] is not defined and node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-local""
    src: ""templates/cloud-config/user-data-local""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  vars:
    cloudconfig_users: ""{{ res_def['cloud_config']['users'] | default([]) }}""
  when: res_def['cloud_config'] is defined and node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-remote""
  template:
    src: ""templates/cloud-config/user-data-remote""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  remote_user: ""{{ res_def['remote_user'] | default('root') }}""
  delegate_to: ""{{ uri_hostname }}""
  when: res_def['cloud_config'] is defined and node_exists['failed'] is defined and uri_hostname != 'localhost'

- name: ""Prepare cloud-config/user-data-remote""
  template:
    src: ""templates/cloud-config/user-data-fixed-remote""
- name: ""Prepare cloud-config/meta-data remote""
- name: ""Generate ci data cd image for cloud-init local""
- name: ""Generate ci data cd image for cloud-init remote host""
  command: mkisofs -o /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}.iso -V cidata -r -J --quiet /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data  /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/meta-data
  command: mkisofs -o /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}.iso -V cidata -r -J --quiet /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/meta-data
- name: ""Generate add admin script local""
- name: ""Generate add admin script remote""
- name: ""Remove cloud-init cdrom ""
- name: ""Start VM""
- name: ""Start relevant networks""
- name: ""mac_and_ip | extract mac address""
- name: ""mac_and_ip | wait for dhcp ip address""
- name: ""mac_and_ip | wait for dhcp ip address""",55,No issue found,"file: path: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}"" state: ""directory""; template: src: ""templates/cloud-config/user-data-local"" dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""; template: src: ""templates/cloud-config/user-data-remote"" dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""; command: mkisofs -o /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}.iso -V cidata -r -J --quiet /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data  /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/meta-data; remote_user: ""{{ res_def['remote_user'] | default('root') }}""",0,0,1,1
56,"topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""","no_log: ""{{ not debug_mode }}""

- name: """"Async::Provisioning Azure Virtual Subnet""
  azure_rm_subnet:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    name: ""{{ res_def['subnet_name'] | default(omit) }}""
    virtual_network_name: ""{{ res_def['virtual_network_name']}}""
    address_prefix: ""{{ res_def['address_prefix']|default('10.1.0.0/24')}}""
  register: res_def_output
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""
  when: _async",56,No issue found,"client_id: ""{{ auth_var['client_id'] | default(omit) }}""; tenant: ""{{ auth_var['tenant'] | default(omit) }}""; secret: ""{{ auth_var['secret'] | default(omit) }}""; subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""; no_log: ""{{ not debug_mode }}""",0,1,1,1
57,"- ""{{ res_grp['resource_definitions'] }}""","---
- name: ""Unset the authvar from previous run""
  set_fact:
    auth_var: """"

- name: ""set cred profile""
  set_fact:
    cred_profile: ""{{ res_grp['credentials']['profile'] | default('default') }}""

- name: ""Get creds from auth driver""
  auth_driver:
    filename: ""{{ res_grp['credentials']['filename']  }}""
    cred_type: ""ovirt""
    cred_path: ""{{ creds_path | default(default_credentials_path) }}""
    driver: ""file""
  register: auth_var
  ignore_errors: true

- name: ""set auth_var""
  set_fact:
    auth_var: ""{{ auth_var['output'][cred_profile] }}""
  ignore_errors: true

- block:
  - name: Obtain SSO token with using username/password credentials
    ovirt_auth:
      url: ""{{ auth_var['ovirt_url'] }}""
      username: ""{{ auth_var['ovirt_username'] }}""
      ca_file: ""{{ auth_var['ovirt_ca_file'] | default(omit) }}""
      password: ""{{ auth_var['ovirt_password'] }}""
      insecure: ""{{ auth_var['ovirt_ca_file'] is not defined }}""

  - name: ""Provisioning resource definitions of current group""
    include: provision_res_defs.yml res_def={{ res_item.0 }} res_grp_name={{ res_item.1 }}
    with_nested:
      - ""{{ res_grp['resource_definitions']) }}""
      - [""{{ res_grp['resource_group_name'] }}""]
    loop_control:
      loop_var: res_item

  always:
    - name: Always revoke the SSO token
      ovirt_auth:
        state: absent
        ovirt_auth: ""{{ ovirt_auth }}""",57,Potential insecure use of variable interpolation,"auth_driver: filename: ""{{ res_grp['credentials']['filename']  }}""; ovirt_auth: url: ""{{ auth_var['ovirt_url'] }}"" username: ""{{ auth_var['ovirt_username'] }}"" password: ""{{ auth_var['ovirt_password'] }}"" insecure: ""{{ auth_var['ovirt_ca_file'] is not defined }}""; ignore_errors: true",1,0,1,1
58,"address_prefixes: ""{{ res_def['address_prefixes']|default('10.1.0.0/16')}}""
    address_prefixes: ""{{ res_def['address_prefixes']|default('10.1.0.0/16')}}""","---
- name: ""Provisioning Azure Virtual Network when not async""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: ""{{ res_def['address_prefixes']|default(10.1.0.0/16)}}""
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  register: res_def_output

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure Virtual Network""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: 10.1.0.0/16
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""
  when: _async",58,No issue found,"client_id: ""{{ auth_var['client_id'] | default(omit) }}""; tenant: ""{{ auth_var['tenant'] | default(omit) }}""; secret: ""{{ auth_var['secret'] | default(omit) }}""; subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""; no_log: ""{{ not debug_mode }}""",0,0,1,1
59,"copy: src=fetch/{{ fsid }}/etc/ceph/radosgw.gateway.keyring dest=/etc/ceph/radosgw.gateway.keyring owner=root group=root mode=600
  file: path=/etc/ceph/radosgw.gateway.keyring mode=0600 owner=root group=root","- name: Copy RGW bootstrap key
  copy: src=fetch/{{ fsid }}/etc/ceph/keyring.radosgw.gateway dest=/etc/ceph/keyring.radosgw.gateway owner=root group=root mode=600
  when: cephx

- name: Set RGW bootstrap key permissions
  file: path=/etc/ceph/keyring.radosgw.gateway mode=0600 owner=root group=root
  when: cephx",59,No issue found,No issue found,0,0,0,0
60,copy: >,"---
- name: create red hat storage package directories
  file: >
    path={{ item }}
    state=directory
  with_items:
    - ""{{ ceph_stable_rh_storage_mount_path }}""
    - ""{{ ceph_stable_rh_storage_repository_path }}""

- name: fetch the red hat storage iso from the ansible server
  fetch: >
    src={{ ceph_stable_rh_storage_iso_path }}
    dest={{ ceph_stable_rh_storage_iso_path }}
    flat=yes

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=mounted

- name: copy red hat storage iso content
  shell:
    cp -r {{ ceph_stable_rh_storage_mount_path }}/* {{ ceph_stable_rh_storage_repository_path }}
    creates={{ ceph_stable_rh_storage_repository_path }}/README

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=unmounted",60,No issue found,"src={{ ceph_stable_rh_storage_iso_path }}
dest={{ ceph_stable_rh_storage_iso_path }}; shell:
    cp -r {{ ceph_stable_rh_storage_mount_path }}/* {{ ceph_stable_rh_storage_repository_path }}
    creates={{ ceph_stable_rh_storage_repository_path }}/README",0,0,1,1
61,"with_items: ""{{ groups.get(mon_group_name, []) }}""","---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names",61,No issue found,No issue found,0,0,0,0
62,"with_items: ""{{ groups.get(mon_group_name, []) }}""","---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names",62,No issue found,"name: ceph-mds@{{ mds_name }}; with_items: ""{{ groups[mds_group_name] }}""; delegate_to: ""{{ item }}""",0,1,1,0
63,failed_when: false,"---
- name: set config and keys paths
  set_fact:
    ceph_config_keys:
      - /etc/ceph/ceph.conf
      - /etc/ceph/ceph.client.admin.keyring
      - /var/lib/ceph/bootstrap-mds/ceph.keyring

- name: stat for ceph config and keys
  local_action: stat path={{ item }}
  with_items: ceph_config_keys
  changed_when: false
  sudo: false
  ignore_errors: true
  register: statconfig

- name: try to fetch ceph config and keys
  copy: >
    src=fetch/docker_mon_files/{{ item.0 }}
    dest={{ item.0 }}
    owner=root
    group=root
    mode=644
  with_together:
    - ceph_config_keys
    - statconfig.results
  when: item.1.stat.exists == true",63,failed_when: false,src=fetch/docker_mon_files/{{ item.0 }}; owner=root; group=root; mode=644; sudo: false; ignore_errors: true,1,0,1,0
64,"- name: install nss-tools on redhat
    name: nss-tools
- name: install nss-tools on redhat
    name: nss-tools","- name: install libnss3-tools on redhat
  yum:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""yum""

- name: install libnss3-tools on redhat
  dnf:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""dnf""

- name: install libnss3-tools on debian
  apt:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == 'apt'",64,No issue found,No issue found,0,0,0,0
65,"name: '{{ ntp_service_name }}'
    name: '{{ chrony_daemon_name }}'","enabled: yes

- name: disable ntpd
  failed_when: false
  service:
    name: ntpd
    state: stopped
    enabled: no

- name: disable chronyd
  failed_when: false
  service:
    name: chronyd
    enabled: no
    state: stopped

- name: disable timesyncd
  failed_when: false
  service:
    name: timesyncd
    enabled: no
    state: stopped",65,No issue found,failed_when: false,0,1,1,0
66,"ceph_authtool_cmd: ""{{ container_binary + ' run --net=host --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""","print(base64.b64encode(header + key).decode())""
  run_once: true # must run on a single mon only
    secret: ""{{ monitor_keyring.stdout }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: copy the initial key in /etc/ceph (for containers)
  command: >
    cp /var/lib/ceph/tmp/{{ cluster }}.mon..keyring /etc/ceph/{{ cluster }}.mon.keyring
  changed_when: false
  when:
    - cephx
    - containerized_deployment
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: set_fact ceph-authtool container command
  set_fact:
    ceph_authtool_cmd: ""{{ container_binary + ' run --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""
  command: >
    {{ ceph_authtool_cmd }}
     /var/lib/ceph/tmp/{{ cluster }}.mon.keyring --import-keyring /etc/ceph/{{ cluster }}.client.admin.keyring
- name: set_fact ceph-mon container command
  set_fact:
    ceph_mon_cmd: ""{{ container_binary + ' run --rm --net=host -v /var/lib/ceph/:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-mon ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' +ceph_client_docker_image_tag if containerized_deployment else 'ceph-mon' }}""

  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    --keyring /var/lib/ceph/tmp/{{ cluster }}.mon..keyring
  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    - not cephx",66,run --net=host; -v /var/lib/ceph:/var/lib/ceph:z; -v /etc/ceph/:/etc/ceph/:z; {{ container_binary + ' run --net=host --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }},"print(base64.b64encode(header + key).decode()); secret: ""{{ monitor_keyring.stdout }}""; cp /var/lib/ceph/tmp/{{ cluster }}.mon..keyring /etc/ceph/{{ cluster }}.mon.keyring; {{ ceph_authtool_cmd }} /var/lib/ceph/tmp/{{ cluster }}.mon.keyring --import-keyring /etc/ceph/{{ cluster }}.client.admin.keyring; {{ ceph_mon_cmd }} --cluster {{ cluster }} --setuser ceph --setgroup ceph --mkfs -i {{ monitor_name }} --fsid {{ fsid }} --keyring /var/lib/ceph/tmp/{{ cluster }}.mon..keyring",1,0,1,1
67,"default_release: ""{{ ceph_stable_release_uca | default('') }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else '' }}""","- name: install redhat ceph-mgr package
  package:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
  when:
    - ansible_os_family == 'RedHat'

- name: install ceph mgr for debian
  apt:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
    default_release: ""{{ ceph_stable_release_uca | default(omit) }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else ''}}""
  when:
    - ansible_os_family == 'Debian'",67,No issue found,No issue found,0,0,0,0
68,"- name: include multisite checks
- name: include master multisite tasks
- name: include secondary multisite tasks
- name: add zone to rgw stanza in ceph.conf","- name: Include multisite checks
  include: checks.yml
- name: Include master multisite tasks
  include: master.yml
  when: ""rgw_zonemaster is defined and rgw_zonemaster""
  static: False
- name: Include secondary multisite tasks
  include: secondary.yml
  when: ""rgw_zonesecondary is defined and rgw_zonesecondary""
  static: False
- name: Add zone to RGW stanza in ceph.conf
  lineinfile:
    dest: /etc/ceph/ceph.conf
    regexp: ""{{ ansible_host }}""
    insertafter: ""^[client.rgw.{{ ansible_host }}]""
    line: ""rgw_zone = {{ rgw_zone }}""
    state: present
  notify:
    - restart rgw",68,No issue found,No issue found,0,0,0,0
69,"with_items: ""{{ groups.get(mds_group_name, []) }}""","---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names",69,No issue found,No issue found,0,0,0,0
70,"with_items: ""{{ groups.get(mds_group_name, []) }}""","---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names",70,No issue found,No issue found,0,1,0,0
71,ceph_mon_docker_tag: latest,ceph_osd_docker_tag: latest,71,ceph_mon_docker_tag: latest,ceph_osd_docker_tag: latest,1,0,1,0
72,"template: >
    src=s3gw.fcgi.j2","---

- name: Add Ceph extra
  apt_repository: >
    repo=""deb http://ceph.com/packages/ceph-extras/debian {{ ansible_lsb.codename }} main""
    state=present

- name: ""Install Apache, fastcgi and Rados Gateway""
  apt: >
    pkg={{ item }}
    state=present
  with_items:
    - apache2
    - libapache2-mod-fastcgi
    - radosgw


- name: Install default httpd.conf
  template: src=httpd.conf dest=/etc/apache2/httpd.conf owner=root group=root

- name: Enable some apache mod rewrite and fastcgi
  command: ""{{ item }}""
  with_items:
    - a2enmod rewrite
    - a2enmod fastcgi

- name: Install Rados Gateway vhost
  template: >
    src=rgw.conf
    dest=/etc/apache2/sites-available/rgw.conf
    owner=root
    group=root


- name: Create RGW directory
  file: >
    path=/var/lib/ceph/radosgw/{{ ansible_fqdn }}
    state=directory
    owner=root
    group=root
    mode=0644

- name: Enable Rados Gateway vhost and disable default site
  command: ""{{ item }}""
  with_items:
    - a2ensite rgw.conf
    - a2dissite default
  notify:
    - restart apache2

- name: Install s3gw.fcgi script
  copy: >
    src=s3gw.fcgi
    dest=/var/www/s3gw.fcgi
    mode=0555
    owner=root
    group=root

- name: Check if RGW is started
  command: /etc/init.d/radosgw status
  register: rgwstatus
  ignore_errors: True

- name: Start RGW
  command: /etc/init.d/radosgw start
  when: rgwstatus.rc != 0",72,No issue found,"apt_repository: > repo=""deb http://ceph.com/packages/ceph-extras/debian {{ ansible_lsb.codename }} main"" state=present; template: src=httpd.conf dest=/etc/apache2/httpd.conf owner=root group=root; template: > src=rgw.conf dest=/etc/apache2/sites-available/rgw.conf owner=root group=root; file: > path=/var/lib/ceph/radosgw/{{ ansible_fqdn }} state=directory owner=root group=root mode=0644; copy: > src=s3gw.fcgi dest=/var/www/s3gw.fcgi mode=0555 owner=root group=root; command: /etc/init.d/radosgw start when: rgwstatus.rc != 0",0,0,1,1
73,"- (journal_collocation and raw_multi_journal)
      or (journal_collocation and osd_directory)
      or (journal_collocation and bluestore)
      or (raw_multi_journal and osd_directory)
      or (raw_multi_journal and bluestore)
      or (osd_directory and bluestore)","- (journal_collocation and not raw_multi_journal)
      or (journal_collocation and not osd_directory)
      or (journal_collocation and not bluestore)
      or (raw_multi_journal and not osd_directory)
      or (raw_multi_journal and not bluestore)
      or (osd_directory and not bluestore)
      or bluestore",73,No issue found,No issue found,0,0,0,0
74,name: python-pip,"- name: install pip on debian
  apt:
    name: pip
    state: present
  when: ansible_os_family == 'Debian'

- name: install pip on redhat
  yum:
    name: python-pip
    state: present
  when: ansible_os_family == 'RedHat'",74,No issue found,No issue found,0,0,0,0
75,"- (mon_socket is defined and mon_socket.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)","when:
    - (mon_socket_stat is defined and mon_socket_stat.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)

- name: include configure_ceph_command_aliases.yml
  include_tasks: configure_ceph_command_aliases.yml
  when:
    - containerized_deployment",75,No issue found,No issue found,0,0,0,0
76,"- import_role:
        name: ceph-infra
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-engine
      when: containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool","---
- hosts: mons
  gather_facts: false
  vars:
    delegate_facts_host: true
  pre_tasks:
    - name: gather facts
      setup:
      when: not delegate_facts_host | bool
    - import_role:
        name: ceph-defaults
    - name: gather and delegate facts
      setup:
      delegate_to: ""{{ item }}""
      delegate_facts: true
      with_items: ""{{ groups[mon_group_name] }}""
      run_once: true
      when: delegate_facts_host | bool
  tasks:
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-validate

- hosts: mons
  gather_facts: false
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool
    - import_role:
        name: ceph-config
    - import_role:
        name: ceph-infra
    - import_role:
        name: ceph-mon

- hosts: osds
  gather_facts: true
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-config",76,No issue found,No issue found,0,0,0,0
77,dest: /etc/default/ceph,"dest: /etc/ceph/{{ cluster }}.conf

- name: configure cluster name
  lineinfile:
    dest: /etc/sysconfig/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""RedHat""

- name: configure cluster name
  lineinfile:
    dest: /etc/default/ceph/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""Debian""",77,No issue found,"dest: /etc/ceph/{{ cluster }}.conf; line: ""CLUSTER={{ cluster }}""; line: ""CLUSTER={{ cluster }}""",0,0,1,0
78,"(ceph_health_raw.stdout | default('{}') | from_json)['state'] in ['leader', 'peon']","(ceph_health_raw.stdout | default({}) | from_json)['state'] in ['leader', 'peon']",78,No issue found,No issue found,0,1,0,0
79,"src: ""{{ grafana_yum_repo_template }}""
    dest: ""/etc/yum.repos.d/{{ grafana_yum_repo_template | basename | regex_replace('\\.j2$', '') }}""","- name: Add Grafana repository file [RHEL/CentOS]
  template:
    src: grafana.yum.repo.j2
    dest: /etc/yum.repos.d/grafana.repo
    force: yes
    backup: yes
- name: Import Grafana GPG signing key [Debian/Ubuntu]
  apt_key:
    url: ""https://packagecloud.io/gpg.key""
    state: present
    validate_certs: false
  environment:
    http_proxy: ""{{ http_proxy | default('') }}""
    https_proxy: ""{{ https_proxy | default('') }}""
  when: ansible_pkg_mgr == ""apt""

- name: Add Grafana repository [Debian/Ubuntu]
  apt_repository:
    repo: deb https://packagecloud.io/grafana/stable/debian/ jessie main
    state: present
    update_cache: yes

- name: Install Grafana
  package:
    name: grafana
    state: present
  notify: restart grafana",79,"src: ""{{ grafana_yum_repo_template }}""; dest: ""/etc/yum.repos.d/{{ grafana_yum_repo_template | basename | regex_replace('\.j2$', '') }}""",validate_certs: false,1,0,1,1
80,"prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check-config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check-rules %s""","- name: Set validator commands for prometheus 2.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '>=')

- name: Set validator commands for prometheus 1.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '<')",80,No issue found,"prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""; prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""",0,0,1,0
81,"author: ""Sebastian Gumprich""
        - 6.5
  categories:
    - security
dependencies: []","---
galaxy_info:
  author: Sebastian Gumprich
  description: 'This Ansible role provides numerous security-related ssh configurations, providing all-round base protection.'
  company: Hardening Framework Team
  license: Apache License 2.0
  min_ansible_version: '1.9'
  platforms:
    - name: EL
      versions:
        - 6.4
	- 6.5
    - name: Oracle Linux
      versions:
        - 6.4
        - 6.5
    - name: Ubuntu
      versions:
        - 12.04
        - 14.04
    - name: Debian
      versions:
        - 6
        - 7
   categories:
    - system",81,No issue found,No issue found,0,0,0,0
82,- restart win zabbix agent,"- restart win zabbix-agent
    - restart mac zabbix agent",82,No issue found,No issue found,0,0,0,0
83,nan,"chain: INPUT
    source: ""{{ zabbix_agent_server }}""",83,No issue found,No issue found,0,0,0,0
84,"dest: ""/etc/zabbix/scripts/""","- name: ""Installing user-defined scripts""
  copy:
    src: ""scripts/{{ item }}""
    dest: ""/etc/zabbix/scripts/{{ item }}""
    owner: zabbix
    group: zabbix
    mode: 0644
  notify: restart zabbix-agent
  become: yes
  with_items: ""{{ zabbix_agent_userparameters }}""
  when: zabbix_agent_custom_scripts",84,No issue found,"src: ""scripts/{{ item }}""; with_items: ""{{ zabbix_agent_userparameters }}""; when: zabbix_agent_custom_scripts",0,0,1,0
85,"when: es_start_service and (es_enable_xpack and ""security"" in es_xpack_features) and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))","- include: ./xpack/security/elasticsearch-security-native.yml
  when: es_start_service and (es_enable_xpack and '""security"" in es_xpack_features') and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))

- include: elasticsearch-template.yml
  when: es_templates
  tags:
      - templates",85,No issue found,No issue found,0,0,0,0
86,"lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^DATA_DIR"" insertafter=""^#DATA_DIR"" line=""DATA_DIR={{ es_data_dir }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_USER"" insertafter=""^#ES_USER"" line=""ES_USER={{ es_user }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_GROUP"" insertafter=""^#ES_GROUP"" line=""ES_GROUP={{ es_group }}""","- name: RedHat - configure memory
  lineinfile: dest=/etc/default/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  when: es_heap_size is defined
  register: elasticsearch_configure",86,No issue found,No issue found,0,0,0,0
87,"mode: ""2750""","owner: root
    mode: 2750
  copy: src={{ item }} dest={{ es_conf_dir }}/templates owner=root group={{ es_group }} mode=0660",87,No issue found,copy: src={{ item }} dest={{ es_conf_dir }}/templates owner=root group={{ es_group }} mode=0660,0,0,1,0
88,nan,no_log: True,88,No issue found,No issue found,0,0,0,0
89,"author: ""Florian Utz""","author: """"
      - xenial",89,No issue found,No issue found,0,0,0,0
90,"set -o pipefail;
      set -o pipefail;","- not ubuntu1804cis_skip_for_travis
      - not ubuntu1804cis_skip_for_travis
  shell: |
      set -o pipefail
      df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type d -perm -0002 2>/dev/null | xargs chmod a+t
  args:
      executable: /bin/bash
      - not ubuntu1804cis_allow_autofs
      - autofs_service_status.stdout == ""loaded""
  shell: |
      set -o pipefail
      dmesg | grep -E ""NX|XD"" | grep "" active""
  args:
      executable: /bin/bash",90,No issue found,No issue found,0,0,0,0
91,"service:
      name: rsyslog
      enabled: yes
      - ubuntu1804cis_syslog == ""rsyslog""","command: ""systemctl enable rsyslog""
      - rsyslog_service_status.stdout != ""enabled""",91,No issue found,"command: ""systemctl enable rsyslog""; - rsyslog_service_status.stdout != ""enabled""",0,0,1,0
92,name: rsync,"- name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh, rlogin, rexec""
      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh""
        service:
          name: rsh.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rsh_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6

      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rlogin""
        service:
          name: rlogin.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rlogin_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6

      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rexec""
        service:
          name: rexec.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rexec_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.6

- name: ""SCORED | 2.1.7 | PATCH | Ensure talk server is not enabled""
  service:
    name: ntalk
    state: stopped
    enabled: false
  when:
    - ubuntu1804cis_ntalk_server == false
    - ntalk_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_1_7
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.7

- name: ""SCORED | 2.1.8 | PATCH | Ensure telnet server is not enabled""
  service:
    name: telnet
    state: stopped
    enabled: false
  when:
    - ubuntu1804cis_telnet_server == false
    - telnet_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_1_8
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.8

- name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
  block:
      - name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
      - name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
            state: stopped
      - ubuntu1804cis_rule_2_1_9
      - rule_2.1.9
- name: ""SCORED | 2.1.10 | PATCH | Ensure xinetd is not enabled""
      - ubuntu1804cis_rule_2_1_10
      - rule_2.1.10

- name: ""SCORED | 2.1.11 | PATCH | Ensure openbsd-inetd is not installed""
  apt:
    name: openbsd-inetd
    state: absent
  when:
    - openbsd_inetd_service_status.stdout == ""ok installed""
    - ubuntu1804cis_rule_2_1_11
  tags:
    - level1
    - patch
    - scored
    - rule_2.1.11
- name: ""SCORED | 2.2.16 | PATCH | Ensure rsync service is not enabled ""
    name: rsyncd
    state: stopped
    enabled: false
    - not ubuntu1804cis_rsyncd_server
    - rsyncd_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_2_16
    - level1
    - scored
    - patch
    - rule_2.2.16
- name: ""SCORED | 2.2.17 | PATCH | Ensure NIS Server is not enabled""
      name: ypserv
      - not ubuntu1804cis_nis_server
      - ypserv_service_status.stdout == ""loaded""",92,No issue found,No issue found,0,0,0,0
93,changed_when: tenant_assoc.status == 201,"---
- name: Build stripes
  become: yes
  shell: ""yarn install && yarn build -- output""
  args:
    chdir: ""{{ stripes_conf_dir }}""
  listen: ""Rebuild stripes""

- name: get package.json
  slurp: src={{ stripes_conf_dir }}/package.json
  register: platform_raw
  listen: ""Rebuild stripes""

- set_fact: platform={{ platform_raw.content|b64decode|from_json }}
  listen: ""Rebuild stripes""

- name: Generate module descriptors
  become: yes
  shell: node {{ stripes_conf_dir }}/node_modules/@folio/stripes-core/util/package2md.js {{ stripes_conf_dir }}/node_modules/{{ item.key }}/package.json > {{ stripes_conf_dir }}/module-descriptors/{{ item.key.split('/')[1] }}.json
  when: item.key.split('/')[0] == '@folio' and item.key != '@folio/stripes-components' and item.key != '@folio/stripes-core'
  with_dict: ""{{ platform.dependencies }}""
  listen: ""Rebuild stripes""

- name: Get module descriptor filenames
  shell: ls {{ stripes_conf_dir }}/module-descriptors
  register: mod_descrs
  changed_when: false
  listen: ""Register modules""

- name: Slurp module descriptors
  slurp: src={{ stripes_conf_dir }}/module-descriptors/{{ item }}
  with_items: ""{{ mod_descrs.stdout_lines }}""
  register: mod_descrs_raw
  listen: ""Register modules""

- set_fact: mod_descrs=[]
  listen: ""Register modules""

- set_fact:
    mod_descrs: ""{{ mod_descrs }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descrs_raw.results }}""
  listen: ""Register modules""

- name: Check module registration
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: mod_reg_status
  listen: ""Register modules""

- name: Register modules with Okapi
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules""
    method: POST
    body_format: json
    body: ""{{ item.1|to_json }}""
    status_code: 201
  when: mod_reg_status.results[item.0].status == 404
  register: mod_register
  changed_when: mod_register.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

- name: Check tenant-module association
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/item.id""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: tenant_modules
  listen: ""Register modules""

- name: Enable modules for tenant
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""
    method: POST
    body_format: json
    body: '{ ""id"" : ""{{ item.1.id }}"" }'
    status_code: 201
  when: tenant_modules.results[item.0].status == 404
  register: tenant_assoc
  changed_when: tenant_assoc == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""",93,No issue found,"shell: ""yarn install && yarn build -- output""; shell: node {{ stripes_conf_dir }}/node_modules/@folio/stripes-core/util/package2md.js {{ stripes_conf_dir }}/node_modules/{{ item.key }}/package.json > {{ stripes_conf_dir }}/module-descriptors/{{ item.key.split('/')[1] }}.json; shell: ls {{ stripes_conf_dir }}/module-descriptors; uri: {url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}"", status_code: 200, 404}; uri: {url: ""{{ stripes_okapi_url }}/_/proxy/modules"", method: POST, body_format: json, body: ""{{ item.1|to_json }}"", status_code: 201}; uri: {url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/item.id"", status_code: 200, 404}; uri: {url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules"", method: POST, body_format: json, body: '{ ""id"" : ""{{ item.1.id }}"" }', status_code: 201}",0,0,1,1
94,"file: 
    path: /etc/nginx/sites-enabled/default
    state: absent
    notify: Restart nginx","- name: disable nginx default vhost
  become: yes
  file: path=/etc/nginx/sites-enabled/default
  state: absent
  notify: Restart nginx",94,No issue found,No issue found,0,0,0,0
95,"register: mod_descrs_files
- name: Create mod_descr_list variable to order modules
  with_items: ""{{ mod_descrs_files.stdout_lines }}""
- name: Reset mod_descr_list variable
  set_fact: mod_descr_list=[]
- name: Build mod_descr_list for registration
  set_fact:
    mod_descr_list: ""{{ mod_descr_list }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""","---
- name: Build stripes
  become: yes
  shell: ""yarn install && yarn build -- output""
  args:
    chdir: ""{{ stripes_conf_dir }}""
  listen: ""Rebuild stripes""

- name: Record stripes rebuild variable
  set_fact:
    stripes_rebuild: true
  listen: ""Rebuild stripes""

- name: Get module descriptor filenames
  shell: ls {{ stripes_conf_dir }}/ModuleDescriptors
  register: mod_descrs
  changed_when: false
  listen: ""Register modules""

- name: Slurp module descriptors
  slurp: src={{ stripes_conf_dir }}/ModuleDescriptors/{{ item }}
  with_items: ""{{ mod_descrs.stdout_lines }}""
  register: mod_descrs_raw
  listen: ""Register modules""

- set_fact: mod_descrs=[]
  listen: ""Register modules""

- set_fact:
    mod_descrs: ""{{ mod_descrs }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descrs_raw.results }}""
  listen: ""Register modules""

- name: Check module registration
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: mod_reg_status
  listen: ""Register modules""

- name: Register modules with Okapi
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules""
    method: POST
    body_format: json
    body: ""{{ item.1|to_json }}""
    status_code: 201
  when: mod_reg_status.results[item.0].status == 404
  register: mod_register
  changed_when: mod_register.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

- name: Check tenant-module association
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: tenant_modules
  listen: ""Register modules""

- name: Enable modules for tenant
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""
    method: POST
    body_format: json
    body: '{ ""id"" : ""{{ item.1.id }}"" }'
    status_code: 201
  when: tenant_modules.results[item.0].status == 404
  register: tenant_assoc
  changed_when: tenant_assoc.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""",95,"with_items: ""{{ mod_descrs_files.stdout_lines }}""; mod_descr_list: ""{{ mod_descr_list }} + [ {{ item.content|b64decode|from_json }} ]""; with_items: ""{{ mod_descr_list }}""; with_indexed_items: ""{{ mod_descr_list }}""; with_items: ""{{ mod_descr_list }}""; with_indexed_items: ""{{ mod_descr_list }}""","shell: ""yarn install && yarn build -- output""; shell: ls {{ stripes_conf_dir }}/ModuleDescriptors; slurp: src={{ stripes_conf_dir }}/ModuleDescriptors/{{ item }}; uri: url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""; uri: url: ""{{ stripes_okapi_url }}/_/proxy/modules""; uri: url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/{{ item.id }}""; uri: url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""",1,0,1,1
96,"- include: bootstrap_user.yml create_user=galaxy_tools_create_bootstrap_user
- include: bootstrap_user.yml delete_user=galaxy_tools_delete_bootstrap_user
  when: galaxy_tools_delete_bootstrap_user","- include: bootstrap_user.yml
  when: galaxy_tools_create_bootstrap_user and not galaxy_tools_api_key

  when: galaxy_tools_install_tools

- include: bootstrap_user.yml
  when: galaxy_tools_delete_bootstrap_user and not galaxy_tools_api_key",96,No issue found,No issue found,0,0,0,0
97,- hosts: master,"---
- hosts: all
  remote_user: root
  gather_facts: no

  tasks:
  - name: Add devices to heketi nodes
    heketi: action=adddevice sshuser=""{{ ssh_user }}""  userkey=""{{ user_key }}"" server=""{{ servername }}""
            node=""{{ node }}"" devices=""{{ item.devices }}""
    with_items: hdict
    register: result

  - debug: msg=""{{ result.results[0]['msg'] }}""",97,No issue found,"remote_user: root; sshuser=""{{ ssh_user }}""; userkey=""{{ user_key }}""",0,0,1,0
98,"name: ""{{ graylog_mongodb_package_dependencies_python2 }}""
  when: ansible_python_version is version('3.0.0', '<')

- name: ""Package dependencies should be installed""
  yum:
    name: ""{{ graylog_mongodb_package_dependencies_python3 }}""
    state: present
  when: ansible_python_version is version('3.0.0', '>=')","- name: Package dependencies should be installed
  yum: name={{ item }} state=installed
  with_items: ""{{ graylog_mongodb_package_dependencies | default([]) }}""",98,No issue found,yum: name={{ item }} state=installed,0,0,1,0
99,min_ansible_version: 2.3,min_ansible_version: 2.2,99,No issue found,No issue found,0,0,0,0
100,"loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:","- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath",100,No issue found,regexp='{{ item.0.regexp }}'; line='{{ item.0.line }}'; dest='{{ item.1 }}'; when: nginx_fastcgi_fix_realpath,0,0,1,0
101,"loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:","author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web",101,No issue found,No issue found,0,0,0,0
102,"description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch","- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath",102,No issue found,regexp='{{ item.0.regexp }}'; line='{{ item.0.line }}'; dest='{{ item.1 }}'; when: nginx_fastcgi_fix_realpath,0,0,1,0
103,"description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch","author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web",103,No issue found,No issue found,0,0,0,0
104,"BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshConnectivityHealth.asciidoc","sops:
  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshConnectivityHealth.asciidoc",104,No issue found,No issue found,0,0,0,0
105,"name: crud
      name: spring-boot-rest-http-crud","apiVersion: template.openshift.io/v1
    iconClass: icon-node
    tags: nodejs, crud
    openshift.io/display-name: Fruit CRUD Application
    openshift.io/provider-display-name: Red Hat, Inc.
    openshift.io/documentation-url: https://github.com/integr8ly/walkthrough-applications.git
    description: Basic CRUD application for fruit
- kind: DeploymentConfig
  apiVersion: apps.openshift.io/v1
    name: crud-app
      app: crud-app
    revisionHistoryLimit: 10
    test: false
      app: crud-app
          app: crud-app
        - name: crud-app
          image: quay.io/integreatly/fruit-crud-app:1.0.1
          resources: {}
          terminationMessagePath: ""/dev/termination-log""
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
        restartPolicy: Always
        terminationGracePeriodSeconds: 30
        dnsPolicy: ClusterFirst
        securityContext: {}
        schedulerName: default-scheduler
- kind: Service
  apiVersion: v1
    name: crud-app
    ports:
    - protocol: TCP
      port: 8080
      app: crud-app
- kind: Route
  apiVersion: route.openshift.io/v1
    name: spring-boot-rest-http-crud
    to:
      kind: Service
      name: crud-app
    port:
      targetPort: 8080
    tls:
      termination: edge
    wildcardPolicy: None",105,No issue found,"image: quay.io/integreatly/fruit-crud-app:1.0.1; imagePullPolicy: IfNotPresent; securityContext: {}; terminationMessagePath: ""/dev/termination-log""; tls:
  termination: edge",0,1,1,0
106,"shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-service""
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-service""","---
- name: ""Delete project namespace: {{ msbroker_namespace }}""
  shell: oc delete project {{ msbroker_namespace }}
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Delete CRDs
  shell: ""oc delete crd {{ item }}""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0
  with_items: syndesises.syndesis.io

- name: Clean up clusterservicebroker
  shell: ""oc delete clusterservicebrokers.servicecatalog.k8s.io managed-services-broker""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role
  shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role binding
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0",106,"shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-service""; shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-service""","shell: oc delete project {{ msbroker_namespace }}; shell: ""oc delete crd {{ item }}""; shell: ""oc delete clusterservicebrokers.servicecatalog.k8s.io managed-services-broker""; shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-services""; shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-services""",1,0,1,1
107,nan,"ups_namespace: ""{{ eval_ups_namespace | default('unifiedpush') }}""
ups_app_namespaces: ""{{ eval_mdc_namespace | default('mobile-developer-console') }}""
ups_resources:
  - ""{{ ups_operator_resources }}/service_account.yaml""
  - ""{{ ups_operator_resources }}/role.yaml""
  - ""{{ ups_operator_resources }}/role_binding.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_androidvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_iosvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_pushapplication_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_unifiedpushserver_crd.yaml""
ups_operator_deployment: ""{{ ups_operator_resources }}/operator.yaml""
ups_template_dir: /tmp
ups_server_name: unifiedpush
ups_backup: ""{{ backup_restore_install | default(false) }}""
ups_backup_name: ups-daily-at-midnight
ups_backup_schedule: ""{{ backup_schedule }}""
ups_backup_secret: ""s3-credentials""
ups_backup_secret_namespace: ""{{ backup_namespace }}""
ups_encryption_secret: ''
ups_encryption_secret_namespace: ""{{ backup_namespace }}""
ups_backup_rbac_template:
  - ""{{ backup_resources_location }}/rbac/role-binding-template.yaml""
ups_backup_rbac_resources:
  - ""{{ backup_resources_location }}/rbac/service-account.yaml""
  - ""{{ backup_resources_location }}/rbac/role.yaml""
ups_svc_monitor_resources:
  - ""{{ ups_operator_resources }}/service_monitor.yaml""",107,No issue found,"ups_template_dir: /tmp; ups_backup_secret: ""s3-credentials""; ups_encryption_secret: ''",0,1,1,0
108,shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\},"- name: ""include rhsso vars""
  include_vars: ../../rhsso/defaults/main.yml
    launcher_sso_openshift_idp_client_secret: ""{{ 99999 | random | to_uuid }}""
- name: Retrieve secret for launcher openshift sso client
  shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\} | base64 -d
  failed_when: openshift_client_secret_response.stderr != """"
  until: openshift_client_secret_response.stdout
  retries: 50
  delay: 3
  changed_when: openshift_client_secret_response.stdout",108,Use of shell module; Use of variable in shell command; Potential exposure of sensitive data,include_vars: ../../rhsso/defaults/main.yml; shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\} | base64 -d,1,1,1,1
109,"- name: ""include launcher vars""
  include_vars: ../../launcher/defaults/main.yml

- name: Get Launcher SSO secure route
  shell: oc get route/{{ launcher_sso_prefix }} -o template --template \{\{.spec.host\}\} -n {{ launcher_namespace }}
  register: rhsso_secure_route
  retries: 60
  delay: 5
  until: rhsso_secure_route.rc == 0

- name: ""Generate secret for launcher client""
  set_fact:
    launcher_client_secret: ""{{ (ansible_date_time.epoch + launcher_namespace) | hash('sha512') }}""

- set_fact:
    launcher_sso_route: ""{{ rhsso_secure_route.stdout }}""

  shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_cluster_admin_username}}","- name: ""Create project namespace: {{ rhsso_namespace }}""
  shell: oc new-project {{ rhsso_namespace }}
  register: output
  failed_when: output.stderr != '' and 'already exists' not in output.stderr
  changed_when: output.rc == 0
- name: ""Ensure 1.2 tag is present for redhat sso in openshift namespace""
  shell: oc tag --source=docker registry.access.redhat.com/redhat-sso-7/sso72-openshift:1.2 openshift/redhat-sso72-openshift:1.2
  register: result
  until: result.stdout
  retries: 50
  delay: 1
  failed_when: not result.stdout
  changed_when: False
- name: ""Ensure 1.2 tag has an imported image in openshift namespace""
  shell: oc -n openshift import-image redhat-sso72-openshift:1.2
  register: result
  until: result.stdout
  retries: 50
  delay: 1
  failed_when: not result.stdout
  changed_when: False
- name: ""Create required objects""
  shell: ""oc create -f https://raw.githubusercontent.com/{{rhsso_operator_repo}}/keycloak-operator/{{rhsso_operator_commit_tag}}/deploy/{{ item }} -n {{ rhsso_namespace }}""
  with_items: ""{{ rhsso_operator_required_objects }}""
  register: rhsso_required_objects_result
  failed_when: rhsso_required_objects_result.stderr != '' and 'AlreadyExists' not in rhsso_required_objects_result.stderr

- name: ""Create operator deployment config template""
  template:
    src: ""operator-dc.yaml""
    dest: /tmp/operator-dc.yaml

- name: ""create operator deployment config""
  shell: ""oc create -f /tmp/operator-dc.yaml -n {{ rhsso_namespace }}""
  register: rhsso_dc
  failed_when: rhsso_dc.stderr != '' and 'AlreadyExists' not in rhsso_dc.stderr

- name: ""Create keycloak resource template""
  template:
    src: ""keycloak.json.j2""
    dest: ""/tmp/keycloak.json""

- name: ""Create keycloak resource""
  shell: oc create -f /tmp/keycloak.json -n {{ rhsso_namespace }}
  register: rhsso_keycloak
  failed_when: rhsso_keycloak.stderr != '' and 'AlreadyExists' not in rhsso_keycloak.stderr

- name: ""Generate secret for rhsso client""
  set_fact:
    rhsso_client_secret: ""{{ (ansible_date_time.epoch + rhsso_namespace) | hash('sha512') }}""

- name: ""include threescale vars""
  include_vars: ../../3scale/defaults/main.yml

- name: ""Generate secret for 3scale client""
  set_fact:
    threescale_client_secret: ""{{ (ansible_date_time.epoch + threescale_namespace) | hash('sha512') }}""

- name: ""Create keycloak realm resource template""
  template:
    src: ""keycloak-realm.json.j2""
    dest: ""/tmp/keycloak-realm.json""

- name: Seed evaluation users
  include: _inject_user.yml template=""/tmp/keycloak-realm.json"" email={{ rhsso_seed_users_email_format|format(item|int) }} username={{ rhsso_seed_users_name_format|format(item|int)}} password={{ rhsso_seed_users_password }}
  with_sequence: count={{ rhsso_seed_users_count }}

- name: ""Create keycloak realm resource""
  shell: oc create -f /tmp/keycloak-realm.json -n {{ rhsso_namespace }}
  register: rhsso_kcr
  failed_when: rhsso_kcr.stderr != '' and 'AlreadyExists' not in rhsso_kcr.stderr


- name: ""Verify rhsso realm is provisioned""
  shell: sleep 5; oc get keycloakrealm {{ rhsso_realm }} -o template --template \{\{.status.phase\}\}  -n {{ rhsso_namespace }}  |  grep  'reconcile'
  register: result
  until: result.stdout
  retries: 50
  delay: 10
  failed_when: not result.stdout
  changed_when: False

- name: configure logout
  import_tasks: logout.yml

- name: Add cluster admin role to evals admin
  shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_evals_admin_username}}",109,include_vars: ../../launcher/defaults/main.yml; shell: oc get route/{{ launcher_sso_prefix }} -o template --template \{\{.spec.host\}\} -n {{ launcher_namespace }}; shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_cluster_admin_username}},"shell: oc new-project {{ rhsso_namespace }}; shell: oc tag --source=docker registry.access.redhat.com/redhat-sso-7/sso72-openshift:1.2 openshift/redhat-sso72-openshift:1.2; shell: oc -n openshift import-image redhat-sso72-openshift:1.2; shell: ""oc create -f https://raw.githubusercontent.com/{{rhsso_operator_repo}}/keycloak-operator/{{rhsso_operator_commit_tag}}/deploy/{{ item }} -n {{ rhsso_namespace }}""; shell: ""oc create -f /tmp/operator-dc.yaml -n {{ rhsso_namespace }}""; shell: oc create -f /tmp/keycloak.json -n {{ rhsso_namespace }}; shell: oc create -f /tmp/keycloak-realm.json -n {{ rhsso_namespace }}; shell: sleep 5; oc get keycloakrealm {{ rhsso_realm }} -o template --template \{\{.status.phase\}\}  -n {{ rhsso_namespace }}  |  grep  'reconcile'; shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_evals_admin_username}}",1,1,1,1
110,"shell: ""oc set env dc/tutorial-web-app \
  GITEA_TOKEN='{{ gitea_token }}' \
  GITEA_HOST='http://{{ gitea_ingress_host.stdout }}' \
  -n {{ webapp_namespace }} \
  --overwrite=true""
  when: check_webapp_installed_cmd.rc == 0 and gitea_ingress_host.stdout != ''

- name: Wait for pods
  shell: sleep 5; oc get pods --namespace {{ webapp_namespace }}  |  grep  ""Creating""
  register: result
  until: not result.stdout
  retries: 50
  delay: 10
  failed_when: result.stdout
  changed_when: False","- name: Make sure we are using the gitea namespace
  shell: ""oc project {{ gitea_namespace }}""


- name: ""Wait for Gitea pods to be ready""
  shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[*].status.containerStatuses[?(@.ready==true)].ready}' | wc -w""
  register: gitea_result
  until: gitea_result.stdout.find(""1"") != -1
  retries: 30
- name: Get the name of the gitea pod
  shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[0].metadata.name}'""
  register: gitea_pod_name

- name: Create the gitea admin user
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name={{ gitea_admin_username }} --password={{ gitea_admin_password }} --admin --email=admin@example.com --config /home/gitea/conf/app.ini""
  register: create_admin_user_cmd
  failed_when: create_admin_user_cmd.stderr != '' and 'already exists' not in create_admin_user_cmd.stderr
  changed_when: create_admin_user_cmd.rc == 0

- name: Fetch all old tokens
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens""
    method: GET
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
  register: gitea_old_access_tokens

- name: Delete all old access tokens
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens/{{ item.id }}""
    method: DELETE
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
    status_code: 204
  with_items: ""{{ gitea_old_access_tokens.json }}""

- name: Create a new admin token for the admin user
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens""
    method: POST
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    body: {""name"": ""{{ gitea_admin_token }}""}
    body_format: json
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
    return_content: yes
    status_code: 201
  register: admin_token_result

- name: Extract sha value from admin token result
  set_fact:
    gitea_token: ""{{ admin_token_result.json | json_query('sha1') }}""

- name: Print out the gitea admin token
  debug:
    msg: ""Gitea admin token is {{ gitea_token }}""

- name: Create gitea walkthrough users
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals{{ item }} --password=Password1 --email=evals{{ item }}@example.com --config /home/gitea/conf/app.ini""
  register: create_walkthrough_user_cmd
  failed_when: create_walkthrough_user_cmd.stderr != '' and 'already exists' not in create_walkthrough_user_cmd.stderr
  changed_when: create_walkthrough_user_cmd.rc == 0
  with_sequence: count={{ eval_seed_users_count }}

- name: Create a user for the evals admin
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals-admin --password=Password1 --email=evals-admin@example.com --config /home/gitea/conf/app.ini""
  register: create_eval_admin_user_cmd
  failed_when: create_eval_admin_user_cmd.stderr != '' and 'already exists' not in create_eval_admin_user_cmd.stderr
  changed_when: create_eval_admin_user_cmd.rc == 0

- name: Get the gitea ingress host
  shell: oc get ingress --namespace=gitea --selector='app=gitea' -o jsonpath='{.items[0].spec.rules[0].host}'
  register: gitea_ingress_host

- name: Set gitea host as webapp env var
  shell: oc set env dc/tutorial-web-app GITEA_HOST=""{{ gitea_ingress_host.stdout }}"" -n {{ webapp_namespace }} --overwrite=true
  when: gitea_ingress_host.stdout != ''

  shell: oc set env dc/tutorial-web-app GITEA_TOKEN=""{{ gitea_token }}"" -n {{ webapp_namespace }} --overwrite=true
  when: check_webapp_installed_cmd.rc == 0",110,"GITEA_TOKEN='{{ gitea_token }}'; GITEA_HOST='http://{{ gitea_ingress_host.stdout }}'; shell: sleep 5; oc get pods --namespace {{ webapp_namespace }}  |  grep  ""Creating""","shell: ""oc project {{ gitea_namespace }}""; shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[*].status.containerStatuses[?(@.ready==true)].ready}' | wc -w""; shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[0].metadata.name}'""; shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name={{ gitea_admin_username }} --password={{ gitea_admin_password }} --admin --email=admin@example.com --config /home/gitea/conf/app.ini""; uri: url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens""; shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals{{ item }} --password=Password1 --email=evals{{ item }}@example.com --config /home/gitea/conf/app.ini""; shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals-admin --password=Password1 --email=evals-admin@example.com --config /home/gitea/conf/app.ini""; shell: oc set env dc/tutorial-web-app GITEA_HOST=""{{ gitea_ingress_host.stdout }}"" -n {{ webapp_namespace }} --overwrite=true; shell: oc set env dc/tutorial-web-app GITEA_TOKEN=""{{ gitea_token }}"" -n {{ webapp_namespace }} --overwrite=true",1,1,1,1
111,shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10Mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} --param=BACKEND_IMAGE_TAG={{ launcher_backend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,111,LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD=; LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth; LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public,LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME=; LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD=; LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL=; LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL=,1,0,1,1
112,nan,"---
- name: Get RH-SSO secure route
  local_action: command oc get route/secure-sso -o template --template \{\{.spec.host\}\} -n {{ rhsso_namespace }}
  register: rhsso_secure_route

- set_fact:
    rhsso_route: ""{{ rhsso_secure_route.stdout }}""

- name: Retrieve RH-SSO Client Config
  local_action: command cat /tmp/client-config.json
  register: client_config_raw

- set_fact:
    client_config: ""{{ client_config_raw.stdout }}""

- name: Add RH-SSO identity provider to master config
  blockinfile:
    path: ""{{ openshift_master_config }}""
    insertafter: ""identityProviders""
    backup: yes
    block: |2
        - name: rh_sso
          challenge: false
          login: true
          mappingInfo: add
          provider:
            apiVersion: v1
            kind: OpenIDIdentityProvider
            clientID: {{ rhsso_client_id }}
            clientSecret: {{ client_config.json.value }}
            urls:
              authorize: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/auth
              token: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/token
              userInfo: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/userinfo
            claims:
              id:
              - sub
              preferredUsername:
               - preferred_username
              name:
              - name
              email:
              - email
  register: master_config_update
  become: yes

- name: restart openshift master api service
  service:
    name: atomic-openshift-master-api
    state: restarted
  become: yes
  when: master_config_update.changed

- name: restart openshift master controller service
  service:
    name: atomic-openshift-master-controllers
    state: restarted
  become: yes
  when: master_config_update.changed

- name: Delete local client config file
  file: path=/tmp/client-config.json state=absent",112,No issue found,local_action: command oc get route/secure-sso -o template --template \{\{.spec.host\}\} -n {{ rhsso_namespace }}; local_action: command cat /tmp/client-config.json; clientID: {{ rhsso_client_id }}; clientSecret: {{ client_config.json.value }}; become: yes; file: path=/tmp/client-config.json state=absent,0,0,1,0
113,"- name: Add labels to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}"", ""integreatly-middleware-service"":""true""}}}'
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0
- name: Add labels to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\"", \""integreatly-middleware-service\"":\""true\""}}}'""
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0","- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """"",113,No issue found,"shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'; shell: ""oc patch ns {{ che_infra_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'""",0,0,1,0
114,nan,"when:
        - user_rhsso | default(true) | bool
        - mdc | default(true) | bool",114,No issue found,No issue found,0,1,0,0
115,"shell: /usr/local/bin/master-restart controllers

-
  name: ""Get all users created by rhsso""
  shell: oc get users | grep 'rh_sso' | awk '{print $1}'
  register: users
  failed_when: false

-
  name: ""Delete users""
  shell:  ""oc delete users {{ users.stdout | replace('\n', ' ') }}""
  when: users.stdout != ''
  failed_when: false","- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """"",115,"shell: /usr/local/bin/master-restart controllers; shell: oc get users | grep 'rh_sso' | awk '{print $1}'; shell:  ""oc delete users {{ users.stdout | replace('
', ' ') }}""","shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'; shell: ""oc patch ns {{ che_infra_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'""",1,0,1,1
116,"loop: ""{{ real_users }}""","- name: USB controllers warning
  command: >-
    zenity --warning --text {{ usb_warning_msg }}
  become: yes
  become_user: ""{{ item.user }}""
  with_items: ""{{ real_users }}""
  ignore_errors: yes",116,No issue found,"command: >- zenity --warning --text {{ usb_warning_msg }}; become_user: ""{{ item.user }}""; with_items: ""{{ real_users }}""; ignore_errors: yes",0,0,1,1
117,"node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address }}""","---
node_interface: ""enp0s8""
node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address",117,No issue found,No issue found,0,0,0,0
118,"- name: Install flannel
- name: Prepare and write flannel configuration to etcd
  include: config.yml
- name: Enable flannel on node
  service: name=flanneld enabled=yes
- name: Start flannel on node
  service: name=flanneld state=started
  register: flannel_started
  notify:
     - Restart docker engine","---
- name: Install flannel service (RHEL/CentOS)
  when: ansible_os_family == ""RedHat""
  yum:
    name: flannel
    state: latest
    update_cache: yes

- name: Update flannel config
  template: src=""flanneld.j2"" dest={{ flannel_dir }}/flanneld
  register: change_flannel

- name: Set facts about etcdctl command
  set_fact:
    peers: ""{% for hostname in groups['etcd'] %}http://{{ hostname }}:2379{% if not loop.last %},{% endif %}{% endfor %}""
    conf_file: ""/tmp/config.json""
    conf_loc: ""/{{ flannel_key }}/config""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Create flannel config file to go in etcd
  template: src=flannel-config.json dest={{ conf_file }}
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Load the flannel config file into etcd
  when: etcd_peer_url_scheme == 'http'
  shell: ""/usr/bin/etcdctl --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Load the flannel config file into secure etcd
  when: etcd_peer_url_scheme == 'https'
  shell: ""/usr/bin/etcdctl -cert-file={{ etcd_peer_cert_file }} --ca-file={{ etcd_peer_ca_file }} --key-file={ etcd_peer_key_file }} --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Start and enable flannel on node
  when: change_flannel|succeeded
  service: name=flanneld enabled=no state=started

- name: Reload flanneld
  when: change_flannel|changed or etcd_cert|changed
  service: name=flanneld state=restarted",118,No issue found,"conf_file: ""/tmp/config.json""; template: src=flannel-config.json dest={{ conf_file }}; shell: ""/usr/bin/etcdctl --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""; shell: ""/usr/bin/etcdctl -cert-file={{ etcd_peer_cert_file }} --ca-file={{ etcd_peer_ca_file }} --key-file={ etcd_peer_key_file }} --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""; copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}",0,0,1,1
119,nan,"directories:
    directories:
      - kubedns/kubedns-autoscale-dp.yml
    directories:
    directories:
    directories:
        kind: namespace
    directories:
      - logging/elasticsearch
      - logging/elasticsearch/elasticsearch-sa.yml
      - logging/elasticsearch/elasticsearch-rbac.yml
      - logging/elasticsearch/elasticsearch-svc.yml
      - logging/elasticsearch/elasticsearch-sts.yml
      - logging/kibana/kibana-anonymous-rbac.yml
    directories:
      - monitoring/gpu-exporter
      - monitoring/prometheus-adapter
        kind: namespace
        namespace: """"
      - monitoring/grafana/grafana-res-definitions.yml
      - monitoring/grafana/grafana-gpu-cluster-definitions.yml
      - monitoring/grafana/grafana-gpu-node-definitions.yml
      - monitoring/grafana/grafana-gpu-pod-definitions.yml
      - monitoring/grafana/grafana-cluster-definitions.yml
      - monitoring/gpu-exporter/gpu-exporter-svc.yml
      - monitoring/gpu-exporter/gpu-exporter-ds.yml
      - monitoring/prometheus-adapter/prometheus-adapter-sa.yml
      - monitoring/prometheus-adapter/prometheus-adapter-rbac.yml
      - monitoring/prometheus-adapter/prometheus-adapter-svc.yml
      - monitoring/prometheus-adapter/prometheus-adapter-cm.yml
      - monitoring/prometheus-adapter/prometheus-adapter-apiservice.yml
      - monitoring/prometheus-adapter/prometheus-adapter-dp.yml
      - monitoring/servicemonitor/kube-state-metrics-sm.yml
      - monitoring/servicemonitor/gpu-exporter-sm.yml
      - monitoring/servicemonitor/grafana-sm.yml",119,No issue found,No issue found,0,0,0,0
120,"when: ""'pve-no-subscription' in proxmox_repository_line""","- block:

  - name: Remove automatically installed PVE Enterprise repo configuration
    apt_repository:
      repo: ""deb https://enterprise.proxmox.com/debian jessie pve-enterprise""
      filename: pve-enterprise
      state: absent

  - name: Remove subscription popup dialog in web UI
    replace:
      dest: /usr/share/pve-manager/ext6/pvemanagerlib.js
      regexp: ""^          if .data.status !== 'Active'. {""
      replace: ""          if (false) {""
      backup: yes

  when: 'pve-no-subscription' in proxmox_repository_line
  when: proxmox_check_for_kernel_update

  when:
    - proxmox_reboot_on_kernel_update
    - __proxmox_kernel | changed

- name: Remove old Debian kernels
  apt:
    name: ""{{ item }}""
    state: absent
  with_items:
    - linux-image-amd64
    - linux-image-3.16.0-4-amd64
  when: proxmox_remove_old_kernels

- name: LDAP fix for authenticated search
  lineinfile:
    line: ""    $ldap->bind('{{ proxmox_ldap_bind_user }}', password => '{{ proxmox_ldap_bind_password }}');""
    insertbefore: ""ldap->search\\(""
    dest: /usr/share/perl5/PVE/Auth/LDAP.pm
  notify:
    - restart pvedaemon
  when:
    - proxmox_ldap_bind_user is defined
    - proxmox_ldap_bind_password is defined",120,No issue found,"when: 'pve-no-subscription' in proxmox_repository_line
  when: proxmox_check_for_kernel_update

  when:
    - proxmox_reboot_on_kernel_update
    - __proxmox_kernel | changed; line: ""    $ldap->bind('{{ proxmox_ldap_bind_user }}', password => '{{ proxmox_ldap_bind_password }}');""
    insertbefore: ""ldap->search\\(""
    dest: /usr/share/perl5/PVE/Auth/LDAP.pm",0,0,1,1
121,"- name: Mkdir for java installation
      win_file:
        path: '{{ java_path }}\{{ java_folder }}'
        state: directory
    - name: Create temporary directory
      win_tempfile:
        state: directory
      register: temp_dir_path
    - name: Unarchive to temporary directory
      win_unzip:
        src: '{{ java_artifact }}'
        dest: '{{ temp_dir_path }}'
    - name: Find java_folder in temp
      win_find:
        paths: '{{ temp_dir_path }}'
        recurse: false
        file_type: directory
      register: java_temp_folder
    - name: Copy from temporary directory
      win_copy:
        src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
        dest: '{{ java_path }}\{{ java_folder }}'
        remote_src: true

    - name: Check choco
      win_chocolatey:
        name: chocolatey
        state: present

    - name: 'Install vcredist package prior to using SAP JVM'
      win_chocolatey:
        name: vcredist2013
      register: choco_install
      retries: 15
      delay: 5
      until: choco_install is succeeded","---
- name: Check that the java_folder exists
  win_stat:
    path: '{{ java_path }}\{{ java_folder }}/bin'
  register: java_folder_bin

- name: Install java from tarball
  block:
  - name: Mkdir for java installation
    win_file:
      path: '{{ java_path }}\{{ java_folder }}'
      state: directory

  - name: Create temporary directory
    win_tempfile:
      state: directory
    register: temp_dir_path

  - name: Unarchive to temporary directory
    win_unzip:
      src: '{{ java_artifact }}'
      dest: '{{ temp_dir_path }}'

  - name: Find java_folder in temp
    win_find:
      paths: '{{ temp_dir_path }}'
      recurse: false
      file_type: directory
    register: java_temp_folder

  - name: Copy from temporary directory
    win_copy:
      src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
      dest: '{{ java_path }}\{{ java_folder }}'
      remote_src: true
  when: not java_folder_bin.stat.exists",121,No issue found,"path: '{{ java_path }}\{{ java_folder }}/bin'; src: '{{ java_artifact }}'; dest: '{{ temp_dir_path }}'; src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'; dest: '{{ java_path }}\{{ java_folder }}'",0,0,1,0
122,"| regex_findall('(https://download[\.\w]+/java/GA/jdk'
 java_major_version|string + '[.\d]+/[\d\w]+/'
 java_major_version|string + '/GPL/openjdk-'
 java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')","---
- name: Set java minor version
  set_fact:
    minor: ""{{ java_minor_version | default('.*', True) }}""

- name: 'Fetch root page {{ openjdk_root_page }}'
  uri:
    url: '{{ openjdk_root_page }}/{{ java_major_version }}/'
    return_content: True
  register: root_page

- name: Find release url
  set_fact:
    release_url: >-
      {{ root_page['content']
        | regex_findall('(https://download\.oracle\.com/java/GA/jdk'
          + java_major_version|string + '[.\d]+/[\d\w]+/'
          + java_major_version|string + '/GPL/openjdk-'
          + java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')
      }}

- name: Exit if OpenJDK version is not General-Availability Release
  fail:
    msg: 'OpenJDK version {{ java_major_version }} not GA Release'
  when: release_url[1] is not defined

- name: 'Get artifact checksum {{ release_url[1] }}'
  uri:
    url: '{{ release_url[1] }}'
    return_content: True
  register: artifact_checksum

- name: Show artifact checksum
  debug:
    var: artifact_checksum.content

- name: 'Download artifact from {{ release_url[0] }}'
  get_url:
    url: '{{ release_url[0] }}'
    dest: '{{ download_path }}'
    checksum: 'sha256:{{ artifact_checksum.content }}'
  register: file_downloaded
  retries: 20
  delay: 5
  until: file_downloaded is succeeded

- name: Set downloaded artifact variable
  set_fact:
    java_artifact: '{{ file_downloaded.dest }}'

- name: Split artifact name
  set_fact:
    parts: >-
      {{ java_artifact
        | regex_findall('^(.*j[dkre]{2})-([0-9]+)[u.]([0-9.]+)[-_]([a-z]+)-(x64|i586)')
        | first | list }}

- name: Set variables based on split
  set_fact:
    java_package: '{{ parts[0][-3:] }}'
    java_major_version: '{{ parts[1] }}'
    java_minor_version: '{{ parts[2] }}'
    java_os: '{{ parts[3] }}'
    java_arch: '{{ parts[4] }}'",122,regex_findall('(https://download[\.\w]+/java/GA/jdk' java_major_version|string + '[.\d]+/[\d\w]+/' java_major_version|string + '/GPL/openjdk-' java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)'),"minor: ""{{ java_minor_version | default('.*', True) }}""; url: '{{ openjdk_root_page }}/{{ java_major_version }}/'; url: '{{ release_url[1] }}'; url: '{{ release_url[0] }}'; dest: '{{ download_path }}'; java_artifact: '{{ file_downloaded.dest }}'",1,0,1,0
123,"- { src: php.ini.j2, dest: /etc/php5/conf.d/90-php-docker.ini }
  - { src: php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}","- name: install PHP5 packages

- name: Place PHP configuration files in place.
  template: src={{ item.src }} dest={{ item.dest }} owner=root group=root mode=644
  with_items:
  - { src: php/php.ini.j2, dest: /etc/php5/apache2/90-php-docker.ini }
  - { src: php/php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: php/xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}
  notify: restart apache",123,No issue found,template: src={{ item.src }} dest={{ item.dest }} owner=root group=root mode=644,0,0,1,0
124,"become: yes
  become: yes
  become: yes
  become: yes","- name: ensure rabbitmq is installed
  apt: pkg=rabbitmq-server state=installed
  sudo: yes

- name: activate rabbitmq_management plugin
  shell: ""/usr/sbin/rabbitmq-plugins enable rabbitmq_management""
  sudo: yes

- name: restart rabbitmq
  service: name=rabbitmq-server state=restarted
  sudo: yes

- name: get rabbitmqadmin script
  get_url: url=http://localhost:15672/cli/rabbitmqadmin dest=/usr/local/bin/rabbitmqadmin mode=755
  sudo: yes",124,No issue found,sudo: yes; get_url: url=http://localhost:15672/cli/rabbitmqadmin dest=/usr/local/bin/rabbitmqadmin mode=755,0,1,1,1
125,"pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int < 16 else 'python3-venv' }}""","when: ansible_lsb.major_release|int >= 8

- name: install python 3 virtualenv package
  apt:
    pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int >= 16 else 'python3-venv' }}""
    state: latest
  become: yes",125,No issue found,No issue found,0,1,0,0
126,"file: dest=""{{ solr_config_dir }}"" state=directory group=solr mode=""g+rwX"" recurse=yes","- name: create solr group
  group: name=solr state=present
  become: yes

  user: name=solr group=solr groups=""www-data"" comment=""Solr Daemon"" home=""{{ solr_install_dir }}""
- name: solr config directory permission
  file: dest=""{{ solr_config_dir }}"" state=directory owner=vagrant group=solr mode=""g+rwX"" recurse=yes
  become: yes

- name: solr install directory permission",126,"mode=""g+rwX""","user: name=solr group=solr groups=""www-data"" comment=""Solr Daemon"" home=""{{ solr_install_dir }}""; file: dest=""{{ solr_config_dir }}"" state=directory owner=vagrant group=solr mode=""g+rwX"" recurse=yes",1,1,1,0
127,"shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20 ] ; then exit 1 ; fi ; exit 0""
  args:
    executable: /bin/bash","- name: ""Wait for the running state of ToscaSubmitter""
  shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20] ; then echo '20 seconds expired, unsuccessfully. An error occurred.' ; exit 1 ; fi""
  register: output
  changed_when: output.stdout != """"
  when: f.stat.exists",127,No issue found,shell module; docker ps command without user specification; unhandled exceptions,0,1,1,1
128,"service:
    name: iptables
    state: restarted
    enabled: yes
  service:
    name: micado
    state: restarted
    enabled: yes","- name: Enable packet filtering
  shell: systemctl enable --now iptables",128,No issue found,shell: systemctl enable --now iptables,0,1,1,0
129,"- name: Create app and data folders
  file:
    name: ""{{ item }}""
    state: directory
  with_items:
    - ""{{ bamboo_master_application_folder }}""
    - ""{{ bamboo_master_data_folder }}""

    home: ""{{ bamboo_master_data_folder }}""
- name: Set permissions for app and data folders","---
- name: ""Install JDK""
  yum:
    name: ""java-{{ openjdk_version }}-openjdk""
    state: installed

- name: Add local user
  user:
    name: ""{{ bamboo_user }}""

- name: Create app and data folders
  file:
    name: ""{{ item }}""
    state: directory
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  with_items:
    - ""{{ bamboo_home_directory }}""
    - ""{{ bamboo_root_directory }}""

- name: Download and unpack bamboo
  unarchive:
    src: ""https://www.atlassian.com/software/bamboo/downloads/binary/atlassian-bamboo-{{ bamboo_version }}.tar.gz""
    dest: ""{{ bamboo_root_directory }}""
    remote_src: True
    keep_newer: yes
  changed_when: False

- name: Configure bamboo server (server.xml)
  template:
    src: server.xml.j2
    dest: ""{{ bamboo_application_directory }}/conf/server.xml""
  notify: restart bamboo

- name: Create current version file
  file:
    name: ""{{ bamboo_root_directory }}/current""
    state: touch
    mode: 0644
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  changed_when: False
    
- name: Update current version file
  template:
    src: current.j2
    dest: ""{{ bamboo_root_directory }}/current""
  notify: restart bamboo
  register: current_version

- name: Stop bamboo if newer version is going to be installed
  systemd:
    name: bamboo
    state: stopped
  when: current_version.changed

- name: Copy logrotate script for syslog
  copy:
    src: syslog
    dest: /etc/logrotate.d/syslog

- name: Add cronjob for cleanup Bamboo logs
  cron:
    name: cleanupbamboologs
    special_time: daily
    state: present
    job: ""/usr/bin/find {{ bamboo_application_directory }}/logs/ -name *.log -type f -mtime +7 -exec rm  {} \\;""

- name: Add cronjob for cleanup Bamboo build-dir
  cron:
    name: cleanupbamboobuilddir
    hour: 03
    minute: 15
    weekday: 0 # sunday
    state: present
    job: ""/usr/bin/find {{ bamboo_home_directory }}/xml-data/build-dir/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \\;""

- name: Add cronjob for cleanup maven repository cache
  cron:
    name: cleanupbamboomavenrepo
    hour: 03
    minute: 15
    weekday: 0 # sunday
    state: present
    job: ""/usr/bin/find {{ bamboo_home_directory }}/.m2/repository/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \\;""

- name: Install bamboo systemd unit script
  template:
    src: bamboo.service.j2
    dest: /etc/systemd/system/bamboo.service
    mode: 0744
  notify: restart bamboo

- name: Set bamboo.home property variable
  template:
    src: bamboo-init.properties
    dest: ""{{ bamboo_application_directory }}/atlassian-bamboo/WEB-INF/classes/bamboo-init.properties""
  notify: restart bamboo

- name: Set JVM settings
  replace:
    dest: ""{{ bamboo_application_directory }}/bin/setenv.sh""
    regexp: ""{{ item.regexp }}""
    replace: ""{{ item.replace }}""
  with_items:
    - regexp: 'JVM_MINIMUM_MEMORY=""[0-9]+[bkmg]'
      replace: 'JVM_MINIMUM_MEMORY=""{{ bamboo_jvm_heap_min }}'
    - regexp: 'JVM_MAXIMUM_MEMORY=""[0-9]+[bkmg]'
      replace: 'JVM_MAXIMUM_MEMORY=""{{ bamboo_jvm_heap_max }}'
  notify: restart bamboo

- name: Fix permissions on application folder
  file:
    path: ""{{ item }}""
    recurse: yes
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  changed_when: False
  with_items:
    - ""{{ bamboo_application_directory }}""",129,No issue found,"src: ""https://www.atlassian.com/software/bamboo/downloads/binary/atlassian-bamboo-{{ bamboo_version }}.tar.gz""; job: ""/usr/bin/find {{ bamboo_application_directory }}/logs/ -name *.log -type f -mtime +7 -exec rm  {} \;""; job: ""/usr/bin/find {{ bamboo_home_directory }}/xml-data/build-dir/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \;""; job: ""/usr/bin/find {{ bamboo_home_directory }}/.m2/repository/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \;""",0,1,1,1
130,"command: ""sudo /opt/influxdb/influxd config -config {{influxdb_generated_config}}""
  register: influxdb_merged_config
  tags:
    - influxdb

- name: Write merged config
  copy:
    content: ""{{influxdb_merged_config.stdout}}""
    dest: ""{{influxdb_config_file}}""
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
  tags:
    - influxdb

- name: Ensure directories have correct permissions
  command: ""sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{item}}""
  with_items:
    - ""{{influxdb_meta_dir}}""
    - ""{{influxdb_data_dir}}""
    - ""{{influxdb_hh_dir}}""
    - ""{{influxdb_wal_dir}}""","when: ansible_distribution in ['Ubuntu', 'Debian']
- name: Create data dir
    path: ""{{influxdb_data_dir}}""
    state: directory
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
- name: Create meta dir
    path: ""{{influxdb_meta_dir}}""
- name: Create hh dir
    path: ""{{influxdb_hh_dir}}""
- name: Create config directory
    dest: ""{{influxdb_generated_config}}""
- name: Run config update
  command: ""sudo su -c \""{{influxdb_opt_dir}}/influxd config -config {{influxdb_generated_config}} > {{influxdb_config_file}}\"" {{influxdb_user}}""",130,"command: ""sudo /opt/influxdb/influxd config -config {{influxdb_generated_config}}""; command: ""sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{item}}""","Create meta dir task does not specify the state, owner, or group.; Create hh dir task does not specify the state, owner, or group.; Create config directory task does not specify the state, owner, or group.; Run config update task uses sudo su -c which can lead to privilege escalation.",1,1,1,1
131,"file:
    path: ""{{ degoss_test_dir }}""
    state: directory
  loop: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
    clean: ""{{ degoss_clean | bool }}""
    clean_on_failure: ""{{ degoss_clean_on_failure | bool }}""
    debug: ""{{ degoss_debug | bool }}""","- include: versions/latest.yml
  when: goss_version == ""latest""

- include: versions/pinned.yml
  when: goss_version != ""latest""

- name: establish download url
  set_fact:
    goss_download_url: ""{{ goss_github_repo_url }}/releases/download/v{{ goss_real_version }}/goss-linux-amd64""

- name: create goss directories
  file: path={{ item }} state=directory
  with_items:
    - ""{{ degoss_tmp_root }}""
    - ""{{ degoss_test_root }}""
    - ""{{ degoss_goss_install_dir }}""
  changed_when: degoss_changed_when

- name: install
  get_url:
    url: ""{{ goss_download_url }}""
    dest: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    mode: 0755
  changed_when: degoss_changed_when

- name: link
  file:
    state: link
    src: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    dest: ""{{ degoss_goss_bin }}""
    force: true
  changed_when: degoss_changed_when

- name: deploy test files
  copy: src={{ goss_file }} dest={{ degoss_test_root }}
  with_items: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
  changed_when: degoss_changed_when

- name: run tests
  goss: executable={{ degoss_goss_bin }} path=""{{ goss_file }}"" format=""{{ goss_output_format }}""
  failed_when: false
  register: goss_output

- name: clean
  file: path={{ degoss_tmp_root }} state=absent
  when: degoss_no_clean is undefined and not degoss_no_clean
  changed_when: degoss_changed_when

- name: report errors
  fail: msg=""Goss Tests Failed.""
  when: goss_output.goss_failed
  tags: [format_goss_output]",131,"path: ""{{ degoss_test_dir }}""; loop: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""; clean: ""{{ degoss_clean | bool }}""; clean_on_failure: ""{{ degoss_clean_on_failure | bool }}""; debug: ""{{ degoss_debug | bool }}""","goss_download_url: ""{{ goss_github_repo_url }}/releases/download/v{{ goss_real_version }}/goss-linux-amd64""; url: ""{{ goss_download_url }}""; src={{ goss_file }} dest={{ degoss_test_root }}; executable={{ degoss_goss_bin }} path=""{{ goss_file }}"" format=""{{ goss_output_format }}""",1,1,1,1
132,-signer {{ codesign_cert_filename }} -inkey {{ codesign_key_filename }} -certfile {{ cert_file_filename }} -outform DER \,"---
  - name: Gather list of source files
    command: ls {{ netbootxyz_root }}
    register: source_files

  - name: Create directories for signatures
    file:
      path: ""{{ item }}""
      state: directory
    with_items:
      - ""{{ sigs_dir }}""

  - name: Generate signatures for source files
    shell: |
      openssl cms -sign -binary -noattr -in {{ netbootxyz_root }}/{{ item }} \ 
      -signer {{ codesign_cert_location }} -inkey {{ codesign_key_location }} -certfile {{ cert_file_location }} -outform DER \
      -out {{ sigs_dir }}/{{ item }}.sig
    args:
      chdir: ""{{ cert_dir }}""
      warn: false
    with_items:
      - ""{{ source_files.stdout_lines }}""
    tags:
    - skip_ansible_lint",132,signer {{ codesign_cert_filename }}; -inkey {{ codesign_key_filename }}; -certfile {{ cert_file_filename }},"command: ls {{ netbootxyz_root }}; shell: |
      openssl cms -sign -binary -noattr -in {{ netbootxyz_root }}/{{ item }} \ 
      -signer {{ codesign_cert_location }} -inkey {{ codesign_key_location }} -certfile {{ cert_file_location }} -outform DER \ 
      -out {{ sigs_dir }}/{{ item }}.sig",1,1,1,1
133,"src: ""{{ slurm_build_dir }}/etc/{{ item }}""
    src: ""{{ slurm_build_dir }}/etc/{{ item }}""","---
- name: install build dependencies
  apt:
    name: ""{{ item }}""
  with_items: ""{{ slurm_build_deps }}""
  when: ansible_distribution == 'Ubuntu'

- name: remove slurm packages
  apt:
    name: ""{{ item }}""
    state: absent
  with_items:
    - slurm-wlm
    - slurmctld
    - slurmdbd
    - slurmd
  when: ansible_distribution == 'Ubuntu'

- name: install build dependencies
  yum:
    name: ""{{ item }}""
    state: present
  with_items: ""{{ slurm_build_deps }}""
  when: ansible_os_family == 'RedHat'

- name: remove slurm packages
  yum:
    name: ""{{ item }}""
    state: present
  with_items:
    - slurm
  when: ansible_os_family == 'RedHat'

- name: make build directory
  file:
    path: ""{{ slurm_build_dir }}""
    state: directory

- name: download source
  unarchive:
    src: ""{{ slurm_src_url }}""
    remote_src: yes
    dest: ""{{ slurm_build_dir }}""
    extra_opts:
      - --strip-components=1

- name: uninstall old version
  command: make -j uninstall
  args:
    chdir: ""{{ slurm_build_dir }}""
  ignore_errors: yes
  tags:
    - uninstall

- name: clean src dir
  command: make distclean
  args:
    chdir: ""{{ slurm_build_dir }}""
  ignore_errors: yes

- name: configure
  command: ""./configure --prefix={{ slurm_install_prefix }} --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build
  shell: ""make -j$(nproc) > build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build contrib
  shell: ""make -j$(nproc) contrib >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: install
  shell: ""make -j$(nproc) install >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: install contrib
  shell: ""make -j$(nproc) install-contrib >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build pam_slurm_adopt
  shell: ""make -j$(nproc) >> ../../build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}/contribs/pam_slurm_adopt""

- name: install pam_slurm_adopt
  shell: ""make -j$(nproc) install >> ../../build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}/contribs/pam_slurm_adopt""

- name: copy service files
  copy:
    src: ""{{ item }}""
    dest: ""/etc/systemd/system/{{ item }}""
    remote_src: yes
  args:
    chdir: ""{{ slurm_build_dir }}""
  with_items:
    - slurmctld.service
    - slurmdbd.service
  when: is_controller

- name: copy service files
  copy:
    src: ""{{ item }}""
    dest: ""/etc/systemd/system/{{ item }}""
    remote_src: yes
  args:
    chdir: ""{{ slurm_build_dir }}""
  with_items:
    - slurmd.service
  when: is_compute

- name: restart munge
  service:
    name: munge
    state: restarted

- name: restart slurmd
  service:
    name: slurmd
    state: restarted
    enabled: yes
  when: is_compute

- name: restart slurmdbd
  service:
    name: slurmdbd
    state: restarted
    enabled: yes
  when: is_controller

- name: restart slurmctld
  service:
    name: slurmctld
    state: restarted
    enabled: yes
  when: is_controller",133,"src: ""{{ slurm_build_dir }}/etc/{{ item }}""","name: ""{{ item }}"" in apt and yum modules; src: ""{{ slurm_src_url }}"" in unarchive module; command: ""./configure --prefix={{ slurm_install_prefix }} --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm""; shell: ""make -j$(nproc) > build.log 2>&1""; shell: ""make -j$(nproc) contrib >> build.log 2>&1""; shell: ""make -j$(nproc) install >> build.log 2>&1""; shell: ""make -j$(nproc) install-contrib >> build.log 2>&1""; shell: ""make -j$(nproc) >> ../../build.log 2>&1""; shell: ""make -j$(nproc) install >> ../../build.log 2>&1""",1,1,1,1
134,"- ""{{ ansible_os_family|lower }}.yml""","- name: gather os specific variables
  include_vars: ""{{ item }}""
  with_first_found:
    - files:
      - ""{{ ansible_distribution|lower }}-{{ ansible_distribution_major_version|lower }}.yml""
      - ""{{ ansible_distribution|lower }}.yml""
      paths:
      - ../vars
      skip: true
  tags: vars",134,{{ ansible_os_family|lower }}.yml,"include_vars: ""{{ item }}""; {{ ansible_distribution|lower }}-{{ ansible_distribution_major_version|lower }}.yml; {{ ansible_distribution|lower }}.yml; ../vars",1,0,1,0
135,"command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}'""","- name: add options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Added' in check_installation_options.stdout""
  when: item.1.command == 'add'
  tags: [configuration, wordpress, wordpress-options]

- name: update options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  changed_when: ""'unchanged' not in check_installation_options.stdout""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'update'
  tags: [configuration, wordpress, wordpress-options]

- name: delete options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Could not delete' not in check_installation_options.stderr""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'delete'",135,wp-cli --allow-root; {{ item.0.path }}; {{ item.1.command }}; {{ item.1.name }}; {{ item.1.value }},wp-cli --allow-root; failed_when: False; {{ item.0.path }}; {{ item.1.command }}; {{ item.1.name }}; {{ item.1.value }},1,1,1,1
136,"shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1.name }} --activate""
  when: check_installation_plugins is defined and item.item.1.name and item.rc != 0
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  when: item.1.name

- name: activate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin activate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-activate-plugin]

- name: deactivate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin deactivate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and not item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-deactivate-plugin]","---
- name: identify installation (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  register: check_installation_plugins
  failed_when: False
  changed_when: False
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-is-installed-plugin]

- name: install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1 }} --activate""
  with_items: check_installation_plugins.results
  when: check_installation_plugins is defined and item.item.1 and item.rc != 0
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin]

- name: check install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin-check]",136,"shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""; shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1.name }} --activate""; shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin activate {{ item.1.name }}""; shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin deactivate {{ item.1.name }}""",wp-cli --allow-root; {{ item.0.path }}; {{ item.item.0.path }}; {{ item.item.1 }},1,1,1,1
137,"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",137,No issue found,"springapp_random_source: ""file:///dev/urandom""",0,0,1,0
138,"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_heapsize: ""128m""",138,No issue found,No issue found,0,0,0,0
139,"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",139,No issue found,No issue found,0,0,0,0
140,"user: name=authz-server home={{ authz_server_dir }} shell=/sbin/nologin
- include: install-release.yml
- include: install-snapshot.yml
- include: install-local.yml","---
- name: Create user
  user: name=authz-server home={{ authz_server_dir }}
  tags: authz-server

- name: Create logging directory
  file: path=/var/log/{{ java_app.name }} state=directory owner=authz-server group=authz-server mode=0755
  tags: authz-server

- name: Get the md5 sum of current jar
  stat: path={{ authz_server_dir }}/{{ authz_server_jar }}
  register: current_jar_stat
  tags: authz-server

- name: Create directory where we download application packages (e.g. jar/war files)
  file: path=~/app-downloads state=directory
  tags: authz-server

- name: Deploy | Download release
  get_url:
    url: ""{{ maven_repo }}/org/openconext/authz-server/{{ authz_server_version }}/authz-server-{{ authz_server_version }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp == '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Download snapshot
  get_url:
    url: ""{{ maven_snapshot_repo }}/org/openconext/authz-server/{{ authz_server_version }}-SNAPSHOT/authz-server-{{ authz_server_version }}-{{ authz_server_snapshot_timestamp }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp != '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Upload from local
  copy: src={{ authz_server_local_jar }} dest=~/app-downloads/{{ authz_server_jar }}
  when: authz_server_local_jar != ''
  tags: authz-server

- name: Get the md5 sum of the candidate
  stat: path=~/app-downloads/{{ authz_server_jar }}
  register: candidate_jar_stat
  tags: authz-server

- name: Replace the jar if it has changed
  shell: cp ~/app-downloads/{{ authz_server_jar }} {{ authz_server_dir }}/{{ authz_server_jar }}
  when: current_jar_stat.stat.exists == false or candidate_jar_stat.stat.md5 != current_jar_stat.stat.md5
  notify: restart authz-server
  tags: authz-server

- name: Copy logging config
  template: src=logback.xml.j2 dest={{ authz_server_dir }}/logback.xml owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy application config
  template: src=application.properties.j2 dest={{ authz_server_dir }}/application.properties owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy start script
  template: src=templates/spring-boot.j2 dest=/etc/init.d/{{ java_app.name }} mode=0755
  notify: restart authz-server
  tags: authz-server",140,No issue found,"user: name=authz-server home={{ authz_server_dir }}; file: path=/var/log/{{ java_app.name }} state=directory owner=authz-server group=authz-server mode=0755; get_url: url: ""{{ maven_repo }}/org/openconext/authz-server/{{ authz_server_version }}/authz-server-{{ authz_server_version }}.jar"" dest: ""~/app-downloads/{{ authz_server_jar }}"" force: yes; get_url: url: ""{{ maven_snapshot_repo }}/org/openconext/authz-server/{{ authz_server_version }}-SNAPSHOT/authz-server-{{ authz_server_version }}-{{ authz_server_snapshot_timestamp }}.jar"" dest: ""~/app-downloads/{{ authz_server_jar }}"" force: yes; copy: src={{ authz_server_local_jar }} dest=~/app-downloads/{{ authz_server_jar }}; shell: cp ~/app-downloads/{{ authz_server_jar }} {{ authz_server_dir }}/{{ authz_server_jar }}; template: src=logback.xml.j2 dest={{ authz_server_dir }}/logback.xml owner=authz-server group=authz-server mode=0740; template: src=application.properties.j2 dest={{ authz_server_dir }}/application.properties owner=authz-server group=authz-server mode=0740; template: src=templates/spring-boot.j2 dest=/etc/init.d/{{ java_app.name }} mode=0755",0,0,1,1
141,"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",141,No issue found,"springapp_random_source: ""file:///dev/urandom""",0,0,1,0
142,"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_heapsize: ""128m""",142,No issue found,No issue found,0,0,0,0
143,"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",143,No issue found,No issue found,0,0,0,0
144,service: name=authz-admin state=restarted sleep=45,service: name=authz-admin state=restarted sleep=45`,144,No issue found,No issue found,0,0,0,0
145,name: 'urn:schac:attribute-def:schacPersonalUniqueCode',"janus_service_registry_core:
    admin:
        name: Surfconext
        email: info@surfconext.nl
    auth: default-sp
    useridattr: NameID
    user:
        autocreate: true
    dashboard:
        inbox:
            paginate_by: 20
    push:
        remote:
            test:
                name: ""OpenConext EngineBlock""
                url: ""https://{{ engine_api_janus_user }}:{{ engine_api_janus_password | vault }}@{{ engine_api_domain }}/api/connections""
    entity:
        prettyname: 'name:en'
        useblacklist: false
        usewhitelist: true
        validateEntityId: false
    enable:
        saml20-sp: true
        saml20-idp: true
        shib13-sp: false
        shib13-idp: false
    workflowstates:
        testaccepted:
            name:
                en: Test
                da: Test
                es: 'testaccepted - es'
            description:
                en: 'All test should be performed in this state'
                da: 'I denne tilstand skal al test foretages'
                es: 'Desc 1 es'
            abbr: 'TA'
        prodaccepted:
            name:
                en: Production
                da: Produktion
                es: 'prodaccepted - es'
            description:
                en: 'The connection is on the Production system'
                da: 'Forbindelsen er på Produktions systemet'
                es: 'Desc 5 es'
            textColor: green
            abbr: 'PA'
    workflowstate:
        default: testaccepted
    attributes:
        eduPersonTargetedID:
            name: 'urn:mace:dir:attribute-def:eduPersonTargetedID'
        eduPersonPrincipalName:
            name: 'urn:mace:dir:attribute-def:eduPersonPrincipalName'
        displayName:
            name: 'urn:mace:dir:attribute-def:displayName'
        'cn (common name)':
            name: 'urn:mace:dir:attribute-def:cn'
        givenName:
            name: 'urn:mace:dir:attribute-def:givenName'
        'sn (surname)':
            name: 'urn:mace:dir:attribute-def:sn'
        mail:
            name: 'urn:mace:dir:attribute-def:mail'
        schacHomeOrganization:
            name: 'urn:mace:terena.org:attribute-def:schacHomeOrganization'
        schacHomeOrganizationType:
            name: 'urn:mace:terena.org:attribute-def:schacHomeOrganizationType'
        schacPersonalUniqueCode:
            name: 'urn:mace:terena.org:attribute-def:schacPersonalUniqueCode'
        eduPersonAffiliation:
            name: 'urn:mace:dir:attribute-def:eduPersonAffiliation'
            specify_values: true
        eduPersonScopedAffiliation:
            name: 'urn:mace:dir:attribute-def:eduPersonScopedAffiliation'
            specify_values: true
        eduPersonEntitlement:
            name: 'urn:mace:dir:attribute-def:eduPersonEntitlement'
            specify_values: true
        isMemberOf:
            name: 'urn:mace:dir:attribute-def:isMemberOf'
            specify_values: true
        uid:
            name: 'urn:mace:dir:attribute-def:uid'
        preferredLanguage:
            name: 'urn:mace:dir:attribute-def:preferredLanguage'
        'nlEduPersonOrgUnit (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlEduPersonOrgUnit'
            specify_values: true
        'nlEduPersonStudyBranch (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlEduPersonStudyBranch'
            specify_values: true
        'nlStudielinkNummer (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlStudielinkNummer'
        'nlDigitalAuthorIdentifier (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlDigitalAuthorIdentifier'
        'collabPersonId (deprecated)':
            name: 'urn:oid:1.3.6.1.4.1.1076.20.40.40.1'
    md:
        mapping:
            'UIInfo:Logo:0:height': 'logo:0:height'
            'UIInfo:Logo:0:width': 'logo:0:width'
            'UIInfo:Logo:0:url': 'logo:0:url'
            'UIInfo:Keywords:en': 'keywords:en'
            'UIInfo:Keywords:nl': 'keywords:nl'
            'UIInfo:Description:en': 'description:en'
            'UIInfo:Description:nl': 'description:nl'
    usertypes:
        - admin
        - readonly
        - technical
    messenger:
        external:
            mail:
                class: 'janus:SimpleMail'
                name: Mail
                option:
                    headers: ""MIME-Version: 1.0\r\nContent-type: text/html; charset=iso-8859-1\r\nFrom: JANUS <no-reply@example.org>\r\nReply-To: JANUS Admin <admin@example.org>\r\nX-Mailer: PHP/5.4.6-1ubuntu1.8""
    mdexport:
        default_options:
            entitiesDescriptorName: Federation
            maxCache: 86400
            maxDuration: 432000
            sign:
                enable: false
                privatekey: server.pem
                privatekey_pass: null
                certificate: server.crt
    encryption:
        enable: false
    access:
        createnewentity:
            workflow_states:
                all:
                    - all
                    - '-readonly'
        changeentityid:
            workflow_states:
                all:
                    - admin
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
            default: true
        allentities:
            workflow_states:
                all:
                    - admin
                    - readonly
        exportallentities:
            workflow_states:
                all:
                    - admin
        arpeditor:
            workflow_states:
                all:
                    - admin
        admintab:
            workflow_states:
                all:
                    - admin
        adminusertab:
            workflow_states:
                all:
                    - admin
        federationtab:
            workflow_states:
                all:
                    - ''
        showsubscriptions:
            workflow_states:
                all:
                    - ''
        addsubscriptions:
            workflow_states:
                all:
                    - ''
        editsubscriptions:
            workflow_states:
                all:
                    - ''
        deletesubscriptions:
            workflow_states:
                all:
                    - ''
        experimental:
            workflow_states:
                all:
                    - ''
        changeentitytype:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        exportmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        blockremoteentity:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        changeworkflow:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        changemanipulation:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        changearp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        editarp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        addarp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        addmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        validatemetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        deletemetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        modifymetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        importmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        entityhistory:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        disableconsent:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
    metadata_refresh_cron_tags:
        - daily
    validate_entity_certificate_cron_tags:
        - daily
    validate_entity_endpoints_cron_tags:
        - daily
    ca_bundle_file: /etc/pki/tls/certs/ca-bundle.crt
    metadatafields:
        saml20-idp:
            'name:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'displayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'description:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            certData:
                type: text
                default: ''
                default_allow: false
                required: true
            certData2:
                type: text
                default: ''
                default_allow: false
                required: false
            certData3:
                type: text
                default: ''
                default_allow: false
                required: false
            SingleLogoutService_Binding:
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            SingleLogoutService_Location:
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'contacts:#:contactType':
                type: select
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                select_values:
                    - technical
                    - support
                    - administrative
                    - billing
                    - other
                default: ''
                default_allow: false
            'contacts:#:givenName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:surName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:emailAddress':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                validate: isemail
                type: text
                default: ''
                default_allow: false
            'contacts:#:telephoneNumber':
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationDisplayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationURL:#':
                validate: isurl
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'logo:0:url':
                required: true
                default: 'https://.png'
                default_allow: false
                type: text
            'logo:0:width':
                required: true
                default: '120'
                type: text
                default_allow: true
            'logo:0:height':
                required: true
                default: '60'
                type: text
                default_allow: true
            redirect.sign:
                type: boolean
                required: true
                default: false
                default_allow: true
            'coin:publish_in_edugain':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:publish_in_edugain_date':
                validate: isdatetime
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:additional_logging':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:institution_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'SingleSignOnService:0:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: true
                default_allow: true
            'SingleSignOnService:0:Location':
                required: true
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'SingleSignOnService:1:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            'SingleSignOnService:1:Location':
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'coin:guest_qualifier':
                type: select
                required: true
                select_values:
                    - All
                    - Some
                    - None
                default: All
                default_allow: true
            'coin:schachomeorganization':
                type: text
                default: ''
                default_allow: false
                required: false
            NameIDFormat:
                type: select
                required: false
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default_allow: true
            'keywords:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'coin:disable_scoping':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:hidden':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'shibmd:scope:#:allowed':
                supported:
                    - 0
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                type: text
                default: ''
                default_allow: false
                required: false
            'shibmd:scope:#:regexp':
                type: boolean
                supported:
                    - 0
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                default: ''
                default_allow: false
                required: false
        saml20-sp:
            'name:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'displayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'description:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            certData:
                type: text
                default: ''
                default_allow: false
                required: false
            certData2:
                type: text
                default: ''
                default_allow: false
                required: false
            certData3:
                type: text
                default: ''
                default_allow: false
                required: false
            SingleLogoutService_Binding:
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            SingleLogoutService_Location:
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'contacts:#:contactType':
                type: select
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                select_values:
                    - technical
                    - support
                    - administrative
                    - billing
                    - other
                default: ''
                default_allow: false
            'contacts:#:givenName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:surName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:emailAddress':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                validate: isemail
                type: text
                default: ''
                default_allow: false
            'contacts:#:telephoneNumber':
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationDisplayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationURL:#':
                validate: isurl
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'logo:0:url':
                required: true
                default: 'https://.png'
                default_allow: false
                type: text
            'logo:0:width':
                required: true
                default: '120'
                type: text
                default_allow: true
            'logo:0:height':
                required: true
                default: '60'
                type: text
                default_allow: true
            redirect.sign:
                type: boolean
                required: true
                default: false
                default_allow: true
            'coin:publish_in_edugain':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:publish_in_edugain_date':
                validate: isdatetime
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:additional_logging':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:institution_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:trusted_proxy':
                type: boolean
                default: false
                default_allow: true
                required: false
            'AssertionConsumerService:0:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                required: true
                default_allow: true
            'AssertionConsumerService:#:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                required: false
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                default_allow: true
            'AssertionConsumerService:0:Location':
                required: true
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:#:Location':
                required: false
                validate: isurl
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:0:index':
                required: false
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:#:index':
                required: false
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                type: text
                default: ''
                default_allow: false
            NameIDFormat:
                type: select
                required: true
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default_allow: true
            'NameIDFormats:#':
                supported:
                    - 0
                    - 1
                    - 2
                type: select
                required: false
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                default_allow: true
            'coin:no_consent_required':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:eula':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'url:#':
                required: true
                supported:
                    - en
                    - nl
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'coin:gadgetbaseurl':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:secret':
                validate: lengteq20
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:two_legged_allowed':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consumer_key':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consumer_secret':
                validate: lengteq20
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:key_type':
                type: select
                select_values:
                    - HMAC_SHA1
                    - RSA_PRIVATE
                default: HMAC_SHA1
                default_allow: true
                required: false
            'coin:oauth:app_title':
                default: 'Application Title'
                default_allow: false
                type: text
                required: false
            'coin:oauth:app_description':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:app_thumbnail':
                validate: isurl
                default: 'https://www.surfnet.nl/thumb.png'
                default_allow: false
                type: text
                required: false
            'coin:oauth:app_icon':
                validate: isurl
                default: 'https://www.surfnet.nl/icon.gif'
                default_allow: false
                type: text
                required: false
            'coin:oauth:callback_url':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consent_not_required':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:ss:idp_visible_only':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:application_url':
                default: 'Application URL'
                default_allow: false
                type: text
                required: false
            'coin:implicit_vo_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:transparant_issuer':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:do_not_add_attribute_aliases':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:display_unconnected_idps_wayf':
                type: boolean
                default: false
                default_allow: true
                required: false
    workflow:
        testaccepted:
            prodaccepted:
                role:
                    - admin
        prodaccepted:
            testaccepted:
                role:
                    - admin",145,No issue found,No issue found,0,0,0,0
146,"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",146,No issue found,"springapp_random_source: ""file:///dev/urandom""",0,0,1,0
147,"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_heapsize: ""128m""",147,No issue found,No issue found,0,0,0,0
148,"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",148,No issue found,No issue found,0,0,0,0
149,"- name: copy load engineblock sql
  template: src=files/{{ env_name }}/{{ engine_initial_sql }}.j2 dest=/tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''

- name: run load engineblock sql
  shell: mysql -u {{ engine_database_user }} -p{{ engine_database_password | vault }} -h localhost -D {{ engine_database_name }} < /tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''
  register: mysql_output
  changed_when: False # script should be idempotent

  register: migrate_output
  changed_when: ""'no update needed' not in migrate_output.stderr""","- name: Run EngineBlock migrations
  command: ./bin/migrate
  args:
    chdir: ""{{ engine_release_dir }}""
  changed_when: False # TODO How to check when migrate is up to date?",149,template: src=files/{{ env_name }}/{{ engine_initial_sql }}.j2 dest=/tmp/{{ engine_initial_sql }}; shell: mysql -u {{ engine_database_user }} -p{{ engine_database_password | vault }} -h localhost -D {{ engine_database_name }} < /tmp/{{ engine_initial_sql }},"command: ./bin/migrate; chdir: ""{{ engine_release_dir }}""",1,0,1,1
150,"dest: ""{{ shared_path }}/""                  # Dest on host specified in {{ rsync_to }}
    set_remote_user: false
    ansible_ssh_extra_args: ""-l {{ unicorn_user }} -o ControlMaster=auto -o ControlPersist=60s -o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout=600""","---

- name: rsync asset files with ssh forwarding
  synchronize:
    src: ""{{ shared_path }}/{{ migrate_dir }}""  # Source on target host specified in --limit
    dest: ""{{ shared_path }}/{{ migrate_dir }}"" # Dest on host specified in {{ rsync_to }}
    mode: pull
    rsync_opts:
      - ""--chown={{ unicorn_user }}:{{ unicorn_user }}""
  vars:
    ansible_ssh_extra_args: '-o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout 600'
  loop:
    - assets
    - spree
    - system
  loop_control:
    loop_var: migrate_dir
  delegate_to: ""{{ groups[rsync_to][0] }}""",150,"set_remote_user: false; ansible_ssh_extra_args: ""-o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no""",ansible_ssh_extra_args: '-o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout 600'; --chown={{ unicorn_user }}:{{ unicorn_user }},1,1,1,1
151,"- name: Ignore empty rendered volume templates
  set_fact:
    path: ""{{ item }}""
  register: filtered_volumes
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined and lookup('template', item) | length > 1
  tags: deploy

- name: Update volume templates list for this app
  set_fact:
    volumes: ""{{ filtered_volumes | json_query('results[*].ansible_facts.path') | list }}""
  when: app.volumes is defined
  
  with_items: ""{{ volumes }}""","---

- name: Print app name
  debug: msg=""App name {{ app.name }}""
  tags: route

- name: Make sure application volumes exist
  openshift_raw:
    force: true
    definition: ""{{ lookup('template', item) | from_yaml }}""
    state: present
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined
  tags: volume",151,No issue found,"App name {{ app.name }}; {{ lookup('template', item) | from_yaml }}; {{ app.volumes }}",0,1,1,0
152,when: not registries_vault.stat.exists,"---

- name: Retrieve registries vault file
  stat:
    path: ""group_vars/customer/{{ customer }}/{{ env_type }}/secrets/registries.vault.yml""
  register: registries_vault

- block:
    - name: Check if registries vault exists
      debug:
        msg: ""No registries vault is associated with the project""
    - meta: end_play
  when: registries_vault.stat.exists

- name: Import registries variable
  include_vars:
    file: ""{{ registries_vault.stat.path }}""

- include_tasks: tasks/create_docker_registry_secret.yml
  loop: ""{{ registries }}""
  loop_control:
    loop_var: registry",152,No issue found,"path: ""group_vars/customer/{{ customer }}/{{ env_type }}/secrets/registries.vault.yml""; file: ""{{ registries_vault.stat.path }}""; loop: ""{{ registries }}""",0,0,1,0
153,"line: ""OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""","---
- name: ""Setting Docker Facts""
  set_fact:
    docker_storage_block_device: ""{{ docker_storage_block_device | default(default_docker_storage_block_device) }}""
    docker_storage_volume_group: ""{{ docker_storage_volume_group | default(default_docker_storage_volume_group) }}""

- name: ""Install Docker""
  yum: 
    name: docker
    state: latest
  notify:
    - enable docker

- name: ""Confige Docker""
  lineinfile: 
    dest: /etc/sysconfig/docker 
    regexp: '^OPTIONS=.*$' 
    line: ""^OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""

- name: ""Check for existing Docker Storage device""
  command: pvs
  register: pvs

- name: ""Set Docker Storage fact if already configured""
  set_fact:
    docker_storage_setup: true
  when: pvs.stdout | search('{{ docker_storage_block_device }}.*{{ docker_storage_volume_group }}')

- name: ""Configure Docker Storage Setup""
  template:
    src: docker-storage-setup.j2
    dest: /etc/sysconfig/docker-storage-setup
  when: docker_storage_setup is undefined
  
- name: ""Run Docker Storage Setup""
  command: docker-storage-setup
  when: docker_storage_setup is undefined
  notify:
  - restart docker

- name: ""Extend the Volume Group for Docker Storage""
  command: lvextend -l 90%VG /dev/{{ docker_storage_volume_group }}/docker-pool
  when: docker_storage_setup is undefined
  notify:
  - restart docker",153,--selinux-enabled; --insecure-registry 172.30.0.0/16,"line: ""^OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""; command: docker-storage-setup; command: lvextend -l 90%VG /dev/{{ docker_storage_volume_group }}/docker-pool",1,0,1,1
154,"| intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host_type_master'])) }}""
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host_type_node'])) }}""","vars:
    env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersection(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersection(groups['tag_clusterid_' ~ cluster_id]
                                     | intersection( groups['tag_host_type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersection(groups['tag_clusterid_' ~ cluster_id]
                                   | intersection(groups['tag_host_type_node'])) }}""
    with_items: ""{{ env_cluster_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ master_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ node_hosts }}""",154,No issue found,No issue found,0,1,0,0
155,"env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host-type_node'])) }}""","env_cluster_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                           | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                      | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups.tags.['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                    | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                | intersect(groups.tags.['tag_host-type_node'])) }}""",155,No issue found,No issue found,0,0,0,0
156,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",156,No issue found,"local_action: stat path=""{{ node_custom_config }}/nova/policy.json""; src: ""{{ node_custom_config }}/nova/policy.json""; dest: ""{{ node_config_directory }}/nova/policy.json""",0,0,1,0
157,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",157,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""","local_action: stat path=""{{ node_custom_config }}/heat/policy.json""; src: ""{{ node_custom_config }}/heat/policy.json""; dest: ""{{ node_config_directory }}/heat/policy.json""",1,1,1,0
158,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",158,No issue found,"local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""; src: ""{{ node_custom_config }}/senlin/policy.json""; dest: ""{{ node_config_directory }}/senlin/policy.json""",0,0,1,0
159,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",159,No issue found,"local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""; src: ""{{ node_custom_config }}/cinder/policy.json""; dest: ""{{ node_config_directory }}/cinder/policy.json""",0,0,1,0
160,"- { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - [{ name: tempest, group: tempest }]
    - [{ name: tempest, group: tempest }]","---
- name: Ensuring the containers up
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- include: config.yml

- name: Check the configs
  command: docker exec {{ item.name }} /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- name: Containers config strategy
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_env""
  register: container_envs
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- name: Remove the containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE"" or item[1]['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'
    - item[1]['KOLLA_CONFIG_STRATEGY'] != 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results",160,No issue found,- include: config.yml; command: docker exec {{ item.name }} /usr/local/bin/kolla_set_configs --check; - include: start.yml,0,0,1,1
161,"monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_id }}""","---
- name: Wait for Monasca Grafana to load
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/login""
    status_code: 200
  register: result
  until: result.get('status') == 200
  retries: 10
  delay: 2
  run_once: true

- name: Define Monasca Grafana control plane organisation name
  set_fact:
    monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_name }}""

- name: List Monasca Grafana organisations
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_orgs

- name: Create default control plane organisation if it doesn't exist
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body_format: json
    body:
      name: '{{ monasca_grafana_control_plane_org }}'
    force_basic_auth: true
  when: monasca_grafana_control_plane_org not in monasca_grafana_orgs.json|map(attribute='name')|unique

- name: Lookup Monasca Grafana control plane organisation ID
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/name/{{ monasca_grafana_control_plane_org }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_conf_org

- name: Add {{ monasca_grafana_admin_username }} user to control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/{{ monasca_grafana_conf_org.json.id }}/users'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
    force_basic_auth: true
    body_format: json
    status_code: 200, 409
  register: monasca_grafana_add_user_response
  run_once: True
  changed_when: monasca_grafana_add_user_response.status == 200
  failed_when: monasca_grafana_add_user_response.status not in [200, 409] or
               monasca_grafana_add_user_response.status == 409 and (""User is already"" not in  monasca_grafana_add_user_response.json.message|default(""""))

- name: Switch Monasca Grafana to the control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/user/using/{{ monasca_grafana_conf_org.json.id }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    force_basic_auth: true

- name: Enable Monasca Grafana datasource for control plane organisation
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/datasources""
    method: POST
    user: ""{{ monasca_grafana_admin_username }}""
    password: ""{{ monasca_grafana_admin_password }}""
    body: ""{{ item.value.data | to_json }}""
    body_format: json
    force_basic_auth: true
    status_code: 200, 409
  register: monasca_grafana_datasource_response
  run_once: True
  changed_when: monasca_grafana_datasource_response.status == 200
  failed_when: monasca_grafana_datasource_response.status not in [200, 409] or
               monasca_grafana_datasource_response.status == 409 and (""Data source with same name already exists"" not in  monasca_grafana_datasource_response.json.message|default(""""))
  with_dict: ""{{ monasca_grafana_data_sources }}""
  when: item.value.enabled | bool",161,No issue found,password: '{{ monasca_grafana_admin_password }}'; user: '{{ monasca_grafana_admin_username }}',0,0,1,1
162,"- name: Restart keystone-ssh container
    service_name: ""keystone-ssh""
- name: Restart keystone-fernet container
    service_name: ""keystone-fernet""
- name: Restart keystone container
    service_name: ""keystone""","---
- name: Restart keystone container
  vars:
    service_name: ""keystone""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_conf: ""{{ keystone_confs.results|selectattr('item.key', 'equalto', service_name)|first  }}""
    policy_json: ""{{ keystone_policy_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_conf.changed | bool
      or keystone_domains.changed | bool
      or policy_json.changed | bool
      or keystone_wsgi.changed | bool
      or keystone_paste_ini.changed | bool
      or keystone_container.changed | bool

- name: Restart keystone-fernet container
  vars:
    service_name: ""keystone-fernet""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_conf: ""{{ keystone_confs.results|selectattr('item.key', 'equalto', service_name)|first  }}""
    policy_json: ""{{ keystone_policy_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_fernet_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_conf.changed | bool
      or policy_json.changed | bool
      or keystone_fernet_confs.changed | bool
      or keystone_fernet_container.changed | bool

- name: Restart keystone-ssh container
  vars:
    service_name: ""keystone-ssh""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_ssh_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_ssh_confs.changed | bool
      or keystone_ssh_container.changed | bool",162,No issue found,"service: ""{{ keystone_services[service_name] }}""; config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""; keystone_conf: ""{{ keystone_confs.results|selectattr('item.key', 'equalto', service_name)|first }}""; policy_json: ""{{ keystone_policy_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""; keystone_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""; common_options: ""{{ docker_common_options }}""; name: ""{{ service.container_name }}""; image: ""{{ service.image }}""; volumes: ""{{ service.volumes|reject('equalto', '')|list }}""",0,0,1,1
163,"True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-server'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      ( inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      and not enable_nova_fake | bool
      ) or
      ( inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      and enable_nova_fake | bool
      )
      True if orchestration_engine == 'KUBERNETES' else
      ( inventory_hostname in groups['compute'] or
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      and not enable_nova_fake | bool
      ) or
      ( inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      and enable_nova_fake | bool
      )
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-dhcp-agent'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['neutron-l3-agent']
      or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-lbaas-agent'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['neutron-metadata-agent']
      or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-vpnaas-agent'] }}""","neutron_services:
  openvswitch-db-server:
    container_name: ""openvswitch_db""
    image: ""{{ openvswitch_db_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    volumes:
      - ""{{ node_config_directory }}/openvswitch-db-server/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
      - ""openvswitch_db:/var/lib/openvswitch/""
  openvswitch-vswitchd:
    container_name: ""openvswitch_vswitchd""
    image: ""{{ openvswitch_vswitchd_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    privileged: True
    volumes:
      - ""{{ node_config_directory }}/openvswitch-vswitchd/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-server:
    container_name: ""neutron_server""
    image: ""{{ neutron_server_image_full }}""
    enabled: true
    group: ""neutron-server""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-server'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-server/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""kolla_logs:/var/log/kolla/""
  neutron-openvswitch-agent:
    container_name: ""neutron_openvswitch_agent""
    image: ""{{ neutron_openvswitch_agent_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    privileged: True
    host_in_groups: >-
      {{
        ( inventory_hostname in groups['compute']
          or (enable_manila | bool and inventory_hostname in groups['manila-share'])
          or inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          or inventory_hostname in groups['neutron-vpnaas-agent']
          and not enable_nova_fake | bool
        ) or
        ( inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          or inventory_hostname in groups['neutron-vpnaas-agent']
          and enable_nova_fake | bool
        )
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-openvswitch-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-sfc-agent:
    container_name: ""neutron_sfc_agent""
    image: ""{{ neutron_sfc_agent_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'sfc' }}""
    privileged: True
    host_in_groups: >-
      {{
        ( inventory_hostname in groups['compute']
          or (enable_manila | bool and inventory_hostname in groups['manila-share'])
          or inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          and not enable_nova_fake | bool
        ) or
        ( inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          and enable_nova_fake | bool
        )
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-sfc-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-linuxbridge-agent:
    container_name: ""neutron_linuxbridge_agent""
    image: ""{{  neutron_linuxbridge_agent_image_full }}""
    privileged: True
    enabled: ""{{ neutron_plugin_agent == 'linuxbridge' }}""
    environment:
      KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""
      NEUTRON_BRIDGE: ""br-ex""
      NEUTRON_INTERFACE: ""{{ neutron_external_interface }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-linuxbridge-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-dhcp-agent:
    container_name: ""neutron_dhcp_agent""
    image: ""{{ neutron_dhcp_agent_image_full }}""
    privileged: True
    enabled: True
    group: ""neutron-dhcp-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-dhcp-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-dhcp-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run/:/run/:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-l3-agent:
    container_name: ""neutron_l3_agent""
    image: ""{{ neutron_l3_agent_image_full }}""
    privileged: True
    enabled: ""{{ not enable_neutron_vpnaas | bool }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['neutron-l3-agent']
        or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-l3-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-lbaas-agent:
    container_name: ""neutron_lbaas_agent""
    image: ""{{ neutron_lbaas_agent_image_full }}""
    privileged: True
    enabled: ""{{ enable_neutron_lbaas | bool }}""
    group: ""neutron-lbaas-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-lbaas-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-lbaas-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-metadata-agent:
    container_name: ""neutron_metadata_agent""
    image: ""{{ neutron_metadata_agent_image_full }}""
    privileged: True
    enabled: true
    host_in_groups: >-
      {{
        inventory_hostname in groups['neutron-metadata-agent']
        or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-metadata-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-vpnaas-agent:
    container_name: ""neutron_vpnaas_agent""
    image: ""{{ neutron_vpnaas_agent_image_full }}""
    privileged: True
    enabled: ""{{ enable_neutron_vpnaas | bool }}""
    group: ""neutron-vpnaas-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-vpnaas-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-vpnaas-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""/lib/modules:/lib/modules:ro""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""",163,No issue found,"privileged: True; volumes: - ""{{ node_config_directory }}/openvswitch-vswitchd/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-openvswitch-agent/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-sfc-agent/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-linuxbridge-agent/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-dhcp-agent/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-l3-agent/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-lbaas-agent/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-metadata-agent/:{{ container_config_directory }}/:ro""; volumes: - ""{{ node_config_directory }}/neutron-vpnaas-agent/:{{ container_config_directory }}/:ro""",0,0,1,1
164,"when: inventory_hostname in groups['glance-api']
  when: inventory_hostname in groups['glance-api']","- include: ceph.yml
  when: enable_ceph | bool

  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']",164,Duplicate condition 'when: inventory_hostname in groups['glance-api']' is present,No issue found,1,1,0,0
165,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",165,No issue found,"local_action: stat path=""{{ node_custom_config }}/nova/policy.json""; src: ""{{ node_custom_config }}/nova/policy.json""; dest: ""{{ node_config_directory }}/nova/policy.json""",0,0,1,0
166,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",166,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""","local_action: stat path=""{{ node_custom_config }}/heat/policy.json""; src: ""{{ node_custom_config }}/heat/policy.json""; dest: ""{{ node_config_directory }}/heat/policy.json""",1,1,1,0
167,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",167,No issue found,No issue found,0,0,0,0
168,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",168,No issue found,"local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""; src: ""{{ node_custom_config }}/cinder/policy.json""; dest: ""{{ node_config_directory }}/cinder/policy.json""",0,0,1,0
169,- include: deploy.yml,"- name: Ensuring the containers up
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false

- include: config.yml

- name: Check the configs
  command: docker exec telegraf /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results

- name: Containers config strategy
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_env""
  register: container_envs

- name: Remove the containers
  kolla_docker:
    name: ""telegraf""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE""

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""telegraf""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'",169,No issue found,No issue found,0,0,0,0
170,"name: ""{{ watcher_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",170,"name: ""{{ watcher_database_user }}""","login_password: ""{{ database_password }}""; password: ""{{ wather_database_password }}""; host: ""%""",1,1,1,1
171,"name: ""{{ watcher_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",171,"name: ""{{ watcher_database_user }}""","login_password: ""{{ database_password }}""; password: ""{{ trove_database_password }}""; host: ""%""",1,1,1,1
172,"name: ""{{ watcher_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",172,"name: ""{{ watcher_database_user }}""","login_password: ""{{ database_password }}""; password: ""{{ octavia_database_password }}""; host: ""%""",1,1,1,1
173,"name: ""{{ trove_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",173,"name: ""{{ trove_database_user }}""","login_password: ""{{ database_password }}""; password: ""{{ wather_database_password }}""; host: ""%""",1,1,1,1
174,"name: ""{{ trove_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",174,"name: ""{{ trove_database_user }}""","login_password: ""{{ database_password }}""; password: ""{{ trove_database_password }}""; host: ""%""",1,1,1,1
175,"name: ""{{ trove_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",175,"name: ""{{ trove_database_user }}""","login_password: ""{{ database_password }}""; password: ""{{ octavia_database_password }}""; host: ""%""",1,1,1,1
176,"update_cache: yes
    update_cache: yes","- name: Update yum cache
  yum:
    update_cache: yes
  become: True
  when: ansible_os_family == 'RedHat'


- name: Check which containers are running
  command: docker ps -f 'status=running' -q
  become: true
  failed_when: false
  changed_when: false
  register: running_containers

  register: apt_install_result
  register: yum_install_result


- block:
    - name: Wait for Docker to start
      command: docker info
      become: true
      changed_when: false
      register: result
      until: result is success
      retries: 6
      delay: 10

    - name: Ensure containers are running after Docker upgrade
      command: ""docker start {{ running_containers.stdout }}""
      become: true
  when:
    - install_result is changed
    - running_containers.rc == 0
    - running_containers.stdout != ''
  vars:
    install_result: ""{{ yum_install_result if ansible_os_family == 'RedHat' else apt_install_result }}""
  when:
    - ansible_distribution|lower == ""ubuntu""
    - item != """"
  when:
    - ansible_os_family == 'RedHat'
    - item != """"",176,No issue found,"command: docker ps -f 'status=running' -q; command: docker info; command: ""docker start {{ running_containers.stdout }}""; register: apt_install_result; register: yum_install_result; when: ansible_distribution|lower == ""ubuntu""; when: ansible_os_family == 'RedHat'",0,0,1,1
177,"vars:
    monasca_orgs_body:
      name: '{{ monasca_grafana_control_plane_org }}'
      body: ""{{ monasca_orgs_body | to_json }}""
  vars:
    monasca_user_body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
      body: ""{{ monasca_user_body | to_json }}""","---
- name: Wait for Monasca Grafana to load
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/login""
    status_code: 200
  register: result
  until: result.get('status') == 200
  retries: 10
  delay: 2
  run_once: true

- name: Define Monasca Grafana control plane organisation name
  set_fact:
    monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_name }}""

- name: List Monasca Grafana organisations
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_orgs

- name: Create default control plane organisation if it doesn't exist
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body_format: json
    body:
      name: '{{ monasca_grafana_control_plane_org }}'
    force_basic_auth: true
  when: monasca_grafana_control_plane_org not in monasca_grafana_orgs.json|map(attribute='name')|unique

- name: Lookup Monasca Grafana control plane organisation ID
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/name/{{ monasca_grafana_control_plane_org }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_conf_org

- name: Add {{ monasca_grafana_admin_username }} user to control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/{{ monasca_grafana_conf_org.json.id }}/users'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
    force_basic_auth: true
    body_format: json
    status_code: 200, 409
  register: monasca_grafana_add_user_response
  run_once: True
  changed_when: monasca_grafana_add_user_response.status == 200
  failed_when: monasca_grafana_add_user_response.status not in [200, 409] or
               monasca_grafana_add_user_response.status == 409 and (""User is already"" not in  monasca_grafana_add_user_response.json.message|default(""""))

- name: Switch Monasca Grafana to the control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/user/using/{{ monasca_grafana_conf_org.json.id }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    force_basic_auth: true

- name: Enable Monasca Grafana datasource for control plane organisation
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/datasources""
    method: POST
    user: ""{{ monasca_grafana_admin_username }}""
    password: ""{{ monasca_grafana_admin_password }}""
    body: ""{{ item.value.data | to_json }}""
    body_format: json
    force_basic_auth: true
    status_code: 200, 409
  register: monasca_grafana_datasource_response
  run_once: True
  changed_when: monasca_grafana_datasource_response.status == 200
  failed_when: monasca_grafana_datasource_response.status not in [200, 409] or
               monasca_grafana_datasource_response.status == 409 and (""Data source with same name already exists"" not in  monasca_grafana_datasource_response.json.message|default(""""))
  with_dict: ""{{ monasca_grafana_data_sources }}""
  when: item.value.enabled | bool",177,name: '{{ monasca_grafana_control_plane_org }}'; loginOrEmail: '{{ monasca_grafana_admin_username }}',password: '{{ monasca_grafana_admin_password }}'; url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'; url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/name/{{ monasca_grafana_control_plane_org }}'; url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/{{ monasca_grafana_conf_org.json.id }}/users'; url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/user/using/{{ monasca_grafana_conf_org.json.id }}'; url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/datasources',1,1,1,1
178,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",178,No issue found,"local_action: stat path=""{{ node_custom_config }}/nova/policy.json""; src: ""{{ node_custom_config }}/nova/policy.json""; dest: ""{{ node_config_directory }}/nova/policy.json""",0,0,1,0
179,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",179,No issue found,"local_action: stat path=""{{ node_custom_config }}/heat/policy.json""; src: ""{{ node_custom_config }}/heat/policy.json""; dest: ""{{ node_config_directory }}/heat/policy.json""",0,1,1,0
180,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",180,No issue found,"local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""; src: ""{{ node_custom_config }}/senlin/policy.json""; dest: ""{{ node_config_directory }}/senlin/policy.json""",0,0,1,0
181,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",181,No issue found,"local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""; src: ""{{ node_custom_config }}/cinder/policy.json""; dest: ""{{ node_config_directory }}/cinder/policy.json""",0,1,1,0
182,"name: ""{{ octavia_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",182,No issue found,"login_password: ""{{ database_password }}""; password: ""{{ wather_database_password }}""; host: ""%""",0,0,1,1
183,"name: ""{{ octavia_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",183,No issue found,"login_password: ""{{ database_password }}""; password: ""{{ trove_database_password }}""; host: ""%""",0,0,1,1
184,"name: ""{{ octavia_database_user }}""","kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",184,"name: ""{{ octavia_database_user }}""","login_password: ""{{ database_password }}""; password: ""{{ octavia_database_password }}""; host: ""%""",1,0,1,1
185,nan,"---
- name: Restart rabbitmq container
  vars:
    service_name: ""rabbitmq""
    service: ""{{ rabbitmq_services[service_name] }}""
    config_json: ""{{ rabbitmq_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    rabbitmq_container: ""{{ check_rabbitmq_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes }}""
  when:
    - action != ""config""
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or rabbitmq_confs.changed | bool
      or rabbitmq_container.changed | bool",185,No issue found,No issue found,0,0,0,0
186,"- name: Send SIGHUP to nova services
  become: true
  command: docker exec -t {{ item.value.container_name }} kill -1 1
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - not item.key.startswith('nova-compute')
    - not item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""
- name: Restart nova compute and proxy services
  become: true
  kolla_docker:
    action: restart_container
    common_options: ""{{ docker_common_options }}""
    name: ""{{ item.value.container_name }}""
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - item.key.startswith('nova-compute')
      or item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""","---
- name: Sighup nova-api
  command: docker exec -t nova_api kill -1 1
  when: inventory_hostname in groups['nova-api']

- name: Sighup nova-conductor
  command: docker exec -t nova_conductor kill -1 1
  when: inventory_hostname in groups['nova-conductor']

- name: Sighup nova-consoleauth
  command: docker exec -t nova_consoleauth kill -1 1
  when: inventory_hostname in groups['nova-consoleauth']

- name: Sighup nova-novncproxy
  command: docker exec -t nova_novncproxy kill -1 1
  when:
    - inventory_hostname in groups['nova-novncproxy']
    - nova_console == 'novnc'

- name: Sighup nova-scheduler
  command: docker exec -t nova_scheduler kill -1 1
  when: inventory_hostname in groups['nova-scheduler']

- name: Sighup nova-spicehtml5proxy
  command: docker exec -t nova_spicehtml5proxy kill -1 1
  when:
    - inventory_hostname in groups['nova-spicehtml5proxy']
    - nova_console == 'spice'

- name: Sighup nova-compute
  command: docker exec -t nova_compute kill -1 1
  when: inventory_hostname in groups['compute']",186,command: docker exec -t {{ item.value.container_name }} kill -1 1; kolla_docker: action: restart_container,command: docker exec -t nova_api kill -1 1; command: docker exec -t nova_conductor kill -1 1; command: docker exec -t nova_consoleauth kill -1 1; command: docker exec -t nova_novncproxy kill -1 1; command: docker exec -t nova_scheduler kill -1 1; command: docker exec -t nova_spicehtml5proxy kill -1 1; command: docker exec -t nova_compute kill -1 1,1,1,1,1
187,restart_policy: no,"---
- name: Running trove bootstrap container
  kolla_docker:
    action: ""start_container""
    common_options: ""{{ docker_common_options }}""
    detach: False
    environment:
      KOLLA_BOOTSTRAP:
      KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""
    image: ""{{ trove_api_image_full }}""
    labels:
      BOOTSTRAP:
    name: ""bootstrap_trove""
    restart_policy: ""never""
    volumes:
      - ""{{ node_config_directory }}/trove-api/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""kolla_logs:/var/log/kolla/""
      - ""trove:/var/lib/trove/""
  run_once: True
  delegate_to: ""{{ groups['trove-api'][0] }}""",187,No issue found,"common_options: ""{{ docker_common_options }}""; KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""; image: ""{{ trove_api_image_full }}""; volumes: - ""{{ node_config_directory }}/trove-api/:{{ container_config_directory }}/:ro""; delegate_to: ""{{ groups['trove-api'][0] }}""",0,0,1,0
188,"- ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/aodh.conf""","---
- name: Ensuring config directories exist
  file:
    path: ""{{ node_config_directory }}/{{ item }}""
    state: ""directory""
    recurse: yes
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over config.json files for services
  template:
    src: ""{{ item }}.json.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/config.json""
  with_items:
    - ""aodh-api""
    - ""aodh-listener""
    - ""aodh-evaluator""
    - ""aodh-notifier""

- name: Copying over aodh.conf
  merge_configs:
    vars:
      service_name: ""{{ item }}""
    sources:
      - ""{{ role_path }}/templates/aodh.conf.j2""
      - ""{{ node_custom_config }}/global.conf""
      - ""{{ node_custom_config }}/database.conf""
      - ""{{ node_custom_config }}/messaging.conf""
      - ""{{ node_custom_config }}/aodh.conf""
      - ""{{ node_custom_config }}/aodh/{{ item }}.conf""
      - ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/{{ item }}.conf""
    dest: ""{{ node_config_directory }}/{{ item }}/aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over wsgi-aodh files for services
  template:
    src: ""wsgi-aodh.conf.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/wsgi-aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""",188,{{ node_custom_config }}/aodh/{{ inventory_hostname }}/aodh.conf,"path: ""{{ node_config_directory }}/{{ item }}""; src: ""{{ item }}.json.j2""; dest: ""{{ node_config_directory }}/{{ item }}/config.json""; sources: [""{{ role_path }}/templates/aodh.conf.j2"", ""{{ node_custom_config }}/global.conf"", ""{{ node_custom_config }}/database.conf"", ""{{ node_custom_config }}/messaging.conf"", ""{{ node_custom_config }}/aodh.conf"", ""{{ node_custom_config }}/aodh/{{ item }}.conf"", ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/{{ item }}.conf""]; dest: ""{{ node_config_directory }}/{{ item }}/aodh.conf""; src: ""wsgi-aodh.conf.j2""; dest: ""{{ node_config_directory }}/{{ item }}/wsgi-aodh.conf""",1,1,1,0
189,"cloudkitty_processor_dimensions: ""{{ default_container_dimensions }}""","dimensions: ""{{ cloudkitty_api_dimensions }}""
    dimensions: ""{{ cloudkitty_processor_dimensions }}""
cloudkitty_processor_diensions: ""{{ default_container_dimensions }}""
cloudkitty_api_dimensions: ""{{ default_container_dimensions }}""",189,No issue found,"dimensions: ""{{ cloudkitty_api_dimensions }}""; dimensions: ""{{ cloudkitty_processor_dimensions }}""; cloudkitty_processor_diensions: ""{{ default_container_dimensions }}""",0,0,1,0
190,"command: docker exec -t kolla_toolbox /usr/bin/ansible localhost
  command: docker exec -t kolla_toolbox /usr/bin/ansible localhost","- name: Creating Nova-api database
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_db
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'""
  register: database_api
  changed_when: ""{{ database_api.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""

- name: Reading json from variable
  set_fact:
    database_api_created: ""{{ (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""

- name: Creating Nova-api database user and setting permissions
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_user
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'
        password='{{ nova_api_database_password }}'
        host='%'
        priv='{{ nova_api_database_name }}.*:ALL'
        append_privs='yes'""
  register: database_api_user_create
  changed_when: ""{{ database_api_user_create.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api_user_create.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api_user_create.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""",190,docker exec -t kolla_toolbox /usr/bin/ansible localhost,login_password='{{ database_password }}'; password='{{ nova_api_database_password }}'; host='%'; priv='{{ nova_api_database_name }}.*:ALL',1,1,1,1
191,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",191,No issue found,"local_action: stat path=""{{ node_custom_config }}/nova/policy.json""; src: ""{{ node_custom_config }}/nova/policy.json""; dest: ""{{ node_config_directory }}/nova/policy.json""",0,0,1,0
192,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",192,No issue found,"local_action: stat path=""{{ node_custom_config }}/heat/policy.json""; src: ""{{ node_custom_config }}/heat/policy.json""; dest: ""{{ node_config_directory }}/heat/policy.json""",0,0,1,0
193,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",193,No issue found,"local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""; src: ""{{ node_custom_config }}/senlin/policy.json""; dest: ""{{ node_config_directory }}/senlin/policy.json""",0,0,1,0
194,"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",194,No issue found,"local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""; src: ""{{ node_custom_config }}/cinder/policy.json""; dest: ""{{ node_config_directory }}/cinder/policy.json""",0,1,1,0
195,"- { src: ""os_aggregate_storage.j2"", dest: ""/etc/logrotate.d/{{ rsyslog_client_log_rotate_file }}"" }","---

- name: Stop rsyslog
  service:
    name: ""rsyslog""
    state: ""stopped""
  failed_when: false
  tags:
    - rsyslog-client-config

- name: Rsyslog Setup
  copy:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""
  with_items:
    - { src: ""50-default.conf"", dest: ""/etc/rsyslog.d/50-default.conf"" }
  tags:
    - rsyslog-client-config

- name: Find all log files
  shell: |
    find -L '{{ rsyslog_client_log_dir }}' -type f -name '*.log'
  register: log_files
  when: >
    rsyslog_client_log_dir is defined
  tags:
    - rsyslog-client-config

- name: Set fact for combined log files list and found logs
  set_fact:
    rsyslog_client_log_files: ""{{ log_files.stdout_lines | union(rsyslog_client_log_files) }}""
  when: >
    rsyslog_client_log_files is defined and
    log_files | success
  tags:
    - rsyslog-client-config

- name: Set fact for found log files list
  set_fact:
    rsyslog_client_log_files: ""{{ log_files.stdout_lines }}""
  when: >
    rsyslog_client_log_files is not defined and
    log_files | success
  tags:
    - rsyslog-client-config

- name: Check rsyslog_client_log_files is defined
  fail:
    msg: >
      There were no log files defined in `rsyslog_client_log_files`. Please use that
      variable to set the files that you want rsyslog to begin shipping logs for. This
      variable is a list that requires the full path to all log files. You can also set
      `rsyslog_client_log_dir` which will find all ""*.log"" files using the path
      provided.
  when: >
    rsyslog_client_log_files is not defined

- name: Write rsyslog config for found log files
  template:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""
  with_items:
    - { src: ""99-rsyslog.conf.j2"", dest: ""/etc/rsyslog.d/{{ rsyslog_client_config_name }}"" }
    - { src: ""os_aggregate_storage.j2"", dest: ""/etc/logrotate.d/os_aggregate_storage"" }
    - { src: ""rsyslog.conf.j2"", dest: ""/etc/rsyslog.conf"" }
  tags:
    - rsyslog-client-config

- name: Start rsyslog
  service:
    name: ""rsyslog""
    state: ""started""
  tags:
    - rsyslog-client-config",195,Use of variable in file path; Potential for arbitrary file overwrite,"shell: |
    find -L '{{ rsyslog_client_log_dir }}' -type f -name '*.log'
  register: log_files; copy:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""; template:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""",1,0,1,1
196,"vhost: ""{{ notify_vhost }}""","---


- name: Ensure Notify Rabbitmq vhost
  rabbitmq_vhost:
    name: ""{{ notify_vhost }}""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  tags:
    - common-rabbitmq
  when:
    - oslomsg_notify_transport == ""rabbit""

- name: Ensure Notify Rabbitmq user
  rabbitmq_user:
    user: ""{{ notify_user }}""
    password: ""{{ notify_password }}""
    vhost: ""{{ vhost }}""
    configure_priv: "".*""
    read_priv: "".*""
    write_priv: "".*""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  no_log: true
  tags:
    - common-rabbitmq
  when:
    - oslomsge_notify_transport == ""rabbit""",196,No issue found,"configure_priv: "".*""; read_priv: "".*""; write_priv: "".*""",0,0,1,1
197,until: install_packages is success,"- name: Prepare MQ/DB services
  hosts: nova_conductor
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:

    - name: Configure rabbitmq vhost/user (nova)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure rabbitmq vhost/user (nova/telemetry)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure MySQL user (nova)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-placement)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api cell0)
      include: common-tasks/mysql-db-user.yml
      run_once: yes



- name: Install nova-conductor services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_conductor""
    nova_serial: ""{{ nova_conductor_serial | default(['1', '100%']) }}""



- name: Install nova-scheduler/nova-consoleauth services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_scheduler:nova_consoleauth:!nova_conductor""
    nova_serial: ""{{ nova_scheduler_serial | default(['1', '100%']) }}""



- name: Install nova API services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_os_compute:nova_api_placement:!nova_conductor:!nova_scheduler:!nova_consoleauth""
    nova_serial: ""{{ nova_api_serial | default(['1', '100%']) }}""



- name: Install nova console/metadata services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_metadata:nova_console:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement""
    nova_serial: ""{{ nova_console_serial | default(['1', '100%']) }}""



- name: Install nova compute
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_compute:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement:!nova_api_metadata:!nova_console""
    nova_serial: ""{{ nova_compute_serial | default('20%') }}""



- name: Refresh local facts after all software changes are made
  hosts: nova_all
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: ""!all""

    - name: Gather software version list
      set_fact:
        nova_all_software_versions: ""{{ (groups['nova_all'] | map('extract', hostvars, ['ansible_local', 'openstack_ansible', 'nova', 'venv_tag'])) | list }}""
      delegate_to: localhost
      run_once: yes

    - name: Set software deployed fact
      set_fact:
        nova_all_software_deployed: ""{{ (nova_all_software_versions | select('defined')) | list == nova_all_software_versions }}""
      delegate_to: localhost
      run_once: yes

    - name: Set software updated fact
      set_fact:
        nova_all_software_updated: ""{{ ((nova_all_software_versions | unique) | length == 1) and (nova_all_software_deployed | bool) }}""
      delegate_to: localhost
      run_once: yes


- name: Reload all nova services to ensure new RPC object version is used
  hosts: ""nova_all:!nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_serial | default('100%') }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service reload
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova""
        service_action: ""reloaded""
        service_negate: ""{{ ['nova-placement-api.service'] + nova_service_negate | default([]) }}""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Restart the nova placement API service to ensure new RPC object version is used
  hosts: ""nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_api_serial | default(['1', '100%']) }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service restart
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova-placement-api""
        service_action: ""restarted""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Perform online database migrations
  hosts: nova_conductor
  gather_facts: no
  user: root
  tasks:
    - name: Perform online data migrations
      command: ""{{ nova_bin }}/nova-manage db online_data_migrations""
      become: yes
      become_user: ""{{ nova_system_user_name }}""
      when:
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_online_data_migrations'] | bool""
      changed_when: false
      run_once: yes
      register: data_migrations

    - name: Disable the online migrations requirement
      ini_file:
        dest: ""/etc/ansible/facts.d/openstack_ansible.fact""
        section: nova
        option: need_online_data_migrations
        value: False
      when:
        - data_migrations | succeeded",197,No issue found,"user: root; command: ""{{ nova_bin }}/nova-manage db online_data_migrations""; become: yes; become_user: ""{{ nova_system_user_name }}""",0,1,1,1
198,"domain create {{ stack_user_domain_name }} --description ""Owns users and projects created by heat""","- name: Create heat domain
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              domain create {{ stack_domain }} --description ""Owns users and projects created by heat""
  ignore_errors: true

- name: Create heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              user create --domain {{ stack_user_domain_name }} --password {{ stack_domain_admin_password }} {{ stack_domain_admin }}
  ignore_errors: true

- name: Retrieve heat domain id
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
                    domain show {{ stack_user_domain_name }} | grep -oE -m 1 ""[0-9a-f]{32}""

- name: Assign admin role to heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin",198,"{{ stack_user_domain_name }}; --description ""Owns users and projects created by heat""","openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} domain create {{ stack_domain }} --description ""Owns users and projects created by heat""; openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} user create --domain {{ stack_user_domain_name }} --password {{ stack_domain_admin_password }} {{ stack_domain_admin }}; openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} domain show {{ stack_user_domain_name }} | grep -oE -m 1 ""[0-9a-f]{32}""; openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin",1,0,1,1
199,"gnocchi_git_install_branch: b3b49c87e866475c149343fd77408bef259c5534 # HEAD of ""master"" as of 02.02.2017","gnocchi_git_install_branch: 908ca555f6a14547082e38e4f114926ec384a1bd # HEAD of ""master"" as of 24.01.2017",199,No issue found,No issue found,0,0,0,0
200,- data_migrations  is succeeded,"- name: Prepare MQ/DB services
  hosts: nova_conductor
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:

    - name: Configure rabbitmq vhost/user (nova)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure rabbitmq vhost/user (nova/telemetry)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure MySQL user (nova)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-placement)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api cell0)
      include: common-tasks/mysql-db-user.yml
      run_once: yes



- name: Install nova-conductor services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_conductor""
    nova_serial: ""{{ nova_conductor_serial | default(['1', '100%']) }}""



- name: Install nova-scheduler/nova-consoleauth services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_scheduler:nova_consoleauth:!nova_conductor""
    nova_serial: ""{{ nova_scheduler_serial | default(['1', '100%']) }}""



- name: Install nova API services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_os_compute:nova_api_placement:!nova_conductor:!nova_scheduler:!nova_consoleauth""
    nova_serial: ""{{ nova_api_serial | default(['1', '100%']) }}""



- name: Install nova console/metadata services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_metadata:nova_console:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement""
    nova_serial: ""{{ nova_console_serial | default(['1', '100%']) }}""



- name: Install nova compute
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_compute:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement:!nova_api_metadata:!nova_console""
    nova_serial: ""{{ nova_compute_serial | default('20%') }}""



- name: Refresh local facts after all software changes are made
  hosts: nova_all
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: ""!all""

    - name: Gather software version list
      set_fact:
        nova_all_software_versions: ""{{ (groups['nova_all'] | map('extract', hostvars, ['ansible_local', 'openstack_ansible', 'nova', 'venv_tag'])) | list }}""
      delegate_to: localhost
      run_once: yes

    - name: Set software deployed fact
      set_fact:
        nova_all_software_deployed: ""{{ (nova_all_software_versions | select('defined')) | list == nova_all_software_versions }}""
      delegate_to: localhost
      run_once: yes

    - name: Set software updated fact
      set_fact:
        nova_all_software_updated: ""{{ ((nova_all_software_versions | unique) | length == 1) and (nova_all_software_deployed | bool) }}""
      delegate_to: localhost
      run_once: yes


- name: Reload all nova services to ensure new RPC object version is used
  hosts: ""nova_all:!nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_serial | default('100%') }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service reload
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova""
        service_action: ""reloaded""
        service_negate: ""{{ ['nova-placement-api.service'] + nova_service_negate | default([]) }}""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Restart the nova placement API service to ensure new RPC object version is used
  hosts: ""nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_api_serial | default(['1', '100%']) }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service restart
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova-placement-api""
        service_action: ""restarted""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Perform online database migrations
  hosts: nova_conductor
  gather_facts: no
  user: root
  tasks:
    - name: Perform online data migrations
      command: ""{{ nova_bin }}/nova-manage db online_data_migrations""
      become: yes
      become_user: ""{{ nova_system_user_name }}""
      when:
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_online_data_migrations'] | bool""
      changed_when: false
      run_once: yes
      register: data_migrations

    - name: Disable the online migrations requirement
      ini_file:
        dest: ""/etc/ansible/facts.d/openstack_ansible.fact""
        section: nova
        option: need_online_data_migrations
        value: False
      when:
        - data_migrations | succeeded",200,No issue found,No issue found,0,1,0,0
201,"pip_install_options_fact: ""{{ pip_install_options|default('') }} --constraint /opt/developer-pip-constraints.txt --constraint /opt/requirements/upper-constraints.txt""
  tags:
    - neutron-install
    - neutron-pip-packages

- name: Set pip_install_options_fact when not in developer mode
  set_fact:
    pip_install_options_fact: ""{{ pip_install_options|default('') }}""
  when:
    - not neutron_developer_mode | bool
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""","extra_args: ""{{ pip_install_options|default('') }}""",201,"pip_install_options_fact: ""{{ pip_install_options|default('') }} --constraint /opt/developer-pip-constraints.txt --constraint /opt/requirements/upper-constraints.txt""; extra_args: ""{{ pip_install_options_fact }}""; extra_args: ""{{ pip_install_options_fact }}""; extra_args: ""{{ pip_install_options_fact }}""","extra_args: ""{{ pip_install_options|default('') }}""",1,0,1,0
202,"- name: Create tmpfiles.d entry
    src: ""neutron-systemd-tmpfiles.j2""
    dest: ""/etc/tmpfiles.d/{{ program_name }}.conf""","---

- name: Create neutron TEMP dirs
  file:
    path: ""{{ item.path }}/{{ program_name }}""
    state: directory
    owner: ""{{ system_user }}""
    group: ""{{ system_group }}""
    mode: ""2755""
  with_items:
    - { path: ""/var/run"" }
    - { path: ""/var/lock"" }

- name: Create tempfile.d entry
  template:
    src: ""neutron-systemd-tempfiles.j2""
    dest: ""/etc/tmpfiles.d/neutron.conf""
    mode: ""0644""
    owner: ""root""
    group: ""root""

- name: Place the systemd init script
  template:
    src: ""neutron-systemd-init.j2""
    dest: ""/etc/systemd/system/{{ program_name }}.service""
    mode: ""0644""
    owner: ""root""
    group: ""root""
  register: systemd_init

- name: Reload the systemd daemon
  command: ""systemctl daemon-reload""
  when: systemd_init | changed
  notify:
    - Restart neutron services",202,No issue found,"mode: ""2755""; command: ""systemctl daemon-reload""",0,1,1,0
203,"- { path: ""{{ nova_lock_path }}"" }","---

- name: create the system group
  group:
    name: ""{{ nova_system_group_name }}""
    state: ""present""
    system: ""yes""
  tags:
    - nova-group

- name: Create the nova system user
  user:
    name: ""{{ nova_system_user_name }}""
    group: ""{{ nova_system_group_name }}""
    comment: ""{{ nova_system_comment }}""
    shell: ""{{ nova_system_shell }}""
    system: ""yes""
    createhome: ""yes""
    home: ""{{ nova_system_home_folder }}""
  tags:
    - nova-user

- name: Create nova dir
  file:
    path: ""{{ item.path }}""
    state: directory
    owner: ""{{ item.owner|default(nova_system_user_name) }}""
    group: ""{{ item.group|default(nova_system_group_name) }}""
    mode: ""{{ item.mode|default('0755') }}""
  with_items:
    - { path: ""/etc/nova"" }
    - { path: ""/etc/nova/rootwrap.d"" }
    - { path: ""/etc/sudoers.d"", mode: ""0750"", owner: ""root"", group: ""root"" }
    - { path: ""/var/cache/nova"" }
    - { path: ""{{ nova_system_home_folder }}"" }
    - { path: ""{{ nova_system_home_folder }}/.ssh"", mode: ""0700"" }
    - { path: ""{{ nova_system_home_folder }}/cache/api"" }
    - { path: ""{{ nova_system_home_folder }}/instances"" }
    - { path: ""/var/lock/nova"" }
    - { path: ""/var/run/nova"" }
  tags:
    - nova-dirs

- name: Test for log directory or link
  shell: |
    if [ -h ""/var/log/nova""  ]; then
      chown -h {{ nova_system_user_name }}:{{ nova_system_group_name }} ""/var/log/nova""
      chown -R {{ nova_system_user_name }}:{{ nova_system_group_name }} ""$(readlink /var/log/nova)""
    else
      exit 1
    fi
  register: log_dir
  failed_when: false
  changed_when: log_dir.rc != 0
  tags:
    - nova-dirs
    - nova-logs

- name: Create nova log dir
  file:
    path: ""{{ item.path }}""
    state: directory
    owner: ""{{ item.owner|default(nova_system_user_name) }}""
    group: ""{{ item.group|default(nova_system_group_name) }}""
    mode: ""{{ item.mode|default('0755') }}""
  with_items:
    - { path: ""/var/log/nova"" }
  when: log_dir.rc != 0
  tags:
    - nova-dirs
    - nova-logs

- name: Drop sudoers file
  template:
    src: ""sudoers.j2""
    dest: ""/etc/sudoers.d/{{ nova_system_user_name }}_sudoers""
    mode: ""0440""
    owner: ""root""
    group: ""root""
  tags:
    - sudoers
    - nova-sudoers",203,{{ nova_lock_path }},"mode: ""{{ item.mode|default('0755') }}""; mode: ""0440""; shell: ""{{ nova_system_shell }}""; chown -h {{ nova_system_user_name }}:{{ nova_system_group_name }} ""/var/log/nova""; chown -R {{ nova_system_user_name }}:{{ nova_system_group_name }} ""$(readlink /var/log/nova)""; dest: ""/etc/sudoers.d/{{ nova_system_user_name }}_sudoers""",1,0,1,1
204,"until: _stop  is success
  until: _start  is success","service:
    enabled: yes
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
- name: Stop services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""stopped""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _stop
  until: _stop | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
- name: Copy new policy file into place
  copy:
    src: ""/etc/nova/policy.json-{{ nova_venv_tag }}""
    dest: ""/etc/nova/policy.json""
    owner: ""root""
    group: ""{{ nova_system_group_name }}""
    mode: ""0640""
    remote_src: yes
  listen: ""Restart nova services""
- name: Start services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""started""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _start
  until: _start | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
- name: Wait for the nova-compute service to initialize
  command: ""openstack --os-cloud default compute service list --service nova-compute --format value --column Host""
  register: _compute_host_list
  retries: 10
  delay: 5
  until: ""ansible_nodename in _compute_host_list.stdout_lines""
  when:
    - ""'nova_compute' in group_names""
    - ""nova_discover_hosts_in_cells_interval | int < 1""
  listen: ""Restart nova services""
  service:
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  register: _restart
  until: _restart | success
  when:
    - inventory_hostname in groups['nova_api_placement']",204,No issue found,"src: ""/etc/nova/policy.json-{{ nova_venv_tag }}""; command: ""openstack --os-cloud default compute service list --service nova-compute --format value --column Host""",0,1,1,0
205,-mtime +{{ image_cache_expire_days|int - 1 }} {% endif %}|,"- name: Clean image cache directory
  shell: >-
    find {{ image_cache_dir }} -type f
    {% if not image_cache_dir_cleanup|bool %}
    -mtime +{{ image_cache_expire_days - 1 }} {% endif %}|
    xargs --no-run-if-empty -t rm -rf",205,image_cache_expire_days|int - 1,find {{ image_cache_dir }} -type f; xargs --no-run-if-empty -t rm -rf,1,0,1,1
206,name: 'subnode-{{ item.0 + 1 }}',"---

- when: inventory == 'all'
  block:
    - name: copy get-overcloud-nodes.py to undercloud
      template:
        src: 'get-overcloud-nodes.py.j2'
        dest: '{{ working_dir }}/get-overcloud-nodes.py'
        mode: 0755

    - name: fetch overcloud node names and IPs
      shell: >
          source {{ working_dir }}/stackrc;
          python {{ working_dir }}/get-overcloud-nodes.py
      register: registered_overcloud_nodes

    - name: list the overcloud nodes
      debug: var=registered_overcloud_nodes.stdout

    - name: fetch the undercloud ssh key
      fetch:
        src: '{{ working_dir }}/.ssh/id_rsa'
        dest: '{{ overcloud_key }}'
        flat: yes
        mode: 0400

    - name: add overcloud node to ansible
      with_dict: '{{ registered_overcloud_nodes.stdout | default({}) }}'
      add_host:
        name: '{{ item.key }}'
        groups: ""overcloud,{{ item.key | regex_replace('overcloud-(?:nova)?([a-zA-Z0-9_]+)-[0-9]+$', '\\1') }}""
        ansible_host: '{{ item.key }}'
        ansible_fqdn: '{{ item.value }}'
        ansible_user: ""{{ overcloud_user | default('heat-admin') }}""
        ansible_private_key_file: ""{{ overcloud_key }}""
        ansible_ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'

- when: inventory == 'multinode'
  block:
    - name: Get subnodes
      command: cat /etc/nodepool/sub_nodes_private
      register: nodes

    - name: Add subnode to ansible inventory
      with_indexed_items: '{{ nodes.stdout_lines | default([]) }}'
      add_host:
        name: 'subnode-{{ item.0 + 2 }}'
        groups: ""overcloud""
        ansible_host: '{{ item.1 }}'
        ansible_fqdn: '{{ item.1 }}'
        ansible_user: ""{{ lookup('env','USER') }}""
        ansible_private_key_file: ""/etc/nodepool/id_rsa""

- name: set_fact for undercloud ip
  set_fact: undercloud_ip={{ hostvars['undercloud'].undercloud_ip }}
  when: hostvars['undercloud'] is defined and hostvars['undercloud'].undercloud_ip is defined

- name: Add supplemental node vm to inventory
  add_host:
    name: supplemental
    groups: supplemental
    ansible_host: supplemental
    ansible_fqdn: supplemental
    ansible_user: '{{ supplemental_user }}'
    ansible_private_key_file: '{{ local_working_dir }}/id_rsa_supplemental'
    ansible_ssh_extra_args: '-F ""{{local_working_dir}}/ssh.config.ansible""'
    supplemental_node_ip: ""{{ supplemental_node_ip }}""
  when: supplemental_node_ip is defined

- name: set_fact for supplemental ip
  set_fact: supplemental_node_ip={{ hostvars['supplemental'].supplemental_node_ip }}
  when: hostvars['supplemental'] is defined and hostvars['supplemental'].supplemental_node_ip is defined

- name: Add undercloud vm to inventory
  add_host:
    name: undercloud
    groups: undercloud
    ansible_host: undercloud
    ansible_fqdn: undercloud
    ansible_user: '{{ undercloud_user }}'
    ansible_private_key_file: '{{ undercloud_key }}'
    ansible_ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.local.ansible""'
    undercloud_ip: ""{{ undercloud_ip }}""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is not defined and undercloud_ip is defined

- name: set undercloud ssh proxy command
  set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ hostvars[groups['virthost'][0]].ansible_private_key_file }}
      {{ ssh_user }}@{{ hostvars[groups['virthost'][0]].ansible_host }}
      -W {{ undercloud_ip }}:22""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is defined and undercloud_ip is defined

- name: set undercloud ssh proxy command
  set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ hostvars['localhost'].ansible_user_dir }}/.quickstart/id_rsa_virt_power
      {{ ssh_user }}@{{ hostvars['localhost'].ansible_default_ipv4.address }}
      -W {{ undercloud_ip }}:22""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is not defined and undercloud_ip is defined

- name: set supplemental ssh proxy command
  set_fact: supplemental_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ local_working_dir }}/id_rsa_virt_power
      {{ ssh_user }}@{{ hostvars[groups['virthost'][0]].ansible_host }}
      -W {{ supplemental_node_ip }}:22""
  when: supplemental_node_ip is defined

- name: create inventory from template
  delegate_to: localhost
  template:
    src: 'inventory.j2'
    dest: '{{ local_working_dir }}/hosts'

- name: regenerate ssh config
  delegate_to: localhost
  template:
    src: 'ssh_config.j2'
    dest: '{{ local_working_dir }}/ssh.config.ansible'
    mode: 0644
  when: undercloud_ip is defined

- name: regenerate ssh config for ssh connections from the virthost
  delegate_to: localhost
  template:
    src: 'ssh_config_localhost.j2'
    dest: '{{ local_working_dir }}/ssh.config.local.ansible'
    mode: 0644
  when: undercloud_ip is defined

- name: check for existence of identity key
  delegate_to: localhost
  stat: path=""{{ local_working_dir }}/id_rsa_virt_power""
  when: undercloud_ip is not defined
  register: result_stat_id_rsa_virt_power

- name: set fact used in ssh_config_no_undercloud.j2 to determine if IdentityFile should be included
  set_fact:
    id_rsa_virt_power_exists: true
  when: undercloud_ip is not defined and result_stat_id_rsa_virt_power.stat.exists == True

- name: regenerate ssh config, if no undercloud has been launched.
  delegate_to: localhost
  template:
    src: 'ssh_config_no_undercloud.j2'
    dest: '{{ local_working_dir }}/ssh.config.ansible'
    mode: 0644
  when: undercloud_ip is not defined",206,No issue found,"fetch the undercloud ssh key; set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no; set_fact: supplemental_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no; set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no",0,1,1,1
207,"- teardown-environment
    - teardown-provision","- name:  Teardown undercloud and overcloud vms
  hosts: virthost
  gather_facts: yes
  roles:
    - libvirt/teardown
  tags:
    - teardown-all
    - teardown-virthost
    - teardown-nodes

- name: Tear down environment
  hosts: virthost
  roles:
    - environment/teardown
  tags:
    - teardown-all
    - teardown-virthost

- name: Teardown user setup on virt host
  hosts: virthost
  roles:
    - provision/teardown
  tags:
    - teardown-all",207,No issue found,No issue found,0,0,0,0
208,"- when: networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length > 0","- when: ""{{ networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length is greaterthan 0}}""
  block:

  - name: Install OVS dependencies
    include_role:
      name: 'parts/ovs'

  - name: Create OVS Bridges
    openvswitch_bridge:
      bridge: ""{{ item.bridge }}""
      state: present
    when: item.virtualport_type is defined and item.virtualport_type == ""openvswitch""
    with_items: ""{{ networks }}""
    become: true",208,No issue found,No issue found,0,0,0,0
209,"when: undercloud_install_cli_options == '--use-heat' or containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_t_h_t_undercloud }}""
        dest: ""./hieradata-overrides-t-h-t-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-t-h-t-undercloud.yaml""
  when:
    - not undercloud_install_cli_options
    - not containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_classic_undercloud }}""
        dest: ""./hieradata-overrides-classic-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-classic-undercloud.yaml""

- name: Create undercloud configuration
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""","- name: Create undercloud configuration
  template:
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""
    mode: 0600

- name: Create undercloud hieradata overrides
  template:
    src: ""{{ undercloud_hieradata_override_file }}""
    dest: ""./quickstart-hieradata-overrides.yaml""
    mode: 0600

- name: Create undercloud install script
  template:
    src: ""{{ undercloud_install_script }}""
    dest: ""{{ working_dir }}/undercloud-install.sh""
    mode: 0755",209,"src: ""{{ hieradata_override_file_t_h_t_undercloud }}""; src: ""{{ hieradata_override_file_classic_undercloud }}""; src: ""{{ undercloud_config_file }}""",No issue found,1,0,0,0
210,nodepool_cirros_checksum: md5:d56d54f110654dfd29b0e8ed56e6cda8,"nodepool_cirros_url: http://download.cirros-cloud.net/0.3.6/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_dest: /opt/cache/files/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_checksum: md5:a30f156a83471adf89ebb70dc96d0c82",210,md5:d56d54f110654dfd29b0e8ed56e6cda8,nodepool_cirros_url: http://download.cirros-cloud.net/0.3.6/cirros-0.3.6-x86_64-disk.img; nodepool_cirros_checksum: md5:a30f156a83471adf89ebb70dc96d0c82,1,0,1,1
211,"tempest_init: ""tempest init {{ tempest_dir }}""
     tempestconf: ""/usr/bin/discover-tempest-config""

- name: Create /var/log/containers/tempest
  file:
     path: /var/log/containers/tempest
     state: directory
  become: true

- name: Create /var/lib/tempestdata
  file:
     path: /var/lib/tempestdata
     state: directory
  become: true","tempest_init: ""tempest init {{ tempest_dir }}""
      tempestconf: ""/usr/bin/discover-tempest-config""",211,"tempest_init: ""tempest init {{ tempest_dir }}""; tempestconf: ""/usr/bin/discover-tempest-config""; become: true","tempest_init: ""tempest init {{ tempest_dir }}""; tempestconf: ""/usr/bin/discover-tempest-config""",1,0,1,0
212,"undercloud_key: ""{{ local_working_dir }}/id_rsa_undercloud""","---
local_working_dir: ""{{ lookup('env', 'HOME') }}/.quickstart""
working_dir: /home/stack

release: mitaka

node:
    prefix:
        - ""{{ 1000 |random }}""
        - ""{{ lookup('env', 'USER') }}""
        - ""{{ lookup('env', 'BUILD_NUMBER') }}""
tmp:
    node_prefix: '{{ node.prefix | reject(""none"") | join(""-"") }}-'

os_username: admin
os_password: password
os_tenant_name: admin
os_auth_url: 'http://10.0.1.10:5000/v2.0'
cloud_name: qeos7

stack_name: 'oooq-{{ prefix }}stack'
rc_file: /home/stack/overcloudrc
node_name: 'undercloud'
ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'
undercoud_key: ""{{ local_working_dir }}/id_rsa_undercloud""
node_groups:
    - 'undercloud'
    - 'tester'
templates_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal/templates""
ovb_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal""
heat_template: ""{{ templates_dir }}/quintupleo.yaml""
environment_list:
    - ""{{ templates_dir }}/resource-registry.yaml""
    - ""{{ local_working_dir }}/{{ prefix }}env.yaml""

existing_key_location: '{{ local_working_dir }}'
remove_image_from_host_cloud: false

bmc_flavor: m1.medium
bmc_image: 'bmc-base'
bmc_prefix: '{{ prefix }}bmc'

baremetal_flavor: m1.large
baremetal_image: 'ipxe-boot'
baremetal_prefix: '{{ prefix }}baremetal'

key_name: '{{ prefix }}key'
private_net: '{{ prefix }}private'
node_count: 2
public_net: '{{ prefix }}public'
provision_net: '{{ prefix }}provision'

undercloud_name: '{{ prefix }}undercloud'
undercloud_image: '{{ prefix }}undercloud.qcow2'
undercloud_flavor: m1.xlarge
external_net:  '10.2.1.0/22'

network_isolation_type: multi-nic

setup_undercloud_connectivity_log: ""{{ working_dir }}/setup_undercloud_connectivity.log""

mtu: 1350
mtu_interface:
  - eth1
pvt_nameserver: 8.8.8.8

external_interface: eth2
external_interface_ip: 10.0.0.1
external_interface_netmask: 255.255.255.0

registered_releases:
  - mitaka
  - newton
  - master
  - rhos-9",212,"undercloud_key: ""{{ local_working_dir }}/id_rsa_undercloud""","os_username: admin; os_password: password; os_tenant_name: admin; os_auth_url: 'http://10.0.1.10:5000/v2.0'; ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'; undercoud_key: ""{{ local_working_dir }}/id_rsa_undercloud""; pvt_nameserver: 8.8.8.8; external_interface: eth2; external_interface_ip: 10.0.0.1; external_interface_netmask: 255.255.255.0",1,1,1,1
213,"fail: msg=""Overcloud nova list does not show expected number of {{ node_to_scale }} services""
  when: post_scale_node_count.stdout|int != {{ final_scale_value|int }}","source {{ working_dir }}/stackrc;
    {{ working_dir }}/scale-deployment.sh &> overcloud_deployment_scale_console.log;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    nova list | grep {{ node_to_scale }} | cut -f2- -d':' | wc -l
  fail: msg=Overcloud nova list does not show expected number of {{ node_to_scale }} services
  when: post_scale_node_count.stdout != {{ final_scale_value }}",213,No issue found,source {{ working_dir }}/stackrc;; {{ working_dir }}/scale-deployment.sh &> overcloud_deployment_scale_console.log;; nova list | grep {{ node_to_scale }} | cut -f2- -d':' | wc -l,0,1,1,1
214,"pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8",214,pkg_extras: python*-setuptools haproxy PyYAML,No issue found,1,1,0,0
215,"pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8",215,pkg_extras: python*-setuptools haproxy PyYAML,"command: python -m SimpleHTTPServer 8787; command: python3 -m http.server 8787; http_proxy: ""{{ lookup('env', 'http_proxy') }}""; https_proxy: ""{{ lookup('env', 'https_proxy') }}""; ANSIBLE_LIBRARY: ""../../../../library""; lint: enabled: false",1,0,1,1
216,"pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8",216,No issue found,No issue found,0,1,0,0
217,"pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8",217,pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML,"command: python -m SimpleHTTPServer 8787; command: python3 -m http.server 8787; http_proxy: ""{{ lookup('env', 'http_proxy') }}""; https_proxy: ""{{ lookup('env', 'https_proxy') }}""; ANSIBLE_LIBRARY: ""../../../../library""; lint: enabled: false",1,0,1,1
218,"name: ""postgresql@{{ postgres_version }}-main.service""","- name: enable postgres
  become: yes
  service:
    name: ""postgresql@{{ postgresql_version }}-main.service""
    state: started
    enabled: yes
  tags:
    - postgres-enable",218,No issue found,No issue found,0,0,0,0
219,"when: checkgiinstall.stdout != ""1"" and oracle_install_version_gi == item.version and oracle_sw_copy|bool and oracle_sw_unpack|bool
  when: checkgiinstall.stdout != ""1"" and  oracle_install_version_gi == item.version and not oracle_sw_copy|bool and oracle_sw_unpack|bool","---

- name: Extract files to stage-area (GI)
  unarchive: src={{ oracle_stage }}/{{ item.filename }}  dest={{ oracle_stage }}/{{ item.version }} copy=no
  with_items: ""{{oracle_sw_image_gi}}""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - oragridswunpack
  when: checkgiinstall.stdout != ""1"" and ""{{ oracle_install_version_gi }}"" == ""{{ item.version }}"" and oracle_sw_copy|bool and oracle_sw_unpack|bool

- name: Extract files to stage-area (GI) (from remote location)
  unarchive: src={{ oracle_stage_remote }}/{{ item.filename }}  dest={{ oracle_stage }}/{{ item.version }} copy=no
  with_items: ""{{oracle_sw_image_gi}}""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
  - oragridswunpack
  when: checkgiinstall.stdout != ""1"" and ""{{ oracle_install_version_gi }}"" == ""{{ item.version }}"" and not oracle_sw_copy|bool and oracle_sw_unpack|bool

- name: Install cvuqdisk rpm
  yum: name=""{{ oracle_stage_install }}/{{ oracle_install_version_gi }}/grid/rpm/{{ cvuqdisk_rpm }}"" state=present
  when: configure_cluster
  tags: cvuqdisk
  ignore_errors: true

- name: Setup response file for install (GI)
  template: src=grid-install.rsp.{{ oracle_install_version_gi }}.j2 dest={{ oracle_rsp_stage }}/{{ oracle_grid_responsefile }} owner=""{{ grid_install_user }}"" group={{ oracle_group }} mode=775 backup=yes
  with_items: ""{{asm_diskgroups}}""
  tags:
    - responsefilegi
  when: master_node and checkgiinstall.stdout != ""1"" and item.diskgroup == oracle_asm_init_dg

- name: Install Grid Infrastructure
  shell: ""{{ oracle_stage_install }}/{{ oracle_install_version_gi }}/grid/runInstaller -responseFile {{ oracle_rsp_stage }}/{{ oracle_grid_responsefile }} -waitforcompletion -ignorePrereq -ignoreSysPrereqs -showProgress -silent""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - oragridinstall
  when: master_node and checkgiinstall.stdout != ""1"" #and oracle_sw_unpack
  register: giinstall

- debug: var=giinstall.stdout_lines
  when: master_node and checkgiinstall.stdout != ""1""

- name: Run oraInstroot script after installation
  shell: ""{{ oracle_inventory_loc }}/orainstRoot.sh""
  sudo: yes
  tags:
    - runroot
  when: checkgiinstall.stdout != ""1""

- name: Run root script after installation (Master Node)
  shell: ""{{ oracle_home_gi }}/root.sh""
  sudo: yes
  tags:
    - runroot
  when: master_node and checkgiinstall.stdout != ""1""
  register: rootmaster

- debug: var=rootmaster.stdout_lines
  when: master_node and checkgiinstall.stdout != ""1""

- name: Run root script after installation (Other Nodes)
  shell: ""sleep {{ item.0 * 60 }}; {{ oracle_home_gi }}/root.sh""
  sudo: yes
  with_indexed_items: ""{{groups[hostgroup]}}""
  tags:
    - runroot
  when: not master_node and checkgiinstall.stdout != ""1"" and inventory_hostname == item.1
  register: rootother

- debug: var=rootother.stdout_lines
  when: not master_node and checkgiinstall.stdout != ""1""

- name: Setup response file for configToolAllCommands
  template: src=configtoolallcommands.rsp.{{ oracle_install_version_gi }}.j2 dest={{ oracle_rsp_stage }}/configtoolallcommands.rsp owner=""{{ grid_install_user }}"" group={{ oracle_group }} mode=755 backup=yes
  tags:
    - responsefileconfigtool
  when: master_node and run_configtoolallcommand  and checkgiinstall.stdout != ""1""

- name: Run configToolAllCommands
  shell: ""{{ oracle_home_gi }}/cfgtoollogs/configToolAllCommands RESPONSE_FILE={{ oracle_rsp_stage }}/configtoolallcommands.rsp""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - runconfigtool
  when: master_node and run_configtoolallcommand and checkgiinstall.stdout != ""1""
  ignore_errors: true
  register: configtool",219,No issue found,"sudo: yes; ignore_errors: true; shell: ""{{ oracle_stage_install }}/{{ oracle_install_version_gi }}/grid/runInstaller -responseFile {{ oracle_rsp_stage }}/{{ oracle_grid_responsefile }} -waitforcompletion -ignorePrereq -ignoreSysPrereqs -showProgress -silent""; shell: ""{{ oracle_inventory_loc }}/orainstRoot.sh""; shell: ""{{ oracle_home_gi }}/root.sh""; shell: ""sleep {{ item.0 * 60 }}; {{ oracle_home_gi }}/root.sh""; shell: ""{{ oracle_home_gi }}/cfgtoollogs/configToolAllCommands RESPONSE_FILE={{ oracle_rsp_stage }}/configtoolallcommands.rsp""",0,0,1,1
220,"grants={{ item.1.grants |default(omit)  }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, schema: {{ item.1.schema }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants | default (omit) }}
          object_privs={{ item.1.object_privs |default (omit)}}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, schema: {{ item.1.schema | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""","---
- name: Manage grants (cdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.oracle_db_name }}
          user={{ db_user }}
          password={{ db_password_cdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_databases }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants

- name: Manage grants (pdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.pdb_name }}
          user={{ db_user }}
          password={{ db_password_pdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_pdbs }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0 is defined and item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants",220,No issue found,user={{ db_user }}; password={{ db_password_cdb}}; user={{ db_user }}; password={{ db_password_pdb}},0,0,1,1
221,"ovirt_scheduling_policies_facts_internal_24:
      ovirt_api_facts_internal_25:
      ovirt_clusters_internal_24:
        ovirt_clusters_internal_24:","---
- block:
  - name: Login to oVirt
    ovirt_auth:
      url: ""{{ engine_url }}""
      username: ""{{ engine_user }}""
      password: ""{{ engine_password }}""
      ca_file: ""{{ engine_cafile | default(omit) }}""
      insecure: ""{{ engine_insecure | default(true) }}""
    when: ovirt_auth is undefined
    register: loggedin
    tags:
      - always

  - name: Get hosts
    ovirt_hosts_facts:
      auth: ""{{ ovirt_auth }}""
      pattern: ""cluster={{ cluster_name | mandatory }} update_available=true {{ host_names | map('regex_replace', '(.*)', 'name=\\1') | list | join(' or ') }} {{ host_statuses | map('regex_replace', '(.*)', 'status=\\1') | list | join(' or ') }}""
  
  - name: Check if there are hosts to be updated
    debug:
      msg: ""No hosts to be updated""
    when: ovirt_hosts | length == 0
  
  - block:
    - name: Init failed_host_names and succeed_host_names list
      set_fact:
        failed_host_names: []
        succeed_host_names: []

    - name: Get cluster facts
      ovirt_clusters_facts:
        auth: ""{{ ovirt_auth }}""
        pattern: ""name={{ cluster_name }}""
  
    - name: Get name of the original scheduling policy
      ovirt_scheduling_policies_facts:
        auth: ""{{ ovirt_auth }}""
        id: ""{{ ovirt_clusters[0].scheduling_policy.id }}""
  
    - name: Remember the cluster scheduling policy
      set_fact:
        cluster_scheduling_policy: ""{{ ovirt_scheduling_policies[0].name }}""
  
    - name: Get list of VMs in cluster
      ovirt_vms_facts:
        auth: ""{{ ovirt_auth }}""
        pattern: ""cluster={{ cluster_name }}""
  
    - name: Set in cluster upgrade policy
      ovirt_clusters:
        auth: ""{{ ovirt_auth }}""
        name: ""{{ cluster_name }}""
        scheduling_policy: InClusterUpgrade
  
    - name: Shutdown VMs which can be stopped
      ovirt_vms:
        auth: ""{{ ovirt_auth }}""
        state: stopped
        name: ""{{ item }}""
        force: true
      with_items:
        - ""{{ stopped_vms | default([]) }}""
  
    - include: pinned_vms.yml
  
    - include: upgrade.yml
      with_items:
        - ""{{ ovirt_hosts }}""
      when: ""item.id not in host_ids or stop_pinned_to_host_vms""

    when: ovirt_hosts | length > 0
    always:
      - name: Set original cluster policy
        ovirt_clusters:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ cluster_name }}""
          scheduling_policy: ""{{ cluster_scheduling_policy }}""
  
      - name: Start again stopped VMs
        ovirt_vms:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ item }}""
        with_items:
          - ""{{ stopped_vms | default([]) }}""
  
      - name: Start again pin to host VMs
        ovirt_vms:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ item }}""
        with_items:
          - ""{{ pinned_vms_names | default([]) }}""
        when: ""stop_pinned_to_host_vms""
  
      - name: Print info about host which was updated
        debug:
          msg: ""Following hosts was successfully updated: {{ succeed_host_names }}""
        when: ""succeed_host_names | length > 0""

      - name: Fail the playbook, if some hosts wasn't updated
        fail:
          msg: ""The cluster upgrade failed. Hosts {{ failed_host_names }} wasn't updated.""
        when: ""failed_host_names | length > 0""

  always:
    - name: Logout from oVirt
      ovirt_auth:
        state: absent
        ovirt_auth: ""{{ ovirt_auth }}""
      when: not loggedin.skipped | default(false)
      tags:
        - always",221,No issue found,"insecure: ""{{ engine_insecure | default(true) }}""; state: stopped
name: ""{{ item }}""
force: true; ovirt_auth:
state: absent
ovirt_auth: ""{{ ovirt_auth }}""
when: not loggedin.skipped | default(false)",0,0,1,1
222,"args:
      warn: false
    command: ""engine-setup --accept-defaults --config-append={{ answer_file_path }} {{ offline }}""
    until: health_page is success","---
- block:
  - name: Set answer file path
    set_fact:
      answer_file_path: ""/tmp/answerfile-{{ lookup('pipe', 'date +%Y%m%d%H%M%SZ') }}.txt""


  - name: Use the default answerfile
    template:
      src: answerfile_{{ ovirt_engine_setup_version }}_basic.txt.j2
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is undefined

  - name: Copy custom answer file
    template:
      src: ""{{ ovirt_engine_setup_answer_file_path }}""
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is defined

  - name: Update setup packages
    package:
      name: ""ovirt*setup*""
      state: latest
    when: ovirt_engine_setup_update_setup_packages

  - name: Update all packages
    package:
      name: ""*""
      state: latest
    when: ovirt_engine_setup_update_all_packages

  - name: Set accept defaults parameter if variable is set
    set_fact:
      accept_defaults: ""{{ '--accept-defaults' if ovirt_engine_setup_accept_defaults else '' }}""

  - name: Run engine-setup with answerfile
    command: ""engine-setup --config-append={{ answer_file_path }} {{ accept_defaults }}""
    tags:
      - skip_ansible_lint

  - name: Make sure `ovirt-engine` service is running
    service:
      name: ovirt-engine
      state: started

  - name: Check if Engine health page is up
    uri:
      url: ""http://{{ ansible_fqdn }}/ovirt-engine/services/health""
      status_code: 200
    register: health_page
    retries: 12
    delay: 10
    until: health_page|success

  always:
    - name: Clean temporary files
      file:
        path: ""{{ answer_file_path }}""
        state: 'absent'",222,No issue found,"answer_file_path: ""/tmp/answerfile-{{ lookup('pipe', 'date +%Y%m%d%H%M%SZ') }}.txt""; command: ""engine-setup --config-append={{ answer_file_path }} {{ accept_defaults }}""; url: ""http://{{ ansible_fqdn }}/ovirt-engine/services/health""",0,0,1,1
223,nan,- /opt/appdata/themes/nzbget:/app/nzbget/webui:shared,223,No issue found,/opt/appdata/themes/nzbget:/app/nzbget/webui:shared,0,0,1,0
224,"regexp: nzb_backup_dir = """"","- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""nzb_backup_dir =""
    replace: ""nzb_backup_dir = /nzb""
  when: sabnzbd_ini.stat.exists == False

- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""admin_dir = admin""
    replace: ""admin_dir = /admin""
  when: sabnzbd_ini.stat.exists == False",224,No issue found,No issue found,0,0,0,0
225,nan,"- name: password
  pause:
    prompt: ""Please create a Universal Password (.htaccess / Wordpress & etc)""
  register: pw

- debug: msg=""Using following pw {{pw_input.user_input}}""

- name: Replace password with user input
  replace:
    path: /opt/appdata/plexguide/var.yml
    regexp: defaultpassword
    replace: ""{{pw_input.user_input}}""",225,No issue found,"debug: msg=""Using following pw {{pw_input.user_input}}""; replace: ""{{pw_input.user_input}}""",0,0,1,1
226,"when: updatecheck.stdout == ""18.09.2,""","---
- name: ""Establish Facts""
  set_fact:
    switch=""on""
    updatecheck=""default""

- name: ""Docker Check""
  stat:
    path: ""/usr/bin/docker""
  register: check

- name: ""Docker Version Check - True""
  shell: ""docker --version | awk '{print $3}'""
  register: updatecheck

- debug:
    msg: "" {{ updatecheck }} ""

- name: ""Switch - On""
  set_fact:
    switch=""off""
  when: updatecheck.stdout == ""18.06.0-ce,""

- debug:
    msg: ""Switch - {{ switch }}""

- debug:
    msg: ""UpdateCheck - {{ updatecheck }}""

- name: Install required packages
  apt: ""name={{item}} state=present""
  with_items:
    - apt-transport-https
    - ca-certificates
    - software-properties-common
  when: switch == ""on""

- name: Add official gpg signing key
  apt_key:
    id: 0EBFCD88
    url: https://download.docker.com/linux/ubuntu/gpg
  when: switch == ""on""

- name: ""Stop All Containers""
  shell: ""docker stop $(docker ps -a -q)""
  ignore_errors: yes
  when:
    - check.stat.exists == True
    - switch == ""on""

- name: Official Repo
  apt_repository:
    repo: ""deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} edge""
  register: apt_docker_repositories
  when: switch == ""on""

- name: Update APT packages list
  apt:
    update_cache: yes
  when: apt_docker_repositories.changed and switch == ""on""

- name: Release docker-ce from hold
  dpkg_selections:
    name: docker-ce
    selection: install
  when: switch == ""on""

- name: Install docker-ce
  apt:
    name: docker-ce=18.06.0~ce~3-0~ubuntu
    state: present
    update_cache: yes
    force: yes
  when: switch == ""on""

- name: Put docker-ce into hold
  dpkg_selections:
    name: docker-ce
    selection: hold
  when: switch == ""on""

- name: Uninstall docker-py pip module
  pip:
    name: docker-py
    state: absent
  ignore_errors: yes
  when: switch == ""on""

- name: Install docker pip module
  pip:
    name: docker
    state: latest
  ignore_errors: yes
  when: switch == ""on""

- name: Check docker daemon.json exists
  stat:
    path: /etc/docker/daemon.json
  register: docker_daemon

- name: Stop docker to enable overlay2
  systemd: state=stopped name=docker enabled=yes
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: Import daemon.json
  copy: ""src=daemon.json dest=/etc/docker/daemon.json force=yes mode=0775""
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: Start docker (Please Wait)
  systemd: state=started name=docker enabled=yes
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: ""Wait for 20 seconds before commencing""
  wait_for:
    timeout: 20
  when: switch == ""on""

- name: Check override folder exists
  stat:
    path: /etc/systemd/system/docker.service.d
  register: docker_override

- name: Create override folder
  file: ""path=/etc/systemd/system/docker.service.d state=directory mode=0775""
  when:
    - docker_override.stat.exists == False
    - switch == ""on""
  tags: docker_standard

- name: Import override file
  copy: ""src=override.conf dest=/etc/systemd/system/docker.service.d/override.conf force=yes mode=0775""
  tags: docker_standard
  when: switch == ""on""

- name: create plexguide network
  docker_network:
    name: ""plexguide""
    state: present
  tags: docker_standard
  when: switch == ""on""

- name: ""Start All Containers""
  shell: ""docker start $(docker ps -a -q)""
  ignore_errors: yes
  when:
    - switch == ""on""
    - check.stat.exists == True",226,No issue found,"shell: ""docker --version | awk '{print $3}'""; shell: ""docker stop $(docker ps -a -q)""; shell: ""docker start $(docker ps -a -q)""; apt_key: id: 0EBFCD88 url: https://download.docker.com/linux/ubuntu/gpg; apt_repository: repo: ""deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} edge""; apt: name: docker-ce=18.06.0~ce~3-0~ubuntu state: present update_cache: yes force: yes; pip: name: docker state: latest; copy: ""src=daemon.json dest=/etc/docker/daemon.json force=yes mode=0775""; copy: ""src=override.conf dest=/etc/systemd/system/docker.service.d/override.conf force=yes mode=0775""",0,0,1,1
227,"traefik.frontend.rule: ""Host:heimdall.{{domain.stdout}}""","- name: Register Domain
  shell: ""cat /var/plexguide/server.domain""
  register: domain
  ignore_errors: True
      traefik.frontend.rule: ""Host:heimdall.{domain.stdout}}""",227,domain.stdout,"shell: ""cat /var/plexguide/server.domain""; ignore_errors: True; traefik.frontend.rule: ""Host:heimdall.{domain.stdout}}""",1,1,1,1
228,nan,"---
  - name: Check MOVE Service
    stat:
      path: ""/etc/systemd/system/move.service""
    register: move

  - name: Stop If Move Service Running
    systemd: state=stopped name=move
    when: move.stat.exists
    
  - name: Install Move Service
    template:
      src: move.js2
      dest: /etc/systemd/system/move.service 
      force: yes
    when: move.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=move daemon_reload=yes enabled=no

  - name: Start Move
    systemd: state=started name=move enabled=yes
    when: move.stat.exists
    
  - name: Check RCLONE Service
    stat:
      path: ""/etc/systemd/system/rclone.service""
    register: rclone

  - name: Stop If RClone Service Running
    systemd: state=stopped name=rclone
    when: rclone.stat.exists
    
  - name: Install RCLONE Service
    template:
      src: rclone.js2
      dest: /etc/systemd/system/rclone.service 
      force: yes
    when: rclone.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=rclone daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=rclone enabled=yes
    when: rclone.stat.exists

  - name: Check UNIONFS Service
    stat:
      path: ""/etc/systemd/system/unionfs.service""
    register: unionfs

  - name: Stop If UNIONFS Service Running
    systemd: state=stopped name=unionfs
    when: unionfs.stat.exists
    
  - name: Install UNIONFS Service
    template:
      src: unionfs.js2
      dest: /etc/systemd/system/unionfs.service 
      force: yes
    when: unionfs.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=unionfs daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=unionfs enabled=yes
    when: unionfs.stat.exists",228,No issue found,template: src: move.js2 dest: /etc/systemd/system/move.service force: yes; template: src: rclone.js2 dest: /etc/systemd/system/rclone.service force: yes; template: src: unionfs.js2 dest: /etc/systemd/system/unionfs.service force: yes,0,0,1,0
229,"extport: ""7997""","---
- name: ""Establish Key Variables""
  set_fact:
    intport: ""8000""
    extport: ""7999""
    pgrole: ""{{role_name}}""
    image: ""coderaiser/cloudcmd""

- name: ""Key Variables Recall""
  include_role:
    name: ""pgmstart""
    tasks_from: ""keyvar.yml""

- name: Create Directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - ""/opt/appdata/cloudblitz/""

- name: Check config file exists
  stat:
    path: ""/opt/appdata/cloudblitz/.cloudcmd.json""
  register: cloud_json

- name: Install configblitz.json
  template:
    src: configblitz.json
    dest: /opt/appdata/cloudblitz/.cloudcmd.json
    force: yes
  when: cloud_json.stat.exists == False

- name: ""Set Default Volume - {{pgrole}}""
  set_fact:
    default_volumes:
      - /:/SERVER
      - /opt/appdata/cloudblitz:/root/

- name: ""Establish Key Variables - {{pgrole}}""
  set_fact:
    default_env:
      PUID: 1000
      PGID: 1000

- name: ""Set Default Labels - {{pgrole}}""
  set_fact:
    default_labels:
      traefik.enable: ""true""
      traefik.frontend.redirect.entryPoint: ""https""
      traefik.frontend.rule: ""Host:{{pgrole}}.{{domain.stdout}}""
      traefik.port: ""{{intport}}""

- include_role:
    name: ""pgmstart""",229,No issue found,"file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""; default_volumes:
  - /:/SERVER
  - /opt/appdata/cloudblitz:/root/; default_env:
  PUID: 1000
  PGID: 1000; traefik.frontend.rule: ""Host:{{pgrole}}.{{domain.stdout}}""",0,0,1,0
230,"- ""443:443""","- name: Create nginx-proxy directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - /opt/nginx-proxy

      - ""443:443""""",230,No issue found,"mode=0775; ""443:443""""",0,0,1,0
231,nan,"---

- name: fio mixed randread and sequential write benchmark on tikv_data_dir disk
  shell: ""cd {{ fio_deploy_dir }} && ./fio -ioengine=psync -bs=32k -fdatasync=1 -thread -rw=randrw -percentage_random=100,0 -size={{ benchmark_size }} -filename=fio_randread_write_test.txt -name='fio mixed randread and sequential write test' -iodepth=4 -runtime=60 -numjobs=4 -group_reporting --output-format=json --output=fio_randread_write_test.json""
  register: fio_randread_write

- name: clean fio mixed randread and sequential write benchmark temporary file
  file:
    path: ""{{ fio_deploy_dir }}/fio_randread_write_test.txt""
    state: absent

- name: get fio mixed test randread iops
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --read-iops""
  register: disk_mix_randread_iops
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test write iops
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --write-iops""
  register: disk_mix_write_iops
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test randread latency
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --read-lat""
  register: disk_mix_randread_lat
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test write latency
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --write-lat""
  register: disk_mix_write_lat
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed randread and sequential write summary
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --summary""
  register: disk_mix_randread_write_smmary
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: fio mixed randread and sequential write benchmark command
  debug:
    msg: ""fio mixed randread and sequential write benchmark command: {{ fio_randread_write.cmd }}.""
  run_once: true

- name: fio mixed randread and sequential write benchmark summary
  debug:
    msg: ""fio mixed randread and sequential write benchmark summary: {{ disk_mix_randread_write_smmary.stdout }}.""

- name: Preflight check - Does fio mixed randread and sequential write iops of tikv_data_dir disk meet requirement - randread
  fail:
    msg: 'fio mixed randread and sequential write test: randread iops of  tikv_data_dir disk is too low: {{ disk_mix_randread_iops.stdout }} < {{ min_ssd_mix_randread_iops }}, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_randread_iops.stdout|int < min_ssd_mix_randread_iops|int

- name: Preflight check - Does fio mixed randread and sequential write iops of tikv_data_dir disk meet requirement - sequential write
  fail:
    msg: 'fio mixed randread and sequential write test: sequential write iops of tikv_data_dir disk is too low: {{ disk_mix_write_iops.stdout }} < {{ min_ssd_mix_write_iops }}, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_write_iops.stdout|int < min_ssd_mix_write_iops|int

- name: Preflight check - Does fio mixed randread and sequential write latency of tikv_data_dir disk meet requirement - randread
  fail:
    msg: 'fio mixed randread and sequential write test: randread latency of  tikv_data_dir disk is too low: {{ disk_mix_randread_lat.stdout }} ns > {{ max_ssd_mix_randread_lat }} ns, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_randread_lat.stdout|int > max_ssd_mix_randread_lat|int

- name: Preflight check - Does fio mixed randread and sequential write latency of tikv_data_dir disk meet requirement - sequential write
  fail:
    msg: 'fio mixed randread and sequential write test: sequential write latency of tikv_data_dir disk is too low: {{ disk_mix_write_lat.stdout }} ns > {{ max_ssd_mix_write_lat }} ns, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_write_lat.stdout|int > max_ssd_mix_write_lat|int",231,No issue found,No issue found,0,0,0,0
232,enable-telemetry: true,"dashboard:
  public-path-prefix: ""/dashboard""
  internal-proxy: false
  disable-telemetry: false",232,No issue found,internal-proxy: false; disable-telemetry: false,0,0,1,0
233,"expr: increase(tidb_server_event_total{type=~""server_start|server_hang""}[15m])  > 0
      expr:  increase(tidb_server_event_total{type=~""server_start|server_hang""}[15m])  > 0
  - alert: tidb_tikvclient_backoff_total
    expr: increase( tidb_tikvclient_backoff_total[10m] )  > 10
      expr:  increase( tidb_tikvclient_backoff_total[10m] )  > 10","- alert: TiDB_schema_error
    expr: increase(tidb_session_schema_lease_error_total{type=""outdated""}[15m]) > 0
      level: emergency
      expr:  increase(tidb_session_schema_lease_error_total{type=""outdated""}[15m]) > 0
      description: 'alert: instance: {{ $labels.instance }} values: {{ $value }}'
      summary: TiDB schema error
  - alert: TiDB_tikvclient_region_err_total
    expr: increase( tidb_tikvclient_region_err_total[10m] )  > 6000
      level: emergency
      expr:  increase( tidb_tikvclient_region_err_total[10m] )  > 6000
      summary: TiDB tikvclient_backoff_count error
  - alert: TiDB_domain_load_schema_total
    expr: increase( tidb_domain_load_schema_total{type=""failed""}[10m] )  > 10
      level: emergency
      expr:  increase( tidb_domain_load_schema_total{type=""failed""}[10m] )  > 10
      summary: TiDB domain_load_schema_total error
  - alert: TiDB_monitor_keep_alive
    expr: increase(tidb_monitor_keep_alive_total[10m]) < 100
      expr:  increase(tidb_monitor_keep_alive_total[10m]) < 100
      summary: TiDB monitor_keep_alive error
  - alert: TiDB_server_panic_total
    expr: increase(tidb_server_panic_total[10m]) > 0
      level: critical
      expr:  increase(tidb_server_panic_total[10m]) > 0
      summary: TiDB server panic total
  - alert: TiDB_memery_abnormal
    expr: go_memstats_heap_inuse_bytes{job=""tidb""} > 1e+10
      expr: go_memstats_heap_inuse_bytes{job=""tidb""} > 1e+10
      description: 'alert: values: {{ $value }}'
      summary: TiDB mem heap is over 1GiB
  - alert: TiDB_query_duration
    expr: histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance)) > 1
      level: warning
      expr:  histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance)) > 1
      summary: TiDB query duration 99th percentile is above 1s
  - alert: TiDB_server_event_error
    expr: increase(tidb_server_server_event{type=~""server_start|server_hang""}[15m])  > 0
      level: warning
      expr:  increase(tidb_server_server_event{type=~""server_start|server_hang""}[15m])  > 0
      summary: TiDB server event error
  - alert: TiDB_tikvclient_backoff_count
    expr: increase( tidb_tikvclient_backoff_count[10m] )  > 10
      level: warning
      expr:  increase( tidb_tikvclient_backoff_count[10m] )  > 10
      summary: TiDB tikvclient_backoff_count error",233,No issue found,No issue found,0,0,0,0
234,nan,"pulp_webserver_trusted_root_certificates_path: /usr/local/share/ca-certificates/
pulp_webserver_trusted_root_certificates_update_bin: update-ca-certificates
pulp_webserver_python_cryptography: python3-cryptography",234,No issue found,No issue found,0,0,0,0
235,"- name: Delegate a master control plane node
  block:
    - name: Lookup control node from file
      command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
      changed_when: false
      register: k3s_control_delegate_raw
    - name: Ensure control node is delegated to for obtaining a token
      set_fact:
        k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""
    - name: Ensure the control node address is registered in Ansible
      set_fact:
        k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""","when: hostvars[item].k3s_control_node is defined
        and hostvars[item].k3s_control_node
  when: k3s_controller_count is defined
        and k3s_controller_count | length > 1

- name: Ensure ansible_host is mapped to inventory_hostname
  lineinfile:
    path: /tmp/inventory.txt
    line: >-
      {{ item }}
      @@@
      {{ hostvars[item].ansible_host | default(hostvars[item].ansible_fqdn) }}
      @@@
      C_{{ hostvars[item].k3s_control_node }}
      @@@
      P_{{ hostvars[item].k3s_primary_control_node | default(False) }}
    create: true
  loop: ""{{ play_hosts }}""
  when: hostvars[item].k3s_control_node is defined

- name: Lookup control node from file
  command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
  changed_when: false
  register: k3s_control_delegate_raw

- name: Ensure control node is delegated to for obtaining a token
  set_fact:
    k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""

- name: Ensure the control node address is registered in Ansible
  set_fact:
    k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""
  when: k3s_control_node_address is not defined",235,"command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""; k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""; k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""","path: /tmp/inventory.txt; command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""",1,1,1,0
236,"zone: ""{{ dns_domain }}""
    zone: ""{{ dns_domain }}.""
    record: ""master-0.{{ env_id }}.{{ dns_domain }}.""
    zone: ""{{ dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ dns_domain }}.""","---
- name: Ensure Route53 zone is present
  route53_zone:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}""
    state: present

- name: Update Route53 with OCP master record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""master.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ master_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode

- name: Update Route53 with OCP infra wildcard record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ infra_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode",236,"zone: ""{{ dns_domain }}""; zone: ""{{ dns_domain }}.""; record: ""master-0.{{ env_id }}.{{ dns_domain }}.""; zone: ""{{ dns_domain }}.""; record: ""*.apps.{{ env_id }}.{{ dns_domain }}.""","aws_access_key: ""{{ aws_access_key }}""; aws_secret_key: ""{{ aws_secret_key }}""",1,1,1,0
237,"- role: rhsm
    - configure_rhsm
    - role: config-bonding
    - role: config-vlans
    - role: config-routes
  tags:
    - configure_infra_hosts_networking

    - role: update-host
  tags:
    - update_host
    - role: config-iscsi-client
    - configure_iscsi_client","- hosts: aws-provisioner
  roles:
  - aws/manage-networks",237,No issue found,No issue found,0,0,0,0
238,"body_format: json
    body:
      name: ""{{ atlassian.jira.permission_scheme.name }}""
      description: ""{{ atlassian.jira.permission_scheme.description }}""
      permissions: ""{{ permission_list }}""
  register: permission_scheme_output
    default_permission_scheme_id: ""{{ permission_scheme_output.json.id }}""","---
- name: Create Jira Permission Scheme
  uri:
    url: ""{{ atlassian.jira.url }}/rest/api/2/permissionscheme""
    method: POST
    user: ""{{ atlassian.jira.username }}""
    password: ""{{ atlassian.jira.password }}""
    return_content: yes
    force_basic_auth: yes
    body_format: json
    header:
      - Accept: 'application/json'
      - Content-Type: 'application/json'
    body: ""{{ lookup('template','permissionScheme.json.j2') }}""
    status_code: 201
  register: permissionScheme

- name: Set fact for Permission Scheme ID
  set_fact:
    PermissionScheme: ""{{ permissionScheme.json.id }}""",238,{{ atlassian.jira.permission_scheme.name }}; {{ atlassian.jira.permission_scheme.description }}; {{ permission_list }}; {{ permission_scheme_output.json.id }},"user: ""{{ atlassian.jira.username }}""; password: ""{{ atlassian.jira.password }}""",1,1,1,0
239,"---

    - aws/manage-networks","- hosts: aws-provisioner
  roles:
  - aws/manage-networks",239,No issue found,No issue found,0,0,0,0
240,"- name: ""Reset facts to ensure we start over""
  set_fact:
    inventory_id: """"
    project_id: """"
    job_template_id: """"

  - job_template_id is not defined or job_template_id == """"","- name: ""Get the job template id based on job template name""
    job_template_id: ""{{ item.id }}""
  - item.name|trim == job_template.name|trim
  - ""{{ existing_job_templates_output.rest_output }}""
  block:
  - name: ""Create job template {{ job_template.name }}""
    uri:
      url: ""{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/job_templates/""
      user: ""{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}""
      password: ""{{ ansible_tower.admin_password }}""
      force_basic_auth: yes
      method: POST
      body: ""{{ lookup('template', 'job-template.j2') }}""
      body_format: 'json'
      headers:
        Content-Type: ""application/json""
        Accept: ""application/json""
      validate_certs: no
      status_code: 200,201,400
    register: job_template_creation_output

  - name: ""Get the created job template id""
    set_fact:
      job_template_id: ""{{ job_template_creation_output.json.id }}""
  when:
  - job_template_id is not defined

- name: ""Add credentials to job template: {{ job_template.name }}""
  include_tasks: process-job-template-credentials.yml
  loop: ""{{ job_template.credentials | default([ job_template.credential|trim ]) }}""
  loop_control:
    loop_var: job_template_credential
  when: (job_template.credentials is defined) or (job_template.credential is defined)",240,"- job_template_id is not defined or job_template_id == """"","password: ""{{ ansible_tower.admin_password }}""; validate_certs: no",1,1,1,1
241,- docker_install|default(False),"---

- name: ""Install, configure and enable Docker""
  include: docker.yml
  when: 
  - docker_install|default('no') == ""yes""",241,No issue found,No issue found,0,0,0,0
242,"dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc""
    dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o""
    dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins""
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc && \
          export GOPATH={{ ansible_env.HOME}}/{{ gopath }} && \
          make BUILDTAGS=""seccomp selinux"" && make install
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o && \
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins && \
    regexp: '^#storage_driver = ""overlay""'
    line: ""{{ item }}""
    insertbefore: '^#storage_option = \['
  with_items:
    - 'storage_option = ['
    - '""overlay2.override_kernel_check=true"",'
    - ']'

- name: change runtime (runc) path
  replace:
    regexp: '^runtime =.*$'
    replace: 'runtime = ""/usr/local/sbin/runc""'
    name: /etc/crio/crio.conf
    backup: yes
          Environment=\""KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio/crio.sock --container-runtime-endpoint /var/run/crio/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'","---

- name: Check Golang version
  shell: /usr/local/go/bin/go version
  register: go_version
  failed_when: false
  changed_when: false

- name: install Golang upstream
  unarchive:
    src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""
    dest: ""/usr/local""
    remote_src: True
  when: '""go1.8.3"" not in go_version.stdout'

- name: Set custom Golang path in RHEL/CentOS/Fedora
  lineinfile:
    dest: /root/.bashrc
    line: 'export PATH=/usr/local/go/bin:$PATH'
    insertafter: 'EOF'
    regexp: 'export PATH=\/usr\/local\/go\/bin:\$PATH'
    state: present

- name: link golang
  file: src=/usr/local/go/bin/go dest=/usr/bin/go state=link

- name: Install required packages for cri-o
  yum:
    name: ""{{ item }}""
    state: latest
  with_items:
    - wget
    - git
    - make
    - gcc
    - tar
    - libseccomp-devel
    - golang
    - go-md2man
    - glib2-devel
    - glibc-static
    - container-selinux
    - btrfs-progs-devel
    - device-mapper-devel
    - glibc-devel
    - gpgme-devel
    - libassuan-devel
    - libgpg-error-devel
    - pkgconfig
    - json-glib-devel
    - skopeo-containers



- name: Make directories
  file:
    path: ""{{ item }}""
    state: directory
  with_items:
       - /usr/local/go
       - /etc/systemd/system/kubelet.service.d/
       - /var/lib/etcd
       - /etc/cni/net.d
       - /etc/containers

- name: clone runc
  git:
    repo: https://github.com/opencontainers/runc
    dest: /root/src/github.com/opencontainers/runc

- name: clone CRI-O
  git:
    repo: https://github.com/kubernetes-incubator/cri-o
    dest: /root/src/github.com/kubernetes-incubator/cri-o
    version: kube-1.6.x

- name: clone CNI
  git:
    repo: https://github.com/containernetworking/plugins
    dest: /root/src/github.com/containernetworking/plugins

- name: build runc
  shell: |
          cd /root/src/github.com/opencontainers/runc && \
          export GOPATH=/root && \
          make BUILDTAGS=""seccomp selinux"" && \
          make install

- name: link runc
  file: src=/usr/local/sbin/runc dest=/usr/bin/runc state=link

- name: build cri-o
  shell: |
          cd /root/src/github.com/kubernetes-incubator/cri-o && \
          make install.tools && \
          make && \
          make install && \
          make install.systemd && \
          make install.config

- name: build CNI stuff
  shell: |
          cd /root/src/github.com/containernetworking/plugins && \
          ./build.sh && \
          mkdir -p /opt/cni/bin && \
          cp bin/* /opt/cni/bin/

- name: run CRI-O with systemd cgroup manager
  replace:
    regexp: 'cgroupfs'
    replace: 'systemd'
    name: /etc/crio/crio.conf
    backup: yes

- name: run with overlay2
  replace:
    regexp: 'storage_driver = """"'
    replace: 'storage_driver = ""overlay2""'
    name: /etc/crio/crio.conf
    backup: yes

- name: add overlay2 storage opts on RHEL/CentOS
  lineinfile:
    dest: /etc/crio/crio.conf
    line: '""overlay2.override_kernel_check=1""'
    insertafter: 'storage_option = \['
    regexp: 'overlay2\.override_kernel_check=1'
    state: present
  
- name: enable and start CRI-O
  systemd:
    name: crio
    state: started
    enabled: yes
    daemon_reload: yes

- name: modprobe br_netfilter
  command: ""modprobe br_netfilter""
  ignore_errors: true

- name: tune sysctl
  lineinfile:
    line: ""net/bridge/bridge-nf-call-iptables = 1""
    dest: /etc/sysctl.conf
    insertafter: 'EOF'
    regexp: '\/net\/bridge\/bridge-nf-call-iptables = 1'
    state: present
  ignore_errors: true

- name: reload sysctl
  command: ""sysctl -p""
  ignore_errors: true

- name: systemd dropin for kubeadm
  shell: |
          sh -c 'echo ""[Service]
          Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'

- name: flush iptables
  command: ""iptables -F""",242,"dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc""; dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o""; dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins""; cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc && \ export GOPATH={{ ansible_env.HOME}}/{{ gopath }} && \ make BUILDTAGS=""seccomp selinux"" && make install; cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o && \ cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins && \; regexp: '^#storage_driver = ""overlay""'; replace: 'runtime = ""/usr/local/sbin/runc""'; Environment=""KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio/crio.sock --container-runtime-endpoint /var/run/crio/crio.sock"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'","shell: /usr/local/go/bin/go version; unarchive: src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""; lineinfile: dest: /root/.bashrc; file: src=/usr/local/go/bin/go dest=/usr/bin/go state=link; git: repo: https://github.com/opencontainers/runc; git: repo: https://github.com/kubernetes-incubator/cri-o; git: repo: https://github.com/containernetworking/plugins; shell: cd /root/src/github.com/opencontainers/runc && export GOPATH=/root && make BUILDTAGS=""seccomp selinux"" && make install; file: src=/usr/local/sbin/runc dest=/usr/bin/runc state=link; shell: cd /root/src/github.com/kubernetes-incubator/cri-o && make install.tools && make && make install && make install.systemd && make install.config; shell: cd /root/src/github.com/containernetworking/plugins && ./build.sh && mkdir -p /opt/cni/bin && cp bin/* /opt/cni/bin/; command: ""modprobe br_netfilter""; command: ""sysctl -p""; shell: sh -c 'echo ""[Service] Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"" > /etc/systemd/system/kubelet.service.d/0-crio.conf'; command: ""iptables -F""",1,1,1,1
243,crio_version: v1.11.1,"---

- name: Check Golang version
  shell: /usr/local/go/bin/go version
  register: go_version
  failed_when: false
  changed_when: false

- name: install Golang upstream
  unarchive:
    src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""
    dest: ""/usr/local""
    remote_src: True
  when: '""go1.8.3"" not in go_version.stdout'

- name: Set custom Golang path in RHEL/CentOS/Fedora
  lineinfile:
    dest: /root/.bashrc
    line: 'export PATH=/usr/local/go/bin:$PATH'
    insertafter: 'EOF'
    regexp: 'export PATH=\/usr\/local\/go\/bin:\$PATH'
    state: present

- name: link golang
  file: src=/usr/local/go/bin/go dest=/usr/bin/go state=link

- name: Install required packages for cri-o
  yum:
    name: ""{{ item }}""
    state: latest
  with_items:
    - wget
    - git
    - make
    - gcc
    - tar
    - libseccomp-devel
    - golang
    - go-md2man
    - glib2-devel
    - glibc-static
    - container-selinux
    - btrfs-progs-devel
    - device-mapper-devel
    - glibc-devel
    - gpgme-devel
    - libassuan-devel
    - libgpg-error-devel
    - pkgconfig
    - json-glib-devel
    - skopeo-containers



- name: Make directories
  file:
    path: ""{{ item }}""
    state: directory
  with_items:
       - /usr/local/go
       - /etc/systemd/system/kubelet.service.d/
       - /var/lib/etcd
       - /etc/cni/net.d
       - /etc/containers

- name: clone runc
  git:
    repo: https://github.com/opencontainers/runc
    dest: /root/src/github.com/opencontainers/runc

- name: clone CRI-O
  git:
    repo: https://github.com/kubernetes-incubator/cri-o
    dest: /root/src/github.com/kubernetes-incubator/cri-o
    version: kube-1.6.x

- name: clone CNI
  git:
    repo: https://github.com/containernetworking/plugins
    dest: /root/src/github.com/containernetworking/plugins

- name: build runc
  shell: |
          cd /root/src/github.com/opencontainers/runc && \
          export GOPATH=/root && \
          make BUILDTAGS=""seccomp selinux"" && \
          make install

- name: link runc
  file: src=/usr/local/sbin/runc dest=/usr/bin/runc state=link

- name: build cri-o
  shell: |
          cd /root/src/github.com/kubernetes-incubator/cri-o && \
          make install.tools && \
          make && \
          make install && \
          make install.systemd && \
          make install.config

- name: build CNI stuff
  shell: |
          cd /root/src/github.com/containernetworking/plugins && \
          ./build.sh && \
          mkdir -p /opt/cni/bin && \
          cp bin/* /opt/cni/bin/

- name: run CRI-O with systemd cgroup manager
  replace:
    regexp: 'cgroupfs'
    replace: 'systemd'
    name: /etc/crio/crio.conf
    backup: yes

- name: run with overlay2
  replace:
    regexp: 'storage_driver = """"'
    replace: 'storage_driver = ""overlay2""'
    name: /etc/crio/crio.conf
    backup: yes

- name: add overlay2 storage opts on RHEL/CentOS
  lineinfile:
    dest: /etc/crio/crio.conf
    line: '""overlay2.override_kernel_check=1""'
    insertafter: 'storage_option = \['
    regexp: 'overlay2\.override_kernel_check=1'
    state: present
  
- name: enable and start CRI-O
  systemd:
    name: crio
    state: started
    enabled: yes
    daemon_reload: yes

- name: modprobe br_netfilter
  command: ""modprobe br_netfilter""
  ignore_errors: true

- name: tune sysctl
  lineinfile:
    line: ""net/bridge/bridge-nf-call-iptables = 1""
    dest: /etc/sysctl.conf
    insertafter: 'EOF'
    regexp: '\/net\/bridge\/bridge-nf-call-iptables = 1'
    state: present
  ignore_errors: true

- name: reload sysctl
  command: ""sysctl -p""
  ignore_errors: true

- name: systemd dropin for kubeadm
  shell: |
          sh -c 'echo ""[Service]
          Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'

- name: flush iptables
  command: ""iptables -F""",243,No issue found,"shell: /usr/local/go/bin/go version; unarchive: src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""; shell: cd /root/src/github.com/opencontainers/runc && export GOPATH=/root && make BUILDTAGS=""seccomp selinux"" && make install; shell: cd /root/src/github.com/kubernetes-incubator/cri-o && make install.tools && make && make install && make install.systemd && make install.config; shell: cd /root/src/github.com/containernetworking/plugins && ./build.sh && mkdir -p /opt/cni/bin && cp bin/* /opt/cni/bin/; command: ""modprobe br_netfilter""; command: ""sysctl -p""; shell: sh -c 'echo ""[Service] Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"" > /etc/systemd/system/kubelet.service.d/0-crio.conf'; command: ""iptables -F""",0,0,1,0
244,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",244,No issue found,No issue found,0,0,0,0
245,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",245,No issue found,No issue found,0,0,0,0
246,"fp_ip_address_from: ""{{ opstools_external_ip.stdout }}""","- name: run port forwarding role
  include_role:
      name: network/forward-port
  vars:
      fp_ip_address_from: ""{{ opstools_management_ip.stdout }}""
      fp_destination_port: 8081
  delegate_to: hypervisor
  when:
      - ""'hypervisor' in groups""
      - install.overcloud.opstools_forward|default(True)",246,No issue found,No issue found,0,0,0,0
247,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",247,No issue found,No issue found,0,0,0,0
248,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",248,No issue found,install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16; install.version|default(undercloud_version) |openstack_release > 15,0,0,1,0
249,"- name: get the vlan number where external network should be served
    - name: create new vlan interface in ovs system
    - name: get the IP address for the external network interface
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalInterfaceDefaultRoute | awk -F' ' '{print $2}'""
      register: stat_ip_result
    - name: configure external gateway's IP for this interface
      shell: ""sudo ip addr add {{ stat_ip_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""

    - debug: var={{ iface_ip_result }}


    - name: get cidr of the external network
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalNetCidr | awk -F' ' '{print $2}'""
      register: route_result

    - name: add new static route for external network
      shell: ""sudo ip route add {{ route_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""","---
- set_fact:
      isolation_file: ""network-isolation{{ (installer.network.protocol == 'ipv6') | ternary('-v6','') }}.yaml""

- name: append the network environment template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/network/network-environment.yaml \'

- name: append the network isolation template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/{{ isolation_file }} \'

- block:
    - name: get the IP address on the external network default route
      shell: ""cat {{ template_base }}/network/network-environment.yaml | grep ExternalNetworkVlanID | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(\""'\"",'') }} tag={{ result.stdout | replace(\""'\"",'') }} -- set interface vlan{{ result.stdout | replace(\""'\"",'') }} type=internal;""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""

    - name: get the IP address on the external network default route
      shell: ""cat {{ installer.overcloud.template_base }}/network-environment.yaml | grep ExternalNetCidr | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ip addr add {{ result.stdout | replace(\""'\"",'') }}/{{ (installer.network.protocol == 'ipv6') | ternary('64','24') }} dev {{ ansible_interfaces | first }}""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""
  when: installer.network.backend == 'vlan'",249,"shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalInterfaceDefaultRoute | awk -F' ' '{print $2}'""; shell: ""sudo ip addr add {{ stat_ip_result.stdout | replace(""'"",'') }} dev vlan{{ vlan_result.stdout | replace(""'"",'') }}""; shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalNetCidr | awk -F' ' '{print $2}'""; shell: ""sudo ip route add {{ route_result.stdout | replace(""'"",'') }} dev vlan{{ vlan_result.stdout | replace(""'"",'') }}""","dest: ""~/overcloud_deploy.sh""; shell: ""cat {{ template_base }}/network/network-environment.yaml | grep ExternalNetworkVlanID | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""; shell: ""sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(""'"",'') }} tag={{ result.stdout | replace(""'"",'') }} -- set interface vlan{{ result.stdout | replace(""'"",'') }} type=internal;""; shell: ""cat {{ installer.overcloud.template_base }}/network-environment.yaml | grep ExternalNetCidr | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""; shell: ""sudo ip addr add {{ result.stdout | replace(""'"",'') }}/{{ (installer.network.protocol == 'ipv6') | ternary('64','24') }} dev {{ ansible_interfaces | first }}""",1,1,1,1
250,"command: ""sosreport --batch -e openstack_ironic --batch -p openstack,openstack_undercloud,openstack_controller --all-logs --experimental -k docker.all=on --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""","---
- name: Define sosrep folder
  set_fact:
      tmp_sos_dir: '/var/sosrep'

- name: Make sure that sosreport is installed on the host
  yum:
      name: sos
      state: latest

- name: Cleanup sosreport dir if exists
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent

- name: Create dir for sosreport
  file:
      name: ""{{ tmp_sos_dir }}""
      state: directory
      mode: 0644

- name: Collect sosreport logs
  command: ""sosreport --batch --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""
  register: sosreport_result
  failed_when: ""'sosreport has been generated' not in sosreport_result.stdout""

- name: get name of the generated file
  find:
      paths: ""{{ tmp_sos_dir }}""
      patterns: ""sosreport-{{ inventory_hostname }}-*.tar.gz""
      recurse: no
  register: sosreport_file

- name: fetch sosreport archive
  fetch:
      flat: yes
      src: ""{{ item.path }}""
      dest: ""{{ dest_dir }}/""
      validate_checksum: no
  ignore_errors: true
  with_items: ""{{ sosreport_file.files }}""

- name: Cleanup sosreport dir after run
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent",250,"command: ""sosreport --batch -e openstack_ironic --batch -p openstack,openstack_undercloud,openstack_controller --all-logs --experimental -k docker.all=on --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""",mode: 0644; validate_checksum: no; ignore_errors: true,1,1,1,1
251,nan,"- include_tasks: pre.yml
        delegate_to: ""{{  vbmc_inventory_host }}""

- block:
      - include_tasks: check.yml
        when: action == 'check'
      - include_tasks: cleanup.yml
        when: action == 'cleanup'
      - include_tasks: remove.yml
        when: action == 'remove'
  delegate_to: ""{{  vbmc_inventory_host }}""",251,No issue found,No issue found,0,1,0,0
252,"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""","- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhApi
        - OS::TripleO::Services::AodhEvaluator
        - OS::TripleO::Services::AodhNotifier
        - OS::TripleO::Services::AodhListener
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendSimpleCrypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendDogtag{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendKmip{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendPkcs11Crypto{% endif %}""
        - OS::TripleO::Services::CACerts
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerCollector{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerExpirer{% endif %}""
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::CephMds{% endif %}""
        - OS::TripleO::Services::CephMon
        - OS::TripleO::Services::CephExternal
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::CephMgr{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::CephRbdMirror{% endif %}""
        - OS::TripleO::Services::CephRgw
        - OS::TripleO::Services::CinderApi
        - OS::TripleO::Services::CinderScheduler
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 11 %}OS::TripleO::Services::Core{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - OS::TripleO::Services::Fluentd{% if install.version|default(undercloud_version) |openstack_release < 13 %}Client{% endif %}
        - OS::TripleO::Services::GlanceApi
        - ""{% if install.version|default(undercloud_version) |openstack_release < 11 %}OS::TripleO::Services::GlanceRegistry{% endif %}""
        - OS::TripleO::Services::GnocchiApi
        - OS::TripleO::Services::GnocchiMetricd
        - OS::TripleO::Services::GnocchiStatsd
        - OS::TripleO::Services::HeatApi
        - OS::TripleO::Services::HeatApiCfn
        - ""{% if install.version|default(undercloud_version) |openstack_release < 12 %}OS::TripleO::Services::HeatApiCloudwatch{% endif %}""
        - OS::TripleO::Services::HeatEngine
        - OS::TripleO::Services::Horizon
        - OS::TripleO::Services::IronicApi
        - OS::TripleO::Services::IronicConductor
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::IronicPxe{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - OS::TripleO::Services::Keepalived
        - OS::TripleO::Services::Kernel
        - OS::TripleO::Services::Keystone
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - OS::TripleO::Services::ManilaApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendIsilon{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendUnity{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendVMAX{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendVNX{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::ManilaBackendGeneric{% endif %}""
        - OS::TripleO::Services::ManilaBackendNetapp
        - OS::TripleO::Services::ManilaBackendCephFs
        - OS::TripleO::Services::ManilaScheduler
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::MySQLClient{% endif %}""
        - OS::TripleO::Services::Memcached
        - OS::TripleO::Services::NeutronApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::NeutronBgpVpnApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::NeutronSfcApi{% endif %}""
        - OS::TripleO::Services::NeutronCorePlugin
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::NeutronL2gwApi{% endif %}""
        - OS::TripleO::Services::NovaApi
        - OS::TripleO::Services::NovaConductor
        - OS::TripleO::Services::NovaConsoleauth
        - OS::TripleO::Services::NovaIronic
        - OS::TripleO::Services::NovaMetadata
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::NovaPlacement{% endif %}""
        - OS::TripleO::Services::NovaScheduler
        - OS::TripleO::Services::NovaVncProxy
        - OS::TripleO::Services::Ntp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaDeploymentConfig{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHealthManager{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHousekeeping{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaWorker{% endif %}""
        - OS::TripleO::Services::OpenDaylightApi
        - OS::TripleO::Services::OpenDaylightOvs
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNDBs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNController{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::PankoApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - OS::TripleO::Services::SaharaApi
        - OS::TripleO::Services::SaharaEngine
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - OS::TripleO::Services::SensuClient
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAgent{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAnalyzer{% endif %}""
        - OS::TripleO::Services::SwiftProxy
        - OS::TripleO::Services::SwiftStorage
        - OS::TripleO::Services::SwiftRingBuilder
        - OS::TripleO::Services::Snmp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::TripleoPackages
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""",252,Potential insecure use of the 'openstack_release' filter; Potential insecure use of the 'default' filter; Potential insecure use of the 'install.version' variable; Potential insecure use of the 'undercloud_version' variable,No issue found,1,0,0,0
253,"- name: append the ceph storage flavor name to the base overcloud deploy script
  when:
    - ""not install.storage.external""
    - ""not install.splitstack""
  when:
    - install.storage.config == 'internal'

- name: append the storage ceph template in splitstack deployment
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'
  when:
    - ""install.splitstack""
    - install.version|default(undercloud_version)|openstack_release < 12","- block:
    - name: import ceph facts
      set_fact:
          ceph_facts: ""{{ lookup('file', '{{ inventory_dir }}/{{ hostvars[(groups.ceph|first)].inventory_hostname }}') }}""

    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-scale {{ storage_nodes }} \'
      when: ""templates.storage_add_scale | default(True)""

    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-flavor {% if groups[""ceph""] is defined %}ceph{% else %}baremetal{% endif %} \'
      when: ""templates.storage_add_scale | default(True)""

  when: ""not install.storage.external""
  vars:
      storage_nodes: ""{{ (install.storage.nodes|default(0)) or (groups['ceph']|default([])|length) or 1 }}""

- name: append the storage template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'

- name: prepare ceph storage template
  vars:
      ceph_compt_version: ""
          {%- if install.version|openstack_release < 8 -%}8
          {%- elif install.version|openstack_release >= 10 -%}10
          {%- else -%}{{ install.version|openstack_release }}{%- endif -%}""
  template:
      src: ""storage/ceph.yml.j2""
      dest: ""{{ template_base }}/ceph.yaml""
      mode: 0755

- name: append the storage ceph custom template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/ceph.yaml \'",253,No issue found,"ceph_facts: ""{{ lookup('file', '{{ inventory_dir }}/{{ hostvars[(groups.ceph|first)].inventory_hostname }}') }}""; dest: ""~/overcloud_deploy.sh""; dest: ""{{ template_base }}/ceph.yaml""; mode: 0755",0,1,1,0
254,"state: ""{{ ir_default_pip_versions[item] is defined | ternary('present', 'latest') }}""","- name: Include configuration vars
  include_vars: ""vars/config/{{ test.setup }}.yml""

- name: Clone tempest_conf
  git:
      repo: ""{{ tempest_conf.repo }}""
      version: ""{{ tempest_conf.revision | default(omit) }}""
      dest: ""{{ tempest_conf.dir }}""
  register: tempest_conf_repo

- name: Create tempest conf venv with latest pip, setuptools and pbr
  pip:
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      name: ""{{ item }}""
      state: latest
  with_items:
      - pip
      - setuptools
      - pbr

- name: Install tempest_conf
  pip:
      name: "".""
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      chdir: ""{{ tempest_conf.dir }}""

- name: Init tempest
  shell: |
      source .venv/bin/activate
      tempest init ""~/{{ test.dir }}""
  args:
      chdir: ""{{ tempest_conf.dir }}""
      creates: ""~/{{ test.dir }}/etc""

- name: Set facts for configuration run
  set_fact:
      config_command: ""discover-tempest-config""
      config_dir: ""{{ tempest_conf.dir }}""",254,No issue found,"include_vars: ""vars/config/{{ test.setup }}.yml""; repo: ""{{ tempest_conf.repo }}""; dest: ""{{ tempest_conf.dir }}""; virtualenv: ""{{ tempest_conf.dir }}/.venv""; chdir: ""{{ tempest_conf.dir }}""; shell: |
      source .venv/bin/activate
      tempest init ""~/{{ test.dir }}""; args:
      chdir: ""{{ tempest_conf.dir }}""
      creates: ""~/{{ test.dir }}/etc""; config_dir: ""{{ tempest_conf.dir }}""",0,0,1,1
255,"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhListener
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendSimpleCrypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendDogtag{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendKmip{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendPkcs11Crypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::CephMgr{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CinderBackendVRTSHyperScale{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Clustercheck{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Congress{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ec2Api{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Etcd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::IronicPxe{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::ManilaBackendGeneric{% endif %}""
        - OS::TripleO::Services::ManilaBackendNetapp
        - OS::TripleO::Services::ManilaBackendCephFs
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::NeutronSfcApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaDeploymentConfig{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHealthManager{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHousekeeping{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaWorker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNDBs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNController{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAgent{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAnalyzer{% endif %}""
        - ""{% if roles_sshd %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::SwiftRingBuilder
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Tacker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Vpp{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""","networks:
        - InternalApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""",255,No issue found,No issue found,0,0,0,0
256,"OS::TripleO::Galera::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/galera_internal.yaml""","galera_role:
    name: Galera
        OS::TripleO::Galera::Net::SoftwareConfig: ${deployment_dir}/network/nic-configs/galera_internal.yaml
        OS::TripleO::Galera::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api${ipv6_postfix_underscore}.yaml
    flavor: galera
    host_name_format: 'galera-%index%'",256,No issue found,No issue found,0,0,0,0
257,cp {{ base_image_path }}/{{ base_image }} {{ base_image_path }}/{{ node_image }},"---
- name: create disk(s) from vm base image
  shell: |
      {% for num in range(1, item.value.amount + 1, 1) %}
      {% for disk_name, disk_values in item.value.disks.iteritems() %}
      {% set template_image = '{0}-{1}.qcow2'.format(item.value.name, disk_name) %}
      {% set node_image = '{0}-{1}-{2}.qcow2'.format(item.value.name, num - 1, disk_name) %}
      {% if not disk_values.import_url %}

      cp {{ base_image_path }}/{{ template_image }} {{ base_image_path }}/{{ node_image }}
      qemu-img create -f qcow2 -o preallocation={{ disk_values.preallocation }} {{ disk_values.path  }}/{{ node_image }} {{ disk_values.size }}
      {% if disk_name == 'disk1' -%}
      virt-resize --expand /dev/sda1 {{ base_image_path }}/{{ base_image }} {{ disk_values.path }}/{{ node_image }}
      virt-customize -a {{ disk_values.path }}/{{ node_image }} \
      {%- for index in range(item.value.interfaces | length - 1) %}
          --run-command 'cp /etc/sysconfig/network-scripts/ifcfg-eth{0,{{ index + 1 }}} && sed -i s/DEVICE=.*/DEVICE=eth{{ index + 1 }}/g /etc/sysconfig/network-scripts/ifcfg-eth{{ index +1 }}' {% if not loop.last %}\
          {% endif %}
      {% endfor %}
      {% endif %}
      {% endif %}
      {% endfor %}
      {% endfor %}
  with_dict: ""{{ provisioner.topology.nodes }}""
  register: ""vm_disks""
  async: 7200
  poll: 0

- name: Wait for our disks to be created
  async_status:
      jid: ""{{ item.ansible_job_id }}""
  register: disk_tasks
  until: disk_tasks.finished
  retries: 300
  with_items: ""{{ vm_disks.results }}""",257,cp {{ base_image_path }}/{{ base_image }} {{ base_image_path }}/{{ node_image }},shell module; base_image_path variable; disk_values.path variable; disk_values.preallocation variable; disk_values.size variable; provisioner.topology.nodes variable; async and poll parameters; retries parameter,1,1,1,1
258,"- {role: ""installer/ospd/loadbalancer/"", when: 'loadbalancer' in groups}","- {role: ""installer/ospd/loadbalancer/"", when: provisioner.topology.nodes.loadbalancer is defined }",258,No issue found,No issue found,0,0,0,0
259,"stdout: ""{{ (network_template|selectattr('name_lower','equalto','external')|first).ip_subnet }}""","when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the external vlan id from the network_environment_file
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      vlan_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'equalto', 'external')|first).vlan }}""
  when: use_network_data|bool
  when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the cidr of the external network from network_data
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      route_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'external')|first).ip_subnet }}""
  when: use_network_data|bool
  when: ansible_os_family == ""RedHat""",259,No issue found,"command: ""cat {{ template_base }}/network/network_data.yaml""; when: use_network_data|bool
  when: not use_network_data|bool; when: use_network_data|bool
  when: ansible_os_family == ""RedHat""",0,0,1,0
260,"when: ""'virthost' in groups""","when: ""groups.virthost is defined""",260,No issue found,No issue found,0,0,0,0
261,"OS::TripleO::Telemetry::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/${nics_subfolder}/messaging_internal.yaml""
        OS::TripleO::Telemetry::Ports::InternalApiPort: ""{{ install.heat.templates.basedir }}/network/ports/internal_api.yaml""","telemetry_role:
    name: Telemetry

    resource_registry:
        OS::TripleO::Messaging::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/messaging_internal.yaml""
        OS::TripleO::Messaging::Ports::InternalApiPort: ""{{ install.heat.templates.basedir }}/network/ports/internal_api.yaml""
    flavor: telemetry
    networks:
        - InternalApi
    host_name_format: 'telemetry-%index%'

    services:
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhApi
        - OS::TripleO::Services::AodhEvaluator
        - OS::TripleO::Services::AodhListener
        - OS::TripleO::Services::AodhNotifier
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::AuditD{% endif %}""
        - OS::TripleO::Services::CACerts
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerCollector{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerExpirer{% endif %}""
        - OS::TripleO::Services::CephClient
        - OS::TripleO::Services::CephExternal
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - OS::TripleO::Services::Collectd
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - OS::TripleO::Services::Fluentd{% if install.version|default(undercloud_version) |openstack_release < 13 %}Client{% endif %}
        - OS::TripleO::Services::GnocchiApi
        - OS::TripleO::Services::GnocchiMetricd
        - OS::TripleO::Services::GnocchiStatsd
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - OS::TripleO::Services::Kernel
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::MongoDb{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::MySQLClient{% endif %}""
        - OS::TripleO::Services::Ntp
        - OS::TripleO::Services::Pacemaker
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::PankoApi{% endif %}""
        - OS::TripleO::Services::Redis
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - OS::TripleO::Services::SensuClient
        - OS::TripleO::Services::Snmp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::TripleoPackages
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""",261,No issue found,No issue found,0,0,0,0
262,"shell: |
            test -e .venv/bin/activate && source .venv/bin/activate
            subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html
            test -e .venv/bin/activate && source .venv/bin/activate","test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ inventory_dir }}/tempest_results/tempest-results-{{ test_suite }}.*.subunit""
      test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ test_output_filename }}.subunit""
      - ""{{ test_output_filename }}.xml""
      - ""{{ test_output_filename }}.html""
            > {{ test_output_filename }}.subunit;
            testr run {{ ('--parallel --concurrency=' + test.threads|string) if test.threads|default('') else '' }} --subunit '{{ '|'.join(parts) }}' > {{ test_output_filename }}.subunit
      - name: generate results report in HTML format
        command: ""subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: generate results report in JunitXML format
        shell: |
            subunit2junitxml --output-to={{ test_output_filename }}.xml \
                < {{ test_output_filename }}.subunit | subunit2pyunit
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.junitxml is defined

        command: ""sed '/^<testsuite/s/name=\""\""/name=\""{{ test_suite }}\""/' -i {{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined
      - name: add the test suite name in the HTML report title
        command: ""sed 's#<title>\\(.*Test Report\\)</title>#<title>\\1 - {{ test_suite }}</title>#' \
                  -i {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: Fetch subunit results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.subunit""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.subunit""
            flat: yes
            fail_on_missing: yes

      - name: Fetch JUnit XML results file
            src: ""{{ test.dir }}/{{ test_output_filename }}.xml""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined

      - name: Fetch HTML results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.html""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.html""
            flat: yes
            fail_on_missing: no
        when:
            results_formats.html is defined",262,shell: |; {{ test_output_filename }}.subunit; {{ test_output_filename }}.html,"test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""; - ""{{ inventory_dir }}/tempest_results/tempest-results-{{ test_suite }}.*.subunit""; command: ""subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html""; shell: |
            subunit2junitxml --output-to={{ test_output_filename }}.xml \
                < {{ test_output_filename }}.subunit | subunit2pyunit; command: ""sed '/^<testsuite/s/name=""""/name=""{{ test_suite }}""/' -i {{ test_output_filename }}.xml""; command: ""sed 's#<title>\(.*Test Report\)</title>#<title>\1 - {{ test_suite }}</title>#' \
                  -i {{ test_output_filename }}.html""; fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.subunit""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.subunit""
            flat: yes
            fail_on_missing: yes; fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.xml""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.xml""; fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.html""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.html""
            flat: yes
            fail_on_missing: no",1,1,1,1
263,"path: ""{{ overcloud_deploy_script|default('~/overcloud_deploy.sh') }}""","---
- name: Unsupported OS version on undercloud
  fail:
      msg: ""InfraRed supports updates for OpenStack version 11""
  when:
      - not (undercloud_version == ""11"")

- name: Checking overcloud_deploy_file
  stat:
      path: ""~/overcloud_deploy.sh""
  register: overcloud_deploy_file

- name: Deployment script not found
  fail:
      msg: ""Overcloud deployment script not found. Expected path: ~/overcloud_deploy.sh ""
  when: not overcloud_deploy_file.stat.exists or not overcloud_deploy_file.stat.executable

- name: Checking file with overcloud credentials
  stat:
      path: ""~/overcloudrc.v3""
  register: oc_credentials_file

- name: Overcloud RC not found
  fail:
      msg: ""File with OC credentials not found. Expected path: ~/overcloudrc.v3""
  when: not oc_credentials_file.stat.exists or not oc_credentials_file.stat.readable",263,overcloud_deploy_script|default('~/overcloud_deploy.sh'),No issue found,1,1,0,0
264,"""OS::TripleO::CephStorage::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""","---
ceph_role:
    name: CephStorage

    resource_registry:
        ""OS::TripleO::Compute::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""

    flavor: ceph
    host_name_format: 'ceph-%index%'

    services:
        - OS::TripleO::Services::CACerts
        - OS::TripleO::Services::CephOSD
        - OS::TripleO::Services::Kernel
        - OS::TripleO::Services::Ntp
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoPackages
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::SensuClient
        - OS::TripleO::Services::FluentdClient
        - OS::TripleO::Services::VipHosts",264,No issue found,No issue found,0,0,0,0
265,"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""","networks:
        - InternalApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""",265,No issue found,No issue found,0,0,0,0
266,"- name: get controller flavor
  shell: ""source ~/stackrc; openstack flavor list -c Name -f value | grep 'controller-'""
  register: controller_flavor

  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local'""","- name: set additional propertiesx
  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:baremetal,node:controller-{{ item.0 }},boot_option:local'""
  with_indexed_items: ""{{ groups['controller'] }}""",266,"source ~/stackrc; openstack flavor list -c Name -f value | grep 'controller-'; source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local'",Shell module usage; Potential command injection via item.1; Potential command injection via item.0; Source command usage; Unquoted variables,1,1,1,1
267,"- name: Override postgres params for CentOs or RedHat when ovirt >= 4.2
    - ansible_distribution in ('CentOS', 'RedHat')
    - ansible_distribution in ('CentOS', 'RedHat')","- name: Include postgres params
  include_vars: default.yml

- name: Override postgres params for CentOs or Red Hat when ovirt >= 4.2
  include_vars: postgres95.yml
  when:
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')

- name: install psycopg2 requirements to run ansible modules managing postgres.
  yum:
    name: ""python-psycopg2""
    state: ""present""

    name: ""{{ postgres_service_name }}""
    name: ""{{ postgres_server }}""
- name: scl enable
  shell: 'scl enable rh-postgresql95 bash'
  when:
    - postgresql_status|failed
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version < '4.2'
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
  args:
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version >= '4.2'
    name: ""{{ postgres_service_name }}""
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: ""{{ postgres_config_file }}""
    dest: ""{{ postgres_config_file }}""
    dest: '/usr/lib/systemd/system/{{ postgres_service_name }}.service'
    path: ""{{ postgres_config_file }}""
    name: ""{{ postgres_service_name }}""
- name: create DWH DB user
  become: true
  postgresql_user:
    name: ""{{ item.user }}""
    password: ""{{ item.password }}""
    - user: ""{{ ovirt_engine_db_user }}""
      password: ""{{ ovirt_engine_db_password }}""
    - user: ""{{ ovirt_engine_dwh_db_user }}""
      password: ""{{ ovirt_engine_dwh_db_password }}""
  when: ovirt_engine_dwh_remote_db == True
- name: create engine & DWH DBs
  become: true
  postgresql_db:
    name: ""{{ item.db_name }}""
    owner: ""{{ item.user }}""
    encoding: UTF-8
    lc_collate: en_US.UTF-8
    lc_ctype: en_US.UTF-8
    template: template0
    - db_name: ""{{ ovirt_engine_db_name }}""
      user: ""{{ ovirt_engine_db_user }}""
    - db_name: ""{{ ovirt_engine_dwh_db_name }}""
      user: ""{{ ovirt_engine_dwh_db_user }}""
    name: ""{{ postgres_service_name }}""",267,No issue found,"yum:
    name: ""python-psycopg2""
    state: ""present""

    name: ""{{ postgres_service_name }}""
    name: ""{{ postgres_server }}""; shell: '{{ postgres_setup_cmd }}'
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version < '4.2'
  tags:
    - skip_ansible_lint; shell: '{{ postgres_setup_cmd }}'
  args:
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version >= '4.2'
    name: ""{{ postgres_service_name }}""
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: ""{{ postgres_config_file }}""
    dest: ""{{ postgres_config_file }}""
    dest: '/usr/lib/systemd/system/{{ postgres_service_name }}.service'
    path: ""{{ postgres_config_file }}""
    name: ""{{ postgres_service_name }}""; postgresql_user:
    name: ""{{ item.user }}""
    password: ""{{ item.password }}""
    - user: ""{{ ovirt_engine_db_user }}""
      password: ""{{ ovirt_engine_db_password }}""
    - user: ""{{ ovirt_engine_dwh_db_user }}""
      password: ""{{ ovirt_engine_dwh_db_password }}""
  when: ovirt_engine_dwh_remote_db == True; postgresql_db:
    name: ""{{ item.db_name }}""
    owner: ""{{ item.user }}""
    encoding: UTF-8
    lc_collate: en_US.UTF-8
    lc_ctype: en_US.UTF-8
    template: template0
    - db_name: ""{{ ovirt_engine_db_name }}""
      user: ""{{ ovirt_engine_db_user }}""
    - db_name: ""{{ ovirt_engine_dwh_db_name }}""
      user: ""{{ ovirt_engine_dwh_db_user }}""
    name: ""{{ postgres_service_name }}""",0,0,1,1
268,name: postgresql,"- name: check state of database
  service:
    name: ovirt-engine
    state: running

- name: check state of engine
  service:
    name: ovirt-engine
    state: running

- name: restart of ovirt-engine service
  service:
    name: ovirt-engine
    state: restarted

- name: check health status of page
  uri:
    url: ""http://{{ovirt_engine_hostname}}/ovirt-engine/services/health""
    status_code: 200
  register: health_page
  retries: 12
  delay: 10
  until: health_page|success",268,No issue found,"url: ""http://{{ovirt_engine_hostname}}/ovirt-engine/services/health""",0,1,1,0
269,"- name: Set runner executor section
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^\s*[runners.{{ gitlab_runner.executor|default(""shell"") }}]'
    line: '  [runners.{{ gitlab_runner.executor|default(""shell"") }}]'
    state: present
    insertafter: '^\s*executor ='
    backrefs: no
  check_mode: no
  notify: restart_gitlab_runner


    line: '    image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*\[runners.docker\]'
    line: '    privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*\[runners.docker\]'
    line: '    volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'

    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'","regexp: '^\s*limit ='
    line: '  limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    insertafter: '^\s*name ='
    backrefs: no
    regexp: '^\s*url ='
    line: '  url = {{ gitlab_runner_coordinator_url | to_json }}'
    insertafter: '^\s*limit ='
    backrefs: no
    regexp: '^\s*executor ='
    line: '  executor = {{ gitlab_runner.executor|default(""shell"") | to_json }}'
    insertafter: '^\s*url ='
    backrefs: no
    regexp: '^\s*image ='
    line: '  image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*privileged ='
    line: '  privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*volumes ='
    line: '  volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Type ='
    line: '  Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Path ='
    line: '  Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*ServerAddress ='
    line: '  ServerAddress = {{ gitlab_runner.cache_s3_server_address|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*AccessKey ='
    line: '  AccessKey = {{ gitlab_runner.cache_s3_access_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*SecretKey ='
    line: '  SecretKey = {{ gitlab_runner.cache_s3_secret_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Shared ='
    line: '  Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketName ='
    line: '  BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketLocation ='
    line: '  BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Insecure ='
    line: '  Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*user ='
    line: '  user = {{ gitlab_runner.ssh_user|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*host ='
    line: '  host = {{ gitlab_runner.ssh_host|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*port ='
    line: '  port = {{ gitlab_runner.ssh_port|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*password ='
    line: '  password = {{ gitlab_runner.ssh_password|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*identity_file ='
    line: '  identity_file = {{ gitlab_runner.ssh_identity_file|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no",269,privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}; volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }},No issue found,1,0,0,0
270,when: gitlab_runner_registration_token | length > 0  # Ensure value is set,"- name: Register GitLab Runner
  include: register-runner.yml
  when: gitlab_runner_registration_token != ''",270,No issue found,No issue found,0,1,0,0
271,"regexp: '^\s*limit ='
    line: '  limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    insertafter: '^\s*name ='
    backrefs: no
    regexp: '^\s*url ='
    line: '  url = {{ gitlab_runner_coordinator_url | to_json }}'
    insertafter: '^\s*limit ='
    backrefs: no
    regexp: '^\s*executor ='
    line: '  executor = {{ gitlab_runner.executor|default(""shell"") | to_json }}'
    insertafter: '^\s*url ='
    backrefs: no
    regexp: '^\s*image ='
    line: '  image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*privileged ='
    line: '  privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*volumes ='
    line: '  volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Type ='
    line: '  Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Path ='
    line: '  Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*ServerAddress ='
    line: '  ServerAddress = {{ gitlab_runner.cache_s3_server_address|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*AccessKey ='
    line: '  AccessKey = {{ gitlab_runner.cache_s3_access_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*SecretKey ='
    line: '  SecretKey = {{ gitlab_runner.cache_s3_secret_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Shared ='
    line: '  Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketName ='
    line: '  BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketLocation ='
    line: '  BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Insecure ='
    line: '  Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*user ='
    line: '  user = {{ gitlab_runner.ssh_user|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*host ='
    line: '  host = {{ gitlab_runner.ssh_host|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*port ='
    line: '  port = {{ gitlab_runner.ssh_port|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*password ='
    line: '  password = {{ gitlab_runner.ssh_password|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*identity_file ='
    line: '  identity_file = {{ gitlab_runner.ssh_identity_file|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no","---
- name: Create temporary file
  tempfile:
    state: file
    path: ""{{ temp_runner_config_dir.path }}""
    prefix: ""gitlab-runner.{{ gitlab_runner_index }}{{ runner_config_index }}.""
  register: temp_runner_config_keyword
  check_mode: no
  changed_when: false

- name: Isolate runner configuration
  copy:
    dest: ""{{ temp_runner_config_keyword.path }}""
    content: ""[[runners]]""
  check_mode: no
  changed_when: false

- name: Set concurrent limit option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)limit ='
    line: '\1limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set coordinator URL
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)url ='
    line: '\1url = {{ gitlab_runner_coordinator_url | to_json }}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set runner executor option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)executor ='
    line: '\1executor = {{ gitlab_runner.executor|default(""shell"") | to_json}}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set runner docker image option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)image ='
    line: '\1image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.docker_image is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set docker privileged option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)privileged ='
    line: '\1privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    state: ""{{ 'present' if gitlab_runner.docker_privileged is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set docker volumes option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)volumes ='
    line: '\1volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    state: ""{{ 'present' if gitlab_runner.docker_volumes is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache type option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Type ='
    line: '\1Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_type is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache path option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Path ='
    line: '\1Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_path is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache shared option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Shared ='
    line: '\1Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    state: ""{{ 'present' if gitlab_runner.cache_shared is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 bucket name option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)BucketName ='
    line: '\1BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_bucket_name is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 bucket location option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)BucketLocation ='
    line: '\1BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_bucket_location is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 insecure option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Insecure ='
    line: '\1Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_insecure is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner",271,No issue found,No issue found,0,1,0,0
272,--cache-s3-secret-key '{{ gitlab_runner_cache_s3_secret_key }}',"{% if gitlab_runner_cache_type is defined %}
    --cache-type '{{ gitlab_runner_cache_type }}'
    --cache-s3-server-address '{{ gitlab_runner_cache_s3_server_address }}'
    --cache-s3-access-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-secret-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-bucket-name '{{ gitlab_runner_cache_s3_bucket_name }}'
    --cache-s3-insecure '{{ gitlab_runner_cache_s3_insecure }}'
    --cache-cache-shared '{{ gitlab_runner_cache_cache_shared }}'
    {% endif %}",272,--cache-s3-secret-key '{{ gitlab_runner_cache_s3_secret_key }}',--cache-s3-secret-key '{{ gitlab_runner_cache_s3_access_key }}',1,0,1,1
273,insertafter: '^\s*url =',"- name: Set environment option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^\s*environment ='
    line: '  environment = {{ gitlab_runner.env_vars|default([]) | to_json }}'
    state: present
    insertafter: '^\s*url='
    backrefs: no
  check_mode: no
  notify: restart_gitlab_runner",273,No issue found,"dest: ""{{ temp_runner_config.path }}""; line: '  environment = {{ gitlab_runner.env_vars|default([]) | to_json }}'",0,1,1,0
274,"dest: ""{{ gitlab_runner_config_file }}""","---
- name: (Windows) Create .gitlab-runner dir
  win_file:
    path: ""{{ gitlab_runner_config_file_location }}""
    state: directory

- name: (Windows) Ensure config.toml exists
  win_file:
    path: ""{{ gitlab_runner_config_file }}""
    state: touch
    modification_time: preserve
    access_time: preserve

- name: (Windows) Set concurrent option
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^(\s*)concurrent =.*'
    line: '$1concurrent = {{ gitlab_runner_concurrent }}'
    state: present
    backrefs: yes
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows

- name: (Windows) Add listen_address to config
  win_lineinfile:
    dest: /etc/gitlab-runner/config.toml
    regexp: '^listen_address =.*'
    line: 'listen_address = ""{{ gitlab_runner_listen_address }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_listen_address | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_windows

- name: (Windows) Add sentry dsn to config
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^sentry_dsn =.*'
    line: 'sentry_dsn = ""{{ gitlab_runner_sentry_dsn }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_sentry_dsn | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows",274,"dest: ""{{ gitlab_runner_config_file }}""","path: ""{{ gitlab_runner_config_file_location }}""; path: ""{{ gitlab_runner_config_file }}""; dest: ""{{ gitlab_runner_config_file }}""; dest: /etc/gitlab-runner/config.toml; dest: ""{{ gitlab_runner_config_file }}""; line: '$1concurrent = {{ gitlab_runner_concurrent }}'; line: 'listen_address = ""{{ gitlab_runner_listen_address }}""'; line: 'sentry_dsn = ""{{ gitlab_runner_sentry_dsn }}""'",1,1,1,1
275,"name: ""{{ item }}""","name: ""{{ item }}""
    with_items: ""{{ rocket_chat_dep_packages }}""
      name: 
    with_items: ""{{ rocket_chat_dep_packages }}""",275,"name: ""{{ item }}""",No issue found,1,1,0,1
276,file:  CentOS-Base,"- name: Configure default CentOS online repos
    yum_repository:
      name: {{ item }}
      enabled: {{ rock_online_install }}
      file:  CentOS-Base.repo
    with_items:
      - base
      - updates
      - extras

    when: with_elasticsearch
    when: (with_elasticsearch and with_bro)
    when: (with_elasticsearch and with_bro) and bro_mapping.status == 404
  - name: Add broctl wrapper for admin use
    copy:
      src: broctl.sh
      dest: /usr/sbin/broctl
      mode: 0754
      owner: root
      group: root
    when: with_bro

    when: with_pulledpork and not rules_file.stat.exists
    when: with_kibana and rock_online_install
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana and (kibana_config.rock_config is undefined or kibana_config.rock_config != rock_dashboards_version)
    when: with_kibana and with_bro
    when: with_kibana and with_suricata and not with_bro
    when: with_kibana
    when: with_kibana
    seboolean: 
      name: httpd_can_network_connect
      state: yes 
      persistent: yes
        for intf in {{ rock_monifs | join(' ') }}; do
    - name: create kafka suricata topic
           --topic suricata-raw",276,No issue found,name: {{ item }}; enabled: {{ rock_online_install }}; src: broctl.sh; dest: /usr/sbin/broctl; mode: 0754; owner: root; group: root; name: httpd_can_network_connect; state: yes; persistent: yes; for intf in {{ rock_monifs | join(' ') }}; do; --topic suricata-raw,0,1,1,0
277,"dest: ""{{ bro_sysconfig_dir }}/broctl.cfg""
      creates: ""{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""
      src: ""{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""
      dest: ""{{ bro_site_dir }}/scripts/rock""","- { pkg: docket, test: ""{{with_docket}}"", state: installed }
        - { port: ""8443/tcp"", test: ""{{ with_docket }}"" }
  - name: Create /opt/bro dir for wandering users
      dest: ""/opt/bro""
      state: directory
  - name: Create note to wandering users
    copy:
      dest: ""/opt/bro/README.md""
      content: |
        Hey! Where's my Bro?
        =========================

        RockNSM has aligned the Bro package to be inline with Fedora packaging
        guidelines in an effort to push the package upstream for maintenance.
        Fedora and EPEL have a great community and we believe others can benefit
        from our hard work.

        Here's where you can find your stuff:

        Bro configuration files
        -----------------------
        /opt/bro/etc -> /etc/bro

        Bro site scripts
        -----------------------
        /opt/bro/share/bro/site -> /usr/share/bro/site

        Bro logs and spool dirs (same as previous ROCK iterations)
        -----------------------
        /opt/bro/logs -> /data/bro/logs
        /opt/bro/spool -> /data/bro/spool

      dest: ""{{ bro_sysconfig_dir }}/node.cfg""
      dest: {{ bro_sysconfig_dir }}/broctl.cfg""
      dest: ""{{ bro_sysconfig_dir }}/networks.cfg""
      path: /scripts
      dest: ""{{ bro_site_dir }}/scripts/README.txt""
      dest: ""{{ bro_site_dir }}/scripts/rock""
      dest: ""{{ bro_site_dir }}/scripts/""
      creates: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""
      src: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""
      dest: """"{{ bro_site_dir }}/scripts/rock""""
      path: ""{{ bro_site_dir }}/scripts/rock""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
  - name: Add bro aliases
      job: ""/usr/bin/broctl cron >/dev/null 2>&1""
    command: /usr/bin/broctl install
      name: bro
    notify: reload broctl
      export kibuser=$(getent passwd 1000 | awk -F: '{print $1}') && export kibpw=$(xkcdpass -a rock) && echo -e ""U: ${kibuser}\nP: ${kibpw}"" > /home/${kibuser}/KIBANA_CREDS.README && printf ""${kibuser}:$(echo ${kibpw} | openssl passwd -apr1 -stdin)\n"" | sudo tee -a /etc/nginx/htpasswd.users > /dev/null 2>&1
    seboolean:
      state: yes
    shell:
      service: name=broctl state=""{{ 'started' if enable_bro else 'stopped' }}""",277,"dest: ""{{ bro_sysconfig_dir }}/broctl.cfg""; dest: ""{{ bro_site_dir }}/scripts/rock""","- { pkg: docket, test: ""{{with_docket}}"", state: installed }; - { port: ""8443/tcp"", test: ""{{ with_docket }}"" }; dest: ""{{ bro_sysconfig_dir }}/node.cfg""; dest: {{ bro_sysconfig_dir }}/broctl.cfg""; dest: ""{{ bro_sysconfig_dir }}/networks.cfg""; dest: ""{{ bro_site_dir }}/scripts/README.txt""; dest: ""{{ bro_site_dir }}/scripts/rock""; dest: ""{{ bro_site_dir }}/scripts/""; creates: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""; src: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""; dest: """"{{ bro_site_dir }}/scripts/rock""""; path: ""{{ bro_site_dir }}/scripts/rock""; dest: ""{{ bro_site_dir }}/local.bro""; dest: ""{{ bro_site_dir }}/local.bro""; dest: ""{{ bro_site_dir }}/local.bro""; dest: ""{{ bro_site_dir }}/local.bro""; job: ""/usr/bin/broctl cron >/dev/null 2>&1""; command: /usr/bin/broctl install; name: bro; notify: reload broctl; export kibuser=$(getent passwd 1000 | awk -F: '{print $1}') && export kibpw=$(xkcdpass -a rock) && echo -e ""U: ${kibuser}\nP: ${kibpw}"" > /home/${kibuser}/KIBANA_CREDS.README && printf ""${kibuser}:$(echo ${kibpw} | openssl passwd -apr1 -stdin)\n"" | sudo tee -a /etc/nginx/htpasswd.users > /dev/null 2>&1; seboolean:; state: yes; shell:; service: name=broctl state=""{{ 'started' if enable_bro else 'stopped' }}""",1,0,1,1
278,"state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""","state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""",278,No issue found,No issue found,0,1,0,0
279,"apt:
    pkg: fail2ban
    state: latest
    update_cache: true
    cache_valid_time: ""{{ apt_cache_valid_time }}""
  template:
    src: ""{{ item }}.j2""
    dest: /etc/fail2ban/{{ item }}
  service:
    name: fail2ban
    state: started
    enabled: yes","---
- name: ensure fail2ban is installed
  apt: pkg=fail2ban state=latest update_cache=true cache_valid_time={{ apt_cache_valid_time }}
  notify:
    - restart fail2ban

- name: ensure fail2ban is configured
  template: src={{ item }}.j2 dest=/etc/fail2ban/{{ item }}
  with_items:
    - jail.local
    - fail2ban.local
  notify:
    - restart fail2ban

- name: ensure fail2ban starts on a fresh reboot
  service: name=fail2ban state=started enabled=yes",279,"cache_valid_time: ""{{ apt_cache_valid_time }}""; src: ""{{ item }}.j2""; dest: /etc/fail2ban/{{ item }}",apt: pkg=fail2ban state=latest update_cache=true cache_valid_time={{ apt_cache_valid_time }}; template: src={{ item }}.j2 dest=/etc/fail2ban/{{ item }},1,1,1,1
280,"local_action: command ansible {{ inventory_hostname }} -m ping{{ (inventory_file == None) | ternary('', ' -i ' + inventory_file | string) }} -u root","local_action: command ansible {{ inventory_hostname }} -m ping -i {{ inventory_file }} -u root
    ansible_ssh_user: ""{{ (root_status.rc == 0) | ternary('root', admin_user) }}""

- name: Announce which user was selected
  debug:
    msg: ""Note: Ansible will attempt connections as user = {{ ansible_ssh_user }}""",280,Running commands as root; Potential command injection via inventory_file variable,"local_action: command ansible {{ inventory_hostname }} -m ping -i {{ inventory_file }} -u root; ansible_ssh_user: ""{{ (root_status.rc == 0) | ternary('root', admin_user) }}""; msg: ""Note: Ansible will attempt connections as user = {{ ansible_ssh_user }}""",1,1,1,1
281,"project_subtree: ""{{ project.subtree }}""","project_subtree: ""{{ project.subtree | default(False) }}""",281,No issue found,No issue found,0,1,0,0
282,"when: sensu_redis_server
  when: sensu_rabbitmq_server","---

  - name: Ensure the Sensu group is present
    group: name={{ sensu_group_name }}
             state=present
             
  - name: Ensure the Sensu user is present
    user: name={{ sensu_user_name }}
          group={{ sensu_group_name }}
          shell=/bin/false
          home={{ sensu_config_path }}
          createhome=yes
          state=present

  - name: Ensure the Sensu config directory is present
    file: dest={{ sensu_config_path }}/conf.d state=directory recurse=yes
          owner={{ sensu_user_name }} group={{ sensu_group_name }}

  - name: Ensure Sensu dependencies are installed
    pkgin: name=build-essential,ruby21-base state=present

  - name: Ensure Uchiwa (dashboard) dependencies are installed
    pkgin: name=go state=present
    when: sensu_include_dashboard

  - name: Ensure Sensu is installed
    gem: name=sensu state={{ sensu_gem_state }} user_install=no
    notify:
      - restart sensu-client service
    
  - name: Ensure Sensu 'plugins' gem is installed
    gem: name=sensu-plugin state={{ sensu_plugin_gem_state }} user_install=no

  - include: ssl.yml tags=ssl

  - include: rabbit.yml tags=rabbitmq
    when: rabbitmq_server

  - include: redis.yml tags=redis
    when: redis_server

  - include: server.yml tags=server
    when: sensu_master

  - include: dashboard.yml tags=dashboard
    when: sensu_include_dashboard
    
  - include: client.yml tags=client

  - include: plugins.yml tags=plugins
    when: sensu_include_plugins",282,No issue found,group: name={{ sensu_group_name }} state=present; user: name={{ sensu_user_name }} group={{ sensu_group_name }} shell=/bin/false home={{ sensu_config_path }} createhome=yes state=present; file: dest={{ sensu_config_path }}/conf.d state=directory recurse=yes owner={{ sensu_user_name }} group={{ sensu_group_name }}; gem: name=sensu state={{ sensu_gem_state }} user_install=no; gem: name=sensu-plugin state={{ sensu_plugin_gem_state }} user_install=no; - include: ssl.yml tags=ssl; - include: rabbit.yml tags=rabbitmq when: rabbitmq_server; - include: redis.yml tags=redis when: redis_server; - include: server.yml tags=server when: sensu_master; - include: dashboard.yml tags=dashboard when: sensu_include_dashboard; - include: client.yml tags=client; - include: plugins.yml tags=plugins when: sensu_include_plugins,0,1,1,0
283,enabled: yes,"- name: Deploy Tessen server configuratiuon
  template:
    dest: ""{{ sensu_config_path }}/conf.d/tessen.json""
    owner: ""{{ sensu_user_name }}""
    group: ""{{ sensu_group_name }}""
    src: sensu-tessen.json.j2
  notify: restart sensu-server service

  service:
    name: ""{{ sensu_server_service_name if not se_enterprise else sensu_enterprise_service_name }}""
    state: started
  enabled: yes
  service:
    name: sensu-api
    state: started
    enabled: yes",283,No issue found,"sensu_config_path, sensu_user_name, sensu_group_name, sensu_server_service_name, sensu_enterprise_service_name, se_enterprise are not defined in the script; The script has two service modules, which is not allowed in Ansible. The second service module will be ignored",0,0,1,0
284,istio_git_repo: https://github.com/istio/istio.git,"istio_git_repo: https://github.com/istio/istio.git
istio_git_branch: 0.6.0

istio_playbook_release_tag: 0.6.0
istio_playbook_auth: false
istio_playbook_jaeger: false
istio_playbook_delete_resources: false
istio_playbook_cluster_flavour: ocp
istio_playbook_dest: /home/istio
istio_playbook_namespace: istio-system
istio_playbook_addon: grafana,prometheus,servicegraph
istio_playbook_samples:",284,No issue found,istio_git_repo: https://github.com/istio/istio.git; istio_git_branch: 0.6.0; istio_playbook_dest: /home/istio,0,0,1,0
285,value: sb-2.1.x,"apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: s2i-springboot-example
spec:
  taskRef:
    name: s2i-jdk8
  inputs:
    resources:
      - name: git-repo
        resourceSpec:
          type: git
          params:
            - name: revision
              value: master
            - name: url
              value: https://github.com/snowdrop/rest-http-example
  outputs:
    resources:
      - name: image
        resourceSpec:
          type: image
          params:
            - name: url
              value: quay.io/snowdrop/spring-boot-example",285,No issue found,No issue found,0,0,0,0
286,"- name: ""Setting service_name fact from config""
    splunk_service_name: ""{{ splunk.service_name }}""
    - ""'service_name' in splunk""
- name: Set Splunk service name
  block:
    - name: Setting SplunkForwarder service
      set_fact:
        splunk_service_name: ""SplunkForwarder.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role == ""splunk_universal_forwarder""
    - name: Setting Splunkd service
      set_fact:
        splunk_service_name: ""Splunkd.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role != ""splunk_universal_forwarder""
    - name: Setting splunk service
      set_fact:
        splunk_service_name: ""splunk""
      when:
        - ansible_system is match(""Linux"")
        - not splunk_systemd

    - name: Setting splunkforwarder Windows service
      set_fact:
        splunk_service_name: ""splunkforwarder""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role == ""splunk_universal_forwarder""

    - name: Setting splunkd Windows service
      set_fact:
        splunk_service_name: ""splunkd""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role != ""splunk_universal_forwarder""
    - splunk_service_name is not defined or not splunk_service_name
    - splunk.enable_service","---
- name: ""Setting service_name to SplunkForwarder.service""
  set_fact:
    splunk_service_name: ""SplunkForwarder.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd
    - splunk.build_location is search('forwarder')

- name: ""Setting service_name to Splunkd.service""
  set_fact:
    splunk_service_name: ""Splunkd.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd is False

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and not ansible_system is match(""Linux"")",286,No issue found,No issue found,0,0,0,0
287,"changed_when:
    - splunk_cluster_bundle_result.stdout.find('Applying') != -1
    - splunk_cluster_bundle_result.stdout.find('bundle') != -1","---
- name: Get indexer count
  set_fact:
    num_indexer_hosts: ""{{ groups['splunk_indexer'] | length }}""

- name: Get default replication factor
  set_fact:
    idxc_search_factor: ""{{ splunk.idxc.search_factor }}""
    idxc_replication_factor: ""{{ splunk.idxc.replication_factor }}""

- name: Lower indexer search/replication factor
  set_fact:
    idxc_search_factor: 1
    idxc_replication_factor: 1
  when: num_indexer_hosts|int < 3

- name: Set indexer discovery
  uri:
    url: ""https://127.0.0.1:{{ splunk.svc_port }}/servicesNS/nobody/system/configs/conf-server""
    method: POST
    user: admin
    password: ""{{ splunk.password }}""
    validate_certs: False
    body: ""name=indexer_discovery&pass4SymmKey={{ splunk.shc.secret }}""
    body_format: json
    headers:
      Content-Type: ""application/x-www-form-urlencoded""
    status_code: 201,409
    timeout: 10
  register: set_indexer_discovery
  changed_when: set_indexer_discovery.status == 201

- name: Set the current node as a Splunk indexer cluster master
  command: ""{{ splunk.exec }} edit cluster-config -mode master -replication_factor {{ splunk.idxc.replication_factor }} -search_factor {{ splunk.idxc.search_factor }} -secret '{{ splunk.idxc.secret }}' -cluster_label '{{ splunk.idxc.label }}' -auth 'admin:{{ splunk.password }}'""
  register: task_result
  until: task_result.rc == 0
  retries: ""{{ retry_num }}""
  delay: 3
  notify:
    - Restart the splunkd service

- name: Flush restart handlers
  meta: flush_handlers

- name: Apply the cluster bundle to the Splunk cluster master
  command: ""{{ splunk.exec }} apply cluster-bundle -auth admin:{{ splunk.password }} --skip-validation --answer-yes""
  register: splunk_cluster_bundle_result
  failed_when: >
    (""No new bundle will be pushed"" not in splunk_cluster_bundle_result.stderr)
    and (""Rolling restart of the peers"" not in splunk_cluster_bundle_result.stderr)
    and (""Applying bundle"" not in splunk_cluster_bundle_result.stdout)
  changed_when: splunk_cluster_bundle_result.stdout.find('Applying bundle') != -1

- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml",287,No issue found,"validate_certs: False; user: admin; password: ""{{ splunk.password }}""; body: ""name=indexer_discovery&pass4SymmKey={{ splunk.shc.secret }}""; command: ""{{ splunk.exec }} edit cluster-config -mode master -replication_factor {{ splunk.idxc.replication_factor }} -search_factor {{ splunk.idxc.search_factor }} -secret '{{ splunk.idxc.secret }}' -cluster_label '{{ splunk.idxc.label }}' -auth 'admin:{{ splunk.password }}'""; command: ""{{ splunk.exec }} apply cluster-bundle -auth admin:{{ splunk.password }} --skip-validation --answer-yes""",0,0,1,1
288,"register: set_symmkey

- include_tasks: trigger_restart.yml
  when: set_symmkey is changed","- name: Set general pass4SymmKey
  ini_file: 
    dest: ""{{ splunk.home }}/etc/system/local/server.conf""
    section: ""general""
    option: ""pass4SymmKey""
    value: ""{{ splunk.pass4SymmKey }}""
  notify:
    - Restart the splunkd service",288,No issue found,pass4SymmKey is stored in plaintext; splunk.home and splunk.pass4SymmKey are not validated before use,0,0,1,1
289,"- include_tasks: prepare_apps_bundle.yml
- include_tasks: initialize_cluster_master.yml","- include_tasks: initialize_cluster_master.yml
- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml
- include_tasks: ../../../roles/splunk_common/tasks/provision_apps.yml
  when:
    - splunk.apps_location
- include_tasks: push_apps_to_indexers.yml
  when:
    - splunk.apps_location",289,No issue found,include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml; include_tasks: ../../../roles/splunk_common/tasks/provision_apps.yml,0,1,1,0
290,"path: ""{{ splunk.app_paths.deployment }}""","- name: Gather all deployment server apps
  find:
    path: ""{{ splunk.home }}/etc/deployment_apps""
    recurse: no
    file_type: directory
  register: deployment_apps

    section: ""serverClass:all:app:{{ item.path | basename }}""
  with_items: ""{{ deployment_apps.files }}""",290,"path: ""{{ splunk.app_paths.deployment }}""","path: ""{{ splunk.home }}/etc/deployment_apps""; section: ""serverClass:all:app:{{ item.path | basename }}""",1,1,1,1
291,- include_tasks: ../../../roles/splunk_common/tasks/check_for_required_restarts.yml,- include_tasks: check_for_required_restarts.yml,291,include_tasks: ../../../roles/splunk_common/tasks/check_for_required_restarts.yml,No issue found,1,0,0,0
292,"- name: ""Wait for port {{ splunk.svc_port }} to become open""
    port: ""{{ splunk.svc_port }}""","- name: ""Wait for port 8089 to become open""
    port: 8089",292,No issue found,No issue found,0,0,0,0
293,"license: MIT
  min_ansible_version: 2.0.1","---
galaxy_info:
  author: Justin Leitgeb
  description: Base image and common roles
  company: Stack Builders
  license: Proprietary
  min_ansible_version: 1.2
  platforms:
    - name: Debian
      versions:
        - all

  galaxy_tags:
    - sb-base

dependencies:
  - role: kamaln7.swapfile
    become: yes
    become_method: sudo
    remote_user: administrator
    swapfile_size: ""{{ swap_file_size }}""
    tags:
      - bootstrap

  - role: ansible-role-unattended-upgrades
    become: yes
    become_method: sudo
    remote_user: administrator
    unattended_origins_patterns:
      - 'origin=Debian,archive=${distro_codename},label=Debian-Security'
    unattended_mail: '{{ uu_email_alerts }}'
    unattended_automatic_reboot: false
    tags:
      - bootstrap

  - role: nickjj.fail2ban
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - bootstrap

  - role: geerlingguy.ntp
    become: yes
    become_method: sudo
    remote_user: administrator
    ntp_timezone: UTC
    tags:
      - bootstrap

  - role: jdauphant.ssl-certs
    become: yes
    become_method: sudo
    remote_user: administrator
    ssl_certs_common_name: ""{{ inventory_hostname }}""

    tags:
      - nginx-http
      - nginx-https

  - role: jdauphant.nginx
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - nginx

  - role: ANXS.postgresql
    tags:
      - basic-postgres",293,No issue found,"swapfile_size: ""{{ swap_file_size }}""; unattended_mail: '{{ uu_email_alerts }}'; ssl_certs_common_name: ""{{ inventory_hostname }}""",0,0,1,0
294,service: name={{openvpn_service}} state=restarted,"---

- name: openvpn restart
  service: name=openvpn state=restarted",294,openvpn_service variable is not sanitized,No issue found,1,1,0,0
295,"- include_tasks: system/firewall-deps.yml
  when:
    openvpn_open_firewall | bool
    or openvpn_route_traffic | bool
    or openvpn_client_to_client_via_ip | bool

- include_tasks: system/open-firewall.yml
  when: openvpn_open_firewall | bool","- include_tasks: system/forwarding.yml

- include_tasks: system/firewall.yml

- include_tasks: system/routing.yml
  when: openvpn_route_traffic | bool",295,No issue found,No issue found,0,0,0,0
296,when: threatstack_hostname and threatstack_policy,"- name: Cloudsight setup default
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }}
  when: not threatstack_hostname and not threatstack_policy
- name: Cloudsight setup policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --policy=""{{ threatstack_policy}}""
  register: setup_result
  when: threatstack_policy and not threatstack_hostname
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and not threatstack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname/policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and threastack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret",296,No issue found,"cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }}; cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --policy=""{{ threatstack_policy}}""; cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}; cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}",0,0,1,1
297,"secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""","---
- set_fact:
    resource_group: ""Algo_{{ region }}""

- name: Create a resource group
  azure_rm_resourcegroup:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    name: ""{{ resource_group }}""
    location: ""{{ region }}""
    tags:
        service: algo

- name: Create a virtual network
  azure_rm_virtualnetwork:
    resource_group: ""{{ resource_group }}""
    name: algo_net
    address_prefixes: ""10.10.0.0/16""
    tags:
        service: algo

- name: Create a subnet
  azure_rm_subnet:
    resource_group: ""{{ resource_group }}""
    name: algo_subnet
    address_prefix: ""10.10.0.0/24""
    virtual_network: algo_net
    tags:
        service: algo

- name: Create an instance
  azure_rm_virtualmachine:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    resource_group: ""{{ resource_group }}""
    admin_username: ubuntu
    virtual_network: algo_net
    name: ""{{ azure_server_name }}""
    ssh_password_enabled: false
    vm_size: Standard_D1
    tags:
      service: algo
    ssh_public_keys:
      - { path: ""/home/ubuntu/.ssh/authorized_keys"", key_data: ""{{ lookup('file', '{{ ssh_public_key }}') }}"" }
    image:
      offer: UbuntuServer
      publisher: Canonical
      sku: '16.04-LTS'
      version: latest
  register: azure_rm_virtualmachine

- set_fact:
    ip_address: ""{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].properties.ipConfigurations[0].properties.publicIPAddress.properties.ipAddress }}""

- name: Add the instance to an inventory group
  add_host:
    name: ""{{ ip_address }}""
    groups: vpn-host
    ansible_ssh_user: ubuntu
    ansible_python_interpreter: ""/usr/bin/python2.7""
    easyrsa_p12_export_password: ""{{ easyrsa_p12_export_password }}""
    cloud_provider: azure
    ipv6_support: no

- name: Wait for SSH to become available
  local_action: ""wait_for port=22 host={{ ip_address }} timeout=320""",297,No issue found,"secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""; tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""; client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""; subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""; admin_username: ubuntu; ssh_password_enabled: false; ansible_python_interpreter: ""/usr/bin/python2.7""; easyrsa_p12_export_password: ""{{ easyrsa_p12_export_password }}""",0,0,1,1
298,"- name: Build python virtual environment
  import_tasks: venv.yml
- name: Include prompts
  import_tasks: prompts.yml
- block:
    - set_fact:
        algo_region: >-
          {% if region is defined %}{{ region }}
          {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
          {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

    - name: Security group created
      cs_securitygroup:
        name: ""{{ algo_server_name }}-security_group""
        description: AlgoVPN security group
      register: cs_security_group

    - name: Security rules created
      cs_securitygroup_rule:
        security_group: ""{{ cs_security_group.name }}""
        protocol: ""{{ item.proto }}""
        start_port: ""{{ item.start_port }}""
        end_port: ""{{ item.end_port }}""
        cidr: ""{{ item.range }}""
      with_items:
        - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

    - name: Keypair created
      cs_sshkeypair:
        name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
        public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
      register: cs_keypair

    - name: Set facts
      set_fact:
        image_id: ""{{ cloud_providers.cloudstack.image }}""
        size: ""{{ cloud_providers.cloudstack.size }}""
        disk: ""{{ cloud_providers.cloudstack.disk }}""
        keypair_name: ""{{ cs_keypair.name }}""

    - name: Server created
      cs_instance:
        name: ""{{ algo_server_name }}""
        root_disk_size: ""{{ disk }}""
        template: ""{{ image_id }}""
        ssh_key: ""{{ keypair_name }}""
        security_groups: ""{{ cs_security_group.name }}""
        zone: ""{{ algo_region }}""
        service_offering: ""{{ size }}""
      register: cs_server

    - set_fact:
        cloud_instance_ip: ""{{ cs_server.default_ip }}""
        ansible_ssh_user: ubuntu
  environment:
    CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
    CLOUDSTACK_REGION: ""{{ algo_cs_region }}""","---
- block:
    - name: Build python virtual environment
      import_tasks: venv.yml

    - name: Include prompts
      import_tasks: prompts.yml

    - block:
      - set_fact:
          algo_region: >-
            {% if region is defined %}{{ region }}
            {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
            {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

      - name: Security group created
        cs_securitygroup:
          name: ""{{ algo_server_name }}-security_group""
          description: AlgoVPN security group
        register: cs_security_group

      - name: Security rules created
        cs_securitygroup_rule:
          security_group: ""{{ cs_security_group.name }}""
          protocol: ""{{ item.proto }}""
          start_port: ""{{ item.start_port }}""
          end_port: ""{{ item.end_port }}""
          cidr: ""{{ item.range }}""
        with_items:
          - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

      - name: Keypair created
        cs_sshkeypair:
          name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
          public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
        register: cs_keypair

      - name: Set facts
        set_fact:
          image_id: ""{{ cloud_providers.cloudstack.image }}""
          size: ""{{ cloud_providers.cloudstack.size }}""
          disk: ""{{ cloud_providers.cloudstack.disk }}""
          keypair_name: ""{{ cs_keypair.name }}""

      - name: Server created
        cs_instance:
          name: ""{{ algo_server_name }}""
          root_disk_size: ""{{ disk }}""
          template: ""{{ image_id }}""
          ssh_key: ""{{ keypair_name }}""
          security_groups: ""{{ cs_security_group.name }}""
          zone: ""{{ algo_region }}""
          service_offering: ""{{ size }}""
        register: cs_server

      - set_fact:
          cloud_instance_ip: ""{{ cs_server.default_ip }}""
          ansible_ssh_user: ubuntu
      environment:
        PYTHONPATH: ""{{ cloudstack_venv }}/lib/python2.7/site-packages/""
        CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
        CLOUDSTACK_REGION: ""{{ algo_cs_region }}""

      rescue:
      - debug: var=fail_hint
        tags: always
      - fail:
        tags: always",298,"cs_securitygroup_rule: { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }; cs_securitygroup_rule: { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }; cs_securitygroup_rule: { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }; cs_securitygroup_rule: { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }; public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""","Security rules created: The script is allowing all IP addresses (0.0.0.0/0) to connect to the server on several ports (22, 4500, 500, wireguard_port). This is a major security risk as it exposes the server to potential attacks.; Keypair created: The public key is being read from a file. If this file is not properly secured, it could be read or modified by unauthorized users.; Server created: The script is using a hardcoded username (ubuntu) for the ansible_ssh_user. If an attacker knows this username, it could be used in a brute force attack.; Environment variables: The script is setting environment variables (PYTHONPATH, CLOUDSTACK_CONFIG, CLOUDSTACK_REGION) that could potentially be read by other processes on the system, exposing sensitive information.",1,1,1,1
299,"-out crl/{{ item }}.crt
      creates: crl/{{ item }}.crt","- name: Get active users
  local_action: >
    shell grep ^V index.txt | grep -v ""{{ IP_subject_alt_name }}"" | awk '{print $5}' | sed 's/\/CN=//g'
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
  register: valid_certs

- name: Revoke non-existing users
  local_action: >
    shell openssl ca -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt &&
      openssl ca -gencrl -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt -out crl/{{ item }}.crt
      touch crl/{{ item }}_revoked
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
    creates: crl/{{ item }}_revoked
  environment:
    subjectAltName: ""DNS:{{ item }}""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""

- name: Copy the revoked certificates to the vpn server
  copy:
    src: configs/{{ IP_subject_alt_name }}/pki/crl/{{ item }}.crt
    dest: ""{{ config_prefix|default('/') }}etc/ipsec.d/crls/{{ item }}.crt""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""
  notify:
    - rereadcrls",299,No issue found,"shell grep ^V index.txt | grep -v ""{{ IP_subject_alt_name }}"" | awk '{print $5}' | sed 's/\/CN=//g'; shell openssl ca -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt && openssl ca -gencrl -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt -out crl/{{ item }}.crt; src: configs/{{ IP_subject_alt_name }}/pki/crl/{{ item }}.crt dest: ""{{ config_prefix|default('/') }}etc/ipsec.d/crls/{{ item }}.crt""",0,0,1,1
300,"name: ""{{ opensmtpd_extra_packages }}""","---

- name: Enable opensmtpd_service
  service:
    name: ""{{ opensmtpd_service }}""
    arguments: ""{{ opensmtpd_flags }}""
    enabled: yes

- name: Install opensmtpd_extra_packages
  openbsd_pkg:
    name: ""{{ item }}""
  with_items: ""{{ opensmtpd_extra_packages }}""",300,"name: ""{{ opensmtpd_extra_packages }}""",No issue found,1,1,0,0
301,"mode: 0644
    group: 'root'
    owner: 'root'",mode: 0600,301,No issue found,No issue found,0,0,0,0
302,- irods-rule-engine-plugin-python-4.2.8.0-1,"- name: Ensure iRODS 4.2.7 packages are absent
      - irods-uu-microservices-4.2.7_0.8.1-1
      - irods-sudo-microservices-4.2.7_1.0.0-1
      - davrods-4.2.7_1.4.2-1
      - irods-server-4.2.8-1
      - irods-runtime-4.2.8-1
      - irods-database-plugin-postgres-4.2.8-1
      - irods-rule-engine-plugin-python-4.2.8-1",302,No issue found,No issue found,0,0,0,0
303,when: ruleset.install_scripts and install_rulesets,"when: ruleset.install_scripts == ""yes"" and install_rulesets == ""yes""",303,No issue found,No issue found,0,0,0,0
304,description: Install and configure viasite/zsh-config and oh-my-zsh,"---
galaxy_info:
  author: Stanislav Popov
  company: Viasite
  description: Install and configure popstas/zsh-config and oh-my-zsh
  license: MIT
  min_ansible_version: 1.8
  platforms:
    - name: Ubuntu
      versions:
        - trusty
        - xenial
  categories:
    - system
dependencies: []",304,No issue found,No issue found,0,0,0,0
305,"- wazuh_manager_config.cluster.node_type == ""master"" or wazuh_manager_config.cluster.node_type == ""worker""","when:       
      - wazuh_manager_config.cluster.node_type == ""master""
      - wazuh_manager_config.cluster.node_type == ""master""",305,No issue found,No issue found,0,0,0,0
306,"baseurl: ""{{ wazuh_manager_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}-5""
    baseurl: ""{{ wazuh_manager_config.repo.yum }}""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}""","- name: RedHat/CentOS 5 | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}-5""
    - (ansible_facts['os_family']|lower == 'redhat')
    - (ansible_os_family = ansible_distribution_major_version|int <= 5)
  register: repo_v5_manager_installed
- name: RedHat/CentOS/Fedora | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}""
  changed_when: false
    - repo_v5_manager_installed is undefined",306,"baseurl: ""{{ wazuh_manager_config.repo.yum }}5/""; gpgkey: ""{{ wazuh_manager_config.repo.gpg }}-5""; baseurl: ""{{ wazuh_manager_config.repo.yum }}""; gpgkey: ""{{ wazuh_manager_config.repo.gpg }}""","baseurl: ""{{ wazuh_agent_config.repo.yum }}5/""; gpgkey: ""{{ wazuh_agent_config.repo.gpg }}-5""; baseurl: ""{{ wazuh_agent_config.repo.yum }}""; gpgkey: ""{{ wazuh_agent_config.repo.gpg }}""; (ansible_os_family = ansible_distribution_major_version|int <= 5)",1,1,1,0
307,"od_node_name: ""{{ elasticsearch_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""","- name: Configure node name
    block:
      - name: Setting node name (Elasticsearch)
        set_fact:
          od_node_name: elasticsearch_node_name
        when:
          elasticsearch_node_name is defined and kibana_node_name is not defined

      - name: Setting node name (Kibana)
        set_fact:
          od_node_name: kibana_node_name
        when:
          kibana_node_name is defined

      - name: Setting node name (Filebeat)
        set_fact:
          od_node_name: filebeat_node_name
        when:
          filebeat_node_name is defined


      - ""{{ od_node_name }}.key""
      - ""{{ od_node_name }}.pem""
      - ""{{ od_node_name }}_http.key""
      - ""{{ od_node_name }}_http.pem""
      - ""{{ od_node_name }}_elasticsearch_config_snippet.yml""
      block: ""{{ lookup('file', '{{ local_certs_path }}/certs/{{ od_node_name }}_elasticsearch_config_snippet.yml') }}""
      -h {{ hostvars[od_node_name]['ip'] }}",307,"od_node_name: ""{{ elasticsearch_node_name }}""; od_node_name: ""{{ kibana_node_name }}""; od_node_name: ""{{ kibana_node_name }}""","od_node_name: elasticsearch_node_name; od_node_name: kibana_node_name; od_node_name: filebeat_node_name; {{ od_node_name }}.key; {{ od_node_name }}.pem; {{ od_node_name }}_http.key; {{ od_node_name }}_http.pem; {{ od_node_name }}_elasticsearch_config_snippet.yml; lookup('file', '{{ local_certs_path }}/certs/{{ od_node_name }}_elasticsearch_config_snippet.yml'); hostvars[od_node_name]['ip']",1,1,1,1
308,"- {
      role: geerlingguy.repo-epel,
      version: 1.2.3,
      tags: [
        ""dependency"",
        ""dependency.epel""
      ],
      when: ansible_os_family == 'RedHat'
  }
  - {
      role: srsp.oracle-java,
      version: 2.19.1,
      tags: [
        ""dependency"",
        ""dependency.java""
      ]
  }
  - {
      role: gantsign.maven,
      version: 4.0.0,
      tags: [
        ""dependency,"",
        ""dependency.maven""
      ]
  }
  - { role: andrewrothstein.terraform,
      version: v2.2.10,
      tags: [
        ""dependency"",
        ""dependency.terraform""
      ]
  }","dependencies:
  - { role: geerlingguy.repo-epel, tags: [""dependency""] }
  - { role: srsp.oracle-java, tags: [""dependency""] }
  - { role: gantsign.maven, tags: [""dependency""] }
  - { role: andrewrothstein.terraform, tags: [""dependency""] }",308,No issue found,No issue found,0,0,0,0