Added_lines_fixing_commit,Fixing_Script_Misconfiguration_Label,Fixing_Script_MisconfigurationFoundbyLLM,Fixing_Script_CWE,varification_fix,Added_lines_bug_inducing_commit,Bug_Inducing_Script_Misconfiguration_Label,Bug_Inducing_Script_MisconfigurationFoundbyLLM,Bug_Inducing_Script_CWE,varification_inducing
"command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""",1,"php occ config:set foo \, php occ config:set foo \\, php occ config:set foo \\\, php occ config:set foo \\\\\\\, php occ config:set foo \\\\\\\\\\\\\\\\, php occ config:set foo \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, php occ config:set foo \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-772,1,"- name: ""[NC] - Set Nextcloud settings in config.php""
  shell: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_config_settings }}""
- name: ""[NC] - Set Redis Server""
  command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_redis_settings }}""
  when: nextcloud_install_redis_server == True

    mode: 0750",0,"command: php occ config:system:set \, mode: 0750, name: \, name: \\, shell: php occ config:system:set \\, shell: php occ config:system:set {{ item.name }} --value=\, shell: php occ config:system:set {{ item.name }} --value=\\, when: nextcloud_install_redis_server == True","CWE-276, CWE-277, CWE-278, CWE-279, CWE-280, CWE-282, CWE-283, CWE-284, CWE-327",1
when: (not _nextcloud_conf.stat.exists) or (_nextcloud_configured.rc is defined and _nextcloud_configured.rc != 0),1,when: (not _nextcloud_conf.stat.exists) or (_nextcloud_configured.rc is defined and _nextcloud_configured.rc!= 0),CWE-259,0,"check_mode: no
  when: _nextcloud_configured.rc != 0",0,"check_mode: no\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, check_mode: no\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when: _nextcloud_configured.rc!= 0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, check_mode: no\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when: _nextcloud_configured.rc!= 0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, check_mode: no\\\\\\\\\\\\\\\\\\\\\\\\n  when: _nextcloud_configured.rc!= 0\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\n, check_mode: no\\\\\\\\n  when: _nextcloud_configured.rc!= 0\\\\\\\\n  \\\\\\\\n\\\\\\\\n\\\\\\\\n, check_mode: no\\\\n  when: _nextcloud_configured.rc!= 0\\\\n  \\\\n\\\\n  \\\\n, check_mode: no\\n  when: _nextcloud_configured.rc!= 0\\n  \\n\\n  \\n, check_mode: no\\n  when: _nextcloud_configured.rc!= 0\\n  \\n\\n\\n, check_mode: no\n  when: _nextcloud_configured.rc!= 0\n  \n\n  \n","CWE-1111, CWE-1234",0
when: (nc_sudo_installed_result.rc is defined) and (nc_sudo_installed_result.rc != 0),1,when: (nc_sudo_installed_result.rc is defined) and (nc_sudo_installed_result.rc!= 0),"CWE-248, CWE-249, CWE-257",0,"register: nc_sudo_installed_result
  when: nc_sudo_installed_result is defined and nc_sudo_installed_result.rc != 0",0,"become: yes\\\\\\\\nbecome_method: sudo\\\\\\\\nbecome_user: root\\\\\\\\nbecome_timeout: 300\\\\\\\\nbecome_flags: \\\\\\\\\n, become: yes\\nbecome_method: sudo\\nbecome_user: root\\nbecome_timeout: 300\\nbecome_flags: \\\, become:\\\\\\nbecome: yes\\\\\\\\nbecome_method: sudo\\\\\\\\nbecome_user: root\\\\\\\\nbecome_timeout: 300\\\\\\nbecome_flags: \\\\, become:\\\\nbecome: yes\\\\nbecome_method: sudo\\\\nbecome_user: root\\\\nbecome_timeout: 300, nc.ssh_password: \, nc.ssh_pubkey_auth_timeout: 1, nc.ssh_pubkey_privileges_escalation: yes\\\\n\\n, nc.ssh_public_key: \\, nc.ssh_pw_host_based_auth: yes, register: nc_sudo_installed_result\n when: nc_sudo_installed_result is defined and nc_sudo_installed_result.rc!= 0\n","CWE-248, CWE-250, CWE-269, CWE-319, CWE-521",1
"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
",1,"action: \, action: \\, action: \\\\, action:",CWE-20,1,"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""",0,action:,CWE-2000,0
"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
",1,"action: \, action: \\, action: \\\\, action: \\\\\\\, action: \\\\\\\\\\\\\\\\\\, action: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, action: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-276,1,"##
 # BROWSER
 #
 
- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
#TODO sudo ln -s /usr/lib/chromium-browser/chromedriver /var/lib/chromedriver
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
#TODO sudo ln -s /usr/bin/chromedrive /var/lib/chromedriver  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes",0,"    - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n  - chromedriver # see selenium role\\n,   with_items: \\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n  - chromedriver # see selenium role\\\\\\n,   with_items:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,   with_items:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role,   with_items:\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - chrom,   with_items:\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\\\n,   with_items:\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n  - chromedriver # see selenium role\n , - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n  - chromedriver # see selenium role\\\\n, with_items:\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n  - chromedriver # see selenium role\\\\\\\\\\n ",CWE-949,0
"action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
",1,"action: \, action: \\, action: \\\, action: \\\\, action: \\\\\\\\\\\\, action: \\\\\\\\\\\\\\, action: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, action: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, action: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, action: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-122, CWE-20, CWE-22",0,"action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""",0,"action:, state=",CWE-77,0
"action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
",1,action:,CWE-200,0,"##
 # BROWSER
 #
 
- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
#TODO sudo ln -s /usr/lib/chromium-browser/chromedriver /var/lib/chromedriver
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
#TODO sudo ln -s /usr/bin/chromedrive /var/lib/chromedriver  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes",0,"chromedriver # see selenium role, ln -s /usr/bin/chromedriver /var/lib/chromedriver, ln -s /usr/lib/chromium-browser/chromedriver /var/lib/chromedriver","CWE-272, CWE-276",0
"- (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare(8, '<'))",1,"ansible_distribution_version!= 'buster/sid' and ansible_distribution_version is version_compare(8, '<')","CWE-20, CWE-327, CWE-601, CWE-602, CWE-700, CWE-701, CWE-749, CWE-775, CWE-777, CWE-798",1,"- (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare
(8, '<'))",0,"8, and, and (ansible_distribution_version is version_compare\n(8, '<'), ansible_distribution_version, ansible_distribution_version!= 'buster/sid', is, version_compare",CWE-1037,1
"owner: ""{{ consul_user }}""
    group: ""{{ consul_group }}""",1,"group: \\, group: \\\\\\\, group: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, owner: \, owner: \\\, owner: \\\\\\\\\\\\\\\\\\, owner: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-200,0,"file:
    dest: ""{{ consul_tls_dir }}""
    state: directory
    owner: root
    group: root
    mode: 0755
    copy:
      src: ""{{ consul_src_files }}/{{ consul_ca_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_ca_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_key }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_key }}""
  template:
    src: config_server_tls.json.j2
    dest: ""{{ consul_config_path }}/server/config_server_tls.json""",0,"copy:\\\\n  src: \\\\, copy:\\n  src: \\, copy:\n  src: \, group: root\\\\n, mode: 0755\\n, mode: 0755\n, owner: root\\n, state: directory\\\\n, state: directory\\n",CWE-778,0
"url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ item }}""
    url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script""",1,"url: \, url: \\, url: \\\\, url: \\\\\\\\, url: \\\\\\\\\\\\\\\\\\\\\\\\, url: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-352,0,"url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}/service/siesta/rest/v1/script/{{ item }}""",0,"http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}/service/siesta/rest/v1/script/{{ item }}, url: \","CWE-326, CWE-732, CWE-772",1
"- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: ""modify auth user admin password {{admin_password}}""
  register: change_password
  until: change_password is not failed
  retries: 5",1,"commands: \, commands: \\, playbook_dir: \\\\, provider:\n  ssh_keyfile: \, register: change_password, retries: 5, transport: cli, until: change_password is not failed, user: admin \\\\","CWE-312, CWE-798",0,"- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: modify auth user admin password admin",0,"commands: modify auth user admin password admin, modify auth user admin password admin, server: {{ ansible_host }}, ssh_keyfile: {{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem, user: admin","CWE-220, CWE-319, CWE-863",0
"- name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""{{ec2_info.arista.filter}}""
        architecture: ""x86_64""
    register: arista_amis",1,"ec2_ami_facts, ec2_info.arista.filter, ec2_region, owners, register: arista_amis","CWE-129, CWE-20, CWE-200, CWE-25, CWE-250, CWE-257, CWE-258, CWE-264, CWE-265",0,"#### CISCO AMI
- name: BLOCK FOR CISCO AMI
  block:
  - name: find ami for cisco (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""cisco-CSR*BYOL*""
        architecture: ""x86_64""
    register: cisco_ami_list

  - name: save ami for cisco (NETWORKING MODE)
    set_fact:
      cisco_ami: >
        {{ cisco_ami_list.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""cisco""'

#### ARISTA AMI
- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""*EOS*""
        architecture: ""x86_64""
    register: arista_amis

  - name: save ami for arista eos (NETWORKING MODE)
    set_fact:
      arista_ami: >
        {{ arista_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""arista""'

#### JUNIPER AMI
- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for juniper vsrx (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""junos-vsrx3-x86-64-18.4R1.8--pm*""
        architecture: ""x86_64""
    register: juniper_amis

  - name: save ami for juniper (NETWORKING MODE)
    set_fact:
      juniper_ami: >
        {{ juniper_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""juniper""'",0,"#### CISCO AMI\n- name: BLOCK FOR CISCO AMI\n  block:\n  - name: find ami for cisco (NETWORKING MODE)\n    ec2_ami_facts:\n      region: \, - name: find ami for cisco (NETWORKING MODE)\\\\n    ec2_ami_facts:\\n      region: \\, - name: find ami for juniper vsrx (NETWORKING MODE)\\n    ec2_ami_facts:\\n      region: \\, arista_ami: \\\\, block:\\\\n  - name: find ami for arista (NETWORKING MODE)\\\\n    ec2_ami_facts:\\\\n      region: \\, cisco_ami: \\, ec2_ami_facts:\\n      region: \\\\, juniper_ami: \, junos-vsrx3-x86-64-18.4R1.8--pm*\\n        architecture: \, register: arista_amis\\n  - name: save ami for arista eos (NETWORKING MODE)\\n    set_fact:\\n      arista_ami: \",CWE-502,1
"- name: fail on purpose now to let user know code server failed
      debug:
        msg: ""VS code integration has failed in provisioner/roles/code_server/tasks/main.yml""
      failed_when: true",1,debug:,CWE-1191,0,"---
- name: remove dns entries for each vs code instance
  include_tasks: teardown.yml
  when: teardown|bool

- name: check to see if SSL cert already applied
  become: no
  get_certificate:
    host: ""{{username}}-code.{{ec2_name_prefix|lower}}.{{workshop_dns_zone}}""
    port: 443
  delegate_to: localhost
  run_once: true
  register: check_cert
  ignore_errors: true
  when:
    - not teardown

- name: perform DNS and SSL certs for ansible control node
  block:
    - name: setup vscode for web browser access
      include_tasks: ""codeserver.yml""
  rescue:
    - debug:
        msg: 'VS code integration has failed'

    - name: make sure tower is on
      shell: ansible-tower-service start
      register: install_tower
      until: install_tower is not failed
      retries: 5

    - name: appends
      set_fact:
        coder_information: |
          - VS code integration has failed, please use direct SSH addresses
      run_once: true
      delegate_to: localhost
      delegate_facts: true
  when:
    - not teardown|bool
    - check_cert is failed",0,"block:, delegate_to: localhost\\n  delegate_facts: true, include_tasks: codeserver.yml, perform DNS and SSL certs for ansible control node\\n  block:, perform DNS and SSL certs for ansible control node\n  block:, run_once: true\\\\n  delegate_to: localhost, run_once: true\\n  delegate_to: localhost, when:\\\\n    - not teardown|bool, when:\\n    - check_cert is failed",CWE-916,1
"command: systemctl daemon-reload
  tags: skip_ansible_lint",1,"command: systemctl daemon-reload, tags: skip_ansible_lint","CWE-1169, CWE-1257, CWE-126, CWE-127, CWE-129, CWE-256",0,"systemd:
    daemon-reload: yes",0,"daemon-reload, systemd, yes","CWE-279, CWE-294, CWE-295",1
"copy: src={{ caddy_home }}/caddy dest=/usr/bin/ mode=0755 remote_src=True
  notify: Restart Caddy",1,"copy: src={{ caddy_home }}/caddy dest=/usr/bin/ mode=0755 remote_src=True, notify: Restart Caddy","CWE-20, CWE-89",0,"unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} copy=no
  unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no

- name: Copy Caddy Binary
  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True",0,"unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- name: Copy Caddy Binary\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True, unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- name: Copy Caddy Binary\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True, unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no\\\\\\\\\\\\\\\\n- name: Copy Caddy Binary\\\\\\\\\\\\\\\\n  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True, unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no\\\\\\\\n- name: Copy Caddy Binary\\\\\\\\n  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True, unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no\\\\n- name: Copy Caddy Binary\\\\n  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True, unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no\\n- name: Copy Caddy Binary\\n  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True, unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no\n- name: Copy Caddy Binary\n  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True","CWE-77, CWE-78, CWE-79, CWE-80, CWE-81, CWE-82, CWE-83, CWE-84, CWE-85, CWE-86",1
"retries: 3
  delay: 2
  retries: 3
  delay: 2",1,"delay: 2, retries: 3",CWE-1046,0,"user:
    name: ""{{ caddy_user }}""
    system: yes
    createhome: yes
    home: ""{{ caddy_home }}""
  get_url:
    url: https://api.github.com/repos/mholt/caddy/git/refs/tags
    dest: ""{{ caddy_home }}/releases.txt""
    force: yes
  copy:
    content: ""{{ caddy_features }}""
    dest: ""{{ caddy_home }}/features.txt""
  get_url:
    url: ""{{ caddy_url | quote }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    force: yes
    timeout: 300
    retries: 3
    delay: 2
  get_url:
    url: ""{{ caddy_url}}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    timeout: 300
    retries: 3
    delay: 2
  command: >
    gpg
      --keyserver-options timeout={{ caddy_pgp_recv_timeout }}
      --keyserver {{ caddy_pgp_key_server }}
      --recv-keys {{ caddy_pgp_key_id }}
  get_url:
    url: ""{{ caddy_sig_url }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz.asc""
    timeout: 60
    force: yes
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
  command: >
    gpg
      --verify {{ caddy_home }}/caddy.tar.gz.asc
      {{ caddy_home }}/caddy.tar.gz
  unarchive:
    src: ""{{ caddy_home }}/caddy.tar.gz""
    dest: ""{{ caddy_home }}""
    copy: no
    owner: ""{{ caddy_user }}""
  unarchive:
   src: ""{{ caddy_home }}/caddy.tar.gz""
   dest: ""{{ caddy_home }}""
   creates: ""{{ caddy_home }}/caddy""
   copy: no
   owner: ""{{ caddy_user }}""
  copy:
    src: ""{{ caddy_home }}/caddy""
    dest: ""{{ caddy_bin }}""
    mode: 0755
    remote_src: true
  file:
    path: ""{{ item }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0770
  file:
    path: ""{{ caddy_log_dir }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0775
  copy:
    content: ""{{ caddy_config }}""
    dest: ""{{ caddy_conf_dir }}/Caddyfile""
    owner: ""{{ caddy_user }}""
  stat:
    path: /run/systemd/system
  template:
    src: ""{{ item }}""
    dest: /etc/init/caddy.conf
    mode: 0644
  template:
    src: caddy.service
    dest: /etc/systemd/system/caddy.service
    mode: 0644
  service:
    name: caddy
    state: started
    enabled: yes",0,"content:, force_basic_auth: yes, mode: 0644, mode: 0770, mode: 0775, state: directory, system: yes, timeout: 300, timeout: 60","CWE-275, CWE-276, CWE-277, CWE-285",1
"shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'""",1,"psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'\, psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'\\, psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'\\\\, psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'\\\\\\\, psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'\\\\\\\\\\\\\\\\\, psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: \, shell: \\\, shell: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-117, CWE-78, CWE-79",1,"when: item.hstore is defined and item.hstore

- name: PostgreSQL | Add uuid-ossp to the database with the requirement
  sudo: yes
  sudo_user: ""{{postgresql_admin_user}}""
  shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;""
  with_items: postgresql_databases
  when: item.uuid_ossp is defined and item.uuid_ossp",0,"name: PostgreSQL | Add uuid-ossp to the database with the requirement \\\\\\\\, psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;, shell: \, shell: \\, sudo: yes \\, sudo_user: \, sudo_user: \\, when: item.hstore is defined and item.hstore \\\\\\\\, when: item.uuid_ossp is defined and item.uuid_ossp \\\\, with_items: postgresql_databases \\\\","CWE-190, CWE-20, CWE-221, CWE-250",1
"name: ""postgresql-contrib-{{postgresql_version}}""",1,"become, debug, gather_facts, name: \, no_log, playbook_name, tag, tags, vars","CWE-770, CWE-772, CWE-773, CWE-775, CWE-778, CWE-787",,"# file: postgresql/tasks/extensions/contrib.yml

- name: PostgreSQL | Extensions | Make sure the development headers are installed
  apt:
    name: libpq-dev
    state: present
  notify:
    - restart postgresql",0,"apt:\\\\n name: libpq-dev, apt:\\n name: libpq-dev\\n state: present, name: libpq-dev, name: libpq-dev\nstate: present, name: postgresql, name: postgresql\\nstate: present, state: present",CWE-119,0
"run_once: True
  run_once: True
- name: 'Create folder {{structured_dir_name}} for structured YAML files'
  run_once: True
  run_once: True

- name: 'Create folder {{structured_cvp_name}} for CVP structured YAML files'
  file:
    path: '{{structured_cvp}}'
    state: directory
    mode: 0755
  delegate_to: localhost
  run_once: True
  run_once: True
  run_once: True
  run_once: True",1,run_once: True,"CWE-12345, CWE-12346, CWE-12347, CWE-12348, CWE-12349, CWE-12350, CWE-12351, CWE-12352, CWE-12353, CWE-12354",,"---
# tasks file for build_directories

- name: 'Cleanup existing folders in {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: absent 
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{structured_dir_name}} for structued YAML files'
  file:
    path: '{{structured_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{eos_config_dir_name}} for EOS Configuration files'
  file:
    path: '{{eos_config_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost",0,"file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n path: '{{structured_dir_name}}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n state: directory\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n mode: 0755\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n delegate_to: localhost, file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n path: '{{eos_config_dir_name}}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\n state: directory\\\\\\\\\\\\\\\\\\\\\\\\\\\\n mode: 0755\\\\\\\\\\\\\\\\\\\\\\\\\\\\n delegate_to: localhost, file:\\\\\\\\\\\\n path: '{{output_dir}}'\\\\\\\\\\\\n state: directory\\\\\\\\\\\\n mode: 0755\\\\\\\\\\\\n delegate_to: localhost, file:\\\\n path: '{{eos_config_dir}}'\\\\n state: directory\\\\n mode: 0755\\\\n delegate_to: localhost, file:\\n path: '{{structured_dir_name}}'\\n state: directory\\n mode: 0755\\n delegate_to: localhost, file:\n path: '{{structured_dir}}'\n state: directory\n mode: 0755\n delegate_to: localhost",CWE-200,0
- name: Create/invoke script virtualenv for create galaxy admin,1,"- shell: sudo groupadd -r {{inventory_hostname}}\\n- shell: sudo groupadd -r {{inventory_hostname}}_user\\n- shell: sudo useradd -r -u 1 -d /home/{{inventory_hostname}}_user -s /bin/bash {{inventory_hostname}}_user\\n- shell: sudo useradd -r -u 2 -d /home/{{inventory_hostname}}_user_1 -s /bin/bash {{inventory_hostname}}_user_1, script: 'virtualenv create\\\\\\\\\\\\nscript:. /etc/ansible/magic.sh\\\\\\\\\\\\n- shell: sudo useradd -m -d /home/{{inventory_hostname}}_user -s /bin/bash {{inventory_hostname}}_user\\\\\\\\\\\\n- shell: sudo chown -R {{inventory_hostname}}_user:{{inventory_hostname}}_user /home/{{inventory_hostname}}_user, script: 'virtualenv create\\\\\\\\nscript:. /etc/ansible/magic.sh\\\\\\\\n- shell: sudo useradd -m -d /home/{{inventory_hostname}}_user -s /bin/bash {{inventory_hostname}}_user\\\\\\\\n- shell: sudo chown -R {{inventory_hostname}}_user:{{inventory_hostname}}_user /home/{{inventory_hostname}}_user, script: 'virtualenv create\\\\nscript:. /etc/ansible/magic.sh\\\\n- shell: sudo useradd -m -d /home/{{inventory_hostname}}_user -s /bin/bash {{inventory_hostname}}_user\\\\n- shell: sudo chown -R {{inventory_hostname}}_user:{{inventory_hostname}}_user /home/{{inventory_hostname}}_user, script: 'virtualenv create\\nscript:. /etc/ansible/magic.sh\\n- shell: sudo useradd -m -d /home/{{inventory_hostname}}_user -s /bin/bash {{inventory_hostname}}_user\\n- shell: sudo chown -R {{inventory_hostname}}_user:{{inventory_hostname}}_user /home/{{inventory_hostname}}_user, script: 'virtualenv create\nscript:. /etc/ansible/magic.sh\n- shell: sudo useradd -m -d /home/{{inventory_hostname}}_user -s /bin/bash {{inventory_hostname}}_user\n- shell: sudo chown -R {{inventory_hostname}}_user:{{inventory_hostname}}_user /home/{{inventory_hostname}}_user",CWE-1031,,"- name: Create/invoke script virtualenv
  pip: name={{ item }} virtualenv={{ galaxy_venv_dir }} virtualenv_command=""{{ pip_virtualenv_command | default( 'virtualenv' ) }}""
  with_items:
    - pyyaml
    - bioblend
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""

- name: Create Galaxy admin user
  command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python /usr/local/bin/create_galaxy_user.py --user {{ galaxy_admin }} --password {{ galaxy_admin_pw }} --key {{ galaxy_admin_api_key }}
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""

#- name: Copy the bootstrap user management script
#  copy: src=manage_bootstrap_user.py dest={{ galaxy_server_dir }}/manage_bootstrap_user.py owner={{ galaxy_user_name }}

#- name: Create Galaxy bootstrap user
#  command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python manage_bootstrap_user.py -c {{ galaxy_config_file }} create -e {{ galaxy_admin }} -p """"

#- name: Remove the bootstrap user management script
#  file: dest={{ galaxy_server_dir }}/manage_bootstrap_user.py state=absent",0,"python manage_bootstrap_user.py -c {{ galaxy_config_file }} create -e {{ galaxy_admin }} -p \, sudo_user:","CWE-601, CWE-78",1
"shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /etc/supervisor/conf.d/galaxy.conf""
- debug: var=galaxylogs

- name: get_logs
  shell: ""supervisorctl status""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs",1,"debug: var=galaxylogs, register: galaxylogs\\\\n  ignore_errors: yes, register: galaxylogs\\n  ignore_errors: yes, register: galaxylogs\n  ignore_errors: yes, shell: \, shell: \\, shell: \\\\","CWE-275, CWE-276",0,"- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs

- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs",0,"debug var=galaxy_logs, shell: cat /var/log/supervior/galaxy*","CWE-1161, CWE-319",1
"- {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/allowed_images.yml""}",1,"dest: \\\, dest: \\\\\\\\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, src: \, src: \\\\\\, src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-352,0,"- {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/allowed_images.yml""}",0,"allowed_images.yml.j2, rstudio.ini.j2","CWE-120, CWE-200, CWE-347, CWE-863",1
"- export NATIVE_SPEC=""--ntasks=`/usr/bin/nproc` --share""",1,export NATIVE_SPEC=--ntasks=`/usr/bin/nproc` --share,CWE-1034,0,"supervisor_env_vars:
    - export IP_ADDRESS=`curl icanhazip.com`
    - export GALAXY_CONF_FTP_UPLOAD_SITE=""ftp://$IP_ADDRESS""
    - export MASQUERADE_ADDRESS=$IP_ADDRESS
    - export NTASK=""--ntasks=`/usr/bin/nproc` --share""",0,"export GALAXY_CONF_FTP_UPLOAD_SITE=ftp://$IP_ADDRESS, export IP_ADDRESS=`curl icanhazip.com`, export MASQUERADE_ADDRESS=$IP_ADDRESS, export NTASK=--ntasks=`/usr/bin/nproc` --share","CWE-1197, CWE-305, CWE-306, CWE-307, CWE-310, CWE-320, CWE-89, CWE-94",0
"########## BOTH ########## 


- name: create data directory for cassandra on mounted storage
  file: path={{data_dir}}/cassandra state=directory owner={{cassandra_user}}  group={{cassandra_user}}

- name: create the cassandra yaml from template
  template: src=cassandra/cassandra.yaml dest={{cassandra_yaml}} owner={{cassandra_user}}  group={{cassandra_user}}
    - restart cassandra",1,"- name: create data directory for cassandra on mounted storage\n  file: path={{data_dir}}/cassandra state=directory owner={{cassandra_user}}  group={{cassandra_user}}, - name: create the cassandra yaml from template\\n  template: src=cassandra/cassandra.yaml dest={{cassandra_yaml}} owner={{cassandra_user}}  group={{cassandra_user}}\\n    - restart cassandra, - restart cassandra, owner={{cassandra_user}}  group={{cassandra_user}}",CWE-306,0,"########## REDHAT ########## 

- name: setup yum repo
  copy: src=yum/datastax.repo dest=/etc/yum.repos.d/datastax.repo
  when: ansible_os_family == ""RedHat""   

- name: install packages (Redhat)
  yum: name={{item}} state=present
  with_items:
    - $datastax
    - $cassandra
  tags:
    - packages 
  notify: 
    - restart cassandra
    - configure cassandra
  when: ansible_os_family == ""RedHat"" 

- name: set cassandra to use the ByteOrderedPartitioner (RedHat)
  lineinfile: "" 
    dest=/etc/cassandra/conf/cassandra.yaml  
    regexp=^partitioner 
    line='partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner'""
  notify:
    - restart cassandra
  when: ansible_os_family == ""RedHat""     
 

########## DEBIAN ########## 

- name: install packages (Debian)
  apt: pkg=wget  state=present
  when: ansible_os_family == ""Debian"" 

- name: install packages (Debian)
  apt: pkg=software-properties-common  state=present
  when: ansible_os_family == ""Debian"" 

- name: install packages 2 (Debian)
  apt: pkg=python-software-properties  state=present
  when: ansible_os_family == ""Debian"" 

- name: apt-add-repository (Debian)
  command: apt-add-repository 'deb http://debian.datastax.com/community stable main'
  when: ansible_os_family == ""Debian"" 

- name: get datastax repo key (Debian)
  command: wget http://debian.datastax.com/debian/repo_key --output-document=/tmp/datastax.key
  when: ansible_os_family == ""Debian""   

- name: add datastax repo key  (Debian)
  command: apt-key add /tmp/datastax.key
  when: ansible_os_family == ""Debian""   

- name: install Cassandra (Debian)
  apt: pkg=cassandra=1.2.10 state=present update_cache=yes
  with_items:
    - $cassandra
  tags:
    - packages 
  notify: 
    - restart cassandra
    - configure cassandra
  when: ansible_os_family == ""Debian"" 


- name: set cassandra to use the ByteOrderedPartitioner  (Debian)
  lineinfile: "" 
    dest=/etc/cassandra/cassandra.yaml  
    regexp=^partitioner 
    line='partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner'""
  notify:
    - restart cassandra
  when: ansible_os_family == ""Debian""


- name: increase stack size (Debian only)
  shell: ""sed -e s/Xss180k/Xss256k/g /etc/cassandra/cassandra-env.sh --in-place""
  when: ansible_os_family == ""Debian""

- name: setup production settings for Cassandra (not overwrites files)
  copy: src={{item.src}} dest={{item.dest}}
  with_items:
    - { src: cassandra/90-nproc.conf, dest: /etc/security/limits.d/90-nproc.conf }
    - { src: cassandra/limits.conf, dest: /etc/security/limits.conf }
  notify: 
    - restart cassandra",0,"- name: get datastax repo key (Debian)\n  command: wget http://debian.datastax.com/debian/repo_key --output-document=/tmp/datastax.key\n  when: ansible_os_family == \, name: add datastax repo key  (Debian)\\n  command: apt-key add /tmp/datastax.key\\n  when: ansible_os_family == \\, name: apt-add-repository (Debian)\\\\n  command: apt-add-repository 'deb http://debian.datastax.com/community stable main'\\\\n  when: ansible_os_family == \\, name: get datastax repo key (Debian)\\\\\\\\n  command: wget http://debian.datastax.com/debian/repo_key --output-document=/tmp/datastax.key\\\\\\\\n  when: ansible_os_family == \\, name: increase stack size (Debian only)\\\\n  shell: \, name: install packages (Debian)\\\\\\\\\\\\\\\\n  apt: pkg=wget  state=present\\\\\\\\\\\\\\\\n  when: ansible_os_family == \\\\\\\\\\\\\\\\, name: install packages (Debian)\\\\\\\\n  apt: pkg=wget  state=present\\\\\\\\n  when: ansible_os_family == \\\\\\\\, name: install packages (Debian)\\\\n  apt: pkg=software-properties-common  state=present\\\\n  when: ansible_os_family == \\\\, name: install packages (Debian)\\n  apt: pkg=python-software-properties  state=present\\n  when: ansible_os_family == \\, name: install packages 2 (Debian)\\\\n  apt: pkg=software-properties-common  state=present\\\\n  when: ansible_os_family == \\\\","CWE-1008, CWE-1022",1
"dir: '{{ data_dir }}/spatial-data'
  url: '{{ geoserver_url }}'
  geoserver_data_dir: ""{{ geoserver_data_dir | default('/data/spatial-data/geoserver_data_dir') }}""
biocacheServiceUrl: ""{{ biocache_service_url | default('https://biocache.ala.org.au/ws') }}""
biocacheUrl: ""{{ biocache_url | default('https://biocache.ala.org.au') }}""
spatialService.url: ""{{ spatial_service_url }}""
grails.serverURL: {{spatial_service_url}}",1,"biocacheServiceUrl: '{{ biocache_service_url | default('https://biocache.ala.org.au/ws') }}', biocacheUrl: '{{ biocache_url | default('https://biocache.ala.org.au') }}', geoserver_data_dir: '{{ geoserver_data_dir | default('/data/spatial-data/geoserver_data_dir') }}', grails.serverURL: {{spatial_service_url}}, spatialService.url: '{{ spatial_service_url }}'","CWE-1026, CWE-1027",0,"#
# au.org.ala.spatial.service config
#
data:
  dir: '{{data_dir}}/spatial-data'
geoserver:
  url: '{{geoserver_url}}'
  username: '{{ geoserver_username | default('admin') }}'
  password: '{{ geoserver_password | default('geoserver') }}'
  canDeploy: {{ can_deploy_to_geoserver | default('true') }}
# To use a remote geoserver instance, set geoserver.remote.geoserver_data_dir to the geoserver_data_dir path on the
# remote server. This will cause layer files to be copied to geoserver_data_dir/data/
#  remote:
  geoserver_data_dir: ""{{ geoserver_data_dir | default('{{data_dir}}/geoserver_data_dir') }}""

shpResolutions: {{ shp_resolutions | default([0.5, 0.25, 0.1, 0.05]) }}
grdResolutions: {{ grd_resolutions | default([0.5, 0.25, 0.1, 0.05, 0.01]) }}

biocacheServiceUrl: ""{{biocache_service_url | default('https://biocache.ala.org.au/ws')}}""
biocacheUrl: ""{{biocache_url | default('https://biocache.ala.org.au')}}""
openstreetmap:
  url: ""{{ openstreetmap_tile_url | default('https://tile.openstreetmap.org') }}""

slave.enable: {{ slave_enable | default(true) }}
service.enable: {{ service_enable | default(true) }}

serviceKey: {{ spatial_service_service_key  | default('') }}
batch_sampling_passwords: ""{{ batch_sampling_passwords | default('') }}""
batch_sampling_points_limit: {{ batch_sampling_points_limit | default(1000000) }}
batch_sampling_fields_limit: {{ batch_sampling_fields_limit | default(1000) }}

---
#
# au.org.ala.spatial.slave config
#
spatialService.url: ""{{spatial_service_url}}""
data.dir: ""{{ data_dir }}/spatial-data""
shp2pgsql.path: ""{{ shp2pgsql_path | default('/usr/bin/shp2pgsql') }}""
gdal.dir: ""{{ gdal_dir | default('/usr/bin/') }}""
gdm.dir: ""{{ gdm_dir | default('/data/spatial-data/modelling/gdm/DoGdm') }}""

aloc.xmx: ""{{ aloc_xmx | default('6G') }}""
aloc.threads: ""{{ aloc_threads | default(4) }}""
maxent.mx: ""{{ maxent_mx | default('1G') }}""
maxent.threads: ""{{ maxent_threads | default(4) }}""

sampling.threads: ""{{ sampling_threads | default(4) }}""

slaveKey: ""{{ spatial_service_slave_key | default('') }}""
serviceKey: ""{{ spatial_service_service_key | default('') }}""

# time between pushing status updates to the master for a task
statusTime: ""{{ status_time | default(3000) }}""
retryCount: ""{{ retry_count | default(10) }}""
retryTime: ""{{ retry_time | default(30000) }}""

#
#  CAS SETTINGS
#
#  NOTE: Some of these will be ignored if default_config exists
security:
  cas:
    casServerName: {{ auth_base_url }}
    uriFilterPattern: {{ uri_filter_pattern | default('/manageLayers,/manageLayers/.*,/admin,/admin/.*,/alaAdmin.*') }}
    uriExclusionFilterPattern: {{ uri_exclusion_filter_pattern | default('/assets.*,/images.*,/css.*,/js.*,/less.*,/tasks/status/.*') }}
    authenticateOnlyIfLoggedInFilterPattern: {{ authenticate_only_if_logged_in_filter_pattern | default('/master,/master/.*,/tasks,/tasks/.*') }}
    appServerName: {{ spatial_service_base_url }}
    casServerUrlPrefix: {{ auth_cas_url }}
    loginUrl: {{ auth_cas_url }}/login
    logoutUrl: {{ auth_cas_url }}/logout
    contextPath: {{ spatial_service_context_path }}
    bypass: {{ bypass_cas | default(true) }}
    disableCAS: {{ bypass_cas | default(true) }}
    gateway: {{ gateway_cas | default(false) }}

auth.admin_role: {{ auth_admin_role | default('ROLE_ADMIN') }}
app.http.header.userId: {{ app_http_header_userid | default('X-ALA-userId') }}

headerAndFooter.baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3') }}
ala.baseURL: {{ ala_base_url | default('https://www.ala.org.au') }}
bie.baseURL: {{ bie_base_url | default('https://bie.ala.org.au') }}
bie.searchPath: '/search'

records.url: {{ records_url | default('https://archives.ala.org.au/archives/exports/lat_lon_taxon.zip') }}

api_key: {{ spatial_service_api_key  | default('') }}
lists.url: {{ lists_url | default('https://lists.ala.org.au') }}
collections.url: {{ collections_url | default('https://collections.ala.org.au') }}
sandboxHubUrl: {{ sandbox_url | default('http://sandbox.ala.org.au/ala-hub') }}
sandboxBiocacheServiceUrl: {{sandbox_biocache_service_url | default('http://sandbox.ala.org.au/biocache-service') }}
phyloServiceUrl: {{ phylolink_url | default('https://phylolink.ala.org.au') }}

spatialHubUrl: {{ spatial_hub_url }}

gazField: {{ gaz_field | default('cl915') }}
userObjectsField: {{ user_objects_field | default('cl1083') }}

apiKeyCheckUrlTemplate: ""{{api_key_check_url_template | default('https://auth.ala.org.au/apikey/ws/check?apikey={0}') }}""
spatialService.remote: ""{{spatial_service_remote_url}}""

journalmap.api_key: {{ journalmap_api_key | default('') }}
journalmap.url: {{ journalmap_url | default('https://www.journalmap.org/') }}

# For side by side installation with layers-service, analysis-service
#legacy.workingdir: '/{{ data_dir }}/ala/data/alaspatial/'

#legacy.enabled: true

#legacy compatability type
#""link"" = link legacy files into new locations
#""copy"" = copy legacy files into new locations
#""move"" = move legacy files into new locations
#legacy.type=""link""

#legacy.ANALYSIS_LAYER_FILES_PATH: '{{ data_dir }}/ala/data/layers/analysis/'
#legacy.LAYER_FILES_PATH: '{{ data_dir }}/ala/data/layers/ready'
#legacy.ALASPATIAL_OUTPUT_PATH: '{{ data_dir }}/ala/runtime/output'

grails.plugin.elfinder.rootDir: '{{ data_dir }}/spatial-service'

i18n.override.dir: '{{ data_dir }}/spatial-service/config/i81n/'


#layers-store config

#Threads created for each batch intersection and each individual shape file
#layers_store.BATCH_THREAD_COUNT: 3

#Set LAYER_INDEX_URL to use REMOVE layer intersections.
#layers_store.LAYER_INDEX_URL: https://spatial.ala.org.au/layers-service

#Use local layer files for sampling or the /intersect/batch service provided by LAYER_INDEX_URL
#layers_store.LOCAL_SAMPLING: false
#layers_store.LOCAL_SAMPLING: true

# Set intersect config reload time in ms
#layers_store.CONFIG_RELOAD_WAIT: 12000000

#Comma separated shape file fields to preload, or 'all'
#layers_store.PRELOADED_SHAPE_FILES: all
#layers_store.PRELOADED_SHAPE_FILES: cl22,cl20

# Grid intersection buffer size in bytes.  Must be multiple of 64.
# Only applies to grids > 80MB.
# layers_store.GRID_BUFFER_SIZE=4096
#layers_store.GRID_BUFFER_SIZE: 40960

# Number of GridCacheReader objects to open.
#layers_store.GRID_CACHE_READER_COUNT: 5

# layers_store ingestion
#layers_store.CAN_INGEST_LAYERS: false
#layers_store.CAN_UPDATE_LAYER_DISTANCES: false
#layers_store.CAN_UPDATE_GRID_CACHE: false
#layers_store.CAN_GENERATE_ANALYSIS_FILES: false
#layers_store.CAN_INTERSECT_LAYERS: false
#layers_store.CAN_GENRATE_THUMBNAILS: false

#layers_store.FIELD_STYLES: true

layers_store.GEONETWORK_URL: '{{ geonetwork_url | default('') }}'

distributions.cache.dir: ""{{ data_dir }}/${appName}/mapCache/""
distributions.geoserver.image.url: ""/ALA/wms?service=WMS&version=1.1.0&request=GetMap&sld={{ distribution_image_sld_url | default('https://fish.ala.org.au/data/dist.sld')}}&layers=ALA:aus1,ALA:Distributions&styles=&bbox=109,-47,157,-7&srs=EPSG:4326&format=image/png&width=400&height=400&viewparams=s:""

dataSource:
    url: 'jdbc:postgresql://{{layers_db_host}}/{{layers_db_name}}'
    username: {{layers_db_username}}
    password: {{layers_db_password}}

grails.serverURL: {{spatial_service_base_url}}
grails.app.context: {{spatial_service_context_path}}

skin.orgNameLong: {{ orgNameLong | default('Atlas of Living Australia') }}
skin.orgNameShort: {{ orgNameShort | default('ALA') }}

grails.controllers.upload.maxFileSize: {{ max_request_size | default(524288000) }}
grails.controllers.upload.maxRequestSize: {{ max_request_size | default(524288000) }}",0,"aloc.threads: '{{ aloc_threads | default(4) }}', aloc.xmx: '{{ aloc_xmx | default('6G') }}', auth.admin_role: '{{ auth_admin_role | default('ROLE_ADMIN') }}', auth_base_url: '{{ auth_base_url }}', casServerUrlPrefix: '{{ auth_cas_url }}', gdal.dir: '{{ gdal_dir | default('/usr/bin/') }}', geoserver_username: '{{ geoserver_username | default('admin') }}', serviceKey: '{{ spatial_service_service_key | default('') }}', slaveKey: '{{ spatial_service_slave_key | default('') }}'",CWE-319: Cleartext Storage of Sensitive Information,1
"- name: Add entries for demo into hosts file
  lineinfile: dest=/etc/hosts regexp='^127\.0\.0\.1' line=""127.0.0.1 localhost {{ demo_hostname | default('') }} ala.vagrant.dev ala demo.vagrant1.ala.org.au vagrant1.ala.org.au"" owner=root group=root mode=0644

- name: Ensure data directory exists
    - demo",1,mode=0644,CWE-306,0,"- include: ../../common/tasks/setfacts.yml

- name: ensure data directory exists
  file: path=/srv/{{ demo_hostname }}/www/html state=directory owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - demo

- name: Copy welcome page (Debian)
  template: src=index.html dest=/srv/{{ demo_hostname }}/www/index.html mode=0666
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/biocache-media"" 
  ignore_errors: yes
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/html/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/html/biocache-media"" 
  ignore_errors: yes
  tags: 
    - demo",0,"command: ln -sf /data/biocache-media /srv/demo_hostname/www/biocache-media, file: path=/srv/demo_hostname/www state=directory owner=tomcat_user group=tomcat_user, file: path=/srv/demo_hostname/www/html state=directory owner=tomcat_user group=tomcat_user, ln -sf /data/biocache-media /srv/demo_hostname/www/biocache-media, ln -sf /data/biocache-media /srv/demo_hostname/www/html/biocache-media, template: src=index.html dest=/srv/demo_hostname/www/index.html mode=0666",CWE-22,1
"service-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}
        management-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}/status",1,"management-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}/status, service-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}",CWE-1031,0,"authCookieName: {{ auth_cookie_name | default('ALA-Auth') }}
      enabled: {{ oauth_providers_flickr_enabled | default('true') }}
    inaturalist:
      enabled: {{ oauth_providers_inaturalist_enabled | default('false') }}
      key: {{ oauth_providers_inaturalist_key | default('') }}
      secret: {{ oauth_providers_inaturalist_secret | default('') }}
      callback: ${grails.serverURL}/profile/inaturalistCallback
biocache.search.baseUrl: {{ biocache_base_url }}/occurrences/search
headerAndFooter:
  baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3')}}
  version: {{ header_and_footer_version | default('1')}}
{% if bootadmin_enabled %}
        service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}
#        management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status
{% endif %}
{% if spring_session_redis_clustered %}
{% endif %}",0,"    authCookieName: {{ auth_cookie_name | default('ALA-Auth') }}\\n      enabled: {{ oauth_providers_flickr_enabled | default('true') }}\\n    inaturalist:\\n      enabled: {{ oauth_providers_inaturalist_enabled | default('false') }}\\n      key: {{ oauth_providers_inaturalist_key | default('') }}\\n      secret: {{ oauth_providers_inaturalist_secret | default('') }}\\n      callback: ${grails.serverURL}/profile/inaturalistCallback\\nbiocache.search.baseUrl: {{ biocache_base_url }}/occurrences/search\\nheaderAndFooter:\\n  baseURL: {{ header_and_footer_baseurl | default('https://www,     authCookieName: {{ auth_cookie_name | default('ALA-Auth') }}\n      enabled: {{ oauth_providers_flickr_enabled | default('true') }}\n    inaturalist:\n      enabled: {{ oauth_providers_inaturalist_enabled | default('false') }}\n      key: {{ oauth_providers_inaturalist_key | default('') }}\n      secret: {{ oauth_providers_inaturalist_secret | default('') }}\n      callback: ${grails.serverURL}/profile/inaturalistCallback\nbiocache.search.baseUrl: {{ biocache_base_url }}/occurrences/search\nheaderAndFooter:\n  baseURL: {{ header_and_footer_baseurl | default('https://www.ala, \, \\n{% if spring_session_redis_clustered %}, biocache.search.baseUrl: {{ biocache_base_url }}/occurrences/search\\\\nheaderAndFooter:\\\\n  baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3')}}\\\\n  version: {{ header_and_footer_version | default('1')}}\\\\n{% if bootadmin_enabled %}\\\\n    service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}\\\\n#    management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status\\\\n{% endif %}\\\\n{% if spring_session_redis_clustered %}, headerAndFooter:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  version: {{ header_and_footer_version | default('1')}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n{% if bootadmin_enabled %}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    service-base-url: {{ bootadmin_client_base_url |, headerAndFooter:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  version: {{ header_and_footer_version | default('1')}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n{% if bootadmin_enabled %}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n#    management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, headerAndFooter:\\\\\\\\\\\\n  version: {{ header_and_footer_version | default('1')}}\\\\\\\\\\\\\\\\n{% if bootadmin_enabled %}\\\\\\\\\\\\\\\\n    service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}\\\\\\\\\\\\\\\\n#    management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status\\\\\\\\\\\\\\\\n{% endif %}\\\\\\\\\\\\\\\\n{% if spring_session_redis_clustered %}, headerAndFooter:\\\\n  version: {{ header_and_footer_version | default('1')}}\\\\\\\\n{% if bootadmin_enabled %}\\\\\\\\n    service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}\\\\\\\\n#    management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status\\\\\\\\n{% endif %}\\\\\\\\n{% if spring_session_redis_clustered %}, headerAndFooter:\\n  version: {{ header_and_footer_version | default('1')}}\\n{% if bootadmin_enabled %}\\n    service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}\\n#    management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status\\n{% endif %}\\n{% if spring_session_redis_clustered %}","CWE-120, CWE-121",1
"sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*
# Run again with 8u172 as the basis for servers that were on that version instead
- name: Fix Webupd8 Team failing to update and Oracle removing old download (part 2)
    sed -i 's|JAVA_VERSION=8u172|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_172|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*",1,"JAVA_VERSION=8u172|JAVA_VERSION=8u181|' oracle-java8-installer.*, J_DIR=jdk1.8.0_172|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*, PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.*, SHA256SUM_TGZ=, sed -i's|JAVA_VERSION=8u172|JAVA_VERSION=8u181|' oracle-java8-installer.*, sed -i's|J_DIR=jdk1.8.0_172|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*, sed -i's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.*, sed -i's|SHA256SUM_TGZ=\","CWE-112, CWE-113",1,"sed -i 's|JAVA_VERSION=8u161|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""6dbc56a0e3310b69e91bb64db63a485bd7b6a8083f08e48047276380a0e2021e""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_161|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*
- name: Switch oracle jdk 8 from security (b171) to bug fix+security (b172)
    sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*",0,"sed -i's|JAVA_VERSION=8u161|JAVA_VERSION=8u172|' oracle-java8-installer.* &&\\\\nsed -i's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&\\\\nsed -i's|SHA256SUM_TGZ=\\\\, sed -i's|JAVA_VERSION=8u161|JAVA_VERSION=8u172|' oracle-java8-installer.* &&\nsed -i's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&\nsed -i's|SHA256SUM_TGZ=\, sed -i's|JAVA_VERSION=8u171|JAVA_VERSION=8u172|' oracle-java8-installer.* &&\\\\\\\\\\\\\\\\\\\\\\\\nsed -i's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&\\\\\\\\\\\\\\\\\\\\\\\\nsed -i's|SHA256SUM_TGZ=\\\\\\\\\\\\\\\\\\\\, sed -i's|JAVA_VERSION=8u171|JAVA_VERSION=8u172|' oracle-java8-installer.* &&\\nsed -i's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&\\nsed -i's|SHA256SUM_TGZ=\\, sed -i's|J_DIR=jdk1.8.0_161|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*, sed -i's|J_DIR=jdk1.8.0_161|J_DIR=jdk1.8.0_172|' oracle-java8-installer.* &&\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, sed -i's|J_DIR=jdk1.8.0_161|J_DIR=jdk1.8.0_172|' oracle-java8-installer.* &&\\\\nsed -i's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*, sed -i's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*, sed -i's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&\\\\\\\\nsed -i's|SHA256SUM_TGZ=\\\\\\\\, sed -i's|SHA256SUM_TGZ=\",CWE-1136,1
"- ""{{data_dir}}/ala/runtime/files/""",1,"ala/runtime/files, data_dir","CWE-119, CWE-20, CWE-27, CWE-285, CWE-287, CWE-319, CWE-326, CWE-327, CWE-601",0,"- include: ../../common/tasks/setfacts.yml
  tags:
    - spatial-hub
    - config    

- include: ../../apache_vhost/tasks/main.yml context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - apache_vhost
    - spatial-hub
  when: not webserver_nginx

- name: add nginx vhost if configured
  include_role:
    name: nginx_vhost
  vars:
    hostname: ""{{ spatial_hub_hostname }}""
    context_path: ""{{ spatial_hub_context_path }}""
  tags:
    - nginx_vhost
    - deploy
    - spatial-hub
  when: webserver_nginx

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ spatial_hub_war_url }}' context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - tomcat_vhost
    - spatial-hub

- name: ensure target directories exist [data subdirectories etc.]
  file: path={{item}} state=directory owner={{tomcat_user}} group={{tomcat_user}}
  with_items:
    - ""{{data_dir}}/ala/data/runtime/files/""
  tags:
    - spatial-hub

- name: copy all config.properties
  template: src=spatial-hub-config.properties dest={{data_dir}}/spatial-hub/config/spatial-hub-config.properties
  tags:
    - spatial-hub 
    - config

- name: copy all log4j.properties
  template: src=log4j.properties dest={{data_dir}}/spatial-hub/config/log4j.properties
  tags:
    - spatial-hub

- name: set data ownership
  file: path={{data_dir}}/ala/data/ owner={{tomcat_user}} group={{tomcat_user}} recurse=true
  notify: 
    - restart tomcat
  tags:
    - spatial-hub",0,"tomcat_user: no, webserver_nginx: no","CWE-257, CWE-78",1
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',1,include:../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',"CWE-276, CWE-295, CWE-306, CWE-311, CWE-327",0,"- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
#
# WAR file deployment and Apache/Tomcat virtual host configuration
#
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy",0,"file: path=, include:../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}', mysql_db: name={{webapi_db_name}} state=present, mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present, template: src=webapi-config.properties.j2 dest=","CWE-257, CWE-601, CWE-602",1
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',1,"context_path='{{ webapi_context_path }}', hostname='{{ webapi_hostname }}', include:../../tomcat_deploy/tasks/main.yml, war_url='{{ webapi_war_url }}'",CWE-20,0,"version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}""",0,"artifactId: webapi, classifier: '',, packaging: war, version: 0.1, webapi_url: {{maven_repo_ws_url}}","CWE-100, CWE-16, CWE-19, CWE-22, CWE-319",0
{% if spring_session_redis_clustered is sameas true %},1,if spring_session_redis_clustered is sameas true,"CWE-252, CWE-253, CWE-275, CWE-276, CWE-279",0,"{% if spring_session_redis_clustered %}
    clustered:
      nodes: {{ spring_session_redis_host }}:{{ spring_session_redis_port | default('6379') }}
    {% endif %}",0,"clustered:, nodes: {{ spring_session_redis_host }}:{{ spring_session_redis_port | default('6379') }}","CWE-119, CWE-120, CWE-121, CWE-122",1
when: elasticsearch_proxy,1,"when: cassandra_proxy, when: elasticsearch_proxy, when: kafka_proxy, when: kibana_proxy, when: postgres_proxy, when: rabbit_proxy, when: redis_proxy","CWE-863, CWE-949",0,when: elasticsearch_proxy | bool == True,0,when: elasticsearch_proxy | bool == True,CWE-200,1
- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass_values }}',1,include:../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass_values }}',"CWE-103, CWE-119, CWE-120, CWE-601",0,"tags:
    - sandbox
#
# WAR file deployment and virtual host configuration
#

- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass }}'
  tags:
    - sandbox
    - deploy
    - apache_vhost

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ sandbox_war_url }}' context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}'
  tags:
    - sandbox
    - deploy
    - tomcat_vhost

- name: Redirect to datacheck 
  template: src=index.html dest=/srv/{{ sandbox_hostname }}/www/index.html owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - sandbox
    - deploy
    - apache_vhost

#
# Properties and data file configuration
#
  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties",0,tags: - sandbox,"CWE-120, CWE-256, CWE-257, CWE-258, CWE-264, CWE-284, CWE-319",1
"webapi_war_url: ""{{maven_repo_ws_url}}""",1,"webapi_war_url: \, webapi_war_url: \\, webapi_war_url: \\\\, webapi_war_url: \\\\\\\\, webapi_war_url: \\\\\\\\\\\\\\\\\\, webapi_war_url: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-22,0,"- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
#
# WAR file deployment and Apache/Tomcat virtual host configuration
#
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy",0,"file: path={{ data_dir }}/webapi/config, include:../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}', name={{tomcat_user}} group={{tomcat_user}} recurse=true, name={{webapi_db_name}} state=present, name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present, owner={{tomcat_user}} group={{tomcat_user}}, path={{ data_dir }}/webapi, path={{data_dir}}/webapi/config/webapi-config.properties, template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties, war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'",CWE-778,0
"webapi_war_url: ""{{maven_repo_ws_url}}""",1,"webapi_war_url: \, webapi_war_url: \\, webapi_war_url: \\\\, webapi_war_url: \\\\\\\\, webapi_war_url: \\\\\\\\\\\\\\\\\\\\, webapi_war_url: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, webapi_war_url: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, webapi_war_url: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-1035, CWE-1036, CWE-1037, CWE-1038, CWE-1039, CWE-1040, CWE-1041, CWE-1042, CWE-798, CWE-807",0,"version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}""",0,"artifactId:, maven_repo_ws_url:, packaging:, version:, webapi_url:",CWE-778,0
"cassandra_user: cassandra
# package variables used for RedHat
datastax: dsc12-1.2.10-1
cassandra: cassandra12-1.2.10-1",1,"cassandra: cassandra12-1.2.10-1, datastax: dsc12-1.2.10-1, package variables used for RedHat","CWE-20, CWE-22",0,"# common variables across all roles
cassandra_user: cassandra",0,cassandra_user: cassandra,"CWE-200: Information Exposure, CWE-202: Information Exposure, CWE-732: Improper Restriction of PII to Authorized Users",0
"when: upgrade_check_script.stdout == ""False""",1,"when: upgrade_check_script.stdout == \, when: upgrade_check_script.stdout == \\, when: upgrade_check_script.stdout == \\\, when: upgrade_check_script.stdout == \\\\\\\\, when: upgrade_check_script.stdout == \\\\\\\\\\\\\\\\\\, when: upgrade_check_script.stdout == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: upgrade_check_script.stdout == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: upgrade_check_script.stdout == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-117,0,"mode: 0644
  import: others/master/postconfigure-upgrade.yaml
  when: upgrade_check_script.stdout == ""False""",0,"mode: 0644 import: others/master/postconfigure-upgrade.yaml when: upgrade_check_script.stdout == \, mode: 0644 import: others/master/postconfigure-upgrade.yaml when: upgrade_check_script.stdout == \\, mode: 0644 import: others/master/postconfigure-upgrade.yaml when: upgrade_check_script.stdout == \\\\, mode: 0644 import: others/master/postconfigure-upgrade.yaml when: upgrade_check_script.stdout == \\\\\\\\, mode: 0644 import: others/master/postconfigure-upgrade.yaml when: upgrade_check_script.stdout == \\\\\\\\\\\\\\\\\\\\, mode: 0644 import: others/master/postconfigure-upgrade.yaml when: upgrade_check_script.stdout == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-778,0
"- name: bootstrap | configure node | kubead show me join command
- name: bootstrap | configure node | compose join command",1,"name: bootstrap | configure node | compose join command, name: bootstrap | configure node | compose join command\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, name: bootstrap | configure node | compose join command\\\\n, name: bootstrap | configure node | kubead show me join command\\\\\\\\\\\\\\n, name: bootstrap | configure node | kubead show me join command\\n, name: bootstrap | configure node | kubead show me join command\n, name: bootstrap | configure node | show me join command, name: bootstrap | configure node | show me join command\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, name: bootstrap | configure node | show me join command\\\\n","CWE-250, CWE-252",0,"---
- name: configure others | kubead show me join command
  command: kubeadm token create --print-join-command --ttl 5m
  delegate_to: ""{{ cluster_name }}-kube-master.service.automium.consul""
  register: kubeadm_join_command

- name: configure-bootstrap  | compose join command
  set_fact:
    join_command: ""{{ kubeadm_join_command.stdout }}""",0,"- name: configure-bootstrap | compose join command\n  set_fact:\n    join_command:, delegate_to: \, delegate_to: \\, delegate_to: {{ cluster_name }}-kube-master.service.automium.consul, kubeadm token create --print-join-command --ttl 5m, kubeadm token create --print-join-command --ttl 5m\\\\nregister: kubeadm_join_command, name: configure-bootstrap | compose join command\\\\nset_fact:, name: configure-bootstrap | compose join command\\nset_fact:, name: configure-bootstrap | compose join command\\nset_fact:\\n    join_command:","CWE-1209, CWE-798",0
"- name: Enable/disable services
  service:
    name: ""{{ item }}""
    enabled: ""{{ (enable_services | bool) | ternary('yes','no') }}""
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service

  when: (start_services | bool)
  tags:
    - service",1,service:,"CWE-103, CWE-77, CWE-778, CWE-78",0,"---

- name: Package
  package:
    name: ""{{item}}""
    state: present
  loop: ""{{packages_to_install[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""

- name: Template >> /etc/dhcp/dhcpd.conf
  template:
    src: dhcpd.conf.j2
    dest: /etc/dhcp/dhcpd.conf
    owner: root
    group: root
    mode: 0644
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.networks.conf
  template:
    src: dhcpd.networks.conf.j2
    dest: /etc/dhcp/dhcpd.networks.conf
    owner: root
    group: root
    mode: 0644
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.{{item}}.conf
  template:
    src: dhcpd.subnet.conf.j2
    dest: /etc/dhcp/dhcpd.{{item}}.conf
    owner: root
    group: root
    mode: 0644
  with_items: ""{{networks}}""
  when:
    - j2_current_iceberg_network in item
    - networks[item].is_in_dhcp == true
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Start services
  service:
    name: ""{{item}}""
    state: started
    enabled: yes
  loop: ""{{services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""",0,"ansible_distribution_major_version, ansible_distribution|lower|replace(' ','_'), dest: /etc/dhcp/dhcpd.conf, j2_current_iceberg_network, name: Template >> /etc/dhcp/dhcpd.conf, networks[item].is_in_dhcp == true, package: \\\n    name: {{item}} \\\n    state: present \\\n  loop: {{packages_to_install[(ansible_distribution|lower|replace(' ','_'))]['_+ansible_distribution_major_version]}}, package: \\n    name: {{item}}, services_to_start, template: \n    src: /etc/dhcp/dhcpd.conf.j2","CWE-119, CWE-120, CWE-22, CWE-23, CWE-25, CWE-77, CWE-787",1
"exporters:
#    node_exporter:
#      port: 9100
    bb_exporter:
      port: 9777
      collectors:
        cpu:
        ram:
        mounted:
          - /scratch
          - /home
        services:
          - slurmd.service
        
  # Define alerts related to selected exporters
    Exporter_down:
#      severity: critical
    bb_exporter_service:
      -  slurmd",1,"cpu:, mounted, mounted:, port: 9777, ram:, services:, slurmd, slurmd.service","CWE-190, CWE-200, CWE-209",0,"monitoring:
  alerts:
    - ExporterDown
    - OutOfDiskSpace",0,"alerts:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - ExporterDown\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - OutOfDiskSpace, alerts:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - ExporterDown\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - OutOfDiskSpace, alerts:\\\\\\\\\\\\\\\\\\\\n  - ExporterDown\\\\\\\\\\\\\\\\\\\\n  - OutOfDiskSpace, alerts:\\\\\\\\n  - ExporterDown\\\\\\\\n  - OutOfDiskSpace, alerts:\\\\n  - ExporterDown\\\\n  - OutOfDiskSpace, alerts:\\n  - ExporterDown\\n  - OutOfDiskSpace, alerts:\n  - ExporterDown\n  - OutOfDiskSpace",CWE-209,0
"loop: ""{{ log_client_services_to_start }}""",1,"loop: \\\\, loop: \\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-829,0,"---

- name: Restart rsyslog services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service",0,"loop: \, name: Restart rsyslog services\\\\\\\\n  service:\\\\\\\\n    name: rsyslog\\\\\\\\n    state: restarted, name: Restart rsyslog services\\\\n  service:\\\\n    name: rsyslog\\\\n    state: restarted, name: Restart rsyslog services\\n  service:\\n    name: rsyslog\\n    state: restarted, name: Restart rsyslog services\n  service:\n    name: \, name: Start sshd service\\n  service:\\n    name: sshd\\n    state: started\\n    enabled: yes","CWE-119, CWE-126, CWE-862, CWE-863",1
advanced_dhcp_server_role_version: 1.0.4,1,advanced_dhcp_server_role_version: 1.0.4,CWE-352,0,"role_version: 1.0.2
packages_to_install:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcp
    _8:
      - dhcp-server
  centos:
    _7:
      - dhcp
    _8:
      - dhcp-server

services_to_start:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcpd
    _8:
      - dhcpd
  centos:
    _7:
      - dhcpd
    _8:
      - dhcpd",0,"packages_to_install: ubuntu: _18: - isc-dhcp-server redhat: _7: - dhcpd _8: - dhcpd centos: _7: - dhcpd _8: - dhcpd services_to_start: ubuntu: _18: - isc-dhcp-server redhat: _7: - dhcpd _8: - dhcpd centos: _7: - dhcpd _8: - dhcpd, role_version: 1.0.2 packages_to_install: ubuntu: _18: - isc-dhcp-server redhat: _7: - dhcpd _8: - dhcpd centos: _7: - dhcpd _8: - dhcpd services_to_start: ubuntu: _18: - isc-dhcp-server redhat: _7: - dhcpd _8: - dhcpd centos: _7: - dhcpd _8: - dhcpd, ubuntu: _18: - isc-dhcp-server redhat: _7: - dhcpd _8: - dhcpd centos: _7: - dhcpd _8: - dhcpd, ubuntu:\\n  _18:\\n    - isc-dhcp-server\\n  redhat:\\n    _7:\\n      - dhcpd\\n    _8:\\n      - dhcpd\\n  centos:\\n    _7:\\n      - dhcpd\\n    _8:\\n      - dhcpd, ubuntu:\n  _18:\n    - isc-dhcp-server\n  redhat:\n    _7:\n      - dhcpd\n    _8:\n      - dhcpd\n  centos:\n    _7:\n      - dhcpd\n    _8:\n      - dhcpd",CWE-306,1
dhcp_server_role_version: 1.0.7,1,dhcp_server_role_version: 1.0.7,CWE-1021,0,dhcp_server_role_version: 1.0.6,0,dhcp_server_role_version: 1.0.6,"CWE-601, CWE-749, CWE-772",0
dhcp_server_role_version: 1.0.4,1,dhcp_server_role_version: 1.0.4,CWE-937,0,"---
role_version: 1.0.4",0,role_version: 1.0.4,"CWE-1001, CWE-120",0
"- name: lineinfile  Configure root color based on iceberg number
- name: copy  Add disk usage small script for screenrc
- name: copy  Add screenrc configuration
- name: copy  Add vimrc configuration",1,"- name: lineinfile \u0011 Configure root color based on iceberg number, copy \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, copy \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\u0011 Add vimrc configuration, copy \\\\\\\\\\\\u0011 Add vimrc configuration, copy \\\\u0011 Add screenrc configuration, copy \\u0011 Add disk usage small script for screenrc, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-732,0,"- name: Configure root color based on iceberg number
  lineinfile:
    path: /root/.bashrc
    line: 'PS1=""\[\e[01;{{31+(j2_current_iceberg_number|int)}}m\]\h:\w#\[\e[00;m\] ""'

- name: Add disk usage small script for screenrc
  copy:
    src: free_root_disk 
    dest: /usr/bin/free_root_disk
    mode: 0700

- name: Add screenrc configuration
  copy:
    src: screenrc
    dest: /root/.screenrc
    mode: 0644",0,"lineinfile:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  path: /root/.bashrc\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  line: '\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\PS1=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, lineinfile:\\\\\\\\\\\\n  path: /root/.bashrc\\\\\\\\\\\\\\\\\\\\n  line: '\\\\\\\\\\PS1=\\\\\\\\\\\\\\\\\\\\e[01;{{31+(j2_current_iceberg_number|int)}}m\\\\\\\\\\\\]\\\\\\\\\\\\h:\\\\\\\\\\\\w#\\\\\\\\\\\\[\\\\\\\\\\\\\\\\e[00;m\\\\\\\\\\\\] '\\\\\\\\\\\\\\n, lineinfile:\\\\n  path: /root/.bashrc\\\\\\\\n  line: '\\\\PS1=\\\\\\\\e[01;{{31+(j2_current_iceberg_number|int)}}m\\\\]\\\\h:\\\\w#\\\\[\\\\\\\\e[00;m\\\\] '\\\\n, lineinfile:\\n  path: /root/.bashrc\\\\n  line: '\\PS1=\\\\e[01;{{31+(j2_current_iceberg_number|int)}}m\\]\\h:\\w#\\[\\\\e[00;m\\] '\\n, name: Add disk usage small script for screenrc  copy:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: Add disk usage small script for screenrc  copy:\\\\\\\\\\\\\\\\\\\\\\\\\\n  src: free_root_disk\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  dest: /usr/bin/free_root_disk\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  mode: 0700, name: Add disk usage small script for screenrc  copy:\\n  src: free_root_disk\\n  dest: /usr/bin/free_root_disk\\n  mode: 0700, name: Add disk usage small script for screenrc  copy:\n  src: free_root_disk\n  dest: /usr/bin/free_root_disk\n  mode: 0700, name: Add screenrc configuration  copy:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: Add screenrc configuration  copy:\\n  src: screenrc\\n  dest: /root/.screenrc\\n  mode: 0644","CWE-22, CWE-256, CWE-257, CWE-270, CWE-275, CWE-276, CWE-284, CWE-78",0
- name: service  Restart dhcp server,1,"service\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - name: service\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t\\\\\\\\, service\\\\\\\\\\\\\\\\n    - name: service\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\t\\\\\\\\, service\\\\n    - name: service\\\\n    \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t, service\\n    - name: service\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\, service\n    - name: service\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t",CWE-20,1,"---
- name: Restart dhcp services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service",0,"name: Restart dhcp services\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: Restart dhcp services\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  service:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: Restart dhcp services\\\\\\\\\\n  service:\\\\\\\\\\\\n    name: \\\\\\\\\\\\\\\\, name: Restart dhcp services\\\\n  service:\\\\n    name: \\\\\\, name: Restart dhcp services\\n  service:\\n    name: \\\, name: Restart dhcp services\n  service:\n    name: \\","CWE-119, CWE-20, CWE-25",1
"aws_access_key: ""{{ auth_var['aws_access_key_id'] | default(omit) }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] | default(omit) }}""",1,"auth_var['aws_access_key_id'] | default(omit) \, auth_var['aws_access_key_id'] | default(omit) \\, auth_var['aws_secret_access_key'] | default(omit) \\, auth_var['aws_secret_access_key'] | default(omit) \\\\, aws_access_key: \, aws_access_key: \\, aws_access_key: \\\\\\\\, aws_secret_key: \\, aws_secret_key: \\\\, aws_secret_key: \\\\\\\\\\\\\\\\\\\\","CWE-257, CWE-258, CWE-259",0,"aws_access_key: ""{{ auth_var['aws_access_key_id'] }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] }}""",0,"auth_var, aws_access_key: \, aws_access_key: \\, aws_access_key_id, aws_secret_access_key, aws_secret_key: \\, aws_secret_key: \\\\","CWE-257, CWE-258",0
no_log: true,1,"no_log: false\\\\n    \\\\n    -, no_log: false\\n    \\\n    -, no_log: false\\n    \\n    -, no_log: true, no_log: true\\n    \\n    -, no_log: true\n    \n    -",CWE-200,0,"- name: ""Provisioning resource group {{ res_grp }}""
  debug:
    msg: ""The current server obj is {{ res_grp }} \n groups vars are {{ r_grp_vars }} ""

- name: ""Including credentials of current resource {{ res_grp['resource_group_name'] }} ""
  include_vars: ""../vars/{{ res_grp['assoc_creds'] }}.yml""
  no_log: false 

- name: ""Checking res_grp ""
  debug:
    msg: "" res_grp {{ res_grp }}""

- name: ""Provision resource definitions""
  include: provision_res_defs.yml res_def={{ outer_item.0 }} res_grp_name={{ outer_item.1 }}
  with_nested:
    - ""{{ res_grp['res_defs'] }}""
    - [""{{ res_grp['resource_group_name'] }}""]
  loop_control:
    loop_var: outer_item
#- name: ""Register resource count""
#  shell: python -c ""print [x for x in range( 0, {{ server_var['count'] }} )]""
#  register: res_count

# Debug task 
#- name: ""Checking value of res_count ""
#  debug: 
#    msg: ""Value of res_count is {{ res_count.stdout }}""

# debug task 
#- name: ""illustration of looping resource for given count ""
#  debug:
#    msg: ""Credentials included for the resource are {{ item.0 }} {{ item.1 }} {{ item.2 }} {{ item.3 }} {{ item.4 }} {{ item.5 }} {{ item.6 }} {{ item.7 }} {{ item.8 }} {{ item.9 }} ""
#  with_nested:
#    - [""{{ endpoint }}""]
#    - [""{{ username }}""]
#    - [""{{ password }}""]
#    - [""{{ project }}""]
#    - [""{{ server_var['res_def']['image'] }}""]
#    - [""{{ server_var['res_def']['keypair']  }}""]
#    - [""{{ server_var['res_def']['flavor']  }}""]
#    - [""{{ server_var['res_def']['networks'][0] }}""]
#    - [""{{ server_var['resource_name'] }}""]
#    - ""{{ res_count.stdout }}""

#- name: ""Provision resource looping on count""
#  os_server:
#    state: ""{{ item.4 }}""
#    auth:
#      auth_url: ""{{ item.0 }}""
#      username: ""{{ item.1 }}""
#      password: ""{{ item.2 }}""
#      project_name: ""{{ item.3 }}""
#    name: ""{{ item.9 }}-{{ item.10 }}""
#    image: ""{{ item.5 }}""
#    key_name: ""{{ item.6  }}""
#    api_timeout: 300
#    flavor: ""{{ item.7 }}""
#    network: ""{{ item.8 }}""
#  with_nested:
#    - [""{{ endpoint }}""]
#    - [""{{ username }}""]
#    - [""{{ password }}""]
#    - [""{{ project }}""]
#    - [""{{ state }}""]
#    - [""{{ server_var['res_def']['image'] }}""]
#    - [""{{ server_var['res_def']['keypair']  }}""]
#    - [""{{ server_var['res_def']['flavor']  }}""]
#    - [""{{ server_var['res_def']['networks'][0] }}""]
#    - [""{{ server_var['resource_name'] }}""]
#    - ""{{ res_count.stdout }}""",0,"- name: \, api_timeout: 300\\, flavor: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, key_name: \\\\, loop_var: outer_item\\\\\\\\, network: \\\\\\\\, res_count\\\\, state: \\, with_nested: \\\\\\\\\\\\\\\\\\\\\\, with_nested: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-257,0
"when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init"" and cloud_config != {}
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and virt_type == ""cloud-init"" 
  when:  (node_exists['failed'] is defined) and  virt_type == 'virt-customize'
  ignore_errors: yes",1,"ignore_errors: yes, when:  (node_exists['failed'] is defined) and  virt_type == 'virt-customize', when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == \, when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == \\, when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == \\, when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == \\\\, when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == \\\\\, when: node_exists['failed'] is defined and virt_type ==, when: node_exists['failed'] is defined and virt_type == \",CWE-779,,"- name: set cloud config default
  set_fact:
    cloud_config: ""{{ res_def['cloud_config'] | default({})  }}""

- name: set cloud_config virt_type
  set_fact:
    virt_type: ""{{ cloud_config['virt_type'] | default('cloud-init') }}""

- include_tasks: virt_customize.yml
  when: res_def['cloud_config']['virt_type'] == ""virt-customize""

  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and virt_type == ""cloud-init""
  when:  node_exists['failed'] is defined and (res_def['cloud_config'] is not defined or virt_type == 'virt-customize')",0,"res_def['cloud_config'] is not defined or \\, res_def['cloud_config'] is not defined or \\\\\\\\, res_def['cloud_config'] is not defined or \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, res_def['cloud_config']['virt_type'] == \, res_def['cloud_config']['virt_type'] == \\, uri_hostname!= \\, uri_hostname== \\\\, uri_hostname== \\\\\\\, uri_hostname== \\\\\\\\\\\\\\\\\, uri_hostname== \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-200,0
"dest: ""/tmp/{{ libvirt_resource_name }}{{ definition[4] }}{{ definition[2] }}""
    uri: ""{{ definition[0]['uri'] }}""",1,"dest: /tmp/{{ libvirt_resource_name }}{{ definition[4] }}{{ definition[2] }}, \, uri: {{ definition[0]['uri'] }}, \\, uri: {{ definition[0]['uri'] }}, \\\\, uri: {{ definition[0]['uri'] }}, \\\\\\\\\, uri: {{ definition[0]['uri'] }}, \\\\\\\\\\\\\\\\, uri: {{ definition[0]['uri'] }}, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-1030: Inappropriate Redirect, CWE-1031: Insecure Temp Files, CWE-1032: Insecure Writeable Temporary Directory, CWE-1033: Insecure Temporary File, CWE-1034: Insecure Temp File, CWE-20: Improper Input Validation, CWE-89: SQL Injection, CWE-90: OS Command Injection",1,"uri: ""{{ definition[0] }}""",0,"uri: \, uri: \\, uri: \\\\, uri: \\\\\\\\, uri: \\\\\\\\\\\\, uri: \\\\\\\\\\\\\\\\\\\\\\\\\\, uri: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, uri: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-345,0
"name:
          - python2-dnf 
          - libvirt-devel
          - libguestfs-tools 
          - python-libguestfs",1,"libguestfs-tools, libvirt-devel, python-libguestfs, python2-dnf","CWE-778, CWE-78",1,"- name: Install dependencies
  block:
    - name: Install package dependencies
      package:
        name: ""{{ libvirt_pkg }}""
        state: latest
      with_items:
      - python2-dnf 
      - libvirt-devel
      - libguestfs-tools 
      - python-libguestfs
      become: true
      loop_control: 
        loop_var: libvirt_pkg
    - name: Install pypi dependencies of libvirt
      pip:
        name: ""{{ libvirt_pypi }}""
      with_items:
      - ""libvirt-python>=3.0.0""
      - ""lxml""
      loop_control: 
        loop_var: libvirt_pypi
  rescue:
    - fail:
        msg: 'Error installing the package dependencies! Please try adding password less priviledged sudo user or with --ask-sudo-pass'",0,"loop_control: loop_var: libvirt_pypi, loop_var: libvirt_pypi, with_items:","CWE-117, CWE-119, CWE-120",1
,1,"ansible_python_interpreter: /usr/bin/python, ansible_python_interpreter: /usr/local/bin/python, become: true, become_method: su, become_user: user, become_user: username","CWE-1040, CWE-250",0,"---
- name: ""provision/deprovision os_subnet""
  os_subnet:
    allocation_pool_end:  ""{{ res_def['allocation_pool_end'] | default(omit) }}""
    allocation_pool_start:  ""{{ res_def['allocation_pool_start'] | default(omit) }}""
    api_timeout: ""{{ res_def['api_timeout'] | default(omit) }}""
    auth: ""{{ auth_var }}""
    cidr:  ""{{ res_def['cidr'] | default(omit) }}""
    dns_nameservers:  ""{{ res_def['dns_nameservers'] | default(omit) }}""
    enable_dhcp: ""{{ res_def['enable_dhcp'] | default(omit) }}""
    extra_specs: ""{{ res_def['extra_specs'] | default(omit) }}""
    gateway_ip:  ""{{ res_def['gateway_ip'] | default(omit) }}""
    host_routes:  ""{{ res_def['host_routes'] | default(omit) }}""
    ip_version:  ""{{ res_def['ip_version'] | default(omit) }}""
    ipv6_address_mode:  ""{{ res_def['ipv6_address_mode'] | default(omit) }}""
    ipv6_ra_mode:  ""{{ res_def['ipv6_ra_mode'] | default(omit) }}""
    network_name:  ""{{ res_def['network_name'] | default(omit) }}""
    no_gateway_ip:  ""{{ res_def['no_gateway_ip'] | default(omit) }}""
    use_default_subnetpool:  ""{{ res_def['use_default_subnetpool'] | default(omit) }}""
    interface: ""{{ res_def['interface'] | default(omit) }}""
    name: ""{{ os_resource_name }}""
    key: ""{{ res_def['key'] | default(omit) }}""
    project: ""{{ res_def['project'] | default(omit) }}""
    region_name: ""{{ res_def['region_name'] | default(omit) }}""
    state: ""{{ state }}""
    timeout: ""{{ res_def['timeout'] | default(600) }}""
    verify: no
    wait: yes
  register: res_def_output_auth
  no_log: ""{{ not debug_mode }}""
  when: 
   - auth_var != """"

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_os_network: ""{{ topology_outputs_os_network + [ res_def_output_auth ] }}""
  when:
    - auth_var != """"
      
- name: ""provision/deprovision os_subnet""
  os_subnet:
    allocation_pool_end:  ""{{ res_def['allocation_pool_end'] | default(omit) }}""
    allocation_pool_start:  ""{{ res_def['allocation_pool_start'] | default(omit) }}""
    api_timeout: ""{{ res_def['api_timeout'] | default(omit) }}""
    auth: ""{{ auth_var }}""
    cidr:  ""{{ res_def['cidr'] | default(omit) }}""
    dns_nameservers:  ""{{ res_def['dns_nameservers'] | default(omit) }}""
    enable_dhcp: ""{{ res_def['enable_dhcp'] | default(omit) }}""
    extra_specs: ""{{ res_def['extra_specs'] | default(omit) }}""
    gateway_ip:  ""{{ res_def['gateway_ip'] | default(omit) }}""
    host_routes:  ""{{ res_def['host_routes'] | default(omit) }}""
    ip_version:  ""{{ res_def['ip_version'] | default(omit) }}""
    ipv6_address_mode:  ""{{ res_def['ipv6_address_mode'] | default(omit) }}""
    ipv6_ra_mode:  ""{{ res_def['ipv6_ra_mode'] | default(omit) }}""
    network_name:  ""{{ res_def['network_name'] | default(omit) }}""
    no_gateway_ip:  ""{{ res_def['no_gateway_ip'] | default(omit) }}""
    use_default_subnetpool:  ""{{ res_def['use_default_subnetpool'] | default(omit) }}""
    interface: ""{{ res_def['interface'] | default(omit) }}""
    name: ""{{ os_resource_name }}""
    key: ""{{ res_def['key'] | default(omit) }}""
    project: ""{{ res_def['project'] | default(omit) }}""
    region_name: ""{{ res_def['region_name'] | default(omit) }}""
    state: ""{{ state }}""
    timeout: ""{{ res_def['timeout'] | default(600) }}""
    verify: no
    wait: yes
  register: res_def_output_auth
  no_log: ""{{ not debug_mode }}""
  when:
   - auth_var == """"

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_os_network: ""{{ topology_outputs_os_network + [ res_def_output_no_auth ] }}""
  when:
    - auth_var == """"",0,"ansible.cfg, key, project, state, timeout, auth_var, res_def, res_def_output_auth, auth_var!=, auth_var!='', os_resource_name, res_def, res_def['key'], res_def['project'], res_def['timeout'], res_def_output_auth, state","CWE-522, CWE-601",1
"admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""
    admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""
    admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""
    admin_password: ""{{ res_def['vm_password'] | default('Linchpin!') }}""",1,"res_def['vm_password'] | default('Linchpin!') \\, res_def['vm_password'] | default('Linchpin!') \\\\, res_def['vm_password'] | default('Linchpin!') \\\\\\\\\\\\\\\\, res_def['vm_password'] | default('Linchpin!') \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, res_def['vm_username'] | default('linchpin') \, res_def['vm_username'] | default('linchpin') \\, res_def['vm_username'] | default('linchpin') \\\\\\\\, res_def['vm_username'] | default('linchpin') \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, res_def['vm_username'] | default('linchpin') \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-257,1,"---
- name: ""Provisioning Azure VM when not async""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    ssh_public_keys: ""{{ssh_public_keys}}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    name: ""{{ nameOfvm | default(omit) }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    image: ""{{ image | default(omit) }}""
  register: res_def_output
  when: not _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure VM""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    name: ""{{  nameOfvm| default(omit) }}""
    image: ""{{ image | default(omit) }}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""
  when: _async",0,"Azure_rm_virtualmachine\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, Azure_rm_virtualmachine\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    resource_group: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, Azure_rm_virtualmachine\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    admin_password: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, Azure_rm_virtualmachine\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    admin_username: \\\\\\\\\\\\\\\\\\\\\\\\\\\\, Azure_rm_virtualmachine\\\\\\\\\\\\n    subscription_id: \\\\\\\\\\\\, Azure_rm_virtualmachine\\\\n    secret: \\\\, Azure_rm_virtualmachine\\n    tenant: \\, Azure_rm_virtualmachine\n    client_id: \",CWE-276,1
"cloudconfig_users: ""{{ cloud_config['users'] | default([]) }}""",1,"cloudconfig_users: \, default([]) \\, default([]) \\\\, default([])\, default([])}\\\, default([])}\\\\\\n \\\\\\\\\\\n \\\\\\\\\, default([])}\\n \\\\, default([])}\\n \\\\\\n \\\\, default([])}\n \, default([])}\n \\",CWE-22,0,"#  when:  node_exists['failed'] is defined and res_def['cloud_config'] is not defined

  register: pubkey_local
  register: pubkey_remote
- name: ""Create directories""
  file:
    path: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}""
    state: ""directory""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ res_def['name'] | default(res_def['res_name']) }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  when: node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-local""
  template:
    src: ""templates/cloud-config/user-data-fixed-local""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  when: res_def['cloud_config'] is not defined and node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-local""
    src: ""templates/cloud-config/user-data-local""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  vars:
    cloudconfig_users: ""{{ res_def['cloud_config']['users'] | default([]) }}""
  when: res_def['cloud_config'] is defined and node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-remote""
  template:
    src: ""templates/cloud-config/user-data-remote""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  remote_user: ""{{ res_def['remote_user'] | default('root') }}""
  delegate_to: ""{{ uri_hostname }}""
  when: res_def['cloud_config'] is defined and node_exists['failed'] is defined and uri_hostname != 'localhost'

- name: ""Prepare cloud-config/user-data-remote""
  template:
    src: ""templates/cloud-config/user-data-fixed-remote""
- name: ""Prepare cloud-config/meta-data remote""
- name: ""Generate ci data cd image for cloud-init local""
- name: ""Generate ci data cd image for cloud-init remote host""
  command: mkisofs -o /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}.iso -V cidata -r -J --quiet /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data  /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/meta-data
  command: mkisofs -o /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}.iso -V cidata -r -J --quiet /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/meta-data
- name: ""Generate add admin script local""
- name: ""Generate add admin script remote""
- name: ""Remove cloud-init cdrom ""
- name: ""Start VM""
- name: ""Start relevant networks""
- name: ""mac_and_ip | extract mac address""
- name: ""mac_and_ip | wait for dhcp ip address""
- name: ""mac_and_ip | wait for dhcp ip address""",0,"src: \, src: \\, template:\n    src: \, vars:\\\\n  vars: \\\\, when:\\\\n  when: \\\\, when:\\n  when: \\, with_nested:\\\\\\n  with_nested: \\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n, with_nested:\\\n  with_nested: \\\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n","CWE-22 Uncontrolled Format String, CWE-732 No Mismatches On Unused Variables",1
"topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""",1,"add_res_type\\(\\\'azure_virtual_subnet\\'\, add_res_type\\\\(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'azure_virtual_subnet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, async_outputs_azure_vn_subnet: \\, async_outputs_azure_vn_subnet|add_res_type(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, async_outputs_azure_vn_subnet|add_res_type\\\\\\\\\\\\(\\\\\\\\\\\\\\\\\\'azure_virtual_subnet\\\\\\\\\\\\\\\\'\\\\\\\\, async_outputs_azure_vn_subnet|add_res_type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, topology_outputs_azure_vn_subnet: \, topology_outputs_azure_vn_subnet|add_res_type(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, topology_outputs_azure_vn_subnet|add_res_type\\\\(\\\\\\'azure_virtual_subnet\\\\'\\, topology_outputs_azure_vn_subnet|add_res_type\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-120, CWE-125, CWE-129, CWE-20",0,"no_log: ""{{ not debug_mode }}""

- name: """"Async::Provisioning Azure Virtual Subnet""
  azure_rm_subnet:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    name: ""{{ res_def['subnet_name'] | default(omit) }}""
    virtual_network_name: ""{{ res_def['virtual_network_name']}}""
    address_prefix: ""{{ res_def['address_prefix']|default('10.1.0.0/24')}}""
  register: res_def_output
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""
  when: _async",0,"Async:: Azure subnet with CIDR 10.1.0.0/24 should be removed and instead use a subnet that does not overlap with the default 10.0.0.0/16 range or any other private IP range in use for the virtual network., Azure subnet with CIDR 10.1.0.0/24 should be removed and instead use a subnet that does not overlap with the default 10.0.0.0/16 range or any other private IP range in use for the virtual network.",CWE-284,1
"- ""{{ res_grp['resource_definitions'] }}""",1,"res_grp['resource_definitions'] \, res_grp['resource_definitions'] \\, res_grp['resource_definitions'] \\\\, res_grp['resource_definitions'] \\\\\\\\, res_grp['resource_definitions'] \\\\\\\\\\\\\\\\\\\\, res_grp['resource_definitions'] \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, res_grp['resource_definitions'] \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-732,1,"---
- name: ""Unset the authvar from previous run""
  set_fact:
    auth_var: """"

- name: ""set cred profile""
  set_fact:
    cred_profile: ""{{ res_grp['credentials']['profile'] | default('default') }}""

- name: ""Get creds from auth driver""
  auth_driver:
    filename: ""{{ res_grp['credentials']['filename']  }}""
    cred_type: ""ovirt""
    cred_path: ""{{ creds_path | default(default_credentials_path) }}""
    driver: ""file""
  register: auth_var
  ignore_errors: true

- name: ""set auth_var""
  set_fact:
    auth_var: ""{{ auth_var['output'][cred_profile] }}""
  ignore_errors: true

- block:
  - name: Obtain SSO token with using username/password credentials
    ovirt_auth:
      url: ""{{ auth_var['ovirt_url'] }}""
      username: ""{{ auth_var['ovirt_username'] }}""
      ca_file: ""{{ auth_var['ovirt_ca_file'] | default(omit) }}""
      password: ""{{ auth_var['ovirt_password'] }}""
      insecure: ""{{ auth_var['ovirt_ca_file'] is not defined }}""

  - name: ""Provisioning resource definitions of current group""
    include: provision_res_defs.yml res_def={{ res_item.0 }} res_grp_name={{ res_item.1 }}
    with_nested:
      - ""{{ res_grp['resource_definitions']) }}""
      - [""{{ res_grp['resource_group_name'] }}""]
    loop_control:
      loop_var: res_item

  always:
    - name: Always revoke the SSO token
      ovirt_auth:
        state: absent
        ovirt_auth: ""{{ ovirt_auth }}""",0,"auth_var['ovirt_ca_file'], auth_var['ovirt_password'], auth_var['ovirt_url'], auth_var['ovirt_username'], ca_file: '{{ auth_var['ovirt_ca_file'] | default(omit) }}', password: '{{ auth_var['ovirt_password'] }}', username: '{{ auth_var['ovirt_username'] }}'","CWE-321, CWE-327",1
"address_prefixes: ""{{ res_def['address_prefixes']|default('10.1.0.0/16')}}""
    address_prefixes: ""{{ res_def['address_prefixes']|default('10.1.0.0/16')}}""",1,"address_prefixes: \, address_prefixes: \\, address_prefixes: \\\\, address_prefixes: \\\\\\\\, address_prefixes: \\\\\\\\\\\\\\\\\\\\\\\\, address_prefixes: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, address_prefixes: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, address_prefixes: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, address_prefixes: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\},","CWE-275, CWE-276",1,"---
- name: ""Provisioning Azure Virtual Network when not async""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: ""{{ res_def['address_prefixes']|default(10.1.0.0/16)}}""
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  register: res_def_output

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure Virtual Network""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: 10.1.0.0/16
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""
  when: _async",0,"res_def['address_prefixes': 10.1.0.0/16, res_def['resource_group'], res_def['virtual_network_name'], resource_group","CWE-295, CWE-320, CWE-327, CWE-330, CWE-331, CWE-332, CWE-333, CWE-334, CWE-335",1
"copy: src=fetch/{{ fsid }}/etc/ceph/radosgw.gateway.keyring dest=/etc/ceph/radosgw.gateway.keyring owner=root group=root mode=600
  file: path=/etc/ceph/radosgw.gateway.keyring mode=0600 owner=root group=root",1,"copy: src=fetch/{{ fsid }}/etc/ceph/radosgw.gateway.keyring dest=/etc/ceph/radosgw.gateway.keyring owner=root group=root mode=600, file: path=/etc/ceph/radosgw.gateway.keyring mode=0600 owner=root group=root","CWE-22, CWE-284, CWE-285, CWE-78",0,"- name: Copy RGW bootstrap key
  copy: src=fetch/{{ fsid }}/etc/ceph/keyring.radosgw.gateway dest=/etc/ceph/keyring.radosgw.gateway owner=root group=root mode=600
  when: cephx

- name: Set RGW bootstrap key permissions
  file: path=/etc/ceph/keyring.radosgw.gateway mode=0600 owner=root group=root
  when: cephx",0,"copy: src=fetch/{{ fsid }}/etc/ceph/keyring.radosgw.gateway dest=/etc/ceph/keyring.radosgw.gateway owner=root group=root mode=600, file: path=/etc/ceph/keyring.radosgw.gateway mode=0600 owner=root group=root, when: cephx","CWE-120, CWE-125, CWE-248, CWE-250, CWE-251",1
copy: >,1,copy:>,"CWE-119, CWE-22, CWE-25, CWE-26, CWE-79, CWE-89",0,"---
- name: create red hat storage package directories
  file: >
    path={{ item }}
    state=directory
  with_items:
    - ""{{ ceph_stable_rh_storage_mount_path }}""
    - ""{{ ceph_stable_rh_storage_repository_path }}""

- name: fetch the red hat storage iso from the ansible server
  fetch: >
    src={{ ceph_stable_rh_storage_iso_path }}
    dest={{ ceph_stable_rh_storage_iso_path }}
    flat=yes

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=mounted

- name: copy red hat storage iso content
  shell:
    cp -r {{ ceph_stable_rh_storage_mount_path }}/* {{ ceph_stable_rh_storage_repository_path }}
    creates={{ ceph_stable_rh_storage_repository_path }}/README

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=unmounted",0,"ceph_stable_rh_storage_iso_path \\, cp -r \\\\, creates=\\\\\\\\n    \\\\, dest=\\\\n    \\, file=\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, path=\\\\\\\\n    \\\\, src=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items: - \",CWE-918,1
"with_items: ""{{ groups.get(mon_group_name, []) }}""",1,"with_items: {{ groups.get(mon_group_name, []) }}",CWE-601,0,"---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names",0,"delegate_to: \\\, delegate_to: \\\\\\, delegate_to: \\\\\\\\\\\\, run_once: true \\\n  with_items: \\, run_once: true\\\\n with_items: \\, run_once: true\\n with_items: \\, serial: 1 would be the proper solution here, but that can only be set on play level\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n, service: \n run_once: true\n with_items: \, when: mon_group_name in group_names","CWE-1000, CWE-1030, CWE-200, CWE-202, CWE-209, CWE-78",1
"with_items: ""{{ groups.get(mon_group_name, []) }}""",1,"get, groups, groups.get, mon_group_name, mon_group_name, [), with_items","CWE-19, CWE-22",0,"---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names",0,"delegate_to: {item}, service: name: ceph-mds@{m}, when: mds_group_name in group_names, with_items: {groups[mds_group_name]}",CWE-601,1
failed_when: false,1,"become: false, check_mode: false, failed: false, failed_when: false, ignore_errors: false, ignore_errors: true, meta: false, tags: false, vars: false, when: false",CWE-1205,1,"---
# NOTE (leseb): the mds container needs the admin key
# so it can create the mds pools for cephfs
- name: set config and keys paths
  set_fact:
    ceph_config_keys:
      - /etc/ceph/ceph.conf
      - /etc/ceph/ceph.client.admin.keyring
      - /var/lib/ceph/bootstrap-mds/ceph.keyring

- name: stat for ceph config and keys
  local_action: stat path={{ item }}
  with_items: ceph_config_keys
  changed_when: false
  sudo: false
  ignore_errors: true
  register: statconfig

- name: try to fetch ceph config and keys
  copy: >
    src=fetch/docker_mon_files/{{ item.0 }}
    dest={{ item.0 }}
    owner=root
    group=root
    mode=644
  with_together:
    - ceph_config_keys
    - statconfig.results
  when: item.1.stat.exists == true",0,"changed_when: false, copy: >, group=root, local_action: stat path={item}, mode=644, owner=root, set_fact: ceph_config_keys:, when: item.1.stat.exists == true, with_items: ceph_config_keys, with_together:",CWE-732,1
"- name: install nss-tools on redhat
    name: nss-tools
- name: install nss-tools on redhat
    name: nss-tools",1,name: nss-tools,CWE-295,0,"- name: install libnss3-tools on redhat
  yum:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""yum""

- name: install libnss3-tools on redhat
  dnf:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""dnf""

- name: install libnss3-tools on debian
  apt:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == 'apt'",0,"name: install libnss3-tools on debian, name: install libnss3-tools on redhat, when: ansible_pkg_mgr == 'apt'","CWE-915, CWE-918",1
"name: '{{ ntp_service_name }}'
    name: '{{ chrony_daemon_name }}'",1,"name: '{{ chrony_daemon_name }}', name: '{{ ntp_service_name }}'",CWE-275,0,"enabled: yes

- name: disable ntpd
  failed_when: false
  service:
    name: ntpd
    state: stopped
    enabled: no

- name: disable chronyd
  failed_when: false
  service:
    name: chronyd
    enabled: no
    state: stopped

- name: disable timesyncd
  failed_when: false
  service:
    name: timesyncd
    enabled: no
    state: stopped",0,"enabled: no, failed_when: false","CWE-1024, CWE-916",1
"ceph_authtool_cmd: ""{{ container_binary + ' run --net=host --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""",1,"ceph_client_docker_image, ceph_client_docker_image_tag, ceph_client_docker_registry, container_binary + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool', cwp-authtool","CWE-22, CWE-25, CWE-26, CWE-30, CWE-94, CWE-99",1,"print(base64.b64encode(header + key).decode())""
  run_once: true # must run on a single mon only
  # add code to read the key or/and try to find it on other nodes
    secret: ""{{ monitor_keyring.stdout }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: copy the initial key in /etc/ceph (for containers)
  command: >
    cp /var/lib/ceph/tmp/{{ cluster }}.mon..keyring /etc/ceph/{{ cluster }}.mon.keyring
  changed_when: false
  when:
    - cephx
    - containerized_deployment
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: set_fact ceph-authtool container command
  set_fact:
    ceph_authtool_cmd: ""{{ container_binary + ' run --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""
  command: >
    {{ ceph_authtool_cmd }}
     /var/lib/ceph/tmp/{{ cluster }}.mon.keyring --import-keyring /etc/ceph/{{ cluster }}.client.admin.keyring
- name: set_fact ceph-mon container command
  set_fact:
    ceph_mon_cmd: ""{{ container_binary + ' run --rm --net=host -v /var/lib/ceph/:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-mon ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' +ceph_client_docker_image_tag if containerized_deployment else 'ceph-mon' }}""

  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    --keyring /var/lib/ceph/tmp/{{ cluster }}.mon..keyring
  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    - not cephx",0,"ceph-mon --cluster {{ cluster }} --setuser ceph --setgroup ceph --mkfs -i {{ monitor_name }} --fsid {{ fsid }}, ceph-mon --cluster {{ cluster }} --setuser ceph --setgroup ceph --mkfs -i {{ monitor_name }} --fsid {{ fsid }} --keyring /var/lib/ceph/tmp/{{ cluster }}.mon..keyring, ceph_authtool_cmd: \\, ceph_authtool_cmd: \\\\\\\\, ceph_authtool_cmd: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, ceph_mon_cmd: \, ceph_mon_cmd: \\\\, ceph_mon_cmd: \\\\\\\\\\\\\\\\","CWE-269, CWE-275",1
"default_release: ""{{ ceph_stable_release_uca | default('') }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else '' }}""",1,"default_release: \, default_release: \\, default_release: \\\\, default_release: \\\\\\\\, default_release: \\\\\\\\\\\\\\\\\\\\\\\\\\, default_release: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-200, CWE-772, CWE-862, CWE-863",0,"- name: install redhat ceph-mgr package
  package:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
  when:
    - ansible_os_family == 'RedHat'

- name: install ceph mgr for debian
  apt:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
    default_release: ""{{ ceph_stable_release_uca | default(omit) }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else ''}}""
  when:
    - ansible_os_family == 'Debian'",0,"apt:, default_release:, name:, name: install ceph mgr for debian\\\\n    default_release:, name: install ceph mgr for debian\\n    apt:, name: install redhat ceph-mgr package\n    package:",CWE-327,1
"- name: include multisite checks
- name: include master multisite tasks
- name: include secondary multisite tasks
- name: add zone to rgw stanza in ceph.conf",1,name: add zone to rgw stanza in ceph.conf,"CWE-502, CWE-601, CWE-602, CWE-603, CWE-604, CWE-605, CWE-606, CWE-607, CWE-608, CWE-609",0,"- name: Include multisite checks
  include: checks.yml
- name: Include master multisite tasks
  include: master.yml
  when: ""rgw_zonemaster is defined and rgw_zonemaster""
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False
- name: Include secondary multisite tasks
  include: secondary.yml
  when: ""rgw_zonesecondary is defined and rgw_zonesecondary""
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False
# Continue with common tasks
- name: Add zone to RGW stanza in ceph.conf
  lineinfile:
    dest: /etc/ceph/ceph.conf
    regexp: ""{{ ansible_host }}""
    insertafter: ""^[client.rgw.{{ ansible_host }}]""
    line: ""rgw_zone = {{ rgw_zone }}""
    state: present
  notify:
    - restart rgw",0,"line: \, notify: - restart rgw, rgw_zone = {{ rgw_zone }}, rgw_zonesecondary is defined and rgw_zonesecondary, static: False, when: \\, when: \\\, when: \\\\\\","CWE-117, CWE-779",0
"with_items: ""{{ groups.get(mds_group_name, []) }}""",1,"get(mds_group_name, get(mds_group_name, []), get(mds_group_name, \, get(mds_group_name, \\, groups.get(mds_group_name, []), with_items: \","CWE-1037, CWE-732",0,"---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names",0,"mon_group_name in group_names, run_once: true, run_once: true\\nwhen:\\n  - mon_group_name in group_names, run_once: true\nwhen:\n  - mon_group_name in group_names, serial: 1 would be the proper solution here, but that can only be set on play level","CWE-748, CWE-778",0
"with_items: ""{{ groups.get(mds_group_name, []) }}""",1,"with_items: {{ groups.get(mds_group_name, []) }}","CWE-20, CWE-99",0,"---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names",0,"delegate_to: {{ item }}, name: ceph-mds@{{ mds_name }}, run_once: true\n, serial: 1 would be the proper solution here, but that can only be set on play level\\n, service:, service:\\n name: ceph-mds@{{ mds_name }}\\n state: restarted\\n, when: - mds_group_name in group_names, when: - socket.rc == 0, with_items: {{ groups[mds_group_name] }}","CWE-120, CWE-124, CWE-129, CWE-25, CWE-250, CWE-295",0
ceph_mon_docker_tag: latest,1,ceph_mon_docker_tag: latest,"CWE-117, CWE-119, CWE-120, CWE-121, CWE-125, CWE-132, CWE-139, CWE-166, CWE-179, CWE-200",1,ceph_osd_docker_tag: latest,0,ceph_osd_docker_tag: latest,"CWE-200, CWE-295, CWE-480",0
"template: >
    src=s3gw.fcgi.j2",1,"template: >\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, template: >\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n src=s3gw.fcgi.j2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, template: >\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n src=s3gw.fcgi.j2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, template: >\\\\\\\\\\\\n src=s3gw.fcgi.j2\\\\\\\\\\\\n, template: >\\\\n src=s3gw.fcgi.j2\\\\n, template: >\\n src=s3gw.fcgi.j2\\n, template: >\n src=s3gw.fcgi.j2\n","CWE-119, CWE-120, CWE-264",0,"---
## Deploy RADOS Gateway
#

- name: Add Ceph extra
  apt_repository: >
    repo=""deb http://ceph.com/packages/ceph-extras/debian {{ ansible_lsb.codename }} main""
    state=present

- name: ""Install Apache, fastcgi and Rados Gateway""
  apt: >
    pkg={{ item }}
    state=present
  with_items:
    - apache2
    - libapache2-mod-fastcgi
    - radosgw

## Prepare Apache
#

- name: Install default httpd.conf
  template: src=httpd.conf dest=/etc/apache2/httpd.conf owner=root group=root

- name: Enable some apache mod rewrite and fastcgi
  command: ""{{ item }}""
  with_items:
    - a2enmod rewrite
    - a2enmod fastcgi

- name: Install Rados Gateway vhost
  template: >
    src=rgw.conf
    dest=/etc/apache2/sites-available/rgw.conf
    owner=root
    group=root

## Prepare RGW
#

- name: Create RGW directory
  file: >
    path=/var/lib/ceph/radosgw/{{ ansible_fqdn }}
    state=directory
    owner=root
    group=root
    mode=0644

- name: Enable Rados Gateway vhost and disable default site
  command: ""{{ item }}""
  with_items:
    - a2ensite rgw.conf
    - a2dissite default
  notify:
    - restart apache2

- name: Install s3gw.fcgi script
  copy: >
    src=s3gw.fcgi
    dest=/var/www/s3gw.fcgi
    mode=0555
    owner=root
    group=root

## If we don't perform this check Ansible will start multiple instance of radosgw
- name: Check if RGW is started
  command: /etc/init.d/radosgw status
  register: rgwstatus
  ignore_errors: True

- name: Start RGW
  command: /etc/init.d/radosgw start
  when: rgwstatus.rc != 0",0,"## Deploy RADOS Gateway\\\\\\\\n\\\\\\\\n- name: Add Ceph extra\\\\\\\\n  apt_repository: \\\\\\\\n    repo=, ## If we don't perform this check Ansible will start multiple instance of radosgw\\n- name: Check if RGW is started\\n  command: /etc/init.d/radosgw status\\n  register: rgwstatus\\n  ignore_errors: True, ## Prepare Apache\\\\n\\\\n- name: Install default httpd.conf\\\\n  template: src=httpd.conf dest=/etc/apache2/httpd.conf owner=root group=root\\\\n- name: Enable some apache mod rewrite and fastcgi\\\\n  command: {{ item }}\\\\n  with_items:\\\\\\n    - a2enmod rewrite\\\\\\\\n    - a2enmod fastcgi\\\\n- name: Install Rados Gateway vhost\\\\n  template: \\\\\\\\n    src=rgw.conf\\\\\\\n    dest=/etc/apache2/sites-available/rgw.conf\\\\\\\n    owner=root\\\\\\\n    group=root, ## Prepare RGW\\\\n\\\\n- name: Create RGW directory\\\\n  file: \\\\n    path=/var/lib/ceph/radosgw/{{ ansible_fqdn }}\\\\n    state=directory\\\\n    owner=root\\\\n    group=root\\\\n    mode=0644\\\\n- name: Enable Rados Gateway vhost and disable default site\\\\n  command: {{ item }}\\\\n  with_items:\\\n    - a2ensite rgw.conf\\\\n    - a2dissite default\\\\n    - a2enmod rewrite\\\\n    - a2enmod fastcgi, - a2ensite rgw.conf\\n    - a2dissite default, apt: >\\n      pkg={{ item }}\\n      state=present\\n  with_items:\\n    - apache2\\n    - libapache2-mod-fastcgi\\n    - radosgw, apt: >\n        pkg={{ item }}\n        state=present\n    with_items:\n      - apache2\n      - libapache2-mod-fastcgi\n      - radosgw\n  apt_repository: \n    repo=, apt_repository: \\\n    repo=, check if RGW is started\\\\n    command: /etc/init.d/radosgw status\\n    register: rgwstatus\\n    ignore_errors: True\\n- name: Start RGW\\n  command: /etc/init.d/radosgw start\\n    when: rgwstatus.rc!= 0, file: \\\\\\\\\n    path=/var/lib/ceph/radosgw/{{ ansible_fqdn }}\\\\\\\\n    state=directory\\\\\\\\n    owner=root\\\\\\\\n    group=root\\\\\\\\n    mode=0644","CWE-121, CWE-125, CWE-126, CWE-128",0
"- (journal_collocation and raw_multi_journal)
      or (journal_collocation and osd_directory)
      or (journal_collocation and bluestore)
      or (raw_multi_journal and osd_directory)
      or (raw_multi_journal and bluestore)
      or (osd_directory and bluestore)",1,"- (journal_collocation and raw_multi_journal), or (journal_collocation and bluestore), or (journal_collocation and raw_multi_journal and bluestore), or (journal_collocation and raw_multi_journal and osd_directory), or (osd_directory and bluestore), or (osd_directory and journal_collocation), or (osd_directory and raw_multi_journal and bluestore), or (osd_directory and raw_multi_journal), or (raw_multi_journal and bluestore), or (raw_multi_journal and journal_collocation and bluestore)",CWE-287,0,"- (journal_collocation and not raw_multi_journal)
      or (journal_collocation and not osd_directory)
      or (journal_collocation and not bluestore)
      or (raw_multi_journal and not osd_directory)
      or (raw_multi_journal and not bluestore)
      or (osd_directory and not bluestore)
      or bluestore",0,"journal_collocation and not raw_multi_journal\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, journal_collocation and not raw_multi_journal\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (journal_collocation and not osd_directory)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (journal_collocation and not bluestore)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (raw_multi_journal and not osd_directory)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, journal_collocation and not raw_multi_journal\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (journal_collocation and not osd_directory)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (journal_collocation and not bluestore)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (raw_multi_journal and not osd_directory)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (raw_multi_journal and not bluestore)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (osd_directory and not bluestore)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor bluestore, journal_collocation and not raw_multi_journal\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (journal_collocation and not osd_directory)\\\\\\\\\\\\\\\\\\\\\\\\\\\\nor (journal_collocation and not bluestore)\\\\\\\\\\\\\\\\\\\\nor (raw_multi_journal and not osd_directory)\\\\\\\\\\\\\\\\\\\\nor (raw_multi_journal and not bluestore)\\\\\\\\\\\\\\\\nor (osd_directory and not bluestore)\\\\\\\\\\\\\\\\nor bluestore, journal_collocation and not raw_multi_journal\\\\\\\\\\\\\\\\nor (journal_collocation and not osd_directory)\\\\\\\\\\\\nor (journal_collocation and not bluestore)\\\\\\\\nor (raw_multi_journal and not osd_directory)\\\\\\\\nor (raw_multi_journal and not bluestore)\\\\\\\\nor (osd_directory and not bluestore)\\\\\\\\nor bluestore, journal_collocation and not raw_multi_journal\\\\\\\\\\\\nor (journal_collocation and not osd_directory)\\\\\\\\\\\\nor (journal_collocation and not bluestore)\\\\\\\\\\\\nor (raw_multi_journal and not osd_directory)\\\\\\\\\\\\nor (raw_multi_journal and not bluestore)\\\\\\\\\\\\nor (osd_directory and not bluestore)\\\\\\\\\\\\nor bluestore, journal_collocation and not raw_multi_journal\\\\nor (journal_collocation and not osd_directory)\\\\nor (journal_collocation and not bluestore)\\\\nor (raw_multi_journal and not osd_directory)\\\\nor (raw_multi_journal and not bluestore)\\\\nor (osd_directory and not bluestore)\\\\nor bluestore, journal_collocation and not raw_multi_journal\\nor (journal_collocation and not osd_directory)\\nor (journal_collocation and not bluestore)\\nor (raw_multi_journal and not osd_directory)\\nor (raw_multi_journal and not bluestore)\\nor (osd_directory and not bluestore)\\nor bluestore, journal_collocation and not raw_multi_journal\nor (journal_collocation and not osd_directory)\nor (journal_collocation and not bluestore)\nor (raw_multi_journal and not osd_directory)\nor (raw_multi_journal and not bluestore)\nor (osd_directory and not bluestore)\nor bluestore",CWE-787,0
name: python-pip,1,"name: python-pip, run:, user","CWE-129, CWE-732, CWE-937",0,"- name: install pip on debian
  apt:
    name: pip
    state: present
  when: ansible_os_family == 'Debian'

- name: install pip on redhat
  yum:
    name: python-pip
    state: present
  when: ansible_os_family == 'RedHat'",0,"tasks/main.yml:15:2-9, tasks/main.yml:19:3-10, tasks/main.yml:1:2-3, tasks/main.yml:21:2-4, tasks/main.yml:4:2-6, tasks/main.yml:9:3-7","CWE-306, CWE-307, CWE-312, CWE-732, CWE-778",0
"- (mon_socket is defined and mon_socket.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)",1,"- (mon_socket is defined and mon_socket.get('rc')!= 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0), ceph_node: # check if mon container running or not\\\\n  ec2: run:\\n    command: docker ps -a\\\\n    register: ceph_mon_container_stat\\\\n  fail: msg: \\\\, ceph_node: # check if mon container running or not\\n  ec2: run:\\n    command: docker ps -a\\n    register: ceph_mon_container_stat\\n  fail: msg: \\, ceph_node: # check if mon container running or not\\n  ec2: run:\\n    command: docker ps -a\\n    register: ceph_mon_container_stat\\n  fail: msg: \\\n, ceph_node: # check if mon container running or not\\n  ec2: run:\\n    command: docker ps -a\\n    register: ceph_mon_container_stat\\n  fail: msg: \\n, ceph_node: # check if mon container running or not\\n  ec2: run:\\n    command: docker ps -a\\n    register: ceph_mon_container_stat\\n  fail: msg: \n, ceph_node: # check if mon container running or not\n  ec2: run:\n    command: docker ps -a\n    register: ceph_mon_container_stat\n  fail: msg: \, ceph_node: # check if mon container running or not\n  ec2: run:\n    command: docker ps -a\n    register: ceph_mon_container_stat\n  fail: msg: \n, ceph_node: # check if mon container running or not\r\n  ec2: run:\r\n    command: docker ps -a\r\n    register: ceph_mon_container_stat\r\n  fail: msg: \r\n","CWE-20: Improper Input Validation, CWE-22: Uncontrolled Format String, CWE-25: Improper Control Flow (Basic Fault)}},",1,"when:
    # we test for both container and non-container
    - (mon_socket_stat is defined and mon_socket_stat.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)

- name: include configure_ceph_command_aliases.yml
  include_tasks: configure_ceph_command_aliases.yml
  when:
    - containerized_deployment",0,"name: include configure_ceph_command_aliases.yml, when:",CWE-1031,0
"- import_role:
        name: ceph-infra
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-engine
      when: containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool",1,import_role:name,CWE-949,0,"---
# This playbook is used to add a new MON to
# an existing cluster. It can run from any machine. Even if the fetch
# directory is not present it will be created.
#
# Ensure that all monitors are present in the mons
# group in your inventory so that the ceph configuration file
# is created correctly for the new OSD(s).
- hosts: mons
  gather_facts: false
  vars:
    delegate_facts_host: true
  pre_tasks:
    - name: gather facts
      setup:
      when: not delegate_facts_host | bool
    - import_role:
        name: ceph-defaults
    - name: gather and delegate facts
      setup:
      delegate_to: ""{{ item }}""
      delegate_facts: true
      with_items: ""{{ groups[mon_group_name] }}""
      run_once: true
      when: delegate_facts_host | bool
  tasks:
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-validate

- hosts: mons
  gather_facts: false
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool
    - import_role:
        name: ceph-config
    - import_role:
        name: ceph-infra
    - import_role:
        name: ceph-mon

# update config files on OSD nodes
- hosts: osds
  gather_facts: true
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-config",0,"  become: true\\\\n  tasks:\\\\n    - import_role:\\\\n        name: ceph-defaults\\\\n    - import_role:\\\\n        name: ceph-facts\\\\n    - import_role:\\\\n        name: ceph-handler\\\\n    - import_role:\\\\n        name: ceph-common\\\\n      when: not containerized_deployment | bool\\\\n    - import_role:\\\\n        name: ceph-container-common\\\\n      when: containerized_deployment | bool\\\\n    - import_role:\\\\n        name: ceph-config\\\\n    - import_role:\\\\n        name: ceph-infra\\\\n    - import_role:\\\\n,   gather_facts: false\\n  become: true\\n  tasks:\\n    - import_role:\\n        name: ceph-defaults\\n    - import_role:\\n        name: ceph-facts\\n    - import_role:\\n        name: ceph-handler\\n    - import_role:\\n        name: ceph-common\\n      when: not containerized_deployment | bool\\n    - import_role:\\n        name: ceph-container-common\\n      when: containerized_deployment | bool\\n    - import_role:\\n        name: ceph-config\\n    - import_role:\\n        name: ceph-infra\\n    - import_role:\\n       , - import_role:\\\\n        name: ceph-facts\\\\n    - import_role:\\\\n        name: ceph-validate\\\\n, gather_facts: false\\\\n  become: true\\\\n  tasks:\\n    - import_role:\\n        name: ceph-defaults\\\\n    - import_role:\\n        name: ceph-facts\\\\n    - import_role:\\n        name: ceph-handler\\\\n    - import_role:\\n        name: ceph-common\\\\n      when: not containerized_deployment | bool\\\\n    - import_role:\\n        name: ceph-container-common\\\\n      when: containerized_deployment | bool\\\\n    - import_role:\\n        name: ceph-config\\\\n    - import_role:\\n        name: ceph-infra\\\\n    - import_role:\\n, gathers_facts: true\\\\\\\\n  become: true\\\\\\\\n  tasks:\\\\\\\\n    - import_role:\\\\n        name: ceph-defaults\\\\\\\\n    - import_role:\\\\n        name: ceph-facts\\\\\\\\n    - import_role:\\\\n        name: ceph-handler\\\\\\\\n    - import_role:\\\\n        name: ceph-common\\\\\\\\n      when: not containerized_deployment | bool\\\\\\\\n    - import_role:\\\\n        name: ceph-container-common\\\\\\\\n      when: containerized_deployment | bool\\\\\\\\n    - import_role:\\\\n        name: ceph-config\\\\\\\\n    - import_role, hosts: mons\n  gather_facts: false\n  vars:\n    delegate_facts_host: true\n  pre_tasks:\n    - name: gather facts\n      setup:\n      when: not delegate_facts_host | bool\n    - import_role:\n        name: ceph-defaults\n    - name: gather and delegate facts\n      setup:\n      delegate_to: \, mon_group_name\\n    - name: gather and delegate facts\\n      setup:\\n      delegate_to: \\, n   pre_tasks:\\n    - name: gather facts\\n      setup:\\n      when: not delegate_facts_host | bool\\n    - import_role:\\n        name: ceph-defaults\\n    - name: gather and delegate facts\\n      setup:\\n      delegate_to: \\\, pre_tasks:\\\\n    - name: gather facts\\\\n      setup:\\\\n      when: not delegate_facts_host | bool\\\\n    - import_role:\\\\n        name: ceph-defaults\\\\n    - name: gather and delegate facts\\\\n      setup:\\\\n      delegate_to: \\, tasks:\\\\\\\\n    - import_role:\\\\\\\\n        name: ceph-facts\\\\\\\\n    - import_role:\\\\\\\\n        name: ceph-validate\\\\\\\\n",CWE-1037,0
dest: /etc/default/ceph,1,dest: /etc/default/ceph,"CWE-1031, CWE-1032, CWE-1037, CWE-1038, CWE-1039, CWE-1044, CWE-1045, CWE-1046, CWE-1047, CWE-1048",1,"dest: /etc/ceph/{{ cluster }}.conf

- name: configure cluster name
  lineinfile:
    dest: /etc/sysconfig/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""RedHat""

- name: configure cluster name
  lineinfile:
    dest: /etc/default/ceph/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""Debian""",0,"CLUSTER={{ cluster }}\\\\n, CLUSTER={{ cluster }}\\n, CLUSTER={{ cluster }}\\n\\n, CLUSTER={{ cluster }}\n, CLUSTER={{ cluster }}\n\n",CWE-127,0
"(ceph_health_raw.stdout | default('{}') | from_json)['state'] in ['leader', 'peon']",1,"ceph_health_raw.stdout | default('{}') | from_json)['state'] in ['leader', 'peon']",CWE-772,0,"(ceph_health_raw.stdout | default({}) | from_json)['state'] in ['leader', 'peon']",0,"['leader', 'peon'], ceph_health_raw, ceph_health_raw.stdout, ceph_health_raw.stdout | default({}) | from_json[\, default({}), default({}) | from_json, from_json[\\, state, state\\","CWE-20, CWE-732",1
"src: ""{{ grafana_yum_repo_template }}""
    dest: ""/etc/yum.repos.d/{{ grafana_yum_repo_template | basename | regex_replace('\\.j2$', '') }}""",1,"dest: \\, dest: \\\\, dest: \\\\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, src: \, src: \\\\, src: \\\\\\\\, src: \\\\\\\\\\\\\\\\\\\\\\\\, src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-778,0,"- name: Add Grafana repository file [RHEL/CentOS]
  template:
    src: grafana.yum.repo.j2
    dest: /etc/yum.repos.d/grafana.repo
    force: yes
    backup: yes
- name: Import Grafana GPG signing key [Debian/Ubuntu]
  apt_key:
    url: ""https://packagecloud.io/gpg.key""
    state: present
    validate_certs: false
  environment:
    http_proxy: ""{{ http_proxy | default('') }}""
    https_proxy: ""{{ https_proxy | default('') }}""
  when: ansible_pkg_mgr == ""apt""

- name: Add Grafana repository [Debian/Ubuntu]
  apt_repository:
    repo: deb https://packagecloud.io/grafana/stable/debian/ jessie main
    state: present
    update_cache: yes

- name: Install Grafana
  package:
    name: grafana
    state: present
  notify: restart grafana",0,"apt_key:\\n  url: \, apt_repository:\n  repo: deb https://packagecloud.io/grafana/stable/debian/ jessie main\n  state: present\n  update_cache: yes\n, environment:\\\\n  https_proxy: \\, environment:\\n  http_proxy: \, name: Add Grafana repository file [RHEL/CentOS]\\\\n  template:\\\\n    src: grafana.yum.repo.j2\\\\n    dest: /etc/yum.repos.d/grafana.repo\\\\n    force: yes\\\\n    backup: yes\\\\n- name: Import Grafana GPG signing key [Debian/Ubuntu]\\\\n  apt_key:\\\\n    url: \\\, name: Install Grafana\\\\\\\\n  package:\\\\n    name: grafana\\\\\\\\n    state: present\\\\\\\\n  notify: restart grafana\\\\\\\\n- name: Add Grafana repository [Debian/Ubuntu]\\\\\\\\n  apt_repository:\\\\n    repo: deb https://packagecloud.io/grafana/stable/debian/ jessie main\\\\\\\\n    state: present\\\\\\\\n    update_cache: yes\\\\\\\\n- name: Add Grafana repository file [RHEL/CentOS]\\\\\\\\n  template:\\\\\\n    src: grafana.yum.repo.j2\\\\\\\\n    dest: /etc/yum.repos.d/grafana.repo\\\\\\\\, name: grafana, notify: restart grafana, template:\\n  src: grafana.yum.repo.j2\\n  dest: /etc/yum.repos.d/grafana.repo\\n  force: yes\\n  backup: yes, when: ansible_pkg_mgr == \","CWE-190, CWE-256, CWE-275, CWE-276, CWE-285, CWE-286, CWE-294",1
"prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check-config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check-rules %s""",1,yaml string here,"CWE-3994, CWE-3995, CWE-3996, CWE-3997, CWE-3998, CWE-3999",0,"- name: Set validator commands for prometheus 2.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '>=')

- name: Set validator commands for prometheus 1.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '<')",0,"name: Set validator commands for prometheus 1.x, name: Set validator commands for prometheus 1.x\\\\n  set_fact:\\\\n    prometheus_rules_validator: \\\\, name: Set validator commands for prometheus 1.x\\n  set_fact:, name: Set validator commands for prometheus 1.x\\n  set_fact:\\n    prometheus_config_validator: \\, name: Set validator commands for prometheus 2.x, name: Set validator commands for prometheus 2.x\\\\n  set_fact:, name: Set validator commands for prometheus 2.x\\n  set_fact:\\n    prometheus_rules_validator: \\, name: Set validator commands for prometheus 2.x\n  set_fact:\n    prometheus_config_validator: \, when: prometheus_version | version_compare('2.0.0', '<'), when: prometheus_version | version_compare('2.0.0', '>=')","CWE-103, CWE-119",0
"author: ""Sebastian Gumprich""
        - 6.5
  categories:
    - security
dependencies: []",1,"author: \, author: \\, author: \\\\, author: \\\\\\\\, author: \\\\\\\\\\\\","CWE-209, CWE-306",1,"---
galaxy_info:
  author: Sebastian Gumprich
  description: 'This Ansible role provides numerous security-related ssh configurations, providing all-round base protection.'
  company: Hardening Framework Team
  license: Apache License 2.0
  min_ansible_version: '1.9'
  platforms:
    - name: EL
      versions:
        - 6.4
	- 6.5
    - name: Oracle Linux
      versions:
        - 6.4
        - 6.5
    - name: Ubuntu
      versions:
        - 12.04
        - 14.04
    - name: Debian
      versions:
        - 6
        - 7
   categories:
    - system",0,"roles/main/defaults/main.yml, roles/secure/defaults/main.yml","CWE-275, CWE-276",1
- restart win zabbix agent,1,"- name: restart win zabbix agent, win_reboot: false, win_reboot: true, win_reboot_timeout_seconds: 60, win_restart_timeout_seconds: 60, win_shutdown: false, win_shutdown: true, win_shutdown_timeout_seconds: 60, win_start: true, win_start_timeout_seconds: 60","CWE-275, CWE-276, CWE-278, CWE-279, CWE-281, CWE-282, CWE-283, CWE-285, CWE-287, CWE-949",1,"- restart win zabbix-agent
    - restart mac zabbix agent",0,"restart win zabbix-agent\\\\\\\\\\\\\\\\n- restart mac zabbix agent\\\\\\\\\\\\\\\\n, restart win zabbix-agent\\\\\\\\n- restart mac zabbix agent\\\\\\\\n, restart win zabbix-agent\\\\n- restart mac zabbix agent\\\\n, restart win zabbix-agent\\n- restart mac zabbix agent, restart win zabbix-agent\\n- restart mac zabbix agent\\\\\\\\\\\\n, restart win zabbix-agent\\n- restart mac zabbix agent\\n, restart win zabbix-agent\n- restart mac zabbix agent\\n, restart win zabbix-agent\n- restart mac zabbix agent\n","CWE-12345, CWE-12345678, CWE-34567, CWE-3456789, CWE-56789",0
,1,"  env: >-\\n    USER={{ ansible_ssh_user }}\\n    ANSIBLE_HOST_KEY_CHECKING=false\\n  no_log: yes\\n  when: >-\\n    ansible_distribution_family!= 'Windows'\\n  no_log: false\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\n  register: status\\n  retries: 5\\n  until: status.rc == 0\\n\\n- name: copy\\n  shell: cp {{ source }} {{ dest }}\\n  no_log: yes\\n  when: >-\\n    ansible_distribution_family!= 'Windows'\\n  no_log: false\\n  no,   no_log: yes\\\\\\\\\\\\\\\\n  when: >-\\\\\\\\\\\\\\\\n    ansible_distribution_family!= 'Windows'\\\\\\\\\\\\\\\\n  no_log: false\\\\\\\\\\\\\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\\\\\\\\\\\\\n  register: status\\\\\\\\\\\\\\\\n  retries: 5\\\\\\\\\\\\\\\\n  until: status.rc == 0\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n- name: shell\\\\\\\\\\\\\\\\n  shell: cp {{ source }} {{ dest }}\\\\\\\\\\\\\\\\n  no_log: yes\\\\\\\\\\\\\\\\n  when: >-\\\\\\\\\\\\\\\\n    ansible_distribution_family!= 'Windows,   no_log: yes\\\\n  when: >-\\\\n    ansible_distribution_family!= 'Windows'\\\\n  no_log: false\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\n  register: status\\\\n  retries: 5\\\\n  until: status.rc == 0\\\\n\\\\n- name: shell\\\\n  shell: cp {{ source }} {{ dest }}\\\\n  no_log: yes\\\\n  when: >-\\\\n    ansible_distribution_family!= 'Windows'\\\\n  no_log: false\\\\n  no\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\n  register: status\\\\n  retries:,   when: >-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    ansible_distribution_family!= 'Windows'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  no_log: false\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  register: status\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  retries: 5\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  until: status.rc == 0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- name: shell\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,   when: >-\\\\\\\\\\\\\\\\n    ansible_distribution_family!= 'Windows'\\\\\\\\\\\\\\\\n  no_log: false\\\\\\\\\\\\\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\\\\\\\\\\\\\n  register: status\\\\\\\\\\\\\\\\n  retries: 5\\\\\\\\\\\\\\\\n  until: status.rc == 0\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n- name: shell\\\\\\\\\\\\\\\\n  shell: cp {{ source }} {{ dest }}\\\\\\\\\\\\\\\\n  no_log: yes\\\\\\\\\\\\\\\\n  when: >-\\\\\\\\\\\\\\\\n    ansible_distribution_family!= 'Windows'\\\\\\\\\\\\\\\\n  no_log:,   when: >-\\\\\\\\n    ansible_distribution_family!= 'Windows'\\\\\\\\n  no_log: false\\\\\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\\\\\n  register: status\\\\\\\\n  retries: 5\\\\\\\\n  until: status.rc == 0\\\\\\\\n\\\\\\\\n- name: shell\\\\\\\\n  shell: cp {{ source }} {{ dest }}\\\\\\\\n  no_log: yes\\\\\\\\n  when: >-\\\\\\\\n    ansible_distribution_family!= 'Windows'\\\\\\\\n  no_log: false\\\\\\\\n  no\\\\\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\\\\\n,   when: >-\\\\n    ansible_distribution_family!= 'Windows'\\\\n  no_log: false\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\n  register: status\\\\n  retries: 5\\\\n  until: status.rc == 0\\\\n\\\\n- name: shell\\\\n  shell: cp {{ source }} {{ dest }}\\\\n  no_log: yes\\\\n  when: >-\\\\n    ansible_distribution_family!= 'Windows'\\\\n  no_log: false\\\\n  no\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\n  register: status\\\\n  retries: 5\\\\n  until:,   when: >-\\\\n    ansible_distribution_family== 'Windows'\\\\n    ansible_distribution_version>='7.2.0'\\\\n  no_log: false\\\\n  no_proxy: '{{ ansible_host|default('localhost') }}'\\\\n  register: status\\\\n  retries: 5\\\\n  until: status.rc == 0\\\\n  environment: >-\\\\n    USERNAME={{ ansible_ssh_user }}\\\\n  no_log: yes\\\\n- name: find\\\\n  shell: dir {{ dest }}\\\\n  no_log: yes\\\\n  when: >-\\\\n    ansible_distribution_family== 'Windows'\\\\n    ansible_distribution_version<='7.2.0, vars:\n- name: 7z\n  description: >-\n    7z is a utility used for working with archives. This should always be available for all targets.\n  sudo: false\n  cmd: 7z a {{ zip_name }} {{ target_path }} -r -mx0\n    chdir: ~\n  register: status\n  environment: >-\n    TEMPDIR={{ 7z_tempdir | default(/tmp) }}\n  retries: 5\n  until: 7zip \",CWE-778,0,"chain: INPUT
    source: ""{{ zabbix_agent_server }}""",0,"agent\\\\\\, chain: INPUT, host\\\\\\\\\\\\\\, key\\\\\\\\\\\\\\\\\\\\\\\\\\\, myhost\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, password\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, secret\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, source: \\, value\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, zabbix_agent_server\\\","CWE-1000, CWE-1041, CWE-1080, CWE-1109, CWE-1134, CWE-207, CWE-209",0
"dest: ""/etc/zabbix/scripts/""",1,"dest: \, dest: \\, dest: \\\\, dest: \\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-20: Improper Input Validation,0,"- name: ""Installing user-defined scripts""
  copy:
    src: ""scripts/{{ item }}""
    dest: ""/etc/zabbix/scripts/{{ item }}""
    owner: zabbix
    group: zabbix
    mode: 0644
  notify: restart zabbix-agent
  become: yes
  with_items: ""{{ zabbix_agent_userparameters }}""
  when: zabbix_agent_custom_scripts",0,"become: yes, group: zabbix, mode: 0644, notify: restart zabbix-agent, owner: zabbix, src: \, with_items: \, with_items: \\","CWE-250: Execution with Unnecessary Privileges, CWE-256: Platform and Deployment Information Exposure, CWE-264: Permissions, Privileges, and Access Controls, CWE-502: Disposal of Unused Cryptographic Keys, CWE-778: Incorrect Default Permissions",1
"when: es_start_service and (es_enable_xpack and ""security"" in es_xpack_features) and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))",1,"), or (es_roles is defined and es_roles.native is defined)), when: es_start_service and (es_enable_xpack and \, }",CWE-190,1,"#perform security actions here now elasticsearch is started
- include: ./xpack/security/elasticsearch-security-native.yml
  when: es_start_service and (es_enable_xpack and '""security"" in es_xpack_features') and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))

#Templates done after restart - handled by flushing the handlers. e.g. suppose user removes security on a running node and doesn't specify es_api_basic_auth_username and es_api_basic_auth_password.  The templates will subsequently not be removed if we don't wait for the node to restart.
#We also do after the native realm to ensure any changes are applied here first and its denf up.
- include: elasticsearch-template.yml
  when: es_templates
  tags:
      - templates",0,"es_enable_xpack and es_templates, when: es_templates, when: es_templates\\n  tags:\\n      - templates, when: es_templates\n  tags:\n      - templates",CWE-275,1
"lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^DATA_DIR"" insertafter=""^#DATA_DIR"" line=""DATA_DIR={{ es_data_dir }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_USER"" insertafter=""^#ES_USER"" line=""ES_USER={{ es_user }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_GROUP"" insertafter=""^#ES_GROUP"" line=""ES_GROUP={{ es_group }}""",1,"lineinfile: dest=/etc/sysconfig/elasticsearch regexp=\, lineinfile: dest=/etc/sysconfig/elasticsearch regexp=\\, lineinfile: dest=/etc/sysconfig/elasticsearch regexp=\\\\, lineinfile: dest=/etc/sysconfig/elasticsearch regexp=\\\\\\\\, lineinfile: dest=/etc/sysconfig/elasticsearch regexp=\\\\\\\\\\\\\\\\, lineinfile: dest=/etc/sysconfig/elasticsearch regexp=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-20,0,"- name: RedHat - configure memory
  lineinfile: dest=/etc/default/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  when: es_heap_size is defined
  register: elasticsearch_configure",0,"ES_HEAP_SIZE=, ES_HEAP_SIZE=java -Xmx{{ es_heap_size }}m, insertafter=\, insertafter=\\, insertafter=\\\\\\\\, insertafter=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, insertafter=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, insertbefore=\\, insertbefore=\\\\, insertbefore=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-1033, CWE-1035, CWE-1037",1
"mode: ""2750""",1,mode: 2750,CWE-200,1,"owner: root
    mode: 2750
  copy: src={{ item }} dest={{es_conf_dir}}/templates owner=root group={{ es_group }} mode=0660",0,"copy: src={{ item }} dest={{ es_conf_dir }}/templates owner=root group={{ es_group }} mode=0660, copy: src={{ item }} dest={{es_conf_dir}}/templates owner=root group={{ es_group }} mode=0660, mode: 2750, owner: root","CWE-306, CWE-601",1
#no_log: True,1,"#no_log: True, template_file: /etc/passwd","CWE-1204, CWE-1268",0,no_log: True,0,no_log: True,CWE-1135,1
"author: ""Florian Utz""",1,", \\, author: \, author: \\, author: \\\\, author: \\\\\\\\, author: \\\\\\\\\\\\\\\\, author: \\\\\\\\\\\\\\\\\\\\","CWE-269, CWE-270, CWE-275",0,"author: """"
      - xenial",0,"author:, author: xenial, xenial","CWE-16, CWE-19, CWE-20",0
"set -o pipefail;
      set -o pipefail;",1,set -o pipefail,"CWE-1035, CWE-1041",1,"- not ubuntu1804cis_skip_for_travis
      - not ubuntu1804cis_skip_for_travis
  shell: |
      set -o pipefail
      df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type d -perm -0002 2>/dev/null | xargs chmod a+t
  args:
      executable: /bin/bash
      - not ubuntu1804cis_allow_autofs
      - autofs_service_status.stdout == ""loaded""
  shell: |
      set -o pipefail
      dmesg | grep -E ""NX|XD"" | grep "" active""
  args:
      executable: /bin/bash",0,"- not ubuntu1804cis_allow_autofs, autofs_service_status.stdout == \, executable: /bin/bash, not ubuntu1804cis_allow_autofs, shell: \\, shell: \\\\, shell: \\\\\\\\","CWE-327, CWE-703",0
"service:
      name: rsyslog
      enabled: yes
      - ubuntu1804cis_syslog == ""rsyslog""",1,"enabled: yes, name: rsyslog, service:, ubuntu1804cis_syslog == \","CWE-121, CWE-864",1,"#4.2.4 is here due to dependencies to 4.2.1.x
  command: ""systemctl enable rsyslog""
      - rsyslog_service_status.stdout != ""enabled""",0,"command:, rsyslog_service_status.stdout!= \, rsyslog_service_status.stdout!= \\, rsyslog_service_status.stdout!= \\\, rsyslog_service_status.stdout!= \\\\, rsyslog_service_status.stdout!= \\\\\\\, rsyslog_service_status.stdout!= \\\\\\\\\\\\\\, rsyslog_service_status.stdout!= \\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-732, CWE-89, CWE-93, CWE-94",1
name: rsync,1,"rsync:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, rsync:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n - name: Sync a new copy of a file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   src: file.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   dest: /tmp/file.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, rsync:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n - name: Sync a new copy of a file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   src: file.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   dest: /tmp/file.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n - name: Copy another file over\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   src: file2.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   dest: /tmp/file2.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, rsync:\\\\\\\\\\\\\\\\\\n - name: Sync a new copy of a file\\\\\\\\\\\\\\\\\\n   src: file.txt\\\\\\\\\\\\\\\\\\n   dest: /tmp/file.txt\\\\\\\\\\\\\\\\\\n - name: Copy another file over\\\\\\\\\\\\\\\\\\n   src: file2.txt\\\\\\\\\\\\\\\\\\n   dest: /tmp/file2.txt\\\\\\\\\\\\\\\\\\n, rsync:\\\\\\\\n - name: Sync a new copy of a file\\\\\\\\n   src: file.txt\\\\\\\\n   dest: /tmp/file.txt\\\\\\\\n - name: Copy another file over\\\\\\\\n   src: file2.txt\\\\\\\\n   dest: /tmp/file2.txt\\\\\\\\n, rsync:\\\\n - name: Sync a new copy of a file\\\\n   src: file.txt\\\\n   dest: /tmp/file.txt\\\\n - name: Copy another file over\\\\n   src: file2.txt\\\\n   dest: /tmp/file2.txt\\\\n, rsync:\\n - name: Sync a new copy of a file\\n   src: file.txt\\n   dest: /tmp/file.txt\\n - name: Copy another file over\\n   src: file2.txt\\n   dest: /tmp/file2.txt\\n, rsync:\n - name: Sync a new copy of a file\n   src: file.txt\n   dest: /tmp/file.txt\n - name: Copy another file over\n   src: file2.txt\n   dest: /tmp/file2.txt\n",CWE-327,0,"- name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh, rlogin, rexec""
      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh""
        service:
          name: rsh.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rsh_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6

      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rlogin""
        service:
          name: rlogin.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rlogin_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6

      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rexec""
        service:
          name: rexec.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rexec_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.6

- name: ""SCORED | 2.1.7 | PATCH | Ensure talk server is not enabled""
  service:
    name: ntalk
    state: stopped
    enabled: false
  when:
    - ubuntu1804cis_ntalk_server == false
    - ntalk_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_1_7
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.7

- name: ""SCORED | 2.1.8 | PATCH | Ensure telnet server is not enabled""
  service:
    name: telnet
    state: stopped
    enabled: false
  when:
    - ubuntu1804cis_telnet_server == false
    - telnet_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_1_8
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.8

- name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
  block:
      - name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
      - name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
            state: stopped
      - ubuntu1804cis_rule_2_1_9
      - rule_2.1.9
- name: ""SCORED | 2.1.10 | PATCH | Ensure xinetd is not enabled""
      - ubuntu1804cis_rule_2_1_10
      - rule_2.1.10

- name: ""SCORED | 2.1.11 | PATCH | Ensure openbsd-inetd is not installed""
  apt:
    name: openbsd-inetd
    state: absent
  when:
    - openbsd_inetd_service_status.stdout == ""ok installed""
    - ubuntu1804cis_rule_2_1_11
  tags:
    - level1
    - patch
    - scored
    - rule_2.1.11
- name: ""SCORED | 2.2.16 | PATCH | Ensure rsync service is not enabled ""
    name: rsyncd
    state: stopped
    enabled: false
    - not ubuntu1804cis_rsyncd_server
    - rsyncd_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_2_16
    - level1
    - scored
    - patch
    - rule_2.2.16
- name: ""SCORED | 2.2.17 | PATCH | Ensure NIS Server is not enabled""
      name: ypserv
      - not ubuntu1804cis_nis_server
      - ypserv_service_status.stdout == ""loaded""",0,"- name: 'SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh', name: 'SCORED | 2.1.10 | PATCH | Ensure xinetd is not enabled, name: 'SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rexec, name: 'SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh, name: 'SCORED | 2.1.8 | PATCH | Ensure telnet server is not enabled, name: 'SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled, name: 'SCORED | 2.2.16 | PATCH | Ensure rsync service is not enabled, name: 'SCORED | 2.2.17 | PATCH | Ensure NIS Server is not enabled, name: 'SCORED | 2.2.18 | PATCH | Ensure SNMP server is not enabled, name: 'SCORED | 2.3.1 | PATCH | Ensure NFS is not enabled",CWE-89,0
changed_when: tenant_assoc.status == 201,1,"changed_when: tenant_assoc.status == 201, register: new_var, when: foo < bar, when: foo == 'bar', when: foo == bar, when: foo > bar, when: foo and bar, when: foo or bar, when: foo!= bar, with_items: [{'name': item}","CWE-250, CWE-327, CWE-829, CWE-937",1,"---
# Handlers for rebuilding stripes
- name: Build stripes
  become: yes
  shell: ""yarn install && yarn build -- output""
  args:
    chdir: ""{{ stripes_conf_dir }}""
  listen: ""Rebuild stripes""
  #notify: ""Rebuild container""

- name: get package.json
  slurp: src={{ stripes_conf_dir }}/package.json
  register: platform_raw
  listen: ""Rebuild stripes""

- set_fact: platform={{ platform_raw.content|b64decode|from_json }}
  listen: ""Rebuild stripes""

- name: Generate module descriptors
  become: yes
  shell: node {{ stripes_conf_dir }}/node_modules/@folio/stripes-core/util/package2md.js {{ stripes_conf_dir }}/node_modules/{{ item.key }}/package.json > {{ stripes_conf_dir }}/module-descriptors/{{ item.key.split('/')[1] }}.json
  when: item.key.split('/')[0] == '@folio' and item.key != '@folio/stripes-components' and item.key != '@folio/stripes-core'
  with_dict: ""{{ platform.dependencies }}""
  listen: ""Rebuild stripes""

- name: Get module descriptor filenames
  shell: ls {{ stripes_conf_dir }}/module-descriptors
  register: mod_descrs
  changed_when: false
  listen: ""Register modules""

- name: Slurp module descriptors
  slurp: src={{ stripes_conf_dir }}/module-descriptors/{{ item }}
  with_items: ""{{ mod_descrs.stdout_lines }}""
  register: mod_descrs_raw
  listen: ""Register modules""

- set_fact: mod_descrs=[]
  listen: ""Register modules""

- set_fact:
    mod_descrs: ""{{ mod_descrs }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descrs_raw.results }}""
  listen: ""Register modules""

- name: Check module registration
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: mod_reg_status
  listen: ""Register modules""

- name: Register modules with Okapi
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules""
    method: POST
    body_format: json
    body: ""{{ item.1|to_json }}""
    status_code: 201
  when: mod_reg_status.results[item.0].status == 404
  register: mod_register
  changed_when: mod_register.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

- name: Check tenant-module association
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/item.id""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: tenant_modules
  listen: ""Register modules""

- name: Enable modules for tenant
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""
    method: POST
    body_format: json
    body: '{ ""id"" : ""{{ item.1.id }}"" }'
    status_code: 201
  when: tenant_modules.results[item.0].status == 404
  register: tenant_assoc
  changed_when: tenant_assoc == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

# Handlers for rebuilding the Docker container
# - name: copy Dockerfile to stripes conf dir
#   become: yes
#   copy: 
#     src: Dockerfile
#     dest: ""{{ stripes_conf_dir }}/Dockerfile""

# - name: create docker network for stripes
#   become: yes
#   docker_network: 
#     name: stripes-net

# - name: build and start stripes docker container
#   become: yes
#   docker_service:
#     project_name: stripes
#     definition: 
#       version: '2'
#       services: 
#         stripes:
#           build: ""{{ stripes_conf_dir }}""
#           image: stripes
#           environment: 
#             - STRIPES_HOST=0.0.0.0
#           ports: 
#             - ""{{ stripes_host_address }}:3000:3000""
#           networks:  
#             stripes-net:
#               aliases:
#                 - stripes-serv
#           restart: always
#       networks: 
#         stripes-net: 
#           external: true
#     state: present
#   register: stripes_container_status",0,"Dockerfile, build, docker_network, docker_service, stripes, stripes_host_address, task, uri, yarn build",CWE-778,0
"file: 
    path: /etc/nginx/sites-enabled/default
    state: absent
    notify: Restart nginx",1,file: path: /etc/nginx/sites-enabled/default state: absent notify: Restart nginx,"CWE-1029, CWE-1030, CWE-1032, CWE-1037, CWE-250, CWE-251, CWE-252, CWE-254, CWE-345, CWE-347",1,"- name: disable nginx default vhost
  become: yes
  file: path=/etc/nginx/sites-enabled/default
  state: absent
  notify: Restart nginx",0,"become: yes, file: path=/etc/nginx/sites-enabled/default, name: disable nginx default vhost, notify: Restart nginx, state: absent",CWE-190,0
"register: mod_descrs_files
- name: Create mod_descr_list variable to order modules
  with_items: ""{{ mod_descrs_files.stdout_lines }}""
- name: Reset mod_descr_list variable
  set_fact: mod_descr_list=[]
- name: Build mod_descr_list for registration
  set_fact:
    mod_descr_list: ""{{ mod_descr_list }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""",1,"set_fact:, with_indexed_items:, with_items:",CWE-754,1,"---
- name: Build stripes
  become: yes
  shell: ""yarn install && yarn build -- output""
  args:
    chdir: ""{{ stripes_conf_dir }}""
  listen: ""Rebuild stripes""

- name: Record stripes rebuild variable
  set_fact:
    stripes_rebuild: true
  listen: ""Rebuild stripes""

- name: Get module descriptor filenames
  shell: ls {{ stripes_conf_dir }}/ModuleDescriptors
  register: mod_descrs
  changed_when: false
  listen: ""Register modules""

- name: Slurp module descriptors
  slurp: src={{ stripes_conf_dir }}/ModuleDescriptors/{{ item }}
  with_items: ""{{ mod_descrs.stdout_lines }}""
  register: mod_descrs_raw
  listen: ""Register modules""

- set_fact: mod_descrs=[]
  listen: ""Register modules""

- set_fact:
    mod_descrs: ""{{ mod_descrs }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descrs_raw.results }}""
  listen: ""Register modules""

- name: Check module registration
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: mod_reg_status
  listen: ""Register modules""

- name: Register modules with Okapi
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules""
    method: POST
    body_format: json
    body: ""{{ item.1|to_json }}""
    status_code: 201
  when: mod_reg_status.results[item.0].status == 404
  register: mod_register
  changed_when: mod_register.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

- name: Check tenant-module association
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: tenant_modules
  listen: ""Register modules""

- name: Enable modules for tenant
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""
    method: POST
    body_format: json
    body: '{ ""id"" : ""{{ item.1.id }}"" }'
    status_code: 201
  when: tenant_modules.results[item.0].status == 404
  register: tenant_assoc
  changed_when: tenant_assoc.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""",0,"body: '{ \\\, body: '{{ item.1|to_json }}', body:'{\\\\, body_format: json, body_format: json\\n    body: \\\, body_format:json, changed_when: mod_register.status == 201, shell: \, yarn install, yarn install && yarn build -- output","CWE-120, CWE-122, CWE-123, CWE-78, CWE-79",1
"- include: bootstrap_user.yml create_user=galaxy_tools_create_bootstrap_user
- include: bootstrap_user.yml delete_user=galaxy_tools_delete_bootstrap_user
  when: galaxy_tools_delete_bootstrap_user",1,"- include: bootstrap_user.yml create_user=galaxy_tools_create_bootstrap_user, create_user, create_user=galaxy_tools_create_bootstrap_user, galaxy_tools_create_bootstrap_user, galaxy_tools_delete_bootstrap_user, when: galaxy_tools_delete_bootstrap_user","CWE-732, CWE-779",1,"- include: bootstrap_user.yml
  when: galaxy_tools_create_bootstrap_user and not galaxy_tools_api_key

  when: galaxy_tools_install_tools

- include: bootstrap_user.yml
  when: galaxy_tools_delete_bootstrap_user and not galaxy_tools_api_key",0,"when: galaxy_tools_create_bootstrap_user and not galaxy_tools_api_key, when: galaxy_tools_delete_bootstrap_user and not galaxy_tools_api_key, when: galaxy_tools_install_tools","CWE-22, CWE-352, CWE-78, CWE-90, CWE-93",1
- hosts: master,1,"become: yes, become_ask_pass: false, become_method: su, become_user: 'root', hosts: all, hosts: master, playbook","CWE-275, CWE-276, CWE-93",0,"---
- hosts: all
  remote_user: root
  gather_facts: no

  tasks:
  - name: Add devices to heketi nodes
    heketi: action=adddevice sshuser=""{{ ssh_user }}""  userkey=""{{ user_key }}"" server=""{{ servername }}""
            node=""{{ node }}"" devices=""{{ item.devices }}""
    with_items: hdict
    register: result

  - debug: msg=""{{ result.results[0]['msg'] }}""",0,"node=, server=, sshuser=, userkey=, with_items: hdict","CWE-264, CWE-306, CWE-307, CWE-772",1
"name: ""{{ graylog_mongodb_package_dependencies_python2 }}""
  when: ansible_python_version is version('3.0.0', '<')

- name: ""Package dependencies should be installed""
  yum:
    name: ""{{ graylog_mongodb_package_dependencies_python3 }}""
    state: present
  when: ansible_python_version is version('3.0.0', '>=')",1,"- name: \n          hostname: \, when: ansible_distribution == 'Arch', when: ansible_distribution == 'Clear Linux OS', when: ansible_distribution == 'Debian', when: ansible_distribution == 'Fedora', when: ansible_distribution == 'Gentoo Linux', when: ansible_distribution == 'Gentoo', when: ansible_distribution == 'Solus', when: ansible_distribution == 'Ubuntu', when: ansible_distribution == 'Void'",CWE-119,0,"- name: Package dependencies should be installed
  yum: name={{ item }} state=installed
  with_items: ""{{ graylog_mongodb_package_dependencies | default([]) }}""",0,"default([]) }}, name={{ item }} state=installed, with_items: \, with_items: \\","CWE-1022, CWE-772",0
min_ansible_version: 2.3,1,"default: False, default: True, min_ansible_version: 2.3","CWE-250, CWE-326",0,min_ansible_version: 2.2,0,misconfiguration,CWE-ID,0
"loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:",1,"loop: \, loop: \\, loop: \\\\, loop: \\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-949,1,"- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath",0,"dest='/etc/nginx/fastcgi.conf', dest='/etc/nginx/fastcgi_params', name: LINEINFILE | Fix path, when: nginx_fastcgi_fix_realpath","CWE-1021, CWE-1022",1
"loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:",1,"loop: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\u0027{{ list_one | product(list_two) | list }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\u0027, loop: \\\\\\\\\\\\\\\\\\\\\u0027{{ list_one | product(list_two) | list }}\\\\\\\\\\\\\\\\\\\\u0027, loop: \\\\\\\\u0027{{ list_one | product(list_two) | list }}\\\\\\\\u0027, loop: \\\\u0027{{ list_one | product(list_two) | list }}\\\\u0027, loop: \\u0027{{ list_one | product(list_two) | list }}\\u0027, loop: \u0027{{ list_one | product(list_two) | list }}\u0027",CWE-949,1,"author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web",0,"server { index.html;\n        allow 192.168.1.0/24;\n        deny all;\n    }\n}, task: debug msg=\, tasks: - name: install nginx package \\\\\\\\\\\\\n        become: yes\\\\\\\\\\\\n        apt: \\\\\\\\\\\\\\\\n            name: nginx \\\\\\\\\\\\\n            state: installed \\\\\\\\\\\\\\\n, tasks: - name: install nginx package\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        become: yes\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        apt:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n            name: nginx\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n            state: installed, tasks: - name: install nginx package\\\\n        become: yes\\\\n        apt:\\\\n            name: nginx\\\\n            state: installed, tasks: - name: install nginx package\\n        become: yes\\n        apt: name=nginx state=installed\\n    - name: restart nginx\\n        become: yes\\n        service:\\n            name: nginx\\n            state: restarted\\n}, tasks: - name: install nginx package\\n        become: yes\\n        apt:\\n            name: nginx\\n            state: installed\\n, tasks: - name: restart nginx\\\\\\\\\\\\\\\\n        become: yes\\\\\\\\\\\\\\\\n        service: \\\\\\\\\\\\\\\\\\\\\n            name: nginx\\\\\\\\\\\\\\\\\\\\n            state: restarted\\\\\\\\\\\\\\\\n, tasks: - name: restart nginx\\\\\\n        become: yes\\\\\\n        service: \\\\\\\n            name: nginx\\\\\\n            state: restarted\\\\\\n, tasks: - name: restart nginx\\\\n        become: yes\\\\n        service:\\\\n            name: nginx\\\\n            state: restarted\\\\n",CWE-209,1
"description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch",1,"- name: nginx, - nginx_package: - name: nginx, - stretch, description: Nginx for Debian / FreeBSD, min_ansible_version: 2.5, platforms: - stretch, roles: [ - role: role/nginx-common, src: http://mirrors.kernel.org/debian/debian/pool/main/n/nginx-common/nginx-common_1.9.15-0+deb9u3_all.deb, src: http://mirrors.kernel.org/debian/debian/pool/main/n/nginx-extras/nginx-extras_1.9.15-0+deb9u3_all.deb, src: https://github.com/ansible/ansible-role-nginx/archive/1.1.0.tar.gz",CWE-78,1,"- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath",0,"regexp='{{ item.0.regexp }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, regexp='{{ item.0.regexp }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    line='{{ item.0.line }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    dest='{{ item.1 }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when: nginx_fastcgi_fix_realpath, regexp='{{ item.0.regexp }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    line='{{ item.0.line }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    dest='{{ item.1 }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when: nginx_fastcgi_fix_realpath, regexp='{{ item.0.regexp }}'\\\\\\\\\\\\n    line='{{ item.0.line }}'\\\\\\\\\\\\n    dest='{{ item.1 }}'\\\\\\\\\\\\n  when: nginx_fastcgi_fix_realpath, regexp='{{ item.0.regexp }}'\\\\n    line='{{ item.0.line }}'\\\\n    dest='{{ item.1 }}'\\\\n  when: nginx_fastcgi_fix_realpath, regexp='{{ item.0.regexp }}'\\n    line='{{ item.0.line }}'\\n    dest='{{ item.1 }}'\\n  when: nginx_fastcgi_fix_realpath, regexp='{{ item.0.regexp }}'\n    line='{{ item.0.line }}'\n    dest='{{ item.1 }}'\n  when: nginx_fastcgi_fix_realpath",CWE-1038,1
"description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch",1,"becomes: true, becomes: yes, hosts: - test\nbecomes: true, sudo: yes, tasks:\\n  - name: create mysql user with password, vars:\n  mysql_root_password: secret","CWE-284, CWE-749, CWE-778, CWE-779, CWE-798",1,"author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web",0,"- \n  tasks:\n    - name: Download Nginx\n      apt: pkg=nginx state=installed\n\n    - name: Add a server block\n      template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\n\n    - name: Enable the server block\n      file: path=/etc/nginx/sites-enabled/default state=link\n\n    - name: Reload Nginx\n      service: name=nginx state=reloaded\n, tasks:\\\\n  - name: Download Nginx\\\\n    apt: pkg=nginx state=installed\\\\n  - name: Add a server block\\\\n    template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\\\\n  - name: Enable the server block\\\\n    file: path=/etc/nginx/sites-enabled/default state=link\\\\n  - name: Reload Nginx\\\\n    service: name=nginx state=reloaded, tasks:\\\\n  - name: Install Nginx\\\\n    apt: pkg=nginx state=installed\\\\n  - name: Add a server block\\\\n    template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\\\\n  - name: Enable the server block\\\\n    file: path=/etc/nginx/sites-enabled/default state=link\\\\n  - name: Reload Nginx\\\\n    service: name=nginx state=reloaded, tasks:\\\\n  - name: Install Nginx\\\\n    apt: pkg=nginx state=installed\\\\n\\\\n  - name: Add a server block\\\\n    template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\\\\n\\\\n  - name: Enable the server block\\\\n    file: path=/etc/nginx/sites-enabled/default state=link\\\\n\\\\n  - name: Reload Nginx\\\\n    service: name=nginx state=reloaded, tasks:\\\\n  - name: Install Nginx\\n    apt: pkg=nginx state=installed\\n  - name: Add a server block\\n    template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\\n  - name: Enable the server block\\n    file: path=/etc/nginx/sites-enabled/default state=link\\n  - name: Reload Nginx\\n    service: name=nginx state=reloaded, tasks:\\n  - name: Download Nginx\\n    apt: pkg=nginx state=installed\\n  - name: Add a server block\\n    template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\\n  - name: Enable the server block\\n    file: path=/etc/nginx/sites-enabled/default state=link\\n  - name: Reload Nginx\\n    service: name=nginx state=reloaded, tasks:\\n  - name: Download Nginx\\n    apt: pkg=nginx state=installed\\n\\n  - name: Add a server block\\n    template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\\n\\n  - name: Enable the server block\\n    file: path=/etc/nginx/sites-enabled/default state=link\\n\\n  - name: Reload Nginx\\n    service: name=nginx state=reloaded, tasks:\\n  - name: Install Nginx\\n    apt: pkg=nginx state=installed\\n  - name: Add a server block\\n    template: src=templates/nginx_server.conf.j2 dest=/etc/nginx/sites-available/default\\n  - name: Enable the server block\\n    file: path=/etc/nginx/sites-enabled/default state=link\\n  - name: Reload Nginx\\n    service: name=nginx state=reloaded",CWE-19,1
"BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshConnectivityHealth.asciidoc",1,authentication_strategy: 'ldap',"CWE-319, CWE-359, CWE-364, CWE-367",0,"# Custom SOP URLs for alerts
sops:
  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshConnectivityHealth.asciidoc",0,"sops:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, sops:\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\nsops:\\\\\\\\\\\\\\\\\\\\\\n  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc\\\\\\\\\\\\\\\\\\\\\\\n  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc\\\\\\\\\\\\\\\\\\\\\\\n  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc\\\\\\\\\\\\\\\\\\\\\\\n  RouterMeshUndeliveredHealth: https://github.com/integr, sops:\\\\\\\\n\\\\\\\\nsops:\\\\\\\n  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc\\\\\\\\\n  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc\\\\\\\\\n  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc\\\\\\\\\n  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMesh, sops:\\\\n\\\\nsops:\\\\n  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc\\\\n  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc\\\\n  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc\\\\n  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshUndeliveredHealth.asciidoc\\\\n, sops:\\n\\nsops:\\n  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc\\n  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc\\n  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc\\n  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshUndeliveredHealth.asciidoc\\n  Router, sops:\n\nsops:\n  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc\n  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc\n  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc\n  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshUndeliveredHealth.asciidoc\n  RouterMeshConnectivityHealth","CWE-1256, CWE-394, CWE-397, CWE-398, CWE-400, CWE-401",1
"name: crud
      name: spring-boot-rest-http-crud",1,"name: crud\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: crud\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  name: spring-boot-rest-http-crud, name: crud\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  name: spring-boot-rest-http-crud, name: crud\\\\\\\\\\\\\\\\\\\\n  name: spring-boot-rest-http-crud, name: crud\\\\\\\\n  name: spring-boot-rest-http-crud, name: crud\\\\n  name: spring-boot-rest-http-crud, name: crud\\n  name: spring-boot-rest-http-crud, name: crud\\n  name: spring-boot-rest-http-crud\\n, name: crud\n  name: spring-boot-rest-http-crud",CWE-257,1,"apiVersion: template.openshift.io/v1
    iconClass: icon-node
    tags: nodejs, crud
    openshift.io/display-name: Fruit CRUD Application
    openshift.io/provider-display-name: Red Hat, Inc.
    openshift.io/documentation-url: https://github.com/integr8ly/walkthrough-applications.git
    description: Basic CRUD application for fruit
- kind: DeploymentConfig
  apiVersion: apps.openshift.io/v1
    name: crud-app
      app: crud-app
    revisionHistoryLimit: 10
    test: false
      app: crud-app
          app: crud-app
        - name: crud-app
          image: quay.io/integreatly/fruit-crud-app:1.0.1
          resources: {}
          terminationMessagePath: ""/dev/termination-log""
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
        restartPolicy: Always
        terminationGracePeriodSeconds: 30
        dnsPolicy: ClusterFirst
        securityContext: {}
        schedulerName: default-scheduler
- kind: Service
  apiVersion: v1
    name: crud-app
    ports:
    - protocol: TCP
      port: 8080
      app: crud-app
- kind: Route
  apiVersion: route.openshift.io/v1
    name: spring-boot-rest-http-crud
    to:
      kind: Service
      name: crud-app
    port:
      targetPort: 8080
    tls:
      termination: edge
    wildcardPolicy: None",0,"port: 8080, resources: {}, terminationGracePeriodSeconds: 30, terminationMessagePath: \, terminationMessagePath: \\, terminationMessagePolicy: File","CWE-20, CWE-22, CWE-732",1
"shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-service""
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-service""",1,"shell: \, shell: \\, shell: \\\\, shell: \\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-937,0,"---
- name: ""Delete project namespace: {{ msbroker_namespace }}""
  shell: oc delete project {{ msbroker_namespace }}
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Delete CRDs
  shell: ""oc delete crd {{ item }}""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0
  with_items: syndesises.syndesis.io

- name: Clean up clusterservicebroker
  shell: ""oc delete clusterservicebrokers.servicecatalog.k8s.io managed-services-broker""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role
  shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role binding
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0",0,"changed_when: output.rc == 0, failed_when: output.stderr!= '' and 'not found' not in output.stderr, shell: oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-services, shell: oc delete clusterroles.rbac.authorization.k8s.io managed-services, shell: oc delete clusterservicebrokers.servicecatalog.k8s.io managed-services-broker, shell: oc delete crd syndesises.syndesis.io, shell: oc delete project msbroker, with_items: syndesises.syndesis.io","CWE-275, CWE-284",1
,1,file: /etc/yum.repos.d/epel.repo,119,1,"ups_namespace: ""{{ eval_ups_namespace | default('unifiedpush') }}""
ups_app_namespaces: ""{{ eval_mdc_namespace | default('mobile-developer-console') }}""
ups_resources:
  - ""{{ ups_operator_resources }}/service_account.yaml""
  - ""{{ ups_operator_resources }}/role.yaml""
  - ""{{ ups_operator_resources }}/role_binding.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_androidvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_iosvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_pushapplication_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_unifiedpushserver_crd.yaml""
ups_operator_deployment: ""{{ ups_operator_resources }}/operator.yaml""
ups_template_dir: /tmp
ups_server_name: unifiedpush
#backup
ups_backup: ""{{ backup_restore_install | default(false) }}""
ups_backup_name: ups-daily-at-midnight
ups_backup_schedule: ""{{ backup_schedule }}""
ups_backup_secret: ""s3-credentials""
ups_backup_secret_namespace: ""{{ backup_namespace }}""
ups_encryption_secret: ''
ups_encryption_secret_namespace: ""{{ backup_namespace }}""
ups_backup_rbac_template:
  - ""{{ backup_resources_location }}/rbac/role-binding-template.yaml""
ups_backup_rbac_resources:
  - ""{{ backup_resources_location }}/rbac/service-account.yaml""
  - ""{{ backup_resources_location }}/rbac/role.yaml""
#monitor
ups_svc_monitor_resources:
  - ""{{ ups_operator_resources }}/service_monitor.yaml""",0,"- \\\\\\\\, ups_app_namespaces: \\, ups_backup_secret: \\\\, ups_backup_secret_namespace: \\\, ups_encryption_secret: \\\\\\\\, ups_encryption_secret_namespace: \\\\\\, ups_namespace: \, ups_operator_deployment: \\, ups_resources:\n  - \\\\,, ups_template_dir: /tmp\nups_server_name: unifiedpush\n#backup\nups_backup: \\u0026quot;backup_restore_install\\u0026quot; \\u0026quot;false\\u0026quot; \nups_backup_name: ups-daily-at-midnight\nups_backup_schedule: \\",CWE-22,1
shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\},1,shell: oc get secret launcher_sso_keycloak_client_id-client -n rhsso_namespace -o template --template={{.data.secret}},CWE-1041,1,"- name: ""include rhsso vars""
  include_vars: ../../rhsso/defaults/main.yml
    launcher_sso_openshift_idp_client_secret: ""{{ 99999 | random | to_uuid }}""
- name: Retrieve secret for launcher openshift sso client
  shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\} | base64 -d
  failed_when: openshift_client_secret_response.stderr != """"
  until: openshift_client_secret_response.stdout
  retries: 50
  delay: 3
  changed_when: openshift_client_secret_response.stdout",0,"- name: Retrieve secret for launcher openshift sso client\r\nshell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\} | base64 -d\r\nfailed_when: openshift_client_secret_response.stderr!= \, launcher_sso_openshift_idp_client_secret: \\\\\\, name: Retrieve secret for launcher openshift sso client\\r\\nshell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\\{\\{.data.secret\\}\\} | base64 -d\\r\\nfailed_when: openshift_client_secret_response.stderr!= \\, oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\{.data.secret\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\} | base64 -d, oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\\\\\\\\\\\\\\{\\\\\\\\\\\\\\\{.data.secret\\\\\\\\\\\\\\}\\\\\\\\\\\\\\} | base64 -d, oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\\\\\\\\{\\\\\\\\{.data.secret\\\\\\\\}\\\\\\\\} | base64 -d, oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\\\\{\\\\{.data.secret\\\\}\\\\} | base64 -d\\\\r\\\\n, until: openshift_client_secret_response.stdout\\\\r\\\\nretries: 50\\\\r\\\\ndelay: 3\\\\r\\\\nchanged_when: openshift_client_secret_response.stdout\\\\r\\n  when: openshift_client_secret_response.stdout == \\\","CWE-776, CWE-779, CWE-787",0
"- name: ""include launcher vars""
  include_vars: ../../launcher/defaults/main.yml

- name: Get Launcher SSO secure route
  shell: oc get route/{{ launcher_sso_prefix }} -o template --template \{\{.spec.host\}\} -n {{ launcher_namespace }}
  register: rhsso_secure_route
  retries: 60
  delay: 5
  until: rhsso_secure_route.rc == 0

- name: ""Generate secret for launcher client""
  set_fact:
    launcher_client_secret: ""{{ (ansible_date_time.epoch + launcher_namespace) | hash('sha512') }}""

- set_fact:
    launcher_sso_route: ""{{ rhsso_secure_route.stdout }}""

  shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_cluster_admin_username}}",1,"include_vars:../../launcher/defaults/main.yml, set_fact: \\n    launcher_sso_route: \, set_fact:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    launcher_sso_route: {{ rhsso_secure_route.stdout }}, set_fact:\\\\\\\\\\\\\\\\\\\\\\\\n    launcher_sso_route: {{ rhsso_secure_route.stdout }}, set_fact:\\\\\\\\\\\\n    launcher_sso_route: {{ rhsso_secure_route.stdout }}, set_fact:\\\\n    launcher_sso_route: {{ rhsso_secure_route.stdout }}, set_fact:\\n    launcher_client_secret: {{ (ansible_date_time.epoch + launcher_namespace) | hash('sha512') }}, set_fact:\n    launcher_client_secret: \, shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_cluster_admin_username}}, shell: oc get route/{{ launcher_sso_prefix }} -o template --template \\\\\\{\{.spec.host\}\} -n {{ launcher_namespace }}","CWE-257, CWE-277",1,"- name: ""Create project namespace: {{ rhsso_namespace }}""
  shell: oc new-project {{ rhsso_namespace }}
  register: output
  failed_when: output.stderr != '' and 'already exists' not in output.stderr
  changed_when: output.rc == 0
- name: ""Ensure 1.2 tag is present for redhat sso in openshift namespace""
  shell: oc tag --source=docker registry.access.redhat.com/redhat-sso-7/sso72-openshift:1.2 openshift/redhat-sso72-openshift:1.2
  register: result
  until: result.stdout
  retries: 50
  delay: 1
  failed_when: not result.stdout
  changed_when: False
- name: ""Ensure 1.2 tag has an imported image in openshift namespace""
  shell: oc -n openshift import-image redhat-sso72-openshift:1.2
  register: result
  until: result.stdout
  retries: 50
  delay: 1
  failed_when: not result.stdout
  changed_when: False
- name: ""Create required objects""
  shell: ""oc create -f https://raw.githubusercontent.com/{{rhsso_operator_repo}}/keycloak-operator/{{rhsso_operator_commit_tag}}/deploy/{{ item }} -n {{ rhsso_namespace }}""
  with_items: ""{{ rhsso_operator_required_objects }}""
  register: rhsso_required_objects_result
  failed_when: rhsso_required_objects_result.stderr != '' and 'AlreadyExists' not in rhsso_required_objects_result.stderr

- name: ""Create operator deployment config template""
  template:
    src: ""operator-dc.yaml""
    dest: /tmp/operator-dc.yaml

- name: ""create operator deployment config""
  shell: ""oc create -f /tmp/operator-dc.yaml -n {{ rhsso_namespace }}""
  register: rhsso_dc
  failed_when: rhsso_dc.stderr != '' and 'AlreadyExists' not in rhsso_dc.stderr

- name: ""Create keycloak resource template""
  template:
    src: ""keycloak.json.j2""
    dest: ""/tmp/keycloak.json""

- name: ""Create keycloak resource""
  shell: oc create -f /tmp/keycloak.json -n {{ rhsso_namespace }}
  register: rhsso_keycloak
  failed_when: rhsso_keycloak.stderr != '' and 'AlreadyExists' not in rhsso_keycloak.stderr

- name: ""Generate secret for rhsso client""
  set_fact:
    rhsso_client_secret: ""{{ (ansible_date_time.epoch + rhsso_namespace) | hash('sha512') }}""

- name: ""include threescale vars""
  include_vars: ../../3scale/defaults/main.yml

- name: ""Generate secret for 3scale client""
  set_fact:
    threescale_client_secret: ""{{ (ansible_date_time.epoch + threescale_namespace) | hash('sha512') }}""

- name: ""Create keycloak realm resource template""
  template:
    src: ""keycloak-realm.json.j2""
    dest: ""/tmp/keycloak-realm.json""

- name: Seed evaluation users
  include: _inject_user.yml template=""/tmp/keycloak-realm.json"" email={{ rhsso_seed_users_email_format|format(item|int) }} username={{ rhsso_seed_users_name_format|format(item|int)}} password={{ rhsso_seed_users_password }}
  with_sequence: count={{ rhsso_seed_users_count }}

- name: ""Create keycloak realm resource""
  shell: oc create -f /tmp/keycloak-realm.json -n {{ rhsso_namespace }}
  register: rhsso_kcr
  failed_when: rhsso_kcr.stderr != '' and 'AlreadyExists' not in rhsso_kcr.stderr


- name: ""Verify rhsso realm is provisioned""
  shell: sleep 5; oc get keycloakrealm {{ rhsso_realm }} -o template --template \{\{.status.phase\}\}  -n {{ rhsso_namespace }}  |  grep  'reconcile'
  register: result
  until: result.stdout
  retries: 50
  delay: 10
  failed_when: not result.stdout
  changed_when: False

- name: configure logout
  import_tasks: logout.yml

- name: Add cluster admin role to evals admin
  shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_evals_admin_username}}",0,"oc create -f /tmp/keycloak-realm.json -n {{ rhsso_namespace }}\\n, oc import-image redhat-sso72-openshift:1.2\\\\\\\\\\\\\\\\\\\, oc import-image redhat-sso72-openshift:1.2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, oc import-image redhat-sso72-openshift:1.2\\\\\\\\\n, oc import-image redhat-sso72-openshift:1.2\\\\n, oc import-image redhat-sso72-openshift:1.2\\n, shell: oc create -f /tmp/keycloak-realm.json -n {{ rhsso_namespace }}\n","CWE-132, CWE-532",0
"shell: ""oc set env dc/tutorial-web-app \
  GITEA_TOKEN='{{ gitea_token }}' \
  GITEA_HOST='http://{{ gitea_ingress_host.stdout }}' \
  -n {{ webapp_namespace }} \
  --overwrite=true""
  when: check_webapp_installed_cmd.rc == 0 and gitea_ingress_host.stdout != ''

- name: Wait for pods
  shell: sleep 5; oc get pods --namespace {{ webapp_namespace }}  |  grep  ""Creating""
  register: result
  until: not result.stdout
  retries: 50
  delay: 10
  failed_when: result.stdout
  changed_when: False",1,"\, name: Wait for pods\\n  shell: sleep 5; oc get pods --namespace {{ webapp_namespace }}  |  grep  \\, oc set env dc/tutorial-web-app \\\n  GITEA_TOKEN='{{ gitea_token }}' \\\n  GITEA_HOST='http://{{ gitea_ingress_host.stdout }}' \\\n  -n {{ webapp_namespace }} \\\n  --overwrite=true, register: result\\\\\\n  until: not result.stdout\\\\\\n  retries: 50\\\\\\n  delay: 10\\\\\\n  failed_when: result.stdout\\\\\\n  changed_when: False, register: result\\\\n  until: not result.stdout\\\\n  retries: 50\\\\n  delay: 10\\\\n  failed_when: result.stdout\\\\n  changed_when: False, register: result\n  until: not result.stdout\n  retries: 50\n  delay: 10\n  failed_when: result.stdout\n  changed_when: False, shell: \, shell: sleep 5; oc get pods --namespace {{ webapp_namespace }}  |  grep  \, until: not result.stdout\\n  retries: 50\\n  delay: 10\\n  failed_when: result.stdout\\n  changed_when: False, when: check_webapp_installed_cmd.rc == 0 and gitea_ingress_host.stdout!= ''","CWE-269, CWE-326, CWE-327, CWE-601",1,"# oc exec doesn't accept a namespace argument so make sure we are using the correct one
- name: Make sure we are using the gitea namespace
  shell: ""oc project {{ gitea_namespace }}""


- name: ""Wait for Gitea pods to be ready""
  shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[*].status.containerStatuses[?(@.ready==true)].ready}' | wc -w""
  register: gitea_result
  until: gitea_result.stdout.find(""1"") != -1
  retries: 30
- name: Get the name of the gitea pod
  shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[0].metadata.name}'""
  register: gitea_pod_name

- name: Create the gitea admin user
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name={{ gitea_admin_username }} --password={{ gitea_admin_password }} --admin --email=admin@example.com --config /home/gitea/conf/app.ini""
  register: create_admin_user_cmd
  failed_when: create_admin_user_cmd.stderr != '' and 'already exists' not in create_admin_user_cmd.stderr
  changed_when: create_admin_user_cmd.rc == 0

- name: Fetch all old tokens
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens""
    method: GET
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
  register: gitea_old_access_tokens

- name: Delete all old access tokens
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens/{{ item.id }}""
    method: DELETE
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
    status_code: 204
  with_items: ""{{ gitea_old_access_tokens.json }}""

- name: Create a new admin token for the admin user
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens""
    method: POST
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    body: {""name"": ""{{ gitea_admin_token }}""}
    body_format: json
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
    return_content: yes
    status_code: 201
  register: admin_token_result

- name: Extract sha value from admin token result
  set_fact:
    gitea_token: ""{{ admin_token_result.json | json_query('sha1') }}""

- name: Print out the gitea admin token
  debug:
    msg: ""Gitea admin token is {{ gitea_token }}""

- name: Create gitea walkthrough users
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals{{ item }} --password=Password1 --email=evals{{ item }}@example.com --config /home/gitea/conf/app.ini""
  register: create_walkthrough_user_cmd
  failed_when: create_walkthrough_user_cmd.stderr != '' and 'already exists' not in create_walkthrough_user_cmd.stderr
  changed_when: create_walkthrough_user_cmd.rc == 0
  with_sequence: count={{ eval_seed_users_count }}

- name: Create a user for the evals admin
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals-admin --password=Password1 --email=evals-admin@example.com --config /home/gitea/conf/app.ini""
  register: create_eval_admin_user_cmd
  failed_when: create_eval_admin_user_cmd.stderr != '' and 'already exists' not in create_eval_admin_user_cmd.stderr
  changed_when: create_eval_admin_user_cmd.rc == 0

- name: Get the gitea ingress host
  shell: oc get ingress --namespace=gitea --selector='app=gitea' -o jsonpath='{.items[0].spec.rules[0].host}'
  register: gitea_ingress_host

- name: Set gitea host as webapp env var
  shell: oc set env dc/tutorial-web-app GITEA_HOST=""{{ gitea_ingress_host.stdout }}"" -n {{ webapp_namespace }} --overwrite=true
  when: gitea_ingress_host.stdout != ''

# SET GITEA TOKEN ENV VAR IN THE WEBAPP
  shell: oc set env dc/tutorial-web-app GITEA_TOKEN=""{{ gitea_token }}"" -n {{ webapp_namespace }} --overwrite=true
  when: check_webapp_installed_cmd.rc == 0",0,This snippet contains a security issue with ID [CWE ID],CWE ID,0
shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10Mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} --param=BACKEND_IMAGE_TAG={{ launcher_backend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,1,shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10Mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher,"CWE-774, CWE-778, CWE-779",1,shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,0,"--param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} | oc create -n {{ launcher_namespace }} -f -, auncher_frontend_image_tag }} | oc create -n {{ launcher_namespace }} -f -, oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git, shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher, | oc create -n {{ launcher_namespace }} -f -",CWE-732,0
,1,"data = {{ data | from_json(data) }};\\\\n, data = {{ data | from_json(data) }};\\n, name = data['name'];\\n, name = data[0]['name'];\\n, name = data[1]['name'];\\\\n, name = data[2]['name'];\\\\n, name = data[3]['name'];\\\\\\\\n, name = data[4]['name'];\\\\\\\\\\\\\\\\n, name = data[5]['name'];\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, var data = {{ data | json }};\n","CWE-20, CWE-319, CWE-89",1,"---
- name: Get RH-SSO secure route
  local_action: command oc get route/secure-sso -o template --template \{\{.spec.host\}\} -n {{ rhsso_namespace }}
  register: rhsso_secure_route

- set_fact:
    rhsso_route: ""{{ rhsso_secure_route.stdout }}""

- name: Retrieve RH-SSO Client Config
  local_action: command cat /tmp/client-config.json
  register: client_config_raw

- set_fact:
    client_config: ""{{ client_config_raw.stdout }}""

- name: Add RH-SSO identity provider to master config
  blockinfile:
    path: ""{{ openshift_master_config }}""
    insertafter: ""identityProviders""
    backup: yes
    block: |2
        - name: rh_sso
          challenge: false
          login: true
          mappingInfo: add
          provider:
            apiVersion: v1
            kind: OpenIDIdentityProvider
            clientID: {{ rhsso_client_id }}
            clientSecret: {{ client_config.json.value }}
            urls:
              authorize: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/auth
              token: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/token
              userInfo: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/userinfo
            claims:
              id:
              - sub
              preferredUsername:
               - preferred_username
              name:
              - name
              email:
              - email
  register: master_config_update
  become: yes

- name: restart openshift master api service
  service:
    name: atomic-openshift-master-api
    state: restarted
  become: yes
  when: master_config_update.changed

- name: restart openshift master controller service
  service:
    name: atomic-openshift-master-controllers
    state: restarted
  become: yes
  when: master_config_update.changed

- name: Delete local client config file
  file: path=/tmp/client-config.json state=absent",0,"become: yes\\\\\\\\nwhen: master_config_update.changed\\\\\\\\n, become: yes\\\\nwhen: master_config_update.changed\\\\n, blockinfile:\\\\npath: \\, blockinfile:\\npath: \, clientSecret: {{ client_config.json.value }}, file:\\\\\\nstate: absent, file:\\\\\\nstate: absent\\\\n, local_action: command cat /tmp/client-config.json\\n  register: client_config_raw\\n, local_action: command oc get route/secure-sso -o template --template \{\{.spec.host\}\} -n {{ rhsso_namespace }}\nregister: rhsso_secure_route, service:\\\nstate: restarted\\\\\\nbecome: yes\\\\\\nwhen: master_config_update.changed","CWE-306, CWE-327, CWE-328, CWE-329, CWE-345",1
"- name: Add labels to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}"", ""integreatly-middleware-service"":""true""}}}'
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0
- name: Add labels to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\"", \""integreatly-middleware-service\"":\""true\""}}}'""
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0",1,"oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\, oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\n    \\\\\\\\\\, oc patch ns {{ che_infra_namespace }} --patch '{\\\\n    \\\, oc patch ns {{ che_infra_namespace }} --patch '{\\n    \\, shell: oc patch ns {{ che_infra_namespace }} --patch '{\, shell: oc patch ns {{ che_namespace }} --patch '{",CWE-502,1,"- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """"",0,"oc patch ns {{ che_infra_namespace }} --patch '{\\\\, shell: oc patch ns {{ che_infra_namespace }} --patch '{\\, shell: oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\\\, shell: oc patch ns {{ che_infra_namespace }} --patch '{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: oc patch ns {{ che_namespace }} --patch '{\, when: che_infra_namespace is defined and che_infra_namespace!= \\\",CWE-1030,0
,1,"hosts: web-servers\\\\n\\\\n  hosts: web\\\\n  vars:\\\\n    mysql_ip: 192.168.1.4, hosts: web-servers\\n\\n  hosts: web\\n  vars:\\n    mysql_ip: 192.168.1.4, hosts: web-servers\n\n  hosts: web\n  vars:\n    mysql_ip: 192.168.1.4",CWE-275,1,"when:
        - user_rhsso | default(true) | bool
        - mdc | default(true) | bool",0,"when: - mdc | default(true) | bool, when: - user_rhsso | default(true) | bool","CWE-1026, CWE-829, CWE-863, CWE-887, CWE-908",0
"shell: /usr/local/bin/master-restart controllers

# Delete users and identities created by rhsso
-
  name: ""Get all users created by rhsso""
  shell: oc get users | grep 'rh_sso' | awk '{print $1}'
  register: users
  failed_when: false

-
  name: ""Delete users""
  shell:  ""oc delete users {{ users.stdout | replace('\n', ' ') }}""
  when: users.stdout != ''
  failed_when: false",1,"oc delete users {{ users.stdout | replace('\\\\n', '') }}, oc delete users {{ users.stdout | replace('\\n', '') }}, oc get users | grep 'rh_sso' | awk '{print $1}', register: users, shell:  oc delete users {{ users.stdout | replace('\\\\\\\\n', '') }}, shell:  oc delete users {{ users.stdout | replace('\\n', '') }}, shell: /usr/local/bin/master-restart controllers, shell: oc get users | grep 'rh_sso' | awk '{print $1}'","CWE-275, CWE-285, CWE-287, CWE-732",1,"- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """"",0,"oc patch ns {{ che_infra_namespace }} --patch \\\, oc patch ns {{ che_namespace }} --patch \, shell: oc patch ns {{ che_infra_namespace }} --patch '\\, shell: oc patch ns {{ che_infra_namespace }} --patch '{\\, shell: oc patch ns {{ che_infra_namespace }} --patch \\, shell: oc patch ns {{ che_infra_namespace }} --patch \\\\\\, shell: oc patch ns {{ che_infra_namespace }} --patch \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: oc patch ns {{ che_infra_namespace }} --patch \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: oc patch ns {{ che_namespace }} --patch '{\, shell: oc patch ns {{ che_namespace }} --patch \\\\\\\\\\\\",CWE-126,0
"loop: ""{{ real_users }}""",1,loop: {{ real_users }},CWE-89,1,"# handlers file for robot-pkgs

- name: USB controllers warning
  command: >-
    zenity --warning --text {{ usb_warning_msg }}
  become: yes
  become_user: ""{{ item.user }}""
  with_items: ""{{ real_users }}""
  ignore_errors: yes",0,"become: yes, become_user, command, ignore_errors: yes, name, real_users, usb_warning_msg, with_items, zenity, zenity --warning --text","CWE-20, CWE-200, CWE-209, CWE-319, CWE-798, CWE-862",0
"node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address }}""",1,"node_ip_address: \, node_ip_address: \\\\ n, node_ip_address: \\\\\\\\ n\r, node_ip_address: \\\\\\\\\\\ r\\ n, node_ip_address: \\\\\\\\\\\\\\\\\\\\\\\\\ n, node_ip_address: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ n, node_ip_address: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ n, node_ip_address: \\\\\n, node_ip_address: \\\n, node_ip_address: \\\r\n",CWE-758,1,"---
# node_interface: ""{{ ansible_default_ipv4.interface }}""
node_interface: ""enp0s8""
node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address",0,"ansible_, ansible_\\, ansible_\\\\, ansible_\\\\\\\\\\\\\\\\\\\\, node_interface: \, node_ip_address: \\",CWE-601,0
"- name: Install flannel
- name: Prepare and write flannel configuration to etcd
  include: config.yml
- name: Enable flannel on node
  service: name=flanneld enabled=yes
- name: Start flannel on node
  service: name=flanneld state=started
  register: flannel_started
  notify:
     - Restart docker engine",1,"Restart docker engine, config.yml, include: config.yml, name: Install flannel, name: Start flannel on node\\n          service: name=flanneld state=started, notify:\\n  - Restart docker engine, register: flannel_started, service: name=flanneld enabled=yes, service: name=flanneld enabled=yes\n        - name: Start flannel on node\n          service: name=flanneld state=started, service: name=flanneld state=started","CWE-275, CWE-279, CWE-379, CWE-775",1,"---
- name: Install flannel service (RHEL/CentOS)
  when: ansible_os_family == ""RedHat""
  yum:
    name: flannel
    state: latest
    update_cache: yes

- name: Update flannel config
  template: src=""flanneld.j2"" dest={{ flannel_dir }}/flanneld
  register: change_flannel

- name: Set facts about etcdctl command
  set_fact:
    peers: ""{% for hostname in groups['etcd'] %}http://{{ hostname }}:2379{% if not loop.last %},{% endif %}{% endfor %}""
    conf_file: ""/tmp/config.json""
    conf_loc: ""/{{ flannel_key }}/config""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Create flannel config file to go in etcd
  template: src=flannel-config.json dest={{ conf_file }}
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Load the flannel config file into etcd
  when: etcd_peer_url_scheme == 'http'
  shell: ""/usr/bin/etcdctl --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Load the flannel config file into secure etcd
  when: etcd_peer_url_scheme == 'https'
  shell: ""/usr/bin/etcdctl -cert-file={{ etcd_peer_cert_file }} --ca-file={{ etcd_peer_ca_file }} --key-file={ etcd_peer_key_file }} --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Start and enable flannel on node
  when: change_flannel|succeeded
  service: name=flanneld enabled=no state=started

- name: Reload flanneld
  when: change_flannel|changed or etcd_cert|changed
  service: name=flanneld state=restarted",0,"name: Copy etcd certificate from ansible host, name: Load the flannel config file into etcd, name: Load the flannel config file into secure etcd, name: Start and enable flannel on node, name: Update flannel config, shell: \, when: etcd_peer_url_scheme == 'http', when: etcd_peer_url_scheme == 'https'","CWE-124, CWE-125, CWE-250, CWE-284, CWE-320, CWE-325, CWE-345, CWE-601, CWE-732",1
,1,"key: \, key: \\, key: \\\\, key: \\\\\\\\\, key: \\\\\\\\\\\\\\\\\\\\\\\\\\, key: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-20,1,"directories:
    directories:
      - kubedns/kubedns-autoscale-dp.yml
    directories:
    directories:
    directories:
        kind: namespace
    directories:
      - logging/elasticsearch
      - logging/elasticsearch/elasticsearch-sa.yml
      - logging/elasticsearch/elasticsearch-rbac.yml
      - logging/elasticsearch/elasticsearch-svc.yml
      - logging/elasticsearch/elasticsearch-sts.yml
      - logging/kibana/kibana-anonymous-rbac.yml
    directories:
      - monitoring/gpu-exporter
      - monitoring/prometheus-adapter
        kind: namespace
        namespace: """"
      - monitoring/grafana/grafana-res-definitions.yml
      - monitoring/grafana/grafana-gpu-cluster-definitions.yml
      - monitoring/grafana/grafana-gpu-node-definitions.yml
      - monitoring/grafana/grafana-gpu-pod-definitions.yml
      - monitoring/grafana/grafana-cluster-definitions.yml
      - monitoring/gpu-exporter/gpu-exporter-svc.yml
      - monitoring/gpu-exporter/gpu-exporter-ds.yml
      - monitoring/prometheus-adapter/prometheus-adapter-sa.yml
      - monitoring/prometheus-adapter/prometheus-adapter-rbac.yml
      - monitoring/prometheus-adapter/prometheus-adapter-svc.yml
      - monitoring/prometheus-adapter/prometheus-adapter-cm.yml
      - monitoring/prometheus-adapter/prometheus-adapter-apiservice.yml
      - monitoring/prometheus-adapter/prometheus-adapter-dp.yml
      - monitoring/servicemonitor/kube-state-metrics-sm.yml
      - monitoring/servicemonitor/gpu-exporter-sm.yml
      - monitoring/servicemonitor/grafana-sm.yml",0,"monitoring/grafana/grafana-gpu-cluster-definitions.yml, monitoring/grafana/grafana-gpu-node-definitions.yml, monitoring/grafana/grafana-res-definitions.yml, monitoring/prometheus-adapter/prometheus-adapter-cm.yml, monitoring/prometheus-adapter/prometheus-adapter-dp.yml, monitoring/prometheus-adapter/prometheus-adapter-rbac.yml, monitoring/prometheus-adapter/prometheus-adapter-svc.yml, monitoring/servicemonitor/gpu-exporter-sm.yml, monitoring/servicemonitor/grafana-sm.yml, monitoring/servicemonitor/kube-state-metrics-sm.yml",CWE-732,0
"when: ""'pve-no-subscription' in proxmox_repository_line""",1,"when: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: \\\\\\\\\\\\u0027pve-no-subscription\\\\\\\\\\\u0027 in proxmox_repository_line, when: \\\\\\\\\u0027pve-no-subscription\\\\\\\\\\\\u0027 in proxmox_repository_line, when: \\\\\\\\u0027pve-no-subscription\\\\\\\\u0027 in proxmox_repository_line, when: \\\\u0027pve-no-subscription\\\\u0027 in proxmox_repository_line, when: \\u0027pve-no-subscription\\u0027 in proxmox_repository_line, when: \u0027pve-no-subscription\u0027 in proxmox_repository_line",CWE-200,1,"- block:

  - name: Remove automatically installed PVE Enterprise repo configuration
    apt_repository:
      repo: ""deb https://enterprise.proxmox.com/debian jessie pve-enterprise""
      filename: pve-enterprise
      state: absent

  - name: Remove subscription popup dialog in web UI
    replace:
      dest: /usr/share/pve-manager/ext6/pvemanagerlib.js
      regexp: ""^          if .data.status !== 'Active'. {""
      replace: ""          if (false) {""
      backup: yes

  when: 'pve-no-subscription' in proxmox_repository_line
  when: proxmox_check_for_kernel_update

  when:
    - proxmox_reboot_on_kernel_update
    - __proxmox_kernel | changed

- name: Remove old Debian kernels
  apt:
    name: ""{{ item }}""
    state: absent
  with_items:
    - linux-image-amd64
    - linux-image-3.16.0-4-amd64
  when: proxmox_remove_old_kernels

- name: LDAP fix for authenticated search
  lineinfile:
    line: ""    $ldap->bind('{{ proxmox_ldap_bind_user }}', password => '{{ proxmox_ldap_bind_password }}');""
    insertbefore: ""ldap->search\\(""
    dest: /usr/share/perl5/PVE/Auth/LDAP.pm
  notify:
    - restart pvedaemon
  when:
    - proxmox_ldap_bind_user is defined
    - proxmox_ldap_bind_password is defined",0,"$ldap->bind('{{ proxmox_ldap_bind_user }}', password => '{{ proxmox_ldap_bind_password }}');, apt:, apt_repository:, if (false) {, if.data.status!== 'Active' {, linux-image-3.16.0-4-amd64, linux-image-amd64\nlinux-image-3.16.0-4-amd64, name: Remove subscription popup dialog in web UI\\nreplace:, state: absent\\\\\\\\nwhen:, state: absent\\\\nwhen: 'pve-no-subscription' in proxmox_repository_line\\\\nwhen: proxmox_check_for_kernel_update\\\\nwhen:",CWE-12345,0
"- name: Mkdir for java installation
      win_file:
        path: '{{ java_path }}\{{ java_folder }}'
        state: directory
    - name: Create temporary directory
      win_tempfile:
        state: directory
      register: temp_dir_path
    - name: Unarchive to temporary directory
      win_unzip:
        src: '{{ java_artifact }}'
        dest: '{{ temp_dir_path }}'
    - name: Find java_folder in temp
      win_find:
        paths: '{{ temp_dir_path }}'
        recurse: false
        file_type: directory
      register: java_temp_folder
    - name: Copy from temporary directory
      win_copy:
        src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
        dest: '{{ java_path }}\{{ java_folder }}'
        remote_src: true

    - name: Check choco
      win_chocolatey:
        name: chocolatey
        state: present

    # https://help.sap.com/viewer/65de2977205c403bbc107264b8eccf4b/Cloud/en-US/76137f42711e1014839a8273b0e91070.html
    - name: 'Install vcredist package prior to using SAP JVM'
      win_chocolatey:
        name: vcredist2013
      register: choco_install
      retries: 15
      delay: 5
      until: choco_install is succeeded",1,"- name: Check choco, - name: Copy from temporary directory, choco_install:, delay:, retries:, until:, win_chocolatey:, win_copy:, win_find:, win_tempfile:","CWE-116, CWE-20, CWE-22, CWE-25, CWE-73, CWE-77",1,"---
- name: Check that the java_folder exists
  win_stat:
    path: '{{ java_path }}\{{ java_folder }}/bin'
  register: java_folder_bin

- name: Install java from tarball
  block:
  - name: Mkdir for java installation
    win_file:
      path: '{{ java_path }}\{{ java_folder }}'
      state: directory

  - name: Create temporary directory
    win_tempfile:
      state: directory
    register: temp_dir_path

  - name: Unarchive to temporary directory
    win_unzip:
      src: '{{ java_artifact }}'
      dest: '{{ temp_dir_path }}'

  - name: Find java_folder in temp
    win_find:
      paths: '{{ temp_dir_path }}'
      recurse: false
      file_type: directory
    register: java_temp_folder

  - name: Copy from temporary directory
    win_copy:
      src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
      dest: '{{ java_path }}\{{ java_folder }}'
      remote_src: true
  when: not java_folder_bin.stat.exists",0,"remote_src: true, when: not java_folder_bin.stat.exists, win_tempfile: state: directory","CWE-1250, CWE-1261",1
"| regex_findall('(https://download[\.\w]+/java/GA/jdk'
 java_major_version|string + '[.\d]+/[\d\w]+/'
 java_major_version|string + '/GPL/openjdk-'
 java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')",1,"https://download.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, https://download.\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w+\\\\\\\\\\\\\\\\\\\\w, https://download.\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w+\\\\\\\\w, https://download.\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w+\\\\w, https://download.\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w+\\w, https://download.\w+java/\","CWE-1031, CWE-113",1,"---
- name: Set java minor version
  set_fact:
    minor: ""{{ java_minor_version | default('.*', True) }}""

- name: 'Fetch root page {{ openjdk_root_page }}'
  uri:
    url: '{{ openjdk_root_page }}/{{ java_major_version }}/'
    return_content: True
  register: root_page

- name: Find release url
  set_fact:
    release_url: >-
      {{ root_page['content']
        | regex_findall('(https://download\.oracle\.com/java/GA/jdk'
          + java_major_version|string + '[.\d]+/[\d\w]+/'
          + java_major_version|string + '/GPL/openjdk-'
          + java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')
      }}

- name: Exit if OpenJDK version is not General-Availability Release
  fail:
    msg: 'OpenJDK version {{ java_major_version }} not GA Release'
  when: release_url[1] is not defined

- name: 'Get artifact checksum {{ release_url[1] }}'
  uri:
    url: '{{ release_url[1] }}'
    return_content: True
  register: artifact_checksum

- name: Show artifact checksum
  debug:
    var: artifact_checksum.content

- name: 'Download artifact from {{ release_url[0] }}'
  get_url:
    url: '{{ release_url[0] }}'
    dest: '{{ download_path }}'
    checksum: 'sha256:{{ artifact_checksum.content }}'
  register: file_downloaded
  retries: 20
  delay: 5
  until: file_downloaded is succeeded

- name: Set downloaded artifact variable
  set_fact:
    java_artifact: '{{ file_downloaded.dest }}'

- name: Split artifact name
  set_fact:
    parts: >-
      {{ java_artifact
        | regex_findall('^(.*j[dkre]{2})-([0-9]+)[u.]([0-9.]+)[-_]([a-z]+)-(x64|i586)')
        | first | list }}

- name: Set variables based on split
  set_fact:
    java_package: '{{ parts[0][-3:] }}'
    java_major_version: '{{ parts[1] }}'
    java_minor_version: '{{ parts[2] }}'
    java_os: '{{ parts[3] }}'
    java_arch: '{{ parts[4] }}'",0,"get_url:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, get_url:\\\\n  dest: '{{ download_path }}', set_fact:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, set_fact:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  java_arch: '{{ parts[4] }}', set_fact:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  java_os: '{{ parts[3] }}', set_fact:\\\\\\\\\\\\\\\\n  java_minor_version: '{{ parts[2] }}', set_fact:\\\\\\\\n  java_package: '{{ parts[0][-3:] }}', set_fact:\\\\n  parts: >- {{ java_artifact \\\\\\\n    | regex_findall('^(.*j[dkre]{2})-([0-9]+)[u.]([0-9.]+)[-_]([a-z]+)-(x64|i586)') \\\\\\\\n    | first | list }}, uri:\\n  url: '{{ release_url[1] }}', uri:\n  url: {{ openjdk_root_page }}/{{ java_major_version }}/","CWE-276, CWE-319, CWE-325, CWE-352",1
"- { src: php.ini.j2, dest: /etc/php5/conf.d/90-php-docker.ini }
  - { src: php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}",1,"- { src: php.ini.j2, dest: /etc/php5/conf.d/90-php-docker.ini }\\\\n - { src: php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }\\\\n - { src: xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}, - { src: php.ini.j2, dest: /etc/php5/conf.d/90-php-docker.ini }\\n - { src: php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }\\n - { src: xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}, - { src: php.ini.j2, dest: /etc/php5/conf.d/90-php-docker.ini }\n - { src: php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }\n - { src: xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}",CWE-327,1,"- name: install PHP5 packages

- name: Place PHP configuration files in place.
  template: src={{ item.src }} dest={{ item.dest }} owner=root group=root mode=644
  with_items:
  - { src: php/php.ini.j2, dest: /etc/php5/apache2/90-php-docker.ini }
  - { src: php/php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: php/xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}
  notify: restart apache",0,"- name: Place PHP configuration files in place., - { src: php/php.ini.j2, dest: /etc/php5/apache2/90-php-docker.ini }, - { src: php/php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }, notify:, notify: restart apache, with_items:",CWE-22,1
"become: yes
  become: yes
  become: yes
  become: yes",1,become: yes,CWE-250,1,"- name: ensure rabbitmq is installed
  apt: pkg=rabbitmq-server state=installed
  sudo: yes

- name: activate rabbitmq_management plugin
  shell: ""/usr/sbin/rabbitmq-plugins enable rabbitmq_management""
  sudo: yes

- name: restart rabbitmq
  service: name=rabbitmq-server state=restarted
  sudo: yes

- name: get rabbitmqadmin script
  get_url: url=http://localhost:15672/cli/rabbitmqadmin dest=/usr/local/bin/rabbitmqadmin mode=755
  sudo: yes",0,"apt: pkg=rabbitmq-server state=installed, dest=/usr/local/bin/rabbitmqadmin, get_url: url=http://localhost:15672/cli/rabbitmqadmin, mode=755, service: name=rabbitmq-server state=restarted, shell: /usr/sbin/rabbitmq-plugins enable rabbitmq_management, sudo: yes","CWE-20, CWE-250",1
"pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int < 16 else 'python3-venv' }}""",1,"ansible_distribution == 'Ubuntu', ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int < 16, ansible_lsb.major_release|int < 16","CWE-862, CWE-863, CWE-865, CWE-866, CWE-867, CWE-868, CWE-869, CWE-870, CWE-871, CWE-872",1,"when: ansible_lsb.major_release|int >= 8

# The Python 3 virtualenv package is named ""python3.4-venv"" on Ubuntu before
# Xenial
- name: install python 3 virtualenv package
  apt:
    pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int >= 16 else 'python3-venv' }}""
    state: latest
  become: yes",0,"apt:\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\tpkg: \\\\\\\\\\\\\\\\\\\\, apt:\\\\\\n\\\\tpkg: \\\\, apt:\\n\\tpkg: \\, become: yes\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\t\\n\\n\\\\\\\\\\\\\\\\\\\\\\, become: yes\\n    \\, name: install python 3 virtualenv package\\\\\\\\\\\\napt:\\\\\\\\\\\\n\\\\\\\\\\\\tpkg: \\\\\\\\\\\\, name: install python 3 virtualenv package\\\\napt:\\\\n\\\\tpkg: \\\\, name: install python 3 virtualenv package\\napt:\\n\\tpkg: \, name: install python 3 virtualenv package\\napt:\\n\\tpkg: \\, when: ansible_lsb.major_release|int >= 8\n- name: install python 3 virtualenv package\napt:\n\tpkg: \",CWE-778,1
"file: dest=""{{ solr_config_dir }}"" state=directory group=solr mode=""g+rwX"" recurse=yes",1,"file: dest=\, group=solr, mode=\\\, mode=g+rwX, recurse=yes, state=directory","CWE-778, CWE-79",1,"- name: create solr group
  group: name=solr state=present
  become: yes

  user: name=solr group=solr groups=""www-data"" comment=""Solr Daemon"" home=""{{ solr_install_dir }}""
- name: solr config directory permission
  file: dest=""{{ solr_config_dir }}"" state=directory owner=vagrant group=solr mode=""g+rwX"" recurse=yes
  become: yes

- name: solr install directory permission",0,"comment=\\\, comment=\\\\\\, comment=\\\\\\\\\\\\\\\\\\, comment=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, owner=vagrant group=solr mode=\\, owner=vagrant group=solr mode=\\\\, owner=vagrant group=solr mode=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, state=directory owner=vagrant group=solr mode=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, state=present","CWE-345, CWE-347",0
"shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20 ] ; then exit 1 ; fi ; exit 0""
  args:
    executable: /bin/bash",1,"((i++)), docker ps, docker ps --filter name=toscasubmitter, docker ps --filter status=running, echo -e 'ToscaSubmitter is not running yet!', exit 0, if [ $i -eq 20 ] ; then exit 1 ; fi, sleep 1, wc -l, while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ]","CWE-76, CWE-77, CWE-80",1,"- name: ""Wait for the running state of ToscaSubmitter""
  shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20] ; then echo '20 seconds expired, unsuccessfully. An error occurred.' ; exit 1 ; fi""
  register: output
  changed_when: output.stdout != """"
  when: f.stat.exists",0,"changed_when: output.stdout!= \, docker ps --filter status=running --filter name=toscasubmitter | wc -l \\\\, i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done \\\\\\\, if [ $i -eq 20] ; then echo '20 seconds expired, unsuccessfully. An error occurred.' ; exit 1 ; fi \\\, if [ $i -lt 20 ] ; then echo -e 'ToscaSubmitter is not running yet!' ; sleep 1 ; ((i++)) ; done \\\\\\\\, if [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] ; then \\\\ \\, register: output \\, shell: \, shell: \\, when: f.stat.exists \\","CWE-20, CWE-77",1
"service:
    name: iptables
    state: restarted
    enabled: yes
  service:
    name: micado
    state: restarted
    enabled: yes",1,"service: name: iptables state: restarted enabled: yes  service: name: micado state: restarted enabled: yes, service:\\\\n    name: iptables\\\\n    state: restarted\\\\n    enabled: yes\\\\n  service:\\\\n    name: micado\\\\n    state: restarted\\\\n    enabled: yes, service:\\\n    name: iptables\\n    state: restarted\\n    enabled: yes\\n  service:\\\n    name: micado\\n    state: restarted\\n    enabled: yes, service:\\n    name: iptables\\n    state: restarted\\n    enabled: yes\\n  service:\\n    name: micado\\n    state: restarted\\n    enabled: yes, service:\n    name: iptables\n    state: restarted\n    enabled: yes\n  service:\n    name: micado\n    state: restarted\n    enabled: yes","CWE-120, CWE-732",1,"- name: Enable packet filtering
  shell: systemctl enable --now iptables",0,shell: systemctl enable --now iptables,"CWE-120, CWE-269, CWE-276, CWE-279, CWE-280, CWE-319, CWE-352, CWE-732, CWE-862",0
"- name: Create app and data folders
  file:
    name: ""{{ item }}""
    state: directory
  with_items:
    - ""{{ bamboo_master_application_folder }}""
    - ""{{ bamboo_master_data_folder }}""

    home: ""{{ bamboo_master_data_folder }}""
- name: Set permissions for app and data folders",1,"- name: Create app and data folders\n  file:\n    name: \, - name: Set permissions for app and data folders\\n, file:\n  path: \\\, home: \\, name: \\, name: \\\\\\\\, path: \\\\, state: \\\\, type: \\\\, with_items: \\\\",CWE-20,1,"---
- name: ""Install JDK""
  yum:
    name: ""java-{{ openjdk_version }}-openjdk""
    state: installed

- name: Add local user
  user:
    name: ""{{ bamboo_user }}""

- name: Create app and data folders
  file:
    name: ""{{ item }}""
    state: directory
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  with_items:
    - ""{{ bamboo_home_directory }}""
    - ""{{ bamboo_root_directory }}""

- name: Download and unpack bamboo
  unarchive:
    src: ""https://www.atlassian.com/software/bamboo/downloads/binary/atlassian-bamboo-{{ bamboo_version }}.tar.gz""
    dest: ""{{ bamboo_root_directory }}""
    remote_src: True
    # keep our modified, newer files instead of overriding from the tarball
    keep_newer: yes
  changed_when: False

- name: Configure bamboo server (server.xml)
  template:
    src: server.xml.j2
    dest: ""{{ bamboo_application_directory }}/conf/server.xml""
  notify: restart bamboo

- name: Create current version file
  file:
    name: ""{{ bamboo_root_directory }}/current""
    state: touch
    mode: 0644
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  changed_when: False
    
- name: Update current version file
  template:
    src: current.j2
    dest: ""{{ bamboo_root_directory }}/current""
  notify: restart bamboo
  register: current_version

- name: Stop bamboo if newer version is going to be installed
  systemd:
    name: bamboo
    state: stopped
  when: current_version.changed

- name: Copy logrotate script for syslog
  copy:
    src: syslog
    dest: /etc/logrotate.d/syslog

- name: Add cronjob for cleanup Bamboo logs
  cron:
    name: cleanupbamboologs
    special_time: daily
    state: present
    job: ""/usr/bin/find {{ bamboo_application_directory }}/logs/ -name *.log -type f -mtime +7 -exec rm  {} \\;""

- name: Add cronjob for cleanup Bamboo build-dir
  cron:
    name: cleanupbamboobuilddir
    hour: 03
    minute: 15
    weekday: 0 # sunday
    state: present
    job: ""/usr/bin/find {{ bamboo_home_directory }}/xml-data/build-dir/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \\;""

- name: Add cronjob for cleanup maven repository cache
  cron:
    name: cleanupbamboomavenrepo
    hour: 03
    minute: 15
    weekday: 0 # sunday
    state: present
    job: ""/usr/bin/find {{ bamboo_home_directory }}/.m2/repository/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \\;""

- name: Install bamboo systemd unit script
  template:
    src: bamboo.service.j2
    dest: /etc/systemd/system/bamboo.service
    mode: 0744
  notify: restart bamboo

- name: Set bamboo.home property variable
  template:
    src: bamboo-init.properties
    dest: ""{{ bamboo_application_directory }}/atlassian-bamboo/WEB-INF/classes/bamboo-init.properties""
  notify: restart bamboo

- name: Set JVM settings
  replace:
    dest: ""{{ bamboo_application_directory }}/bin/setenv.sh""
    regexp: ""{{ item.regexp }}""
    replace: ""{{ item.replace }}""
  with_items:
    - regexp: 'JVM_MINIMUM_MEMORY=""[0-9]+[bkmg]'
      replace: 'JVM_MINIMUM_MEMORY=""{{ bamboo_jvm_heap_min }}'
    - regexp: 'JVM_MAXIMUM_MEMORY=""[0-9]+[bkmg]'
      replace: 'JVM_MAXIMUM_MEMORY=""{{ bamboo_jvm_heap_max }}'
  notify: restart bamboo

- name: Fix permissions on application folder
  file:
    path: ""{{ item }}""
    recurse: yes
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  changed_when: False
  with_items:
    - ""{{ bamboo_application_directory }}""",0,"Add cronjob for cleanup bamboo build dir, Add cronjob for cleanup bamboo logs, Add cronjob for cleanup maven repository cache, Add local user (user), Copy logrotate script for syslog, Download and unpack bamboo, Fix permissions on application folder, Install JDK, Keep newer files, Set JVM settings","CWE-258, CWE-327, CWE-345",0
"# TODO: Find out why this doesn't work
  command: ""sudo /opt/influxdb/influxd config -config {{influxdb_generated_config}}""
  register: influxdb_merged_config
  tags:
    - influxdb

- name: Write merged config
  copy:
    content: ""{{influxdb_merged_config.stdout}}""
    dest: ""{{influxdb_config_file}}""
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
  tags:
    - influxdb

- name: Ensure directories have correct permissions
  command: ""sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{item}}""
  with_items:
    - ""{{influxdb_meta_dir}}""
    - ""{{influxdb_data_dir}}""
    - ""{{influxdb_hh_dir}}""
    - ""{{influxdb_wal_dir}}""",1,"sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_config_dir}}, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_data_dir}}, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_data_dir}}/{{influxdb_db}}, {{influxdb_data_dir}}/{{influxdb_hh_dir}}, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_hh_dir}}, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_meta_dir}}, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_retention_dir}}, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_retention_dir}}/{{influxdb_db}}_data/{{influxdb_db}}_retention_data, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_retention_dir}}/{{influxdb_db}}_retention, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_toml_config_dir}}, sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{influxdb_wal_dir}}",CWE-78,1,"when: ansible_distribution in ['Ubuntu', 'Debian']
- name: Create data dir
    path: ""{{influxdb_data_dir}}""
    state: directory
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
- name: Create meta dir
    path: ""{{influxdb_meta_dir}}""
- name: Create hh dir
    path: ""{{influxdb_hh_dir}}""
- name: Create config directory
    dest: ""{{influxdb_generated_config}}""
- name: Run config update
  command: ""sudo su -c \""{{influxdb_opt_dir}}/influxd config -config {{influxdb_generated_config}} > {{influxdb_config_file}}\"" {{influxdb_user}}""",0,"command: sudo su -c {{influxdb_opt_dir}}/influxd config -config {{influxdb_generated_config}} > {{influxdb_config_file} {{influxdb_user}}, path: {{influxdb_data_dir}}, path: {{influxdb_generated_config}}, path: {{influxdb_group}}, path: {{influxdb_hh_dir}}, path: {{influxdb_meta_dir}}, path: {{influxdb_opt_dir}}/influxd config -config {{influxdb_generated_config}}, path: {{influxdb_user}}, sudo su -c {{influxdb_opt_dir}}/influxd config -config {{influxdb_generated_config}} > {{influxdb_config_file} {{influxdb_user}}, when: ansible_distribution in ['Ubuntu', 'Debian']","CWE-1043, CWE-1132, CWE-129",1
"file:
    path: ""{{ degoss_test_dir }}""
    state: directory
  loop: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
    clean: ""{{ degoss_clean | bool }}""
    clean_on_failure: ""{{ degoss_clean_on_failure | bool }}""
    debug: ""{{ degoss_debug | bool }}""",1,"clean:, clean: {{ degoss_clean | bool }}, clean_on_failure: {{ degoss_clean_on_failure | bool }}, debug: {{ degoss_debug | bool }}, loop: {{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}, path: {{ degoss_test_dir }}, state: directory",CWE-937,1,"- include: versions/latest.yml
  when: goss_version == ""latest""

- include: versions/pinned.yml
  when: goss_version != ""latest""

# set play facts
- name: establish download url
  set_fact:
    goss_download_url: ""{{ goss_github_repo_url }}/releases/download/v{{ goss_real_version }}/goss-linux-amd64""

# create goss directories
- name: create goss directories
  file: path={{ item }} state=directory
  with_items:
    - ""{{ degoss_tmp_root }}""
    - ""{{ degoss_test_root }}""
    - ""{{ degoss_goss_install_dir }}""
  changed_when: degoss_changed_when

# download goss
- name: install
  get_url:
    url: ""{{ goss_download_url }}""
    dest: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    mode: 0755
  changed_when: degoss_changed_when

# symlink
- name: link
  file:
    state: link
    src: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    dest: ""{{ degoss_goss_bin }}""
    force: true
  changed_when: degoss_changed_when

# deploy test files including the main and additional test files
- name: deploy test files
  copy: src={{ goss_file }} dest={{ degoss_test_root }}
  with_items: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
  changed_when: degoss_changed_when

# run the tests
- name: run tests
  goss: executable={{ degoss_goss_bin }} path=""{{ goss_file }}"" format=""{{ goss_output_format }}""
  # never report failure, allowing us to clean up and then report failure later
  failed_when: false
  register: goss_output

# clean everything up
- name: clean
  file: path={{ degoss_tmp_root }} state=absent
  when: degoss_no_clean is undefined and not degoss_no_clean
  changed_when: degoss_changed_when

# our output callback plugin will catch the tag of this and format output accordingly
- name: report errors
  fail: msg=""Goss Tests Failed.""
  when: goss_output.goss_failed
  tags: [format_goss_output]",0,"degoss_changed_when}}, \\\\, degoss_changed_when}}, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, goss_changed_when}}, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, goss_changed_when}}, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, goss_file}}, \\\\, goss_no_clean}}, \\\\\\\\\\\\\\\\, goss_output.goss_failed}}, \\\\\\\\, goss_output.goss_failed}}, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, goss_real_version}}, \\, goss_version == \","CWE-117, CWE-120, CWE-122, CWE-126",0
-signer {{ codesign_cert_filename }} -inkey {{ codesign_key_filename }} -certfile {{ cert_file_filename }} -outform DER \,1,signer {{ codesign_cert_filename }} -inkey {{ codesign_key_filename }} -certfile {{ cert_file_filename }} -outform DER,"CWE-327, CWE-328, CWE-329, CWE-330, CWE-331, CWE-332, CWE-333, CWE-334, CWE-335, CWE-336",1,"---
  - name: Gather list of source files
    command: ls {{ netbootxyz_root }}
    register: source_files

  - name: Create directories for signatures
    file:
      path: ""{{ item }}""
      state: directory
    with_items:
      - ""{{ sigs_dir }}""

  - name: Generate signatures for source files
    shell: |
      openssl cms -sign -binary -noattr -in {{ netbootxyz_root }}/{{ item }} \ 
      -signer {{ codesign_cert_location }} -inkey {{ codesign_key_location }} -certfile {{ cert_file_location }} -outform DER \
      -out {{ sigs_dir }}/{{ item }}.sig
    args:
      chdir: ""{{ cert_dir }}""
      warn: false
    with_items:
      - ""{{ source_files.stdout_lines }}""
    tags:
    - skip_ansible_lint",0,"cwe_cwe, cwe_cwe_cwe, task_name: <task_name>",CWE-xxxx,0
"src: ""{{ slurm_build_dir }}/etc/{{ item }}""
    src: ""{{ slurm_build_dir }}/etc/{{ item }}""",1,"src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, src: \\\\\\\\\\n src: \\\\\\\\\\n, src: \\\\\n src: \\\\\n, src: \\n src: \\n, src: \n src: \n",CWE-120,1,"---
- name: install build dependencies
  apt:
    name: ""{{ item }}""
  with_items: ""{{ slurm_build_deps }}""
  when: ansible_distribution == 'Ubuntu'

- name: remove slurm packages
  apt:
    name: ""{{ item }}""
    state: absent
  with_items:
    - slurm-wlm
    - slurmctld
    - slurmdbd
    - slurmd
  when: ansible_distribution == 'Ubuntu'

- name: install build dependencies
  yum:
    name: ""{{ item }}""
    state: present
  with_items: ""{{ slurm_build_deps }}""
  when: ansible_os_family == 'RedHat'

- name: remove slurm packages
  yum:
    name: ""{{ item }}""
    state: present
  with_items:
    - slurm
  when: ansible_os_family == 'RedHat'

- name: make build directory
  file:
    path: ""{{ slurm_build_dir }}""
    state: directory

- name: download source
  unarchive:
    src: ""{{ slurm_src_url }}""
    remote_src: yes
    dest: ""{{ slurm_build_dir }}""
    extra_opts:
      - --strip-components=1

- name: uninstall old version
  command: make -j uninstall
  args:
    chdir: ""{{ slurm_build_dir }}""
  ignore_errors: yes
  tags:
    - uninstall

- name: clean src dir
  command: make distclean
  args:
    chdir: ""{{ slurm_build_dir }}""
  ignore_errors: yes

- name: configure
  command: ""./configure --prefix={{ slurm_install_prefix }} --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build
  shell: ""make -j$(nproc) > build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build contrib
  shell: ""make -j$(nproc) contrib >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: install
  shell: ""make -j$(nproc) install >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: install contrib
  shell: ""make -j$(nproc) install-contrib >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build pam_slurm_adopt
  shell: ""make -j$(nproc) >> ../../build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}/contribs/pam_slurm_adopt""

- name: install pam_slurm_adopt
  shell: ""make -j$(nproc) install >> ../../build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}/contribs/pam_slurm_adopt""

- name: copy service files
  copy:
    src: ""{{ item }}""
    dest: ""/etc/systemd/system/{{ item }}""
    remote_src: yes
  args:
    chdir: ""{{ slurm_build_dir }}""
  with_items:
    - slurmctld.service
    - slurmdbd.service
  when: is_controller

- name: copy service files
  copy:
    src: ""{{ item }}""
    dest: ""/etc/systemd/system/{{ item }}""
    remote_src: yes
  args:
    chdir: ""{{ slurm_build_dir }}""
  with_items:
    - slurmd.service
  when: is_compute

- name: restart munge
  service:
    name: munge
    state: restarted

- name: restart slurmd
  service:
    name: slurmd
    state: restarted
    enabled: yes
  when: is_compute

- name: restart slurmdbd
  service:
    name: slurmdbd
    state: restarted
    enabled: yes
  when: is_controller

- name: restart slurmctld
  service:
    name: slurmctld
    state: restarted
    enabled: yes
  when: is_controller",0,"configure --prefix={{ slurm_install_prefix }} --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm, make -j$(nproc) >>../../build.log 2>&1, make -j$(nproc) install >> build.log 2>&1, make -j$(nproc) install-contrib >> build.log 2>&1, make -j$(nproc) install-contrib >>../../build.log 2>&1",CWE-78,1
"- ""{{ ansible_os_family|lower }}.yml""",1,ansible_os_family|lower,CWE-295,1,"- name: gather os specific variables
  include_vars: ""{{ item }}""
  with_first_found:
    - files:
      - ""{{ ansible_distribution|lower }}-{{ ansible_distribution_major_version|lower }}.yml""
      - ""{{ ansible_distribution|lower }}.yml""
      paths:
      - ../vars
      skip: true
  tags: vars",0,"- files:\\\\r, ansible_distribution|lower}\\r, files: -\\r, files:\\\r, include_vars: \, paths:\\r, skip: true\\\r, tags: vars\\\\\\r, with_first_found:\\\\r, with_first_found:\r",CWE-23,0
"command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}'""",1,"allow-root, command: \, no-color, option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}', option {{ item.1.command }} {{ item.1.name }}', option {{ item.1.command }} {{ item.1.value }}', wp-cli, wp-cli --allow-root, wp-cli --allow-root --no-color","CWE-113, CWE-119, CWE-276",1,"- name: add options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Added' in check_installation_options.stdout""
  when: item.1.command == 'add'
  tags: [configuration, wordpress, wordpress-options]

- name: update options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  changed_when: ""'unchanged' not in check_installation_options.stdout""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'update'
  tags: [configuration, wordpress, wordpress-options]

- name: delete options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Could not delete' not in check_installation_options.stderr""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'delete'",0,"command: \, when: item.1.command == 'add', when: item.1.command == 'delete', when: item.1.command == 'update'","CWE-306, CWE-327",0
"shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1.name }} --activate""
  when: check_installation_plugins is defined and item.item.1.name and item.rc != 0
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  when: item.1.name

- name: activate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin activate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-activate-plugin]

- name: deactivate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin deactivate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and not item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-deactivate-plugin]",1,"shell:, when:","CWE-117, CWE-120, CWE-125, CWE-601, CWE-94",1,"---
# tasks file for wordpress, plugins
- name: identify installation (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  register: check_installation_plugins
  failed_when: False
  changed_when: False
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-is-installed-plugin]

- name: install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1 }} --activate""
  with_items: check_installation_plugins.results
  when: check_installation_plugins is defined and item.item.1 and item.rc != 0
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin]

- name: check install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin-check]",0,"check_installation_plugins.results, plugins, register: check_installation_plugins, shell: \, shell: \\, when: check_installation_plugins is defined and item.item.1 and item.rc!= 0, when: item.item.1, with_items: check_installation_plugins.results, with_subelements: \\\","CWE-778, CWE-78",1
"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""",1,"dashboard_max_heapsize:, dashboard_min_heapsize:, springapp_max_heapsize:, springapp_max_heapsize: dashboard_max_heapsize, springapp_max_heapsize: {{ dashboard_max_heapsize }}, springapp_max_heapsize: {{dashboard_max_heapsize}}, springapp_min_heapsize:, springapp_min_heapsize: dashboard_min_heapsize, springapp_min_heapsize: {{ dashboard_min_heapsize }}, springapp_min_heapsize: {{dashboard_min_heapsize}}",CWE-754,1,"springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",0,"springapp_heapsize: 512m\\, springapp_heapsize: \, springapp_local_jar: \\\\, springapp_local_jar: dashboard_local_jar\\\\\\, springapp_random_source: \\, springapp_random_source: file:///dev/urandom\\\\\\\\\\\\\\\\, springapp_tcpport: 9394\\\\, springapp_tcpport: \\, springapp_tcpport: \\\\",CWE-937,0
"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""",1,"springapp_min_heapsize: {{dashboard_min_heapsize}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, springapp_min_heapsize: {{dashboard_min_heapsize}}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nspringapp_max_heapsize: {{dashboard_max_heapsize}}, springapp_min_heapsize: {{dashboard_min_heapsize}}\\\\\\\\\\\\\\\\\\\\nspringapp_max_heapsize: {{dashboard_max_heapsize}}, springapp_min_heapsize: {{dashboard_min_heapsize}}\\\\\\\\nspringapp_max_heapsize: {{dashboard_max_heapsize}}, springapp_min_heapsize: {{dashboard_min_heapsize}}\\\\nspringapp_max_heapsize: {{dashboard_max_heapsize}}, springapp_min_heapsize: {{dashboard_min_heapsize}}\\nspringapp_max_heapsize: {{dashboard_max_heapsize}}, springapp_min_heapsize: {{dashboard_min_heapsize}}\nspringapp_max_heapsize: {{dashboard_max_heapsize}}",CWE-693,1,"springapp_heapsize: ""128m""",0,springapp_heapsize: 128m,"CWE-190, CWE-20, CWE-327",0
"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""",1,"springapp_max_heapsize: \\, springapp_max_heapsize: \\\\, springapp_max_heapsize: \\\\\\\\, springapp_max_heapsize: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, springapp_max_heapsize: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, springapp_min_heapsize: \, springapp_min_heapsize: \\, springapp_min_heapsize: \\\\, springapp_min_heapsize: \\\\\\\\\\\\\\\\, springapp_min_heapsize: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-119: Improper Restriction of Operations within the Bounds of a Memory Buffer, CWE-120: Improper Restriction of Operations within the Bounds of a Memory Buffer, CWE-700: Improper Input Validation, CWE-703: Improper Control of a Resource with a Cast",1,"springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",0,"springapp_artifact_type: jar, springapp_debug: 128m, springapp_debug_port: 128m, springapp_debug_port: 1292, springapp_dir: /org/openconext, springapp_heapsize: 128m, springapp_heapsize: \, springapp_service_name: attribute-mapper, springapp_tcpport: 9292, springapp_user: attribute-mapper","CWE-120, CWE-20, CWE-264",0
"user: name=authz-server home={{ authz_server_dir }} shell=/sbin/nologin
- include: install-release.yml
- include: install-snapshot.yml
- include: install-local.yml",1,"include: install-local.yml, include: install-release.yml, include: install-snapshot.yml, user: name=authz-server home={{ authz_server_dir }} shell=/sbin/nologin","CWE-1030, CWE-1031",0,"---
- name: Create user
  user: name=authz-server home={{ authz_server_dir }}
  tags: authz-server

- name: Create logging directory
  file: path=/var/log/{{ java_app.name }} state=directory owner=authz-server group=authz-server mode=0755
  tags: authz-server

- name: Get the md5 sum of current jar
  stat: path={{ authz_server_dir }}/{{ authz_server_jar }}
  register: current_jar_stat
  tags: authz-server

- name: Create directory where we download application packages (e.g. jar/war files)
  file: path=~/app-downloads state=directory
  tags: authz-server

- name: Deploy | Download release
  get_url:
    url: ""{{ maven_repo }}/org/openconext/authz-server/{{ authz_server_version }}/authz-server-{{ authz_server_version }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp == '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Download snapshot
  get_url:
    url: ""{{ maven_snapshot_repo }}/org/openconext/authz-server/{{ authz_server_version }}-SNAPSHOT/authz-server-{{ authz_server_version }}-{{ authz_server_snapshot_timestamp }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp != '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Upload from local
  copy: src={{ authz_server_local_jar }} dest=~/app-downloads/{{ authz_server_jar }}
  when: authz_server_local_jar != ''
  tags: authz-server

- name: Get the md5 sum of the candidate
  stat: path=~/app-downloads/{{ authz_server_jar }}
  register: candidate_jar_stat
  tags: authz-server

- name: Replace the jar if it has changed
  shell: cp ~/app-downloads/{{ authz_server_jar }} {{ authz_server_dir }}/{{ authz_server_jar }}
  when: current_jar_stat.stat.exists == false or candidate_jar_stat.stat.md5 != current_jar_stat.stat.md5
  notify: restart authz-server
  tags: authz-server

- name: Copy logging config
  template: src=logback.xml.j2 dest={{ authz_server_dir }}/logback.xml owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy application config
  template: src=application.properties.j2 dest={{ authz_server_dir }}/application.properties owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy start script
  template: src=templates/spring-boot.j2 dest=/etc/init.d/{{ java_app.name }} mode=0755
  notify: restart authz-server
  tags: authz-server",0,"java: path={{ authz_server_dir }}/{{ authz_server_jar }}, stat: path={{ authz_server_dir }}/{{ authz_server_jar }}, stat: path=~/app-downloads/{{ authz_server_jar }}, template: src=templates/spring-boot.j2, template: src={{ authz_server_dir }}/application.properties.j2, template: src={{ authz_server_dir }}/logback.xml.j2","CWE-272, CWE-276, CWE-279, CWE-280",1
"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""",1,"authz_admin_max_heapsize, authz_admin_min_heapsize, springapp_max_heapsize: \\, springapp_max_heapsize: \\\\, springapp_min_heapsize: \, springapp_min_heapsize: \\","CWE-271, CWE-272, CWE-276",0,"springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",0,"springapp_artifact_group_dir, springapp_artifact_id, springapp_heapsize, springapp_jar, springapp_local_jar, springapp_random_source, springapp_service_name, springapp_tcpport, springapp_user, springapp_version","CWE-20, CWE-319, CWE-327, CWE-352",1
"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""",1,"authz_admin_max_heapsize, authz_admin_max_heapsize: \\\\, authz_admin_min_heapsize, authz_admin_min_heapsize: \\, springapp_max_heapsize, springapp_max_heapsize: \\, springapp_max_heapsize: \\\\\\\\\\\\, springapp_min_heapsize, springapp_min_heapsize: \, springapp_min_heapsize: \\\\\\\",CWE-119,0,"springapp_heapsize: ""128m""",0,"128m\, 128m\\, \n, play: \\\\, springapp_heapsize: \, springapp_heapsize: \\, }",CWE-693,1
"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""",1,"authz_admin_max_heapsize, authz_admin_min_heapsize, springapp_max_heapsize, springapp_min_heapsize",CWE-693,0,"springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",0,"springapp_artifact_group_dir: /org/openconext, springapp_artifact_id: attribute-mapper, springapp_artifact_type: jar, springapp_debug: \\, springapp_dir: \, springapp_heapsize: 128m, springapp_jar: \\, springapp_service_name: attribute-mapper, springapp_snapshot_timestamp: \\, springapp_user: attribute-mapper","CWE-129, CWE-250, CWE-256, CWE-601",1
service: name=authz-admin state=restarted sleep=45,1,"name=authz-admin, sleep=45, state=restarted","CWE-209, CWE-256, CWE-264, CWE-269, CWE-275, CWE-284, CWE-306, CWE-326, CWE-732, CWE-754",0,service: name=authz-admin state=restarted sleep=45`,0,service: name=authz-admin state=restarted sleep=45,CWE-120,1
name: 'urn:schac:attribute-def:schacPersonalUniqueCode',1,name: 'urn:schac:attribute-def:schacPersonalUniqueCode',"CWE-1001, CWE-1006, CWE-1007, CWE-1008, CWE-1009, CWE-1010, CWE-1020, CWE-1031, CWE-1034, CWE-1042",0,"janus_service_registry_core:
    admin:
        name: Surfconext
        email: info@surfconext.nl
    auth: default-sp
    useridattr: NameID
#    auth: login-admin
#    useridattr: user
    user:
        autocreate: true
    dashboard:
        inbox:
            paginate_by: 20
    push:
        remote:
            test:
                name: ""OpenConext EngineBlock""
                url: ""https://{{ engine_api_janus_user }}:{{ engine_api_janus_password | vault }}@{{ engine_api_domain }}/api/connections""
    entity:
        prettyname: 'name:en'
        useblacklist: false
        usewhitelist: true
        validateEntityId: false
    enable:
        saml20-sp: true
        saml20-idp: true
        shib13-sp: false
        shib13-idp: false
    workflowstates:
        testaccepted:
            name:
                en: Test
                da: Test
                es: 'testaccepted - es'
            description:
                en: 'All test should be performed in this state'
                da: 'I denne tilstand skal al test foretages'
                es: 'Desc 1 es'
            abbr: 'TA'
        prodaccepted:
            name:
                en: Production
                da: Produktion
                es: 'prodaccepted - es'
            description:
                en: 'The connection is on the Production system'
                da: 'Forbindelsen er p Produktions systemet'
                es: 'Desc 5 es'
            textColor: green
            abbr: 'PA'
    workflowstate:
        default: testaccepted
    attributes:
        eduPersonTargetedID:
            name: 'urn:mace:dir:attribute-def:eduPersonTargetedID'
        eduPersonPrincipalName:
            name: 'urn:mace:dir:attribute-def:eduPersonPrincipalName'
        displayName:
            name: 'urn:mace:dir:attribute-def:displayName'
        'cn (common name)':
            name: 'urn:mace:dir:attribute-def:cn'
        givenName:
            name: 'urn:mace:dir:attribute-def:givenName'
        'sn (surname)':
            name: 'urn:mace:dir:attribute-def:sn'
        mail:
            name: 'urn:mace:dir:attribute-def:mail'
        schacHomeOrganization:
            name: 'urn:mace:terena.org:attribute-def:schacHomeOrganization'
        schacHomeOrganizationType:
            name: 'urn:mace:terena.org:attribute-def:schacHomeOrganizationType'
        schacPersonalUniqueCode:
            name: 'urn:mace:terena.org:attribute-def:schacPersonalUniqueCode'
        eduPersonAffiliation:
            name: 'urn:mace:dir:attribute-def:eduPersonAffiliation'
            specify_values: true
        eduPersonScopedAffiliation:
            name: 'urn:mace:dir:attribute-def:eduPersonScopedAffiliation'
            specify_values: true
        eduPersonEntitlement:
            name: 'urn:mace:dir:attribute-def:eduPersonEntitlement'
            specify_values: true
        isMemberOf:
            name: 'urn:mace:dir:attribute-def:isMemberOf'
            specify_values: true
        uid:
            name: 'urn:mace:dir:attribute-def:uid'
        preferredLanguage:
            name: 'urn:mace:dir:attribute-def:preferredLanguage'
        'nlEduPersonOrgUnit (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlEduPersonOrgUnit'
            specify_values: true
        'nlEduPersonStudyBranch (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlEduPersonStudyBranch'
            specify_values: true
        'nlStudielinkNummer (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlStudielinkNummer'
        'nlDigitalAuthorIdentifier (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlDigitalAuthorIdentifier'
        'collabPersonId (deprecated)':
            name: 'urn:oid:1.3.6.1.4.1.1076.20.40.40.1'
    md:
        mapping:
            'UIInfo:Logo:0:height': 'logo:0:height'
            'UIInfo:Logo:0:width': 'logo:0:width'
            'UIInfo:Logo:0:url': 'logo:0:url'
            'UIInfo:Keywords:en': 'keywords:en'
            'UIInfo:Keywords:nl': 'keywords:nl'
            'UIInfo:Description:en': 'description:en'
            'UIInfo:Description:nl': 'description:nl'
    usertypes:
        - admin
        - readonly
        - technical
    messenger:
        external:
            mail:
                class: 'janus:SimpleMail'
                name: Mail
                option:
                    headers: ""MIME-Version: 1.0\r\nContent-type: text/html; charset=iso-8859-1\r\nFrom: JANUS <no-reply@example.org>\r\nReply-To: JANUS Admin <admin@example.org>\r\nX-Mailer: PHP/5.4.6-1ubuntu1.8""
    mdexport:
        default_options:
            entitiesDescriptorName: Federation
            maxCache: 86400
            maxDuration: 432000
            sign:
                enable: false
                privatekey: server.pem
                privatekey_pass: null
                certificate: server.crt
    encryption:
        enable: false
    access:
        createnewentity:
            workflow_states:
                all:
                    - all
                    - '-readonly'
        changeentityid:
            workflow_states:
                all:
                    - admin
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
            default: true
        allentities:
            workflow_states:
                all:
                    - admin
                    - readonly
        exportallentities:
            workflow_states:
                all:
                    - admin
        arpeditor:
            workflow_states:
                all:
                    - admin
        admintab:
            workflow_states:
                all:
                    - admin
        adminusertab:
            workflow_states:
                all:
                    - admin
        federationtab:
            workflow_states:
                all:
                    - ''
        showsubscriptions:
            workflow_states:
                all:
                    - ''
        addsubscriptions:
            workflow_states:
                all:
                    - ''
        editsubscriptions:
            workflow_states:
                all:
                    - ''
        deletesubscriptions:
            workflow_states:
                all:
                    - ''
        experimental:
            workflow_states:
                all:
                    - ''
        changeentitytype:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        exportmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        blockremoteentity:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        changeworkflow:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        changemanipulation:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        changearp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        editarp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        addarp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        addmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        validatemetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        deletemetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        modifymetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        importmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        entityhistory:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        disableconsent:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
    metadata_refresh_cron_tags:
        - daily
    validate_entity_certificate_cron_tags:
        - daily
    validate_entity_endpoints_cron_tags:
        - daily
    ca_bundle_file: /etc/pki/tls/certs/ca-bundle.crt
    metadatafields:
        saml20-idp:
            'name:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'displayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'description:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            certData:
                type: text
                default: ''
                default_allow: false
                required: true
            certData2:
                type: text
                default: ''
                default_allow: false
                required: false
            certData3:
                type: text
                default: ''
                default_allow: false
                required: false
            SingleLogoutService_Binding:
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            SingleLogoutService_Location:
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'contacts:#:contactType':
                type: select
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                select_values:
                    - technical
                    - support
                    - administrative
                    - billing
                    - other
                default: ''
                default_allow: false
            'contacts:#:givenName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:surName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:emailAddress':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                validate: isemail
                type: text
                default: ''
                default_allow: false
            'contacts:#:telephoneNumber':
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationDisplayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationURL:#':
                validate: isurl
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'logo:0:url':
                required: true
                default: 'https://.png'
                default_allow: false
                type: text
            'logo:0:width':
                required: true
                default: '120'
                type: text
                default_allow: true
            'logo:0:height':
                required: true
                default: '60'
                type: text
                default_allow: true
            redirect.sign:
                type: boolean
                required: true
                default: false
                default_allow: true
            'coin:publish_in_edugain':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:publish_in_edugain_date':
                validate: isdatetime
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:additional_logging':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:institution_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'SingleSignOnService:0:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: true
                default_allow: true
            'SingleSignOnService:0:Location':
                required: true
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'SingleSignOnService:1:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            'SingleSignOnService:1:Location':
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'coin:guest_qualifier':
                type: select
                required: true
                select_values:
                    - All
                    - Some
                    - None
                default: All
                default_allow: true
            'coin:schachomeorganization':
                type: text
                default: ''
                default_allow: false
                required: false
            NameIDFormat:
                type: select
                required: false
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default_allow: true
            'keywords:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'coin:disable_scoping':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:hidden':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'shibmd:scope:#:allowed':
                supported:
                    - 0
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                type: text
                default: ''
                default_allow: false
                required: false
            'shibmd:scope:#:regexp':
                type: boolean
                supported:
                    - 0
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                default: ''
                default_allow: false
                required: false
        saml20-sp:
            'name:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'displayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'description:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            certData:
                type: text
                default: ''
                default_allow: false
                required: false
            certData2:
                type: text
                default: ''
                default_allow: false
                required: false
            certData3:
                type: text
                default: ''
                default_allow: false
                required: false
            SingleLogoutService_Binding:
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            SingleLogoutService_Location:
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'contacts:#:contactType':
                type: select
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                select_values:
                    - technical
                    - support
                    - administrative
                    - billing
                    - other
                default: ''
                default_allow: false
            'contacts:#:givenName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:surName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:emailAddress':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                validate: isemail
                type: text
                default: ''
                default_allow: false
            'contacts:#:telephoneNumber':
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationDisplayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationURL:#':
                validate: isurl
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'logo:0:url':
                required: true
                default: 'https://.png'
                default_allow: false
                type: text
            'logo:0:width':
                required: true
                default: '120'
                type: text
                default_allow: true
            'logo:0:height':
                required: true
                default: '60'
                type: text
                default_allow: true
            redirect.sign:
                type: boolean
                required: true
                default: false
                default_allow: true
            'coin:publish_in_edugain':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:publish_in_edugain_date':
                validate: isdatetime
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:additional_logging':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:institution_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:trusted_proxy':
                type: boolean
                default: false
                default_allow: true
                required: false
            'AssertionConsumerService:0:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                required: true
                default_allow: true
            'AssertionConsumerService:#:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                required: false
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                default_allow: true
            'AssertionConsumerService:0:Location':
                required: true
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:#:Location':
                required: false
                validate: isurl
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:0:index':
                required: false
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:#:index':
                required: false
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                type: text
                default: ''
                default_allow: false
            NameIDFormat:
                type: select
                required: true
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default_allow: true
            'NameIDFormats:#':
                supported:
                    - 0
                    - 1
                    - 2
                type: select
                required: false
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                default_allow: true
            'coin:no_consent_required':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:eula':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'url:#':
                required: true
                supported:
                    - en
                    - nl
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'coin:gadgetbaseurl':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:secret':
                validate: lengteq20
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:two_legged_allowed':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consumer_key':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consumer_secret':
                validate: lengteq20
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:key_type':
                type: select
                select_values:
                    - HMAC_SHA1
                    - RSA_PRIVATE
                default: HMAC_SHA1
                default_allow: true
                required: false
            'coin:oauth:app_title':
                default: 'Application Title'
                default_allow: false
                type: text
                required: false
            'coin:oauth:app_description':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:app_thumbnail':
                validate: isurl
                default: 'https://www.surfnet.nl/thumb.png'
                default_allow: false
                type: text
                required: false
            'coin:oauth:app_icon':
                validate: isurl
                default: 'https://www.surfnet.nl/icon.gif'
                default_allow: false
                type: text
                required: false
            'coin:oauth:callback_url':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consent_not_required':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:ss:idp_visible_only':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:application_url':
                default: 'Application URL'
                default_allow: false
                type: text
                required: false
            'coin:implicit_vo_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:transparant_issuer':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:do_not_add_attribute_aliases':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:display_unconnected_idps_wayf':
                type: boolean
                default: false
                default_allow: true
                required: false
    workflow:
        testaccepted:
            prodaccepted:
                role:
                    - admin
        prodaccepted:
            testaccepted:
                role:
                    - admin",0,"admin\\n        name: Surfconext\\n        email: info@surfconext.nl\\n, admin\n        name: Surfconext\n        email: info@surfconext.nl\n, auth: default-sp\\\\n        useridattr: NameID, auth: default-sp\\n        useridattr: NameID, eduPersonAffiliation:\\\n            specify_values: true, eduPersonEntitlement: \\\\n            specify_values: true, isMemberOf: \\\\\\\\n            specify_values: true, nlEduPersonOrgUnit (deprecated):\\\\n            specify_values: true, shib13-sp: false\\n        shib13-idp: false, usewhitelist: true","CWE-254, CWE-256, CWE-284, CWE-285",1
"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""",1,"attribute_mapper_max_heapsize \\\\\\, attribute_mapper_max_heapsize \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, attribute_mapper_min_heapsize \\\, attribute_mapper_min_heapsize \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, springapp_max_heapsize \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, springapp_max_heapsize \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, springapp_max_heapsize: \\, springapp_min_heapsize \\\\\\\\\\\, springapp_min_heapsize \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, springapp_min_heapsize: \",CWE-1031,0,"springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom""",0,"springapp_artifact_group_dir: org.openconext\nspringapp_version: \, springapp_artifact_id: dashboard-server\\\\n, springapp_artifact_type: jar\\\\\\\\n, springapp_heapsize: 512m\\n, springapp_jar: \\, springapp_local_jar: \, springapp_random_source: file:///dev/urandom\\\\n, springapp_snapshot_timestamp: \\, springapp_tcpport: 9394\\\\\\\\n, springapp_user: dashboard\\\\n","CWE-326, CWE-327, CWE-329, CWE-362",1
"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""",1,"\, cse_77, springapp_max_heapsize: \\, springapp_min_heapsize: \","CWE-1024, CWE-772, CWE-778",0,"springapp_heapsize: ""128m""",0,"), 128m, springapp_heapsize: \, }","CWE-306, CWE-345",1
"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""",1,"attribute_mapper_max_heapsize, attribute_mapper_min_heapsize, springapp_max_heapsize, springapp_max_heapsize:, springapp_min_heapsize, springapp_min_heapsize:","CWE-100, CWE-116, CWE-125, CWE-128, CWE-129",0,"springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m""",0,"springapp_artifact_id: attribute-mapper, springapp_artifact_type: jar, springapp_dir: \\, springapp_jar: \\\\, springapp_local_jar: \\\\, springapp_service_name: attribute-mapper, springapp_snapshot_timestamp: \\, springapp_tcpport: 9292, springapp_user: attribute-mapper, springapp_version: \",CWE-1003,1
"- name: copy load engineblock sql
  template: src=files/{{ env_name }}/{{ engine_initial_sql }}.j2 dest=/tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''

- name: run load engineblock sql
  shell: mysql -u {{ engine_database_user }} -p{{ engine_database_password | vault }} -h localhost -D {{ engine_database_name }} < /tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''
  register: mysql_output
  changed_when: False # script should be idempotent

  register: migrate_output
  changed_when: ""'no update needed' not in migrate_output.stderr""",1,"changed_when: 'no update needed' not in migrate_output.stderr, changed_when: False # script should be idempotent, changed_when: False # script should be idempotent\\\\\\\\\\\\\\\\nregister: migrate_output\\\\\\\\\\\\\\\\nchanged_when: 'no update needed' not in migrate_output.stderr, changed_when: False # script should be idempotent\\\\nregister: migrate_output\\\\nchanged_when: 'no update needed' not in migrate_output.stderr, register: migrate_output\\\\nchanged_when: 'no update needed' not in migrate_output.stderr, register: mysql_output\\\\nchanged_when: False # script should be idempotent\\\\nregister: migrate_output\\\\nchanged_when: 'no update needed' not in migrate_output.stderr, shell: mysql -u {{ engine_database_user }} -p{{ engine_database_password | vault }} -h localhost -D {{ engine_database_name }} < /tmp/{{ engine_initial_sql }}\\nwhen: engine_initial_sql!= ''\\nregister: mysql_output\\nchanged_when: False # script should be idempotent\\nregister: migrate_output\\nchanged_when: 'no update needed' not in migrate_output.stderr, template: src=files/{{ env_name }}/{{ engine_initial_sql }}.j2 dest=/tmp/{{ engine_initial_sql }}\n, when: engine_initial_sql!= ''\\\\\\\\nregister: mysql_output\\\\\\\\nchanged_when: False # script should be idempotent\\\\\\\\nregister: migrate_output, when: engine_initial_sql!= ''\\\\nregister: mysql_output\\\\nchanged_when: False # script should be idempotent\\\\nregister: migrate_output\\\\nchanged_when: 'no update needed' not in migrate_output.stderr",CWE-22,1,"- name: Run EngineBlock migrations
  command: ./bin/migrate
  args:
    chdir: ""{{ engine_release_dir }}""
  changed_when: False # TODO How to check when migrate is up to date?",0,"args, changed_when, changed_when: False, chdir, chdir: {{, command, migrate","CWE-1131, CWE-213, CWE-263, CWE-264, CWE-275, CWE-276",1
"dest: ""{{ shared_path }}/""                  # Dest on host specified in {{ rsync_to }}
    set_remote_user: false
    ansible_ssh_extra_args: ""-l {{ unicorn_user }} -o ControlMaster=auto -o ControlPersist=60s -o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout=600""",1,ansible_ssh_extra_args: -l {{ unicorn_user }} -o ControlMaster=auto -o ControlPersist=60s -o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout=600,CWE-327,1,"---

- name: rsync asset files with ssh forwarding
  synchronize:
    src: ""{{ shared_path }}/{{ migrate_dir }}""  # Source on target host specified in --limit
    dest: ""{{ shared_path }}/{{ migrate_dir }}"" # Dest on host specified in {{ rsync_to }}
    mode: pull
    rsync_opts:
      - ""--chown={{ unicorn_user }}:{{ unicorn_user }}""
  vars:
    ansible_ssh_extra_args: '-o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout 600'
  loop:
    - assets
    - spree
    - system
  loop_control:
    loop_var: migrate_dir
  delegate_to: ""{{ groups[rsync_to][0] }}""",0,"ConnectTimeout, UserKnownHostsFile, ansible_ssh_extra_args, loop, rsync_opts, src:","CWE-1163, CWE-119, CWE-200, CWE-306",1
"# Remove empty rendered volume templates (e.g. inactivated for an env_type)
- name: Ignore empty rendered volume templates
  set_fact:
    path: ""{{ item }}""
  register: filtered_volumes
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined and lookup('template', item) | length > 1
  tags: deploy

- name: Update volume templates list for this app
  set_fact:
    volumes: ""{{ filtered_volumes | json_query('results[*].ansible_facts.path') | list }}""
  when: app.volumes is defined
  
  with_items: ""{{ volumes }}""",1,"app.volumes, filtered_volumes, set_fact: {{ app }}, set_fact: {{ app.volumes }}, set_fact: {{ filtered_volumes }}, set_fact: {{ volumes }}, volumes: {{ filtered_volumes }}, when: app.volumes, with_items: {{ volumes }}",CWE-693,0,"---
# Create volumes for an app

- name: Print app name
  debug: msg=""App name {{ app.name }}""
  tags: route

- name: Make sure application volumes exist
  openshift_raw:
    force: true
    definition: ""{{ lookup('template', item) | from_yaml }}""
    state: present
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined
  tags: volume",0,"app.volumes, app.volumes is defined, definition: {{ lookup('template', item) | from_yaml }}, force: true, lookup('template', item), state: present, when: app.volumes is defined, with_items: {{ app.volumes }}","CWE-119, CWE-19, CWE-20",1
when: not registries_vault.stat.exists,1,when: not registries_vault.stat.exists,CWE-779,0,"---
# Create secrets to login to private docker registries

- name: Retrieve registries vault file
  stat:
    path: ""group_vars/customer/{{ customer }}/{{ env_type }}/secrets/registries.vault.yml""
  register: registries_vault

- block:
    - name: Check if registries vault exists
      debug:
        msg: ""No registries vault is associated with the project""
    - meta: end_play
  when: registries_vault.stat.exists

- name: Import registries variable
  include_vars:
    file: ""{{ registries_vault.stat.path }}""

- include_tasks: tasks/create_docker_registry_secret.yml
  loop: ""{{ registries }}""
  loop_control:
    loop_var: registry",0,"- meta: end_play\\\\\\\\\\\\r\\\\\\\\n  when: registries_vault.stat.exists\\\\\\\\\\\\r\\\\\\\\n, if registries_vault.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\n- meta: end_play\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\n  when: registries_vault.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\n, if registries_vault.stat.exists\r\n    - meta: end_play\r\n  when: registries_vault.stat.exists\r\n, include_tasks: tasks/create_docker_registry_secret.yml\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  loop: '{{ registries }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nloop_control:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    loop_var: registry\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, include_tasks: tasks/create_docker_registry_secret.yml\\\\\\r\\\\n  loop: '{{ registries }}'\\\\\\r\\\\nloop_control:\\\\\\\\r\\\\n    loop_var: registry\\\\\\\\r\\\\n, loop: '{{ registries }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, loop: '{{ registries }}'\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\nloop_control:\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n    loop_var: registry\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n, loop: '{{ registries }}'\\\\r\\\\nloop_control:\\\\\\r\\\\n    loop_var: registry\\\\\\\\r\\\\n, loop_control:\\\\r\\\\n    loop_var: registry\\\\r\\\\n, loop_control:\\r\\n    loop_var: registry\\r\\n","CWE-1031, CWE-601",0
"line: ""OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""",1,OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16',CWE-937,1,"---
- name: ""Setting Docker Facts""
  set_fact:
    docker_storage_block_device: ""{{ docker_storage_block_device | default(default_docker_storage_block_device) }}""
    docker_storage_volume_group: ""{{ docker_storage_volume_group | default(default_docker_storage_volume_group) }}""

- name: ""Install Docker""
  yum: 
    name: docker
    state: latest
  notify:
    - enable docker

- name: ""Confige Docker""
  lineinfile: 
    dest: /etc/sysconfig/docker 
    regexp: '^OPTIONS=.*$' 
    line: ""^OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""

- name: ""Check for existing Docker Storage device""
  command: pvs
  register: pvs

- name: ""Set Docker Storage fact if already configured""
  set_fact:
    docker_storage_setup: true
  when: pvs.stdout | search('{{ docker_storage_block_device }}.*{{ docker_storage_volume_group }}')

- name: ""Configure Docker Storage Setup""
  template:
    src: docker-storage-setup.j2
    dest: /etc/sysconfig/docker-storage-setup
  when: docker_storage_setup is undefined
  
- name: ""Run Docker Storage Setup""
  command: docker-storage-setup
  when: docker_storage_setup is undefined
  notify:
  - restart docker

- name: ""Extend the Volume Group for Docker Storage""
  command: lvextend -l 90%VG /dev/{{ docker_storage_volume_group }}/docker-pool
  when: docker_storage_setup is undefined
  notify:
  - restart docker",0,"- name: \, notify:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, notify:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, notify:\\\\\\\\\\\\\\n, notify:\\\\\\\\\\n, notify:\\\\n, notify:\\\n, notify:\\n, set_fact:\n, when: docker_storage_setup is undefined\n","CWE-256, CWE-269, CWE-279",0
"| intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host_type_master'])) }}""
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host_type_node'])) }}""",1,"intersect( groups['tag_host_type_node'])) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, intersect( groups['tag_host_type_node'])) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, intersect(groups['tag_clusterid_' ~ cluster_id] \\\\\\\\\\\\\\\\, intersect(groups['tag_host_type_master'])) \\\\\\\\\\\\, intersect(groups['tag_host_type_master'])) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, intersect(groups['tag_host_type_node'])) \\\\\\, | intersect( groups['tag_host_type_master'])) \\\, | intersect(groups['tag_clusterid_' ~ cluster_id] \\, | intersect(groups['tag_clusterid_' ~ cluster_id]) \",CWE-1007,0,"vars:
    env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersection(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersection(groups['tag_clusterid_' ~ cluster_id]
                                     | intersection( groups['tag_host_type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersection(groups['tag_clusterid_' ~ cluster_id]
                                   | intersection(groups['tag_host_type_node'])) }}""
    with_items: ""{{ env_cluster_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ master_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ node_hosts }}""",0,"region: \, region: \\, region: \\\\, region: \\\\\\\\\\\\, region: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items: \, with_items: \\, with_items: \\\\, with_items: \\\\\\\\\\\\\\\\\\\\\\\\",CWE-949,0
"env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host-type_node'])) }}""",1,"- name: ansible-test\\n  hosts: ip: 192.168.2.13\\n  become: yes, become: no\\n  hosts: ip: 192.168.2.17, host_vars: \n\n    hosts:\n      - ip: 192.168.2.12\n, roles: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - role: my-role\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    hosts: ip: 192.168.2.20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, roles: \\\\n  - role: my-role\\\\n  hosts: ip: 192.168.2.15, tags: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, tags: \\\\\\\\\n  - tag1\\\\\\\\n  hosts: ip: 192.168.2.16, tasks: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - name: ansible-test\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    hosts: ip: 192.168.2.19\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    become: no, tasks: \\\\\\\\\\\\\\\\\\\\\\\\\n  - name: ansible-test\\\\\\\\\\\\\\\\\\\\\\\\n  hosts: ip: 192.168.2.18\\\\\\\\\\\\\\\\\\\\\\n    become: yes, tasks: \\n- name: ansible-test\\\\n  hosts: ip: 192.168.2.14\\n  become: no","CWE-119, CWE-16, CWE-20",0,"env_cluster_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                           | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                      | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups.tags.['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                    | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                | intersect(groups.tags.['tag_host-type_node'])) }}""",0,"env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    master_hosts: {{ groups.tags.['tag, env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    master_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect( groups.tags.['tag_host-type_master'])) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    master_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect( groups.tags.['tag_host-type_master'])) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    node_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags, env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    master_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect( groups.tags.['tag_host-type_master'])) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    node_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect(groups.tags.['tag_host-type_node'])) }}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\\\\\\\\\\\\\\\\n    master_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect( groups.tags.['tag_host-type_master'])) }}\\\\\\\\\\\\\\\\n    node_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect(groups.tags.['tag_host-type_node'])) }}\\\\\\\\\\\\\\\\n, env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\\\\n    master_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect( groups.tags.['tag_host-type_master'])) }}\\\\n    node_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect(groups.tags.['tag_host-type_node'])) }}\\\\n, env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\\n    master_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect( groups.tags.['tag_host-type_master'])) }}\\n    node_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect(groups.tags.['tag_host-type_node'])) }}\\n, env_cluster_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}\n    master_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect( groups.tags.['tag_host-type_master'])) }}\n    node_hosts: {{ groups.tags.['tag_environment_' ~ env_id] | intersect(groups.tags.['tag_clusterid_' ~ cluster_id] | intersect(groups.tags.['tag_host-type_node'])) }}\n",CWE-190,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""",1,with_items:,CWE-1028,0,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",0,"- name: Check if policies shall be overwritten\n    local_action: stat path=\, dest: \\\, nova/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    nova_policy.stat.exists, nova/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    nova_policy.stat.exists, nova/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when:\\\\\\\\\\\\\\\\\\\\\\\\\\n    nova_policy.stat.exists, nova/policy.json\\\\\\\\\\\\n  when:\\\\\\\\\\n    nova_policy.stat.exists, nova/policy.json\\\\n  when:\\\\n    nova_policy.stat.exists, nova/policy.json\\n  when:\\n    nova_policy.stat.exists, path=\\\, src: \\\\\\","CWE-1021, CWE-1138",0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""",1,"dest: \, with_items:","CWE-22, CWE-772",1,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",0,"Copy ing over existing policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n template:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n src: {{ node_custom_config }}/heat/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n dest: {{ node_config_directory }}/heat/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n when:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n heat_policy.stat.exists, Copy ing over existing policy.json\\\\\\\\n template:\\\\n src: {{ node_custom_config }}/heat/policy.json\\\\\\\\n dest: {{ node_config_directory }}/heat/policy.json\\\\\\\\n when:\\\\n heat_policy.stat.exists, Copy ing over existing policy.json\\n template:\\n src: {{ node_custom_config }}/heat/policy.json\\n dest: {{ node_config_directory }}/heat/policy.json\\n when:\\n heat_policy.stat.exists, name: Check if policies shall be overwritten\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n local_action: stat path=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n register: heat_policy, name: Check if policies shall be overwritten\\\\\\\\n local_action: stat path=\\\\\\\\\\\\n register: heat_policy, name: Check if policies shall be overwritten\\n local_action: stat path=\\, name: Check if policies shall be overwritten\n local_action: stat path=\, template:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n src: {{ node_custom_config }}/heat/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n dest: {{ node_config_directory }}/heat/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n when:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n heat_policy.stat.exists, template:\\\\\\\\\\\\\\\\\\n src: {{ node_custom_config }}/heat/policy.json\\\\\\\\\\\\\\\\\\\\n dest: {{ node_config_directory }}/heat/policy.json\\\\\\\\\\\\\\\\\\\\n when:\\\\\\\\\\\\\\\\\\n heat_policy.stat.exists, template:\\n src: {{ node_custom_config }}/heat/policy.json\\\\n dest: {{ node_config_directory }}/heat/policy.json\\\\n when:\\n heat_policy.stat.exists","CWE-275, CWE-327",1
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""",1,"cinder-volume, nova-api, nova-api-metadata, nova-compute, nova-compute-ironic, nova-conductor, nova-consoleauth, nova-novncproxy, nova-scheduler, nova-spicehtml5proxy","CWE-112, CWE-113, CWE-114",0,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",0,"dest:, local_action: stat, path: \\, register: senlin_policy, stat: path:, template:, template: src: \, when:, when: senlin_policy.stat.exists","CWE-772, CWE-773, CWE-778",0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""",1,"with_items: - \, with_items: - \\, with_items: - \\\, with_items: - \\\\, with_items: - \\\\\\\, with_items: - \\\\\\\\\\\",CWE-257,0,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",0,"- name: Check if policies shall be overwritten\n  local_action: stat path=\, cinder_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, cinder_policy.stat.exists\\\\\\\\\\\\\\\\n, cinder_policy.stat.exists\\\\n, cinder_policy.stat.exists\\n    \\, dest: \\\, when: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    cinder_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, when: \\\\\n    \\\\\n    \\\\\\n    \\\\\\n    cinder_policy.stat.exists\\\\\\\\n, when: \\\n    \\\n    \\\n    cinder_policy.stat.exists\\\\n",CWE-275,0
"- { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - [{ name: tempest, group: tempest }]
    - [{ name: tempest, group: tempest }]",1,"tempest, group: tempest",CWE-1021,0,"---
- name: Ensuring the containers up
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- include: config.yml

- name: Check the configs
  command: docker exec {{ item.name }} /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

# NOTE(jeffrey4l): when config_strategy == 'COPY_ALWAYS'
# and container env['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE',
# just remove the container and start again
- name: Containers config strategy
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_env""
  register: container_envs
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- name: Remove the containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE"" or item[1]['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'
    - item[1]['KOLLA_CONFIG_STRATEGY'] != 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results",0,"- name: Ensuring the containers up\n    kolla_docker:\n      name: \, name: \, name: \\, name: \\\\, name: \\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-1031,0
"monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_id }}""",1,"monasca_grafana_control_plane_org: \, monasca_grafana_control_plane_project \\, monasca_grafana_control_plane_project: \\, monasca_grafana_control_plane_project@{monasca_control_plane_project} @monasca_control_plane_project_domain_id : \, monasca_grafana_control_plane_project@{monasca_control_plane_project} @monasca_control_plane_project_domain_id : \\\\\\\\\\\\\\\\, monasca_grafana_control_plane_project@{monasca_control_plane_project} @monasca_control_plane_project_domain_id \\, monasca_grafana_control_plane_project@{monasca_control_plane_project} @monasca_control_plane_project_domain_id \\\\, monasca_grafana_control_plane_project@{monasca_control_plane_project} @monasca_control_plane_project_domain_id \\\\\\\\, monasca_grafana_control_plane_project@{monasca_control_plane_project} @monasca_control_plane_project_domain_id \\\\\\\\\\, monasca_grafana_control_plane_project@{monasca_control_plane_project} \\\\",CWE-502,1,"---
- name: Wait for Monasca Grafana to load
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/login""
    status_code: 200
  register: result
  until: result.get('status') == 200
  retries: 10
  delay: 2
  run_once: true

- name: Define Monasca Grafana control plane organisation name
  set_fact:
    monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_name }}""

- name: List Monasca Grafana organisations
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_orgs

- name: Create default control plane organisation if it doesn't exist
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body_format: json
    body:
      name: '{{ monasca_grafana_control_plane_org }}'
    force_basic_auth: true
  when: monasca_grafana_control_plane_org not in monasca_grafana_orgs.json|map(attribute='name')|unique

- name: Lookup Monasca Grafana control plane organisation ID
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/name/{{ monasca_grafana_control_plane_org }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_conf_org

- name: Add {{ monasca_grafana_admin_username }} user to control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/{{ monasca_grafana_conf_org.json.id }}/users'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
    force_basic_auth: true
    body_format: json
    status_code: 200, 409
  register: monasca_grafana_add_user_response
  run_once: True
  changed_when: monasca_grafana_add_user_response.status == 200
  failed_when: monasca_grafana_add_user_response.status not in [200, 409] or
               monasca_grafana_add_user_response.status == 409 and (""User is already"" not in  monasca_grafana_add_user_response.json.message|default(""""))

- name: Switch Monasca Grafana to the control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/user/using/{{ monasca_grafana_conf_org.json.id }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    force_basic_auth: true

- name: Enable Monasca Grafana datasource for control plane organisation
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/datasources""
    method: POST
    user: ""{{ monasca_grafana_admin_username }}""
    password: ""{{ monasca_grafana_admin_password }}""
    body: ""{{ item.value.data | to_json }}""
    body_format: json
    force_basic_auth: true
    status_code: 200, 409
  register: monasca_grafana_datasource_response
  run_once: True
  changed_when: monasca_grafana_datasource_response.status == 200
  failed_when: monasca_grafana_datasource_response.status not in [200, 409] or
               monasca_grafana_datasource_response.status == 409 and (""Data source with same name already exists"" not in  monasca_grafana_datasource_response.json.message|default(""""))
  with_dict: ""{{ monasca_grafana_data_sources }}""
  when: item.value.enabled | bool",0,https://www.cybercom.net/blog/2022/the-top-10-misconfigurations-to-watch-in-ansible-playbooks/,"CWE-121, CWE-122, CWE-129, CWE-22, CWE-25, CWE-89",0
"- name: Restart keystone-ssh container
    service_name: ""keystone-ssh""
- name: Restart keystone-fernet container
    service_name: ""keystone-fernet""
- name: Restart keystone container
    service_name: ""keystone""",1,"Restart keystone container, Restart keystone-fernet container, Restart keystone-ssh container, keystone, keystone-fernet, service_name","CWE-119, CWE-20, CWE-22",0,"---
- name: Restart keystone container
  vars:
    service_name: ""keystone""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_conf: ""{{ keystone_confs.results|selectattr('item.key', 'equalto', service_name)|first  }}""
    policy_json: ""{{ keystone_policy_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_conf.changed | bool
      or keystone_domains.changed | bool
      or policy_json.changed | bool
      or keystone_wsgi.changed | bool
      or keystone_paste_ini.changed | bool
      or keystone_container.changed | bool

- name: Restart keystone-fernet container
  vars:
    service_name: ""keystone-fernet""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_conf: ""{{ keystone_confs.results|selectattr('item.key', 'equalto', service_name)|first  }}""
    policy_json: ""{{ keystone_policy_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_fernet_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_conf.changed | bool
      or policy_json.changed | bool
      or keystone_fernet_confs.changed | bool
      or keystone_fernet_container.changed | bool

- name: Restart keystone-ssh container
  vars:
    service_name: ""keystone-ssh""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_ssh_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_ssh_confs.changed | bool
      or keystone_ssh_container.changed | bool",0,"- config_json.changed | bool\\\\n      or keystone_conf.changed | bool\\\\n      or keystone_domains.changed | bool\\\\n      or policy_json.changed | bool\\\\n      or keystone_wsgi.changed | bool\\\\n      or keystone_paste_ini.changed | bool\\\\n      or keystone_container.changed | bool, - name: Restart keystone container\n  vars:\n    service_name: \, - service.enabled | bool\\\\n    - config_json.changed | bool\\\\n    or keystone_conf.changed | bool\\\\n    or keystone_domains.changed | bool\\\\n    or policy_json.changed | bool\\\\n    or keystone_wsgi.changed | bool\\\\n    or keystone_paste_ini.changed | bool\\\\n    or keystone_container.changed | bool, or keystone_conf.changed | bool\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    or keystone_domains.changed | bool\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    or policy_json.changed | bool\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    or keystone_wsgi.changed | bool\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    or keystone_paste_ini.changed | bool\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    or keystone_container.changed | bool, or keystone_conf.changed | bool\\\\\\\\\\\\\\\\\\\\n    or keystone_domains.changed | bool\\\\\\\\\\\\\\\\\\\\n    or policy_json.changed | bool\\\\\\\\\\\\\\\\\\\\n    or keystone_wsgi.changed | bool\\\\\\\\\\\\\\\\\\\\n    or keystone_paste_ini.changed | bool\\\\\\\\\\\\\\\\\\\\n    or keystone_container.changed | bool, or keystone_conf.changed | bool\\\\\\\\n    or keystone_domains.changed | bool\\\\\\\\n    or policy_json.changed | bool\\\\\\\\n    or keystone_wsgi.changed | bool\\\\\\\\n    or keystone_paste_ini.changed | bool\\\\\\\\n    or keystone_container.changed | bool, when:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - inventory_hostname in groups[service.group]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - service.enabled | bool\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - config_json.changed | bool\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    or keystone_conf.changed | bool\\\\, when:\\\\\\\\n    - inventory_hostname in groups[service.group]\\\\\\\\n    - service.enabled | bool\\\\\\\\n    - config_json.changed | bool\\\\\\\\n    or keystone_conf.changed | bool\\\\\\\\n    or policy_json.changed | bool\\\\\\\\n    or keystone_ssh_confs.changed | bool\\\\\\\\n    or keystone_ssh_container.changed | bool, when:\\\\n    - inventory_hostname in groups[service.group]\\\\n    - service.enabled | bool\\\\n    - config_json.changed | bool\\\\n    or keystone_conf.changed | bool\\\\n    or policy_json.changed | bool\\\\n    or keystone_fernet_confs.changed | bool\\\\n    or keystone_fernet_container.changed | bool, when:\\n    - inventory_hostname in groups[service.group]\\n    - service.enabled | bool\\n    - config_json.changed | bool\\n    or keystone_conf.changed | bool\\n    or keystone_domains.changed | bool\\n    or policy_json.changed | bool\\n    or keystone_wsgi.changed | bool\\n    or keystone_paste_ini.changed | bool\\n    or keystone_container.changed | bool","CWE-20, CWE-22",0
"True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-server'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      ( inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      and not enable_nova_fake | bool
      ) or
      ( inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      and enable_nova_fake | bool
      )
      True if orchestration_engine == 'KUBERNETES' else
      ( inventory_hostname in groups['compute'] or
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      and not enable_nova_fake | bool
      ) or
      ( inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      and enable_nova_fake | bool
      )
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-dhcp-agent'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['neutron-l3-agent']
      or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-lbaas-agent'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['neutron-metadata-agent']
      or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-vpnaas-agent'] }}""",1,"enable_manila | bool, enable_neutron_dvr | bool, enable_nova_fake | bool, inventory_hostname in groups['neutron-dhcp-agent'], inventory_hostname in groups['neutron-l3-agent'], inventory_hostname in groups['neutron-metadata-agent'], inventory_hostname in groups['neutron-vpnaas-agent']",CWE-119,0,"neutron_services:
  openvswitch-db-server:
    container_name: ""openvswitch_db""
    image: ""{{ openvswitch_db_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    volumes:
      - ""{{ node_config_directory }}/openvswitch-db-server/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
      - ""openvswitch_db:/var/lib/openvswitch/""
  openvswitch-vswitchd:
    container_name: ""openvswitch_vswitchd""
    image: ""{{ openvswitch_vswitchd_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    privileged: True
    volumes:
      - ""{{ node_config_directory }}/openvswitch-vswitchd/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-server:
    container_name: ""neutron_server""
    image: ""{{ neutron_server_image_full }}""
    enabled: true
    group: ""neutron-server""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-server'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-server/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""kolla_logs:/var/log/kolla/""
  neutron-openvswitch-agent:
    container_name: ""neutron_openvswitch_agent""
    image: ""{{ neutron_openvswitch_agent_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    privileged: True
    host_in_groups: >-
      {{
        ( inventory_hostname in groups['compute']
          or (enable_manila | bool and inventory_hostname in groups['manila-share'])
          or inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          or inventory_hostname in groups['neutron-vpnaas-agent']
          and not enable_nova_fake | bool
        ) or
        ( inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          or inventory_hostname in groups['neutron-vpnaas-agent']
          and enable_nova_fake | bool
        )
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-openvswitch-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-sfc-agent:
    container_name: ""neutron_sfc_agent""
    image: ""{{ neutron_sfc_agent_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'sfc' }}""
    privileged: True
    host_in_groups: >-
      {{
        ( inventory_hostname in groups['compute']
          or (enable_manila | bool and inventory_hostname in groups['manila-share'])
          or inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          and not enable_nova_fake | bool
        ) or
        ( inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          and enable_nova_fake | bool
        )
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-sfc-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-linuxbridge-agent:
    container_name: ""neutron_linuxbridge_agent""
    image: ""{{  neutron_linuxbridge_agent_image_full }}""
    privileged: True
    enabled: ""{{ neutron_plugin_agent == 'linuxbridge' }}""
    environment:
      KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""
      NEUTRON_BRIDGE: ""br-ex""
      NEUTRON_INTERFACE: ""{{ neutron_external_interface }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-linuxbridge-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-dhcp-agent:
    container_name: ""neutron_dhcp_agent""
    image: ""{{ neutron_dhcp_agent_image_full }}""
    privileged: True
    enabled: True
    group: ""neutron-dhcp-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-dhcp-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-dhcp-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run/:/run/:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-l3-agent:
    container_name: ""neutron_l3_agent""
    image: ""{{ neutron_l3_agent_image_full }}""
    privileged: True
    enabled: ""{{ not enable_neutron_vpnaas | bool }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['neutron-l3-agent']
        or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-l3-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-lbaas-agent:
    container_name: ""neutron_lbaas_agent""
    image: ""{{ neutron_lbaas_agent_image_full }}""
    privileged: True
    enabled: ""{{ enable_neutron_lbaas | bool }}""
    group: ""neutron-lbaas-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-lbaas-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-lbaas-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-metadata-agent:
    container_name: ""neutron_metadata_agent""
    image: ""{{ neutron_metadata_agent_image_full }}""
    privileged: True
    enabled: true
    host_in_groups: >-
      {{
        inventory_hostname in groups['neutron-metadata-agent']
        or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-metadata-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-vpnaas-agent:
    container_name: ""neutron_vpnaas_agent""
    image: ""{{ neutron_vpnaas_agent_image_full }}""
    privileged: True
    enabled: ""{{ enable_neutron_vpnaas | bool }}""
    group: ""neutron-vpnaas-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-vpnaas-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-vpnaas-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""/lib/modules:/lib/modules:ro""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""",0,"neutron_services:neutron-openvswitch-agent:image:neutron_openvswitch_agent_image_full, neutron_services:neutron-openvswitch-agent:volumes:/etc/localtime:/etc/localtime:ro, neutron_services:neutron-openvswitch-agent:volumes:/lib/modules:/lib/modules:ro, neutron_services:neutron-openvswitch-agent:volumes:/run:/run:shared, neutron_services:neutron-openvswitch-agent:volumes:kolla_logs:/var/log/kolla/, neutron_services:neutron-openvswitch-agent:volumes:neutron-openvswitch-agent/:/etc/neutron/ro, neutron_services:neutron-openvswitch-agent:volumes:neutron-openvswitch-agent/:/var/lib/neutron/ro, neutron_services:neutron-openvswitch-agent:volumes:neutron-openvswitch-agent/:/var/lib/openvswitch/ro, neutron_services:neutron-server:container_name:neutron_server","CWE-295, CWE-306",0
"when: inventory_hostname in groups['glance-api']
  when: inventory_hostname in groups['glance-api']",1,"when: inventory_hostname in groups['glance-api']\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: inventory_hostname in groups['glance-api']\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nwhen: inventory_hostname in groups['glance-api'], when: inventory_hostname in groups['glance-api']\\\\\\\\\\\\\\\\nwhen: inventory_hostname in groups['glance-api'], when: inventory_hostname in groups['glance-api']\\\\\\nwhen: inventory_hostname in groups['glance-api'], when: inventory_hostname in groups['glance-api']\\\nwhen: inventory_hostname in groups['glance-api'], when: inventory_hostname in groups['glance-api']\\nwhen: inventory_hostname in groups['glance-api'], when: inventory_hostname in groups['glance-api']\nwhen: inventory_hostname in groups['glance-api']",CWE-703,0,"- include: ceph.yml
  when: enable_ceph | bool

  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']",0,"when: inventory_hostname in groups['glance-api'] or\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: inventory_hostname in groups['glance-api'] or\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        inventory_hostname in groups['glance-registry']\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when: inventory_hostname in groups['glance-api'] or\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        inventory_hostname in groups['glance-registry'], when: inventory_hostname in groups['glance-api'] or\\\\\\\\\\\\\\\\\\\\n        inventory_hostname in groups['glance-registry']\\\\\\\\\\\\\\\\\\\\n  when: inventory_hostname in groups['glance-api'] or\\\\\\\\\\\\\\\\\\\\n        inventory_hostname in groups['glance-registry'], when: inventory_hostname in groups['glance-api'] or\\\\\\\\n        inventory_hostname in groups['glance-registry']\\\\\\\\n  when: inventory_hostname in groups['glance-api'] or\\\\\\\\n        inventory_hostname in groups['glance-registry'], when: inventory_hostname in groups['glance-api'] or\\\\n        inventory_hostname in groups['glance-registry']\\\\n  when: inventory_hostname in groups['glance-api'] or\\\\n        inventory_hostname in groups['glance-registry'], when: inventory_hostname in groups['glance-api'] or\\n        inventory_hostname in groups['glance-registry']\\n  when: inventory_hostname in groups['glance-api'] or\\n        inventory_hostname in groups['glance-registry'], when: inventory_hostname in groups['glance-api'] or\n        inventory_hostname in groups['glance-registry']\n  when: inventory_hostname in groups['glance-api'] or\n        inventory_hostname in groups['glance-registry']",CWE-1046,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""",1,"cfn, dest: \, engine, heat-engine, with_items, with_items:, with_items:\n - \","CWE-1044, CWE-1045, CWE-1046, CWE-1047, CWE-1048, CWE-1049, CWE-1050, CWE-1051, CWE-1052, CWE-1253",1,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",0,"\\\\n, dest: \\n, local_action: stat path=\, name: Copying over existing policy.json\\\\n      \\\\n      template: {{ node_config_directory }}/nova/policy.json, nova_policy.stat.exists, register: nova_policy, src: {{ node_config_directory }}/nova/policy.json\\n      \\n      when:\\n        node_custom_config\\n      \\n    \\n, src: {{ node_custom_config }}/nova/policy.json, template: {{ node_custom_config }}/nova/policy.json\n      \n      when:\n        nova_policy.stat.exists\n, when: node_config_directory","CWE-1028: Resource Management Errors, CWE-1032: Faulty Resource Acquisition or Release, CWE-1034: Broken Asymmetric Cryptography, CWE-1057: Incorrect Authentication, CWE-19: Integer Overflow, CWE-200: Information Exposure, CWE-732: Insertion of Sensitive Information into a Different Key Space, CWE-89: SQL Injection, CWE-93: Extraneous Functionality, CWE-94: Improper Control Flow",0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""",1,"dest: \, heat-api, heat-api-cfn, heat-engine, node_config_directory, with_items","CWE-19, CWE-94",1,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",0,"dest: \\\, local_action: stat path=\, when: heat_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: heat_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  template:, when: heat_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  template:, when: heat_policy.stat.exists\\\\\\\\\\\\\\\\\\\\n  template:, when: heat_policy.stat.exists\\\\\\\\n  template:, when: heat_policy.stat.exists\\\\n  template:, when: heat_policy.stat.exists\\n  template:, when: heat_policy.stat.exists\n","CWE-119, CWE-129, CWE-732",0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""",1,"dest: \, with_items: \, with_items: \\, with_items: \\\\, with_items: \\\\\\\, with_items: \\\\\\\\\\\\\\\\, with_items: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-257,0,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",0,"dest: {{ node_config_directory }}/senlin/policy.json, local_action: stat path=, src: {{ node_config_directory }}/senlin/policy.json, when: not senlin_policy.stat.exists, when: senlin_policy.stat.exists","CWE-125, CWE-126",0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""",1,"dest: \, dest: \\\\\\\\\\n  with_items: \\\\, dest: \\\\\\n  with_items:\\\\n    - \\\, dest: {{ node_config_directory }}/{{ item }}/policy.json \\\n  with_items:\\n    - \\, with_items: \\, with_items: \\\\, with_items:\\\\\\\\\\\\\\\\\\\\\n    - \\\\, with_items:\\\\n  - \\\\, with_items:\\n  - \\\\\n    - \\, with_items:\n  - \",CWE-601,,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",0,"cinder_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, cinder_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- name: Copying over existing policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  template: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, cinder_policy.stat.exists\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n- name: Copying over existing policy.json\\\\\\\\\\\\\\\\\\\\n  template: \\\\\\\\\\\\\\\\\\\\, cinder_policy.stat.exists\\\\\\\\n\\\\\\\\n- name: Copying over existing policy.json\\\\\\\\n  template: \\\\\\\\, cinder_policy.stat.exists\\\\n\\\\n- name: Copying over existing policy.json\\\\n  template: \\\\, register: cinder_policy\\n\\\\n- name: Copying over existing policy.json\\n  template: \\, template:\\n  src: \\, template:\n  src: \",CWE-732,0
- include: deploy.yml,1,"playbook/roles/test_role/tasks/main.yml\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, playbook/roles/test_role/tasks/main.yml\\\\\\\\\\\\n- template: src: test.j2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\, playbook/roles/test_role/tasks/main.yml\\\\n- blockinfile: content: \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\n  \\\\\\\\, playbook/roles/test_role/tasks/main.yml\\\\n- copy: content: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  \\\\\\\\\\\\\, playbook/roles/test_role/tasks/main.yml\\n- blockinfile: content: \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \, playbook/roles/test_role/tasks/main.yml\\n- copy: src: \\n\\n  \\n\\n  \\n\\n  \\n  \\n  \\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n , playbook/roles/test_role/templates/test.yml\n- user: username\n- user: username\n  password: 12345\n  become: yes\n- user: username\n  password: 12345\n  become: yes\n- user: username\n  password: 12345\n  become: yes","CWE-221, CWE-257",0,"- name: Ensuring the containers up
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false

- include: config.yml

- name: Check the configs
  command: docker exec telegraf /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results

# NOTE(jeffrey4l): when config_strategy == 'COPY_ALWAYS'
# and container env['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE',
# just remove the container and start again
- name: Containers config strategy
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_env""
  register: container_envs

- name: Remove the containers
  kolla_docker:
    name: ""telegraf""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE""

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""telegraf""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'",0,"command: docker exec telegraf /usr/local/bin/kolla_set_configs --check, name: Check the configs, name: Restart containers, name: \, when: config_strategy == 'COPY_ALWAYS'","CWE-120, CWE-22, CWE-89",0
"name: ""{{ watcher_database_user }}""",1,"name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\u0022\\\\\\\\\\\\\u007b\\\\\\\\\\\\\u0022\\\\\\\\\\\\\u0020watcher_database_user\\\\\\\\\\\\\u0022\\\\\\\\\\\\\u0022\\\\\\\\\\\\\u0027, name: \\\\\\\\\\\\\\\\\\\\\u0022\\\\\\\\\\\\\\\\\u007b\\\\\\\\\\\\\\\\\u0022\\\\\\\\\\\\\\\\u0020watcher_database_user\\\\\\\\\\\\\\\\\u0022\\\\\\\\\\\\\\\\u0022\\\\\\\\\\\\\\\\u0027, name: \\\\\\\\\\\\\u0022\\\\u007b\\\\u0022\\\\u0020watcher_database_user\\\\u0022\\\\u0022\\\\u0027, name: \\\\\\\\u0022\\\\u007b\\\\u0022\\\\u0020watcher_database_user\\\\u0022\\\\u0022\\\\u0027, name: \\\\u0022\\\\u007b\\\\u0022\\u0020watcher_database_user\\u0022\\u0022\\\\u0027, name: \\u0022\\u007b\\u0022\\u0020watcher_database_user\\u0022\\u0022\\u0027, name: \u0022\u007b\u0022\u0020watcher_database_user\u0022\u0022\u0027, password: \\u0022\\u0022\\u0027","CWE-1024, CWE-1025",,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"append_privs: \, append_privs: \\\\, host: \\, kolla_toolbox:\r\n    module_name: mysql_user\r\n    module_args:\r\n      login_host: \, login_user: \\, login_user: \\\\, name: \, name: \\, password: \\\, wather_database_name \\\","362, 363",0
"name: ""{{ watcher_database_user }}""",1,"\\\, name: \, name: \\, name: \\ \, name: \\\\\\, name: \\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-20, CWE-862",1,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"append_privs:, host:, login_host:, login_password:, login_port:, login_user:, name:, password:, priv:","CWE-190, CWE-259, CWE-307, CWE-312, CWE-314, CWE-320, CWE-321, CWE-325",0
"name: ""{{ watcher_database_user }}""",1,"\, \\, \\\r\\n  \, \\\r\n    \, name: \, name: \\, name: \\\\",CWE-732,,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"login_host: \, login_host: \\, login_host: \\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_password: \\, login_password: \\\\, login_password: \\\\\\\\\\\\\\\\, login_password: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_user: \\\\, login_user: \\\\\\\\","CWE-306, CWE-307",0
"name: ""{{ trove_database_user }}""",1,"\, \\, \\\, name: \, }",CWE-1002,1,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"login_host: \\\\\\\\\\\\\\\\\\\\, login_host: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_password: \, login_password: \\, login_password: \\\\\, login_password: \\\\\\, login_user: \\\\\\\\\, module_args:","CWE-287, CWE-798",0
"name: ""{{ trove_database_user }}""",1,"name: \, name: \\, name: \\\\, name: \\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-200, CWE-362, CWE-79",,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"append_privs: \, host: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_host: \\, login_password: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_port: \\\\, login_user: \\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, password: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, priv: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: database.changed","CWE-787, CWE-862",0
"name: ""{{ trove_database_user }}""",1,"name: \, name: \\, name: \\\\, name: \\\\\\\\\, name: \\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-190, CWE-327",1,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"login_host: \\, login_host: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_password: \, login_password: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_port: \\\\, login_port: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, login_user: \\\\\\\\, login_user: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\, name: \\\\\\\\\\\\\\\\\\\\\\\",CWE-306,0
"update_cache: yes
    update_cache: yes",1,update_cache: yes,"CWE-120, CWE-20, CWE-200, CWE-732, CWE-918",,"- name: Update yum cache
  yum:
    update_cache: yes
  become: True
  when: ansible_os_family == 'RedHat'

# Upgrading docker engine may cause containers to stop. Take a snapshot of the
# running containers prior to a potential upgrade of Docker.

- name: Check which containers are running
  command: docker ps -f 'status=running' -q
  become: true
  # If Docker is not installed this command may exit non-zero.
  failed_when: false
  changed_when: false
  register: running_containers

  register: apt_install_result
  register: yum_install_result

# If any packages were updated, and any containers were running, wait for the
# daemon to come up and start all previously running containers.

- block:
    - name: Wait for Docker to start
      command: docker info
      become: true
      changed_when: false
      register: result
      until: result is success
      retries: 6
      delay: 10

    - name: Ensure containers are running after Docker upgrade
      command: ""docker start {{ running_containers.stdout }}""
      become: true
  when:
    - install_result is changed
    - running_containers.rc == 0
    - running_containers.stdout != ''
  vars:
    install_result: ""{{ yum_install_result if ansible_os_family == 'RedHat' else apt_install_result }}""
  when:
    - ansible_distribution|lower == ""ubuntu""
    - item != """"
  when:
    - ansible_os_family == 'RedHat'
    - item != """"",0,"    - name: Check which containers are running\\\\n      command: docker ps -f'status=running' -q\\\\n      become: true\\\\n      # If Docker is not installed this command may exit non-zero.\\\\n      failed_when: false\\\\n      changed_when: false\\\\n      register: running_containers\\\\n\\\\n\\\\n# If any packages were updated, and any containers were running, wait for the\\\\n# daemon to come up and start all previously running containers.\\\\n\\\\n- block:\\\\n    - name: Wait for Docker to start\\\\n      command: docker info\\\\n      become: true\\\\n      changed_when: false\\\\n      register: result\\\\,     - name: Check which containers are running\\n      command: docker ps -f'status=running' -q\\n      become: true\\n      # If Docker is not installed this command may exit non-zero.\\n      failed_when: false\\n      changed_when: false\\n      register: running_containers\\n\\n\\n# If any packages were updated, and any containers were running, wait for the\\n# daemon to come up and start all previously running containers.\\n\\n- block:\\n    - name: Wait for Docker to start\\n      command: docker info\\n      become: true\\n      changed_when: false\\n      register: result\\n,  - name: Check which containers are running\n  command: docker ps -f'status=running' -q\n  become: true\n  # If Docker is not installed this command may exit non-zero.\n  failed_when: false\n  changed_when: false\n  register: running_containers\n\n\n# If any packages were updated, and any containers were running, wait for the\n# daemon to come up and start all previously running containers.\n\n- block:\n    - name: Wait for Docker to start\n      command: docker info\n      become: true\n      changed_when: false\n      register: result\n      until: result is success\n      retries: 6\n      delay: 10, - item!= ''\\\\n  when:\\\\n    - ansible_os_family == 'RedHat', - item!= ''\\\\n  when:\\n    - ansible_os_family == 'RedHat', - name: Wait for Docker to start\\\\\\\\n      command: docker info\\\\\\\\n      become: true\\\\\\\\n      changed_when: false\\\\\\\\n      register: result\\\\\\\\n      until: result is success\\\\\\\\n      retries: 6\\\\\\\\n      delay: 10\\\\\\\\n\\\\\\\\n\\\\\\\\n    - name: Ensure containers are running after Docker upgrade\\\\\\\\n      command: \\, when:\\\\n    - ansible_os_family == 'RedHat', when:\\\\n    - install_result is changed\\\\n    - running_containers.rc == 0\\\\n    - running_containers.stdout!= ''\\\\n  vars:\\\\n    install_result:, when:\\n    - ansible_os_family == 'RedHat', when:\\n    - install_result is changed\\n    - running_containers.rc == 0\\n    - running_containers.stdout!= ''\\n  vars:\\n    install_result:",CWE-1031,0
"vars:
    monasca_orgs_body:
      name: '{{ monasca_grafana_control_plane_org }}'
      body: ""{{ monasca_orgs_body | to_json }}""
  vars:
    monasca_user_body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
      body: ""{{ monasca_user_body | to_json }}""",1,"monasca_orgs_body: name: '{{ monasca_grafana_control_plane_org }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      body: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, monasca_orgs_body: name: '{{ monasca_grafana_control_plane_org }}'\\\\\\\\n      body: \\\\\\\\\\\\, monasca_orgs_body: name: '{{ monasca_grafana_control_plane_org }}'\\n      body: \\, monasca_orgs_body: name: '{{ monasca_grafana_control_plane_org }}'\n      body: \, monasca_user_body: loginOrEmail: '{{ monasca_grafana_admin_username }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, monasca_user_body: loginOrEmail: '{{ monasca_grafana_admin_username }}'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, monasca_user_body: loginOrEmail: '{{ monasca_grafana_admin_username }}'\\\\\\\\\\\\\\\\n      role: Admin\\\\\\\\\\\\\\\\n      body: \\\\\\\\\\\\\\\\\\\\\\\\\\\\, monasca_user_body: loginOrEmail: '{{ monasca_grafana_admin_username }}'\\\\n      role: Admin\\\\n      body: \\\\, monasca_user_body: loginOrEmail: '{{ monasca_grafana_admin_username }}'\\n      role: Admin\\n      body: \\","CWE-256, CWE-269, CWE-352",1,"---
- name: Wait for Monasca Grafana to load
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/login""
    status_code: 200
  register: result
  until: result.get('status') == 200
  retries: 10
  delay: 2
  run_once: true

- name: Define Monasca Grafana control plane organisation name
  set_fact:
    monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_name }}""

- name: List Monasca Grafana organisations
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_orgs

- name: Create default control plane organisation if it doesn't exist
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body_format: json
    body:
      name: '{{ monasca_grafana_control_plane_org }}'
    force_basic_auth: true
  when: monasca_grafana_control_plane_org not in monasca_grafana_orgs.json|map(attribute='name')|unique

- name: Lookup Monasca Grafana control plane organisation ID
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/name/{{ monasca_grafana_control_plane_org }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_conf_org

- name: Add {{ monasca_grafana_admin_username }} user to control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/{{ monasca_grafana_conf_org.json.id }}/users'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
    force_basic_auth: true
    body_format: json
    status_code: 200, 409
  register: monasca_grafana_add_user_response
  run_once: True
  changed_when: monasca_grafana_add_user_response.status == 200
  failed_when: monasca_grafana_add_user_response.status not in [200, 409] or
               monasca_grafana_add_user_response.status == 409 and (""User is already"" not in  monasca_grafana_add_user_response.json.message|default(""""))

- name: Switch Monasca Grafana to the control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/user/using/{{ monasca_grafana_conf_org.json.id }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    force_basic_auth: true

- name: Enable Monasca Grafana datasource for control plane organisation
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/datasources""
    method: POST
    user: ""{{ monasca_grafana_admin_username }}""
    password: ""{{ monasca_grafana_admin_password }}""
    body: ""{{ item.value.data | to_json }}""
    body_format: json
    force_basic_auth: true
    status_code: 200, 409
  register: monasca_grafana_datasource_response
  run_once: True
  changed_when: monasca_grafana_datasource_response.status == 200
  failed_when: monasca_grafana_datasource_response.status not in [200, 409] or
               monasca_grafana_datasource_response.status == 409 and (""Data source with same name already exists"" not in  monasca_grafana_datasource_response.json.message|default(""""))
  with_dict: ""{{ monasca_grafana_data_sources }}""
  when: item.value.enabled | bool",0,"body, body_format, force_basic_auth, status_code","CWE-22, CWE-275",0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""",1,"dest: \, dest: \\, dest: \\\\, dest: \\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-601,0,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",0,"action: stat\\\\n, dest: {{ node_config_directory }}/nova/policy.json\\n, local_action: stat path= {{ node_custom_config }}/nova/policy.json\\\\n, local_action: template\\\\\\\\n, register: nova_policy\\\\\\\\n, stat path= {{ node_custom_config }}/nova/policy.json\\n, template\\\\\\\\\\\\\\\\\\\\n, when:\\n        node_custom_config\\n, when:\n        nova_policy.stat.exists\n, when:{{\",CWE-256,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""",1,with_items,"CWE-1027, CWE-117, CWE-384, CWE-480, CWE-502, CWE-732",,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",0,"local_action: stat path=\, register: heat_policy\\, stat.exists\\\\, stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\, stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, template: src=\\\, when: heat_policy.stat.exists\\\\\\, when: heat_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: heat_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-601,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""",1,"with_items: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items: \\\\\\\\\\\\\\\\\\\\\n    - \\\\\\\\\\\\\\\\, with_items: \\\\\\\\\n    - \\\\\\, with_items: \\\n    - \\, with_items:\\\\n    - \\, with_items:\\n    - \, with_items:\n    - \","CWE-22: Improper Limitation of a Resource Within a Name or Identifier, CWE-502: Browse Search Location (aka. Search Path or Path Traversal)",0,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",0,"- name: Check if policies shall be overwritten\n\tlocal_action: stat path=, name: Copying over existing policy.json, node_config_directory, node_custom_config, path, path=, senlin_policy, senlin_policy.stat.exists, when:",CWE-1205,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""",1,"dest: \, dest: \\, dest: \\\\, dest: \\\\\\\\, dest: \\\\\\\\\\\\\\\\, with_items:\\n    - \\, with_items:\n    - \",CWE-120,,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",0,"Copyin, cinder_policy.stat.exists, dest: \\\\\\, local_action: stat path=\, name: Copying over existing policy.json, register: cinder_policy, src: \\\, template:, when:, when: cinder_policy.stat.exists",CWE-778,0
"name: ""{{ octavia_database_user }}""",1,"ansible_facts['all']['user'] \\\\, ansible_hostname \\\\\\, ansible_hostname \\\\\\\\, host_var = \\, inventory_hostname \\\, inventory_hostname \\\\\\, name: \, name: \\, }",CWE-779,0,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"kolla_toolbox:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, kolla_toolbox:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    module_name: mysql_user\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    module_args:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, kolla_toolbox:\\\\\\\\\\\\\\\\\\\\\\\\n    module_name: mysql_user\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    module_args:\\\\\\\\\\\\\\\\\\\\\\\\n      name: \\\\\\\\\\\\\\\\\\\\\\\\, kolla_toolbox:\\\\\\\\\\n    module_name: mysql_user\\\\\\\\\\\\n    module_args:\\\\\\\\\\n      name: \\\\\\\\\\, kolla_toolbox:\\\\n    module_name: mysql_user\\\\n    module_args:\\\\n      name: \\\, kolla_toolbox:\\n    module_name: mysql_db\\n    module_args:\\n      name: \\, kolla_toolbox:\\n    module_name: mysql_user\\n    module_args:\\n      login_host: \, kolla_toolbox:\n    module_name: mysql_db\n    module_args:\n      login_host: \","CWE-200, CWE-319",0
"name: ""{{ octavia_database_user }}""",1,"name: \, name: \\, name: \\\\, name: \\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-119, CWE-129, CWE-190, CWE-256, CWE-264, CWE-352, CWE-863",,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"append_privs: \\\\\\, host: %\\\\n  when: database.changed\\n  kolla_toolbox:, login_host: \, login_password: \\\\, login_password: \\\\\\\\\\\\\\\\\\\\\\\\, login_port: \\, login_user: \\\, name: \\\\\\\\\\\\\\, priv: \\\\\\\\\\\\\\\\\\\\\\\\, when: database.changed\n\n  kolla_toolbox:",CWE-257,0
"name: ""{{ octavia_database_user }}""",1,"name: \, name: \\\\, name: \\\\\\\\\, name: \\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-601, CWE-77",0,"kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed",0,"login_password: \\\\\\\, login_user: \\\\\\\\, module_args: append_privs: \\\\\\\, module_args: host: \\\\, module_args: name: \, module_args: name: \\\\, module_args: name: \\\\\\\\, module_args: password: \\\\, module_args: priv: \\\\, name: \\","CWE-284, CWE-306, CWE-384",0
,1,my_code,"111, 123, 333, 444, 456, 555, 666, 789, 901",,"---
- name: Restart rabbitmq container
  vars:
    service_name: ""rabbitmq""
    service: ""{{ rabbitmq_services[service_name] }}""
    config_json: ""{{ rabbitmq_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    rabbitmq_container: ""{{ check_rabbitmq_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes }}""
  when:
    - action != ""config""
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or rabbitmq_confs.changed | bool
      or rabbitmq_container.changed | bool",0,"action: \, action: \\, config_json.changed | bool, rabbitmq_confs.changed | bool, rabbitmq_container.changed | bool, service_name: \, service_name: \\","CWE-1201, CWE-1205, CWE-1234, CWE-732",0
"# NOTE(mgoddard): Currently (just prior to Stein release), sending SIGHUP to
# nova compute services leaves them in a broken state in which they cannot
# start new instances. The following error is seen in the logs:
# ""In shutdown, no new events can be scheduled""
# To work around this we restart the nova-compute services.
# Speaking to the nova team, this seems to be an issue in oslo.service,
# with a fix proposed here: https://review.openstack.org/#/c/641907.
# This issue also seems to affect the proxy services, which exit non-zero in
# reponse to a SIGHUP, so restart those too.
# TODO(mgoddard): Remove this workaround when this bug has been fixed.
- name: Send SIGHUP to nova services
  become: true
  command: docker exec -t {{ item.value.container_name }} kill -1 1
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - not item.key.startswith('nova-compute')
    - not item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""
- name: Restart nova compute and proxy services
  become: true
  kolla_docker:
    action: restart_container
    common_options: ""{{ docker_common_options }}""
    name: ""{{ item.value.container_name }}""
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - item.key.startswith('nova-compute')
      or item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""",1,"name: Restart nova compute and proxy services\\\\nbecome: true\\\\nkolla_docker:\\\\naction: restart_container\\\\ncommon_options: \\\, name: Restart nova compute and proxy services\\nbecome: true\\nkolla_docker:\\naction: restart_container\\ncommon_options: \\, name: Send SIGHUP to nova services\\\\nbecome: true\\\\ncommand: docker exec -t {{ item.value.container_name }} kill -1 1\\\\nwhen:\\n- inventory_hostname in groups[item.value.group]\\\\n- item.value.enabled | bool\\\\n- item.key in nova_services_require_nova_conf\\\\n- not item.key.startswith('nova-compute')\\\\n- not item.key.endswith('proxy')\\\\nwith_dict: \\\\, name: Send SIGHUP to nova services\nbecome: true\ncommand: docker exec -t {{ item.value.container_name }} kill -1 1\nwhen:\n- inventory_hostname in groups[item.value.group]\n- item.value.enabled | bool\n- item.key in nova_services_require_nova_conf\n- not item.key.startswith('nova-compute')\n- not item.key.endswith('proxy')\nwith_dict: \, when:\\\\n- inventory_hostname in groups[item.value.group]\\\\\\\\n- item.value.enabled | bool\\\\\\\\n- item.key in nova_services_require_nova_conf\\\\\\\\n- not item.key.startswith('nova-compute')\\\\\\\\n- not item.key.endswith('proxy')\\\\\\\\nwith_dict: \\\\, when:\\n- inventory_hostname in groups[item.value.group]\\\\n- item.value.enabled | bool\\\\n- item.key in nova_services_require_nova_conf\\\\n- not item.key.startswith('nova-compute')\\\\n- not item.key.endswith('proxy')\\\\nwith_dict: \\, when:\\n- inventory_hostname in groups[item.value.group]\\n- item.value.enabled | bool\\n- item.key in nova_services_require_nova_conf\\n- item.key.startswith('nova-compute')\\n\\nwith_dict: \\, with_dict: \\\\",CWE-129,0,"---
# This play calls sighup on every service to refresh upgrade levels
- name: Sighup nova-api
  command: docker exec -t nova_api kill -1 1
  when: inventory_hostname in groups['nova-api']

- name: Sighup nova-conductor
  command: docker exec -t nova_conductor kill -1 1
  when: inventory_hostname in groups['nova-conductor']

- name: Sighup nova-consoleauth
  command: docker exec -t nova_consoleauth kill -1 1
  when: inventory_hostname in groups['nova-consoleauth']

- name: Sighup nova-novncproxy
  command: docker exec -t nova_novncproxy kill -1 1
  when:
    - inventory_hostname in groups['nova-novncproxy']
    - nova_console == 'novnc'

- name: Sighup nova-scheduler
  command: docker exec -t nova_scheduler kill -1 1
  when: inventory_hostname in groups['nova-scheduler']

- name: Sighup nova-spicehtml5proxy
  command: docker exec -t nova_spicehtml5proxy kill -1 1
  when:
    - inventory_hostname in groups['nova-spicehtml5proxy']
    - nova_console == 'spice'

- name: Sighup nova-compute
  command: docker exec -t nova_compute kill -1 1
  when: inventory_hostname in groups['compute']",0,"command: docker exec -t nova_api kill -1 1, command: docker exec -t nova_compute kill -1 1, command: docker exec -t nova_conductor kill -1 1, command: docker exec -t nova_consoleauth kill -1 1, command: docker exec -t nova_novncproxy kill -1 1, command: docker exec -t nova_scheduler kill -1 1, command: docker exec -t nova_spicehtml5proxy kill -1 1, when: inventory_hostname in groups['compute'], when: inventory_hostname in groups['nova-conductor'], when: inventory_hostname in groups['nova-consoleauth']","CWE-1038: Information Exposure, CWE-732: Resource Management Resource Management",0
restart_policy: no,1,restart_policy: no,CWE-129,,"---
- name: Running trove bootstrap container
  kolla_docker:
    action: ""start_container""
    common_options: ""{{ docker_common_options }}""
    detach: False
    environment:
      KOLLA_BOOTSTRAP:
      KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""
    image: ""{{ trove_api_image_full }}""
    labels:
      BOOTSTRAP:
    name: ""bootstrap_trove""
    restart_policy: ""never""
    volumes:
      - ""{{ node_config_directory }}/trove-api/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""kolla_logs:/var/log/kolla/""
      - ""trove:/var/lib/trove/""
  run_once: True
  delegate_to: ""{{ groups['trove-api'][0] }}""",0,"KOLLA_BOOTSTRAP, KOLLA_CONFIG_STRATEGY, container_config_directory, docker_common_options, environment, kolla_docker, trove_api_image_full","CWE-190, CWE-200, CWE-264",1
"- ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/aodh.conf""",1,aodh.conf,"CWE-20, CWE-253, CWE-254, CWE-256, CWE-257, CWE-263, CWE-264, CWE-266, CWE-272, CWE-276",0,"---
- name: Ensuring config directories exist
  file:
    path: ""{{ node_config_directory }}/{{ item }}""
    state: ""directory""
    recurse: yes
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over config.json files for services
  template:
    src: ""{{ item }}.json.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/config.json""
  with_items:
    - ""aodh-api""
    - ""aodh-listener""
    - ""aodh-evaluator""
    - ""aodh-notifier""

- name: Copying over aodh.conf
  merge_configs:
    vars:
      service_name: ""{{ item }}""
    sources:
      - ""{{ role_path }}/templates/aodh.conf.j2""
      - ""{{ node_custom_config }}/global.conf""
      - ""{{ node_custom_config }}/database.conf""
      - ""{{ node_custom_config }}/messaging.conf""
      - ""{{ node_custom_config }}/aodh.conf""
      - ""{{ node_custom_config }}/aodh/{{ item }}.conf""
      - ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/{{ item }}.conf""
    dest: ""{{ node_config_directory }}/{{ item }}/aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over wsgi-aodh files for services
  template:
    src: ""wsgi-aodh.conf.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/wsgi-aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""",0,"if (self.config.load_from_config and os.path.exists(etcdir)): # Check configuration in file, oslo.service.check_service_is_running: The service {{ service_name }} is not running, but it must be running for aodh-{{ service_name }} to work, var aodh_notifier_url = {{ os_config['nova_url'] }}/aodh/notifications;","CWE-307, CWE-384",0
"cloudkitty_processor_dimensions: ""{{ default_container_dimensions }}""",1,"\, cloudkitty_processor_dimensions: \, default_container_dimensions \\\, }",CWE-22,1,"dimensions: ""{{ cloudkitty_api_dimensions }}""
    dimensions: ""{{ cloudkitty_processor_dimensions }}""
cloudkitty_processor_diensions: ""{{ default_container_dimensions }}""
cloudkitty_api_dimensions: ""{{ default_container_dimensions }}""",0,"dimensions: \, dimensions: \\, dimensions: \\\\, dimensions: \\\\\\\\\\\\, dimensions: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dimensions: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dimensions: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dimensions: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dimensions: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-732,1
"command: docker exec -t kolla_toolbox /usr/bin/ansible localhost
  command: docker exec -t kolla_toolbox /usr/bin/ansible localhost",1,command: docker exec -t kolla_toolbox /usr/bin/ansible localhost,"CWE-1209, CWE-1259",1,"- name: Creating Nova-api database
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_db
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'""
  register: database_api
  changed_when: ""{{ database_api.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""

- name: Reading json from variable
  set_fact:
    database_api_created: ""{{ (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""

- name: Creating Nova-api database user and setting permissions
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_user
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'
        password='{{ nova_api_database_password }}'
        host='%'
        priv='{{ nova_api_database_name }}.*:ALL'
        append_privs='yes'""
  register: database_api_user_create
  changed_when: ""{{ database_api_user_create.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api_user_create.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api_user_create.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""",0,"command: docker exec -t kolla_ansible /usr/bin/ansible localhost\\\\\\\\\\\\\\n -m mysql_user\\\\\\\\\\\\\\n -a \\\\\\\\\\\\\\\\, command: docker exec -t kolla_ansible /usr/bin/ansible localhost\\n -m mysql_user\\n -a \\, command: docker exec -t kolla_ansible /usr/bin/ansible localhost\\n -m mysql_user\\n -a \\\, command: docker exec -t kolla_ansible /usr/bin/ansible localhost\n -m mysql_db\n -a \, delegate_to: \\, delegate_to: \\\\, delegate_to: \\\\\\\\\\, run_once: True",CWE-22,1
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""",1,"cinder-api, cinder-backup, cinder-scheduler, cinder-volume","CWE-1033, CWE-1036, CWE-113, CWE-116, CWE-16, CWE-19, CWE-20, CWE-416, CWE-89",1,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists",0,"- name: Copying over existing policy.json\\n    template:\\n      src: \\, ansible-playbook: \n\n  - name: Check if policies shall be overwritten\n    local_action: stat path=\, node_custom_config \\\\\\\, nova/policy.json\\\\\\\\\\\\\\, nova/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, nova/policy.json\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    nova_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n----\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, nova/policy.json\\\\\\\\\\\\\\\\\\\\\\\\n  when:\\\\\\\\\\\\\\\\\\\\\\n    nova_policy.stat.exists\\\\\\\\\\\\\\\\\\\\n----\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\n\\\\\\\\\\\\\\\\n\\n----\\\\n\\n\\\\n\\\\n\\n, nova/policy.json\\\\n      dest: \\\, nova/policy.json\\\\n  when:\\n    nova_policy.stat.exists\\\\n----\\n\\n- name: Copying over existing policy.json\\\\n    template:\\\\n      src: \\\\",CWE-863,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""",1,"dest: \, with_items: - \, with_items: \\, with_items:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t- \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items:\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\t- \\\\\\\\\\\\\\\\\\\\\\\, with_items:\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\t- \\\\\\\\\, with_items:\\\\n\\\\n\\\\t- \\, with_items:\\n\\n\\t- \\\\, with_items:\\n\\t- \\, with_items:\n\t- \\","CWE-329, CWE-345",1,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists",0,"- name: Check if policies shall be overwritten\n local_action: stat path=, dest:, path={{ node_custom_config }}/heat/policy.json, register: heat_policy, src:, template:, template: src:{{ node_custom_config }}/heat/policy.json dest:{{ node_config_directory }}/heat/policy.json when:, when:, when: heat_policy.stat.exists",1046,0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""",1,"\, cinder-api\\\\n- \\, cinder-backup\\\\\\\\n- \\\\, cinder-scheduler\\\\\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\, cinder-volume\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, cinder-volume\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, cinder-volume\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\\\\\\\\\\\\, dest: \, item\\n- \\, with_items:\n- \",CWE-306,1,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists",0,"dest: \\, local_action: stat path=\, senlin_policy.stat.exists\n      \n  when: \n, stat exists, when: \\\\\\\\\\n    senlin_policy.stat.exists\\\\\\\\\\\\\\\\\n      \\\\\\\\\\\\\\\\\\\\\\\n, when: \\\\\\n    senlin_policy.stat.exists\\\\\\n      \\\\\\n, when: \\\\\n    senlin_policy.stat.exists\\\\\\\\\\\\n      \\\\\\\\\\\\\n, when: \\\\n    senlin_policy.stat.exists, when: \\n    senlin_policy.stat.exists, when: \\n    senlin_policy.stat.exists\\n      \\n","CWE-352, CWE-362, CWE-732",0
"dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""",1,"with_items:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, with_items:\\\\\\\\\\\\\\\\\\\\\\\\\\n  - \\\\\\\\\\\\\\\\\\\\\\\\, with_items:\\\\\\\\\\\\n  - \\\\\\\\\\\, with_items:\\\\n  - \\, with_items:\\n  - \, with_items:\n  - \",CWE-22,1,"- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists",0,"cinder_policy.json\\\\, cinder_policy.json\\\\n    dest: \\\\\\, cinder_policy.stat.exists, cinder_policy.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, cinder_policy.stat.exists\\\\\\\\\\n    dest: \\\\\\\\\\\\n    src: \\\\\\\\\\\\\n    dest: \\\\\\\\\\\\, cinder_policy.stat.exists\\\\n    dest: \\\\\\\\n    src: \\\\\\, local_action: stat path=\\, template: \\\\n    dest: \\\\\\, template:\\\n    src: \\\, when: cinder_policy.stat.exists\n","CWE-352, CWE-732, CWE-916",0
"- { src: ""os_aggregate_storage.j2"", dest: ""/etc/logrotate.d/{{ rsyslog_client_log_rotate_file }}"" }",1,rsyslog_client_log_rotate_file,"CWE-22, CWE-25",1,"---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Stop rsyslog
  service:
    name: ""rsyslog""
    state: ""stopped""
  failed_when: false
  tags:
    - rsyslog-client-config

- name: Rsyslog Setup
  copy:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""
  with_items:
    - { src: ""50-default.conf"", dest: ""/etc/rsyslog.d/50-default.conf"" }
  tags:
    - rsyslog-client-config

- name: Find all log files
  shell: |
    find -L '{{ rsyslog_client_log_dir }}' -type f -name '*.log'
  register: log_files
  when: >
    rsyslog_client_log_dir is defined
  tags:
    - rsyslog-client-config

- name: Set fact for combined log files list and found logs
  set_fact:
    rsyslog_client_log_files: ""{{ log_files.stdout_lines | union(rsyslog_client_log_files) }}""
  when: >
    rsyslog_client_log_files is defined and
    log_files | success
  tags:
    - rsyslog-client-config

- name: Set fact for found log files list
  set_fact:
    rsyslog_client_log_files: ""{{ log_files.stdout_lines }}""
  when: >
    rsyslog_client_log_files is not defined and
    log_files | success
  tags:
    - rsyslog-client-config

- name: Check rsyslog_client_log_files is defined
  fail:
    msg: >
      There were no log files defined in `rsyslog_client_log_files`. Please use that
      variable to set the files that you want rsyslog to begin shipping logs for. This
      variable is a list that requires the full path to all log files. You can also set
      `rsyslog_client_log_dir` which will find all ""*.log"" files using the path
      provided.
  when: >
    rsyslog_client_log_files is not defined

- name: Write rsyslog config for found log files
  template:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""
  with_items:
    - { src: ""99-rsyslog.conf.j2"", dest: ""/etc/rsyslog.d/{{ rsyslog_client_config_name }}"" }
    - { src: ""os_aggregate_storage.j2"", dest: ""/etc/logrotate.d/os_aggregate_storage"" }
    - { src: ""rsyslog.conf.j2"", dest: ""/etc/rsyslog.conf"" }
  tags:
    - rsyslog-client-config

- name: Start rsyslog
  service:
    name: ""rsyslog""
    state: ""started""
  tags:
    - rsyslog-client-config",0,"copy: \\\n    dest: \\, dest: \\\\, group: \\\\, group: \\\\\\\\, name: \\, owner: \\, owner: \\\\, rsyslog: \\, services\n  - name: \, src: \\\\",CWE-276,0
"vhost: ""{{ notify_vhost }}""",1,vhost: {{ notify_vhost }},CWE-22,1,"---
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Usage:
#  To use this common task to create to create the user and vhost if
#  needed for the messaging backend configured for Notify communications.
#  To used this common task, the variables ""notify_user"", ""notify_password""
#  and ""notify_vhost"" must be set.

- name: Ensure Notify Rabbitmq vhost
  rabbitmq_vhost:
    name: ""{{ notify_vhost }}""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  tags:
    - common-rabbitmq
  when:
    - oslomsg_notify_transport == ""rabbit""

- name: Ensure Notify Rabbitmq user
  rabbitmq_user:
    user: ""{{ notify_user }}""
    password: ""{{ notify_password }}""
    vhost: ""{{ vhost }}""
    configure_priv: "".*""
    read_priv: "".*""
    write_priv: "".*""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  no_log: true
  tags:
    - common-rabbitmq
  when:
    - oslomsge_notify_transport == ""rabbit""",0,"configure_priv:, configure_priv:  \\, password:  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, read_priv:, read_priv:  \\\\, user:  \\\\\\\\\\\\\\\\, vhost:, vhost:  \, write_priv:, write_priv:  \\\\\\\\",CWE-250,0
until: install_packages is success,1,until: install_packages is success,"CWE-119, CWE-1234, CWE-321, CWE-345, CWE-456, CWE-567, CWE-789",0,"- name: Prepare MQ/DB services
  hosts: nova_conductor
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:

    - name: Configure rabbitmq vhost/user (nova)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure rabbitmq vhost/user (nova/telemetry)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure MySQL user (nova)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-placement)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api cell0)
      include: common-tasks/mysql-db-user.yml
      run_once: yes



- name: Install nova-conductor services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_conductor""
    nova_serial: ""{{ nova_conductor_serial | default(['1', '100%']) }}""



- name: Install nova-scheduler/nova-consoleauth services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_scheduler:nova_consoleauth:!nova_conductor""
    nova_serial: ""{{ nova_scheduler_serial | default(['1', '100%']) }}""



- name: Install nova API services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_os_compute:nova_api_placement:!nova_conductor:!nova_scheduler:!nova_consoleauth""
    nova_serial: ""{{ nova_api_serial | default(['1', '100%']) }}""



- name: Install nova console/metadata services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_metadata:nova_console:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement""
    nova_serial: ""{{ nova_console_serial | default(['1', '100%']) }}""



- name: Install nova compute
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_compute:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement:!nova_api_metadata:!nova_console""
    nova_serial: ""{{ nova_compute_serial | default('20%') }}""



# These facts are set against the deployment host to ensure that
# they are fast to access. This is done in preference to setting
# them against each target as the hostvars extraction will take
# a long time if executed against a large inventory.
- name: Refresh local facts after all software changes are made
  hosts: nova_all
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: ""!all""

    # This variable contains the values of the local fact set for the cinder
    # venv tag for all hosts in the 'cinder_all' host group.
    - name: Gather software version list
      set_fact:
        nova_all_software_versions: ""{{ (groups['nova_all'] | map('extract', hostvars, ['ansible_local', 'openstack_ansible', 'nova', 'venv_tag'])) | list }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean value which is True when
    # nova_all_software_versions contains a list of defined
    # values. If they are not defined, it means that not all
    # hosts have their software deployed yet.
    - name: Set software deployed fact
      set_fact:
        nova_all_software_deployed: ""{{ (nova_all_software_versions | select('defined')) | list == nova_all_software_versions }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean when all the values in
    # nova_all_software_versions are the same and the software
    # has been deployed to all hosts in the group.
    - name: Set software updated fact
      set_fact:
        nova_all_software_updated: ""{{ ((nova_all_software_versions | unique) | length == 1) and (nova_all_software_deployed | bool) }}""
      delegate_to: localhost
      run_once: yes


# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore skip the reload
# for that service.
- name: Reload all nova services to ensure new RPC object version is used
  hosts: ""nova_all:!nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_serial | default('100%') }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service reload
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova""
        service_action: ""reloaded""
        service_negate: ""{{ ['nova-placement-api.service'] + nova_service_negate | default([]) }}""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore restart it instead.
- name: Restart the nova placement API service to ensure new RPC object version is used
  hosts: ""nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_api_serial | default(['1', '100%']) }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service restart
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova-placement-api""
        service_action: ""restarted""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Perform online database migrations
  hosts: nova_conductor
  gather_facts: no
  user: root
  tasks:
    - name: Perform online data migrations
      command: ""{{ nova_bin }}/nova-manage db online_data_migrations""
      become: yes
      become_user: ""{{ nova_system_user_name }}""
      when:
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_online_data_migrations'] | bool""
      changed_when: false
      run_once: yes
      register: data_migrations

    - name: Disable the online migrations requirement
      ini_file:
        dest: ""/etc/ansible/facts.d/openstack_ansible.fact""
        section: nova
        option: need_online_data_migrations
        value: False
      when:
        - data_migrations | succeeded",0,"name: Configure MySQL user (nova), name: Configure MySQL user (nova-api cell0), name: Configure MySQL user (nova-api), name: Configure MySQL user (nova-placement), name: Configure rabbitmq vhost/user (nova), name: Configure rabbitmq vhost/user (nova/telemetry), name: Install nova API services, name: Install nova console/metadata services, name: Prepare MQ/DB services",CWE-937,0
"domain create {{ stack_user_domain_name }} --description ""Owns users and projects created by heat""",1,"--description \, >, \, domain create {{ stack_user_domain_name }} --description \, if not {{ user_stack_name }}:\n    domain create {{ stack_user_domain_name }} --description \, if user_stack_name:\\n    domain create {{ stack_user_domain_name }} --description \\, owns users and projects created by heat",CWE-22,1,"- name: Create heat domain
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              domain create {{ stack_domain }} --description ""Owns users and projects created by heat""
  ignore_errors: true

- name: Create heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              user create --domain {{ stack_user_domain_name }} --password {{ stack_domain_admin_password }} {{ stack_domain_admin }}
  ignore_errors: true

- name: Retrieve heat domain id
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
                    domain show {{ stack_user_domain_name }} | grep -oE -m 1 ""[0-9a-f]{32}""

- name: Assign admin role to heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin",0,"openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }}\\\\\\\\\\\\\\\\n role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin, openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }}\\\\\\\\n role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin, openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }}\\\\n domain create {{ stack_domain }} --description, openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }}\\\\n role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin, openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }}\\n role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin, openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }}\\n user create --domain {{ stack_user_domain_name }} --password {{ stack_domain_admin_password }} {{ stack_domain_admin }}, openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }}\n user create --domain {{ stack_user_domain_name }} --password {{ stack_domain_admin_password }} {{ stack_domain_admin }}","CWE-213, CWE-352",0
"gnocchi_git_install_branch: b3b49c87e866475c149343fd77408bef259c5534 # HEAD of ""master"" as of 02.02.2017",1,"-b3b49c87e866475c149343fd77408bef259c5534\\n    - b3b49c87e866475c149343fd77408bef259c5534\\n  }, -b3b49c87e866475c149343fd77408bef259c5534\n    - b3b49c87e866475c149343fd77408bef259c5534\n  }, b3b49c87e866475c149343fd77408bef259c5534, b3b49c87e866475c149343fd77408bef259c5534\\\\n    b3b49c87e866475c149343fd77408bef259c5534\\\\n  }, b3b49c87e866475c149343fd77408bef259c5534\\n    b3b49c87e866475c149343fd77408bef259c5534\\n  }, b3b49c87e866475c149343fd77408bef259c5534\n    b3b49c87e866475c149343fd77408bef259c5534\n  }",CWE-1220,0,"gnocchi_git_install_branch: 908ca555f6a14547082e38e4f114926ec384a1bd # HEAD of ""master"" as of 24.01.2017",0,"as of 24.01.2017, gnocchi_git_install_branch: 908ca555f6a14547082e38e4f114926ec384a1bd # HEAD of \","CWE-0123, CWE-1234, CWE-2345, CWE-3456, CWE-4567, CWE-5678, CWE-6789, CWE-7890, CWE-8901, CWE-9012",0
- data_migrations  is succeeded,1,data_migrations  is succeeded,CWE-779,1,"- name: Prepare MQ/DB services
  hosts: nova_conductor
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:

    - name: Configure rabbitmq vhost/user (nova)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure rabbitmq vhost/user (nova/telemetry)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure MySQL user (nova)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-placement)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api cell0)
      include: common-tasks/mysql-db-user.yml
      run_once: yes



- name: Install nova-conductor services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_conductor""
    nova_serial: ""{{ nova_conductor_serial | default(['1', '100%']) }}""



- name: Install nova-scheduler/nova-consoleauth services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_scheduler:nova_consoleauth:!nova_conductor""
    nova_serial: ""{{ nova_scheduler_serial | default(['1', '100%']) }}""



- name: Install nova API services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_os_compute:nova_api_placement:!nova_conductor:!nova_scheduler:!nova_consoleauth""
    nova_serial: ""{{ nova_api_serial | default(['1', '100%']) }}""



- name: Install nova console/metadata services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_metadata:nova_console:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement""
    nova_serial: ""{{ nova_console_serial | default(['1', '100%']) }}""



- name: Install nova compute
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_compute:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement:!nova_api_metadata:!nova_console""
    nova_serial: ""{{ nova_compute_serial | default('20%') }}""



# These facts are set against the deployment host to ensure that
# they are fast to access. This is done in preference to setting
# them against each target as the hostvars extraction will take
# a long time if executed against a large inventory.
- name: Refresh local facts after all software changes are made
  hosts: nova_all
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: ""!all""

    # This variable contains the values of the local fact set for the cinder
    # venv tag for all hosts in the 'cinder_all' host group.
    - name: Gather software version list
      set_fact:
        nova_all_software_versions: ""{{ (groups['nova_all'] | map('extract', hostvars, ['ansible_local', 'openstack_ansible', 'nova', 'venv_tag'])) | list }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean value which is True when
    # nova_all_software_versions contains a list of defined
    # values. If they are not defined, it means that not all
    # hosts have their software deployed yet.
    - name: Set software deployed fact
      set_fact:
        nova_all_software_deployed: ""{{ (nova_all_software_versions | select('defined')) | list == nova_all_software_versions }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean when all the values in
    # nova_all_software_versions are the same and the software
    # has been deployed to all hosts in the group.
    - name: Set software updated fact
      set_fact:
        nova_all_software_updated: ""{{ ((nova_all_software_versions | unique) | length == 1) and (nova_all_software_deployed | bool) }}""
      delegate_to: localhost
      run_once: yes


# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore skip the reload
# for that service.
- name: Reload all nova services to ensure new RPC object version is used
  hosts: ""nova_all:!nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_serial | default('100%') }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service reload
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova""
        service_action: ""reloaded""
        service_negate: ""{{ ['nova-placement-api.service'] + nova_service_negate | default([]) }}""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore restart it instead.
- name: Restart the nova placement API service to ensure new RPC object version is used
  hosts: ""nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_api_serial | default(['1', '100%']) }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service restart
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova-placement-api""
        service_action: ""restarted""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Perform online database migrations
  hosts: nova_conductor
  gather_facts: no
  user: root
  tasks:
    - name: Perform online data migrations
      command: ""{{ nova_bin }}/nova-manage db online_data_migrations""
      become: yes
      become_user: ""{{ nova_system_user_name }}""
      when:
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_online_data_migrations'] | bool""
      changed_when: false
      run_once: yes
      register: data_migrations

    - name: Disable the online migrations requirement
      ini_file:
        dest: ""/etc/ansible/facts.d/openstack_ansible.fact""
        section: nova
        option: need_online_data_migrations
        value: False
      when:
        - data_migrations | succeeded",0,"Configure MySQL user (nova), Configure MySQL user (nova-api cell0), Configure MySQL user (nova-api), Configure MySQL user (nova-placement), Configure rabbitmq vhost/user (nova), Install nova API services, Install nova compute, Install nova console/metadata services, Install nova-conductor services, Install nova-scheduler/nova-consoleauth services",CWE-295,0
"pip_install_options_fact: ""{{ pip_install_options|default('') }} --constraint /opt/developer-pip-constraints.txt --constraint /opt/requirements/upper-constraints.txt""
  tags:
    - neutron-install
    - neutron-pip-packages

- name: Set pip_install_options_fact when not in developer mode
  set_fact:
    pip_install_options_fact: ""{{ pip_install_options|default('') }}""
  when:
    - not neutron_developer_mode | bool
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""",1,"extra_args: \, extra_args: \\, extra_args: \\\\, extra_args: \\\\\\\\\\\\, extra_args: \\\\\\\\\\\\\\\\\\\\\\\\\\, extra_args: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, extra_args: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, extra_args: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, extra_args: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, extra_args: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-1031,0,"extra_args: ""{{ pip_install_options|default('') }}""",0,"extra_args: \, pip_install_options|default('')\, pip_install_options|default('')\\, pip_install_options|default('')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, pip_install_options|default('')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, pip_install_options|default('')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, pip_install_options|default('')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, pip_install_options|default('')\\\\\\\\\\\\\\\\\\\\n, pip_install_options|default('')\\\\\\\\\n, pip_install_options|default('')\\\\n","CWE-117: Improper Input Handling, CWE-190: Exposure of Sensitive Information to an Unauthorized Actor",0
"- name: Create tmpfiles.d entry
    src: ""neutron-systemd-tmpfiles.j2""
    dest: ""/etc/tmpfiles.d/{{ program_name }}.conf""",1,"\, dest: \, dest: \\, etc/tmpfiles.d/{{ program_name }}.conf, name: Create tmpfiles.d entry, name: Create tmpfiles.d entry\\nsrc: \\\\, name: Create tmpfiles.d entry\n, neutron-systemd-tmpfiles.j2, src: \",CWE-1257,0,"---
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create neutron TEMP dirs
  file:
    path: ""{{ item.path }}/{{ program_name }}""
    state: directory
    owner: ""{{ system_user }}""
    group: ""{{ system_group }}""
    mode: ""2755""
  with_items:
    - { path: ""/var/run"" }
    - { path: ""/var/lock"" }

- name: Create tempfile.d entry
  template:
    src: ""neutron-systemd-tempfiles.j2""
    dest: ""/etc/tmpfiles.d/neutron.conf""
    mode: ""0644""
    owner: ""root""
    group: ""root""

- name: Place the systemd init script
  template:
    src: ""neutron-systemd-init.j2""
    dest: ""/etc/systemd/system/{{ program_name }}.service""
    mode: ""0644""
    owner: ""root""
    group: ""root""
  register: systemd_init

- name: Reload the systemd daemon
  command: ""systemctl daemon-reload""
  when: systemd_init | changed
  notify:
    - Restart neutron services",0,"dest: \\\\\\\\, group: \\\\, group: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, mode: \, mode: \\, mode: \\\\\\\\\\\\\\\, mode: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, owner: \\, owner: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, owner: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-276,0
"- { path: ""{{ nova_lock_path }}"" }",1,"ansible.windows.win_user, file, lineinfile, template, win_executable_search_path, win_shell","CWE-250, CWE-269, CWE-270, CWE-285",1,"---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: create the system group
  group:
    name: ""{{ nova_system_group_name }}""
    state: ""present""
    system: ""yes""
  tags:
    - nova-group

- name: Create the nova system user
  user:
    name: ""{{ nova_system_user_name }}""
    group: ""{{ nova_system_group_name }}""
    comment: ""{{ nova_system_comment }}""
    shell: ""{{ nova_system_shell }}""
    system: ""yes""
    createhome: ""yes""
    home: ""{{ nova_system_home_folder }}""
  tags:
    - nova-user

- name: Create nova dir
  file:
    path: ""{{ item.path }}""
    state: directory
    owner: ""{{ item.owner|default(nova_system_user_name) }}""
    group: ""{{ item.group|default(nova_system_group_name) }}""
    mode: ""{{ item.mode|default('0755') }}""
  with_items:
    - { path: ""/etc/nova"" }
    - { path: ""/etc/nova/rootwrap.d"" }
    - { path: ""/etc/sudoers.d"", mode: ""0750"", owner: ""root"", group: ""root"" }
    - { path: ""/var/cache/nova"" }
    - { path: ""{{ nova_system_home_folder }}"" }
    - { path: ""{{ nova_system_home_folder }}/.ssh"", mode: ""0700"" }
    - { path: ""{{ nova_system_home_folder }}/cache/api"" }
    - { path: ""{{ nova_system_home_folder }}/instances"" }
    - { path: ""/var/lock/nova"" }
    - { path: ""/var/run/nova"" }
  tags:
    - nova-dirs

- name: Test for log directory or link
  shell: |
    if [ -h ""/var/log/nova""  ]; then
      chown -h {{ nova_system_user_name }}:{{ nova_system_group_name }} ""/var/log/nova""
      chown -R {{ nova_system_user_name }}:{{ nova_system_group_name }} ""$(readlink /var/log/nova)""
    else
      exit 1
    fi
  register: log_dir
  failed_when: false
  changed_when: log_dir.rc != 0
  tags:
    - nova-dirs
    - nova-logs

- name: Create nova log dir
  file:
    path: ""{{ item.path }}""
    state: directory
    owner: ""{{ item.owner|default(nova_system_user_name) }}""
    group: ""{{ item.group|default(nova_system_group_name) }}""
    mode: ""{{ item.mode|default('0755') }}""
  with_items:
    - { path: ""/var/log/nova"" }
  when: log_dir.rc != 0
  tags:
    - nova-dirs
    - nova-logs

- name: Drop sudoers file
  template:
    src: ""sudoers.j2""
    dest: ""/etc/sudoers.d/{{ nova_system_user_name }}_sudoers""
    mode: ""0440""
    owner: ""root""
    group: ""root""
  tags:
    - sudoers
    - nova-sudoers",0,", if [ -h \, if [ -h \\, if [ -h \\\\, if [ -h \\\\\\\\, if [ -h \\\\\\\\\\\\, if [ -h \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, if [ -h \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-256, CWE-257, CWE-260, CWE-261, CWE-262, CWE-263, CWE-264",0
"until: _stop  is success
  until: _start  is success",1,"until: _stop  is success\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, until: _stop  is success\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n until: _start  is success, until: _stop  is success\\\\\\\\\\\\\\\\n until: _start  is success, until: _stop  is success\\\\\\\\n until: _start  is success, until: _stop  is success\\\\n until: _start  is success, until: _stop  is success\\n until: _start  is success, until: _stop  is success\\n until: _start  is success\\n, until: _stop  is success\n until: _start  is success",1028,0,"service:
    enabled: yes
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
- name: Stop services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""stopped""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _stop
  until: _stop | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
# Note (odyssey4me):
# The policy.json file is currently read continually by the services
# and is not only read on service start. We therefore cannot template
# directly to the file read by the service because the new policies
# may not be valid until the service restarts. This is particularly
# important during a major upgrade. We therefore only put the policy
# file in place after the service has been stopped.
#
- name: Copy new policy file into place
  copy:
    src: ""/etc/nova/policy.json-{{ nova_venv_tag }}""
    dest: ""/etc/nova/policy.json""
    owner: ""root""
    group: ""{{ nova_system_group_name }}""
    mode: ""0640""
    remote_src: yes
  listen: ""Restart nova services""
- name: Start services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""started""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _start
  until: _start | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
- name: Wait for the nova-compute service to initialize
  command: ""openstack --os-cloud default compute service list --service nova-compute --format value --column Host""
  register: _compute_host_list
  retries: 10
  delay: 5
  until: ""ansible_nodename in _compute_host_list.stdout_lines""
  when:
    - ""'nova_compute' in group_names""
    - ""nova_discover_hosts_in_cells_interval | int < 1""
  listen: ""Restart nova services""
  service:
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  register: _restart
  until: _restart | success
  when:
    - inventory_hostname in groups['nova_api_placement']",0,"copy:\\r\\nsrc: \, daemon_reload:\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n, delay: 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, register: _restart\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\, register: _start\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n, retries: 5\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\, service:\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n, service:\r\nenabled: yes\r\nstate: stopped\r\n, until: _start | success\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\, when:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n","CWE-1030, CWE-1041, CWE-1042, CWE-1043, CWE-1044, CWE-1045, CWE-1046, CWE-1047, CWE-259, CWE-263",0
-mtime +{{ image_cache_expire_days|int - 1 }} {% endif %}|,1,"-mimtime +{{ image_cache_expire_days|int - 1 }}\n, mimtime +{{ image_cache_expire_days|int - 1 }} |\\\\n, mimtime +{{ image_cache_expire_days|int - 1 }} |\\n, mimtime +{{ image_cache_expire_days|int - 1 }}\\n, mimtime +{{ image_cache_expire_days|int - 1 }}|, mimtime +{{ image_cache_expire_days|int - 1 }}|\\n",CWE-502,0,"- name: Clean image cache directory
  shell: >-
    find {{ image_cache_dir }} -type f
    {% if not image_cache_dir_cleanup|bool %}
    -mtime +{{ image_cache_expire_days - 1 }} {% endif %}|
    xargs --no-run-if-empty -t rm -rf",0,find {{ image_cache_dir }} -type f -mtime +{{ image_cache_expire_days - 1 }}| xargs --no-run-if-empty -t rm -rf,"CWE-200, CWE-22, CWE-23, CWE-264, CWE-276, CWE-601, CWE-662, CWE-78, CWE-89",1
name: 'subnode-{{ item.0 + 1 }}',1,name:'subnode-{{ item.0 + 1 }}',"CWE-113, CWE-119",1,"---

- when: inventory == 'all'
  block:
    #required for liberty based deployments
    - name: copy get-overcloud-nodes.py to undercloud
      template:
        src: 'get-overcloud-nodes.py.j2'
        dest: '{{ working_dir }}/get-overcloud-nodes.py'
        mode: 0755

    #required for liberty based deployments
    - name: fetch overcloud node names and IPs
      shell: >
          source {{ working_dir }}/stackrc;
          python {{ working_dir }}/get-overcloud-nodes.py
      register: registered_overcloud_nodes

    - name: list the overcloud nodes
      debug: var=registered_overcloud_nodes.stdout

    - name: fetch the undercloud ssh key
      fetch:
        src: '{{ working_dir }}/.ssh/id_rsa'
        dest: '{{ overcloud_key }}'
        flat: yes
        mode: 0400

    # add host to the ansible group formed from its type
    # novacompute nodes are added as compute for backwards compatibility
    - name: add overcloud node to ansible
      with_dict: '{{ registered_overcloud_nodes.stdout | default({}) }}'
      add_host:
        name: '{{ item.key }}'
        groups: ""overcloud,{{ item.key | regex_replace('overcloud-(?:nova)?([a-zA-Z0-9_]+)-[0-9]+$', '\\1') }}""
        ansible_host: '{{ item.key }}'
        ansible_fqdn: '{{ item.value }}'
        ansible_user: ""{{ overcloud_user | default('heat-admin') }}""
        ansible_private_key_file: ""{{ overcloud_key }}""
        ansible_ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'

- when: inventory == 'multinode'
  block:
    - name: Get subnodes
      command: cat /etc/nodepool/sub_nodes_private
      register: nodes

    - name: Add subnode to ansible inventory
      with_indexed_items: '{{ nodes.stdout_lines | default([]) }}'
      add_host:
        name: 'subnode-{{ item.0 + 2 }}'
        groups: ""overcloud""
        ansible_host: '{{ item.1 }}'
        ansible_fqdn: '{{ item.1 }}'
        ansible_user: ""{{ lookup('env','USER') }}""
        ansible_private_key_file: ""/etc/nodepool/id_rsa""

#required for regeneration of ssh.config.ansible
- name: set_fact for undercloud ip
  set_fact: undercloud_ip={{ hostvars['undercloud'].undercloud_ip }}
  when: hostvars['undercloud'] is defined and hostvars['undercloud'].undercloud_ip is defined

# Add the supplemental to the in-memory inventory.
- name: Add supplemental node vm to inventory
  add_host:
    name: supplemental
    groups: supplemental
    ansible_host: supplemental
    ansible_fqdn: supplemental
    ansible_user: '{{ supplemental_user }}'
    ansible_private_key_file: '{{ local_working_dir }}/id_rsa_supplemental'
    ansible_ssh_extra_args: '-F ""{{local_working_dir}}/ssh.config.ansible""'
    supplemental_node_ip: ""{{ supplemental_node_ip }}""
  when: supplemental_node_ip is defined

- name: set_fact for supplemental ip
  set_fact: supplemental_node_ip={{ hostvars['supplemental'].supplemental_node_ip }}
  when: hostvars['supplemental'] is defined and hostvars['supplemental'].supplemental_node_ip is defined

#readd the undercloud to reset the ansible_ssh parameters set in quickstart
- name: Add undercloud vm to inventory
  add_host:
    name: undercloud
    groups: undercloud
    ansible_host: undercloud
    ansible_fqdn: undercloud
    ansible_user: '{{ undercloud_user }}'
    ansible_private_key_file: '{{ undercloud_key }}'
    ansible_ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.local.ansible""'
    undercloud_ip: ""{{ undercloud_ip }}""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is not defined and undercloud_ip is defined

#required for regeneration of ssh.config.ansible
- name: set undercloud ssh proxy command
  set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ hostvars[groups['virthost'][0]].ansible_private_key_file }}
      {{ ssh_user }}@{{ hostvars[groups['virthost'][0]].ansible_host }}
      -W {{ undercloud_ip }}:22""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is defined and undercloud_ip is defined

#required for regeneration of ssh.config.ansible
- name: set undercloud ssh proxy command
  set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ hostvars['localhost'].ansible_user_dir }}/.quickstart/id_rsa_virt_power
      {{ ssh_user }}@{{ hostvars['localhost'].ansible_default_ipv4.address }}
      -W {{ undercloud_ip }}:22""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is not defined and undercloud_ip is defined

- name: set supplemental ssh proxy command
  set_fact: supplemental_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ local_working_dir }}/id_rsa_virt_power
      {{ ssh_user }}@{{ hostvars[groups['virthost'][0]].ansible_host }}
      -W {{ supplemental_node_ip }}:22""
  when: supplemental_node_ip is defined

- name: create inventory from template
  delegate_to: localhost
  template:
    src: 'inventory.j2'
    dest: '{{ local_working_dir }}/hosts'

- name: regenerate ssh config
  delegate_to: localhost
  template:
    src: 'ssh_config.j2'
    dest: '{{ local_working_dir }}/ssh.config.ansible'
    mode: 0644
  when: undercloud_ip is defined

- name: regenerate ssh config for ssh connections from the virthost
  delegate_to: localhost
  template:
    src: 'ssh_config_localhost.j2'
    dest: '{{ local_working_dir }}/ssh.config.local.ansible'
    mode: 0644
  when: undercloud_ip is defined

# just setup the ssh.config.ansible and hosts file for the virthost
- name: check for existence of identity key
  delegate_to: localhost
  stat: path=""{{ local_working_dir }}/id_rsa_virt_power""
  when: undercloud_ip is not defined
  register: result_stat_id_rsa_virt_power

- name: set fact used in ssh_config_no_undercloud.j2 to determine if IdentityFile should be included
  set_fact:
    id_rsa_virt_power_exists: true
  when: undercloud_ip is not defined and result_stat_id_rsa_virt_power.stat.exists == True

- name: regenerate ssh config, if no undercloud has been launched.
  delegate_to: localhost
  template:
    src: 'ssh_config_no_undercloud.j2'
    dest: '{{ local_working_dir }}/ssh.config.ansible'
    mode: 0644
  when: undercloud_ip is not defined",0,"name: Add overcloud node to ansible\\nwith_dict: '{{ registered_overcloud_nodes.stdout | default({}) }}'\\nadd_host:\\n  name: '{{ item.key }}'\\n  groups: \, name: Add supplemental node vm to inventory\nadd_host:\n  name: supplemental\n  groups: supplemental\n  ansible_host: supplemental\n  ansible_fqdn: supplemental\n  ansible_user: '{{ supplemental_user }}'\n  ansible_private_key_file: '{{ local_working_dir }}/id_rsa_supplemental'\n  ansible_ssh_extra_args: '-F \, name: Add undercloud vm to inventory\\\\nadd_host:\\n  name: undercloud\\\\n  groups: undercloud\\\\n  ansible_host: undercloud\\\\n  ansible_fqdn: undercloud\\\\n  ansible_user: '{{ undercloud_user }}'\\\\n  ansible_private_key_file: '{{ undercloud_key }}'\\\\n  ansible_ssh_extra_args: '-F \\, name: copy get-overcloud-nodes.py to undercloud\\ntemplate:\\n  src: 'get-overcloud-nodes.py.j2'\\n  dest: '{{ working_dir }}/get-overcloud-nodes.py'\\n  mode: 0755, name: fetch overcloud node names and IPs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nshell: >\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n          source {{ working_dir }}/stackrc;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n          python {{ working_dir }}/get-overcloud-nodes.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n        register: registered_overcloud_nodes\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- name: list the overcloud nodes\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndbg: var=registered_overcloud_nodes.stdout\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\, name: fetch overcloud ssh key\\\\nfetch:\\\n  src: '{{ working_dir }}/.ssh/id_rsa'\\\\n  dest: '{{ overcloud_key }}'\\\\n  flat: yes\\\\n  mode: 0400, name: fetch the undercloud ssh key\\\\\\\\nfetch:\\\\\\n  src: '{{ working_dir }}/.ssh/id_rsa'\\\\\\\\n  dest: '{{ overcloud_key }}'\\\\\\\\n  flat: yes\\\\\\\\n  mode: 0400, name: list the overcloud nodes\\\\ndbg: var=registered_overcloud_nodes.stdout, name: set_fact for supplemental ip\\\\\\\\\\\\\\\\nsafet: supplemental_node_ip={{ hostvars['supplemental'].supplemental_node_ip }\\\\\\\\, name: set_fact for undercloud ip\\\\\\\\nsafet: undercloud_ip={{ hostvars['undercloud'].undercloud_ip }\\\\","CWE-290, CWE-327",0
"- teardown-environment
    - teardown-provision",1,"- teardown-environment, - teardown-provision, teardown-environment, teardown-provision","CWE-274, CWE-275, CWE-276",1,"# This teardown role will destroy all vms defined in the overcloud_nodes
# key, and the undercloud
- name:  Teardown undercloud and overcloud vms
  hosts: virthost
  gather_facts: yes
  roles:
    - libvirt/teardown
  tags:
    - teardown-all
    - teardown-virthost
    - teardown-nodes

# This teardown role will destroy libvirt networks
- name: Tear down environment
  hosts: virthost
  roles:
    - environment/teardown
  tags:
    - teardown-all
    - teardown-virthost

# Finally, we conditionally remove basic setup (users,
# groups, directories)to start from scratch
- name: Teardown user setup on virt host
  hosts: virthost
  roles:
    - provision/teardown
  tags:
    - teardown-all",0,"hosts: virthost, roles:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - provision/teardown, roles:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - provision/teardown, roles:\\\\\\\\\\\\\\\\\\n  - provision/teardown, roles:\\\\\\\\n  - provision/teardown, roles:\\\\n  - provision/teardown, roles:\\n  - provision/teardown, roles:\n  - environment/teardown","CWE-256, CWE-257, CWE-306",0
"- when: networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length > 0",1,"when: networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length > 0",CWE-250,1,"# If virtualport_type is defined for any networks, include OVS dependencies
- when: ""{{ networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length is greaterthan 0}}""
  block:

  # Install OVS dependencies
  - name: Install OVS dependencies
    include_role:
      name: 'parts/ovs'

  # Create any OVS Bridges that have been defined
  - name: Create OVS Bridges
    openvswitch_bridge:
      bridge: ""{{ item.bridge }}""
      state: present
    when: item.virtualport_type is defined and item.virtualport_type == ""openvswitch""
    with_items: ""{{ networks }}""
    become: true",0,"any OVS Bridges that have been defined, become: true, state: present","CWE-257, CWE-732, CWE-754",0
"when: undercloud_install_cli_options == '--use-heat' or containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_t_h_t_undercloud }}""
        dest: ""./hieradata-overrides-t-h-t-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-t-h-t-undercloud.yaml""
  when:
    - not undercloud_install_cli_options
    - not containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_classic_undercloud }}""
        dest: ""./hieradata-overrides-classic-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-classic-undercloud.yaml""

- name: Create undercloud configuration
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""",1,"when: not containerized_undercloud|bool, when: not undercloud_install_cli_options, when: undercloud_install_cli_options, when: undercloud_install_cli_options == '--use-heat' or containerized_undercloud|bool",CWE-20,0,"# Creat the scripts that will be used to deploy the undercloud
# environment.
- name: Create undercloud configuration
  template:
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""
    mode: 0600

- name: Create undercloud hieradata overrides
  template:
    src: ""{{ undercloud_hieradata_override_file }}""
    dest: ""./quickstart-hieradata-overrides.yaml""
    mode: 0600

- name: Create undercloud install script
  template:
    src: ""{{ undercloud_install_script }}""
    dest: ""{{ working_dir }}/undercloud-install.sh""
    mode: 0755",0,"dest:, mode:, src:","CWE-272, CWE-276",0
nodepool_cirros_checksum: md5:d56d54f110654dfd29b0e8ed56e6cda8,1,nodepool_cirros_checksum: md5:d56d54f110654dfd29b0e8ed56e6cda8,"CWE-284, CWE-285, CWE-288, CWE-289, CWE-290, CWE-291, CWE-292, CWE-293, CWE-295, CWE-296",1,"nodepool_cirros_url: http://download.cirros-cloud.net/0.3.6/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_dest: /opt/cache/files/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_checksum: md5:a30f156a83471adf89ebb70dc96d0c82",0,"nodepool_cirros_checksum: md5:a30f156a83471adf89ebb70dc96d0c82, nodepool_cirros_dest: /opt/cache/files/cirros-0.3.6-x86_64-disk.img, nodepool_cirros_url: http://download.cirros-cloud.net/0.3.6/cirros-0.3.6-x86_64-disk.img","CWE-1237, CWE-352",1
"tempest_init: ""tempest init {{ tempest_dir }}""
     tempestconf: ""/usr/bin/discover-tempest-config""

- name: Create /var/log/containers/tempest
  file:
     path: /var/log/containers/tempest
     state: directory
  become: true

- name: Create /var/lib/tempestdata
  file:
     path: /var/lib/tempestdata
     state: directory
  become: true",1,"become: true, name: Create /var/lib/tempestdata, name: Create /var/log/containers/tempest","CWE-312, CWE-327",1,"tempest_init: ""tempest init {{ tempest_dir }}""
      tempestconf: ""/usr/bin/discover-tempest-config""",0,"tempest init {{ tempest_dir }}, tempest_dir, tempest_init, tempest_init: tempest init {{ tempest_dir }}, tempestconf, tempestconf:, tempestconf: /usr/bin/discover-tempest-config",CWE-778,0
"undercloud_key: ""{{ local_working_dir }}/id_rsa_undercloud""",1,"id_rsa_undercloud, local_working_dir, undercloud_key: \",CWE-295,0,"---
# defaults for all ovb-stack related tasks
local_working_dir: ""{{ lookup('env', 'HOME') }}/.quickstart""
working_dir: /home/stack

release: mitaka

node:
    prefix:
        - ""{{ 1000 |random }}""
        - ""{{ lookup('env', 'USER') }}""
        - ""{{ lookup('env', 'BUILD_NUMBER') }}""
tmp:
    node_prefix: '{{ node.prefix | reject(""none"") | join(""-"") }}-'

os_username: admin
os_password: password
os_tenant_name: admin
os_auth_url: 'http://10.0.1.10:5000/v2.0'
cloud_name: qeos7

stack_name: 'oooq-{{ prefix }}stack'
rc_file: /home/stack/overcloudrc
node_name: 'undercloud'
ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'
undercoud_key: ""{{ local_working_dir }}/id_rsa_undercloud""
node_groups:
    - 'undercloud'
    - 'tester'
templates_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal/templates""
ovb_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal""
heat_template: ""{{ templates_dir }}/quintupleo.yaml""
environment_list:
    - ""{{ templates_dir }}/resource-registry.yaml""
    - ""{{ local_working_dir }}/{{ prefix }}env.yaml""

existing_key_location: '{{ local_working_dir }}'
remove_image_from_host_cloud: false

bmc_flavor: m1.medium
bmc_image: 'bmc-base'
bmc_prefix: '{{ prefix }}bmc'

baremetal_flavor: m1.large
baremetal_image: 'ipxe-boot'
baremetal_prefix: '{{ prefix }}baremetal'

key_name: '{{ prefix }}key'
private_net: '{{ prefix }}private'
node_count: 2
public_net: '{{ prefix }}public'
provision_net: '{{ prefix }}provision'

# QuintupleO-specific params ignored by virtual-baremetal.yaml
undercloud_name: '{{ prefix }}undercloud'
undercloud_image: '{{ prefix }}undercloud.qcow2'
undercloud_flavor: m1.xlarge
external_net:  '10.2.1.0/22'

network_isolation_type: multi-nic

setup_undercloud_connectivity_log: ""{{ working_dir }}/setup_undercloud_connectivity.log""

mtu: 1350
mtu_interface:
  - eth1
pvt_nameserver: 8.8.8.8

external_interface: eth2
external_interface_ip: 10.0.0.1
external_interface_netmask: 255.255.255.0

registered_releases:
  - mitaka
  - newton
  - master
  - rhos-9",0,"bmc_image: 'bmc-base', cloud_name: qeos7, existing_key_location: \, external_net:  '10.2.1.0/22', key_name: \\\, local_working_dir: \\, network_isolation_type: multi-nic, pvt_nameserver: 8.8.8.8\n\nexternal_interface: eth2\nexternal_interface_ip: 10.0.0.1\nexternal_interface_netmask: 255.255.255.0, templates_dir: \, working_dir: /home/stack\\nrelease: mitaka","CWE-1197, CWE-732",0
"fail: msg=""Overcloud nova list does not show expected number of {{ node_to_scale }} services""
  when: post_scale_node_count.stdout|int != {{ final_scale_value|int }}",1,when: post_scale_node_count.stdout|int!= {{ final_scale_value|int }},"CWE-287, CWE-295, CWE-754",1,"source {{ working_dir }}/stackrc;
    {{ working_dir }}/scale-deployment.sh &> overcloud_deployment_scale_console.log;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    nova list | grep {{ node_to_scale }} | cut -f2- -d':' | wc -l
  fail: msg=Overcloud nova list does not show expected number of {{ node_to_scale }} services
  when: post_scale_node_count.stdout != {{ final_scale_value }}",0,"nova list | grep {{ node_to_scale }} | cut -f2- -d':' | wc -l, source {{ working_dir }}/stackrc;, when: post_scale_node_count.stdout!= {{ final_scale_value }}",CWE-117,0
"pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML",1,"PyYAML, haproxy, pkg_extras, python, python*-setuptools, python-setuptools","CWE-1195, CWE-1196, CWE-937",1,"---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8",0,"    image: centos:7\\\\\\\\n    pkg_extras: python-setuptools haproxy\\\\\\\\n    easy_install:\\\\\\n      - pip\\\\\\\\n    environment: *env\\\\\\\\n\\\\\\\\n  - name: fedora28\\\\\\\\n    hostname: fedora28\\\\\\\\n    image: fedora:28\\\\\\\\n    pkg_extras: python*-setuptools haproxy\\\\\\\\n    environment:\\\\\\n      <<: *env\\\\\\\\n\\\\\\\\nprovisioner:\\\\\\n  name: ansible\\\\\\\\n  log: true\\\\\\\\n  env:\\\\\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\\\\\\\n    ANSIBLE_LIBRARY:, - pip\\\\\\\\n    environment: *env\\\\\\\\n\\\\\\\\n  - name: fedora28\\\\\\\\n    hostname: fedora28\\\\\\\\n    image: fedora:28\\\\\\\\n    pkg_extras: python*-setuptools haproxy\\\\\\\\n    environment:\\\\\\n      <<: *env\\\\\\\\n\\\\\\\\nprovisioner:\\\\\\n  name: ansible\\\\\\\\n  log: true\\\\\\\\n  env:\\\\\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\\\\\\\n    ANSIBLE_LIBRARY: \\\\\n\\\\n\\\\\\\\nsce\\\\\\\\n, ANSIBLE_STDOUT_CALLBACK: yaml\\\\n    ANSIBLE_LIBRARY: \\\, hostname: centos7\n    image: centos:7\n    pkg_extras: python-setuptools haproxy\n    easy_install:\n      - pip\n    environment: *env\n\n  - name: fedora28\n    hostname: fedora28\n    image: fedora:28\n    pkg_extras: python*-setuptools haproxy\n    environment:\n      <<: *env\n\nprovisioner:\n  name: ansible\n  log: true\n  env:\n    ANSIBLE_STDOUT_CALLBACK: yaml\n    ANSIBLE_LIBRARY: \, http_proxy: \\n      {{ lookup('env', 'http_proxy') }}\\\\n      https_proxy: \\n      {{ lookup('env', 'https_proxy') }}\\\\\\\\n\\\\n  - name: fedora28\\\\n    hostname: fedora28\\\\n    image: fedora:28\\\\n    pkg_extras: python*-setuptools haproxy\\\\n    environment:\\\\n      <<: *env\\\\n\\\\nprovisioner:\\\\n  name: ansible\\\\n  log: true\\\\n  env:\\\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\\\n    ANSIBLE_LIBRARY: \\\\\\\n\\\\nscenario:\\\\n  test_sequence:\\\\n, https_proxy: \\, log: true\\\\n  env:\\\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\\\n    ANSIBLE_LIBRARY: \\\\, name: ansible\\\\\\\\\\\\\\\\n  log: true\\\\\\\\\\\\\\\\n  env:\\\\\\\\\\\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\\\\\\\\\\\\\\\n    ANSIBLE_LIBRARY: \\\\\\\\\\n\\\\\\\\\\\\\\\\nscenario:\\\\\\\\\\\\n  test_sequence:\\\\\\\\\\\\\\n    - destroy\\\\\\\\\\\\n    - create\\\\\\\\\\\\n    - prepare\\\\\\\\\\\\n    - converge\\\\\\\\\\\\n    - verify\\\\\\\\\\\\n    - destroy\\\\n\\\\\\\\\\\\nlint:\\\\\\\\\\\\n  enabled: true\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nverifier:\\\\\\\\\\\\n  name: testinfra\\\\\\\\\\\\n  lint:\\, name: ansible\\\\n  log: true\\\\n  env:\\\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\\\n    ANSIBLE_LIBRARY: \\\n\\\\nscenario:\\n  test_sequence:\\\n    - destroy\\\\n    - create\\\\n    - prepare\\\\n    - converge\\\\n    - verify\\\\n    - destroy\\n\\nlint:\\n  enabled: true\\\\n\\nverifier:\\n  name: testinfra\\n  lint:\\n    name: flake8\\n\\n, pkg_extras: python-setuptools haproxy\\n    easy_install:\\n      - pip\\n    environment: *env\\n\\n  - name: fedora28\\n    hostname: fedora28\\n    image: fedora:28\\n    pkg_extras: python*-setuptools haproxy\\n    environment:\\n      <<: *env\\n\\nprovisioner:\\n  name: ansible\\n  log: true\\n  env:\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\n    ANSIBLE_LIBRARY: \\","CWE-20, CWE-22",0
"pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML",1,"PyYAML, haproxy, pkg_extras, pkg_extras: haproxy, pkg_extras: python*-setuptools, pkg_extras: python*-setuptools haproxy PyYAML, pkg_extras: python-setuptools, pkg_extras: python-setuptools haproxy PyYAML, python*-setuptools, python-setuptools",CWE-123,1,"---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8",0,"    command: python3 -m http.server 8787\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    environment: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      <<: *env\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,     environment: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      http_proxy: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      https_proxy: \\\\\\\\\\\\\\\\\\\\\,     environment: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      http_proxy: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      https_proxy: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - name: fedora28\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    hostname: fedora28\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    image: fedora:28\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,   test_sequence: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    - destroy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: ansible\\\\n  log: true\\\\n  env: \\\\\\\\n    ANSIBLE_STDOUT_CALLBACK: yaml\\\\n    ANSIBLE_LIBRARY: \\, name: centos7\n  hostname: centos7\n  image: centos:7\n  override_command: True\n  command: python -m SimpleHTTPServer 8787\n  pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby\n  environment: &env\n    http_proxy: \, name: fedora28\\n  hostname: fedora28\\n  image: fedora:28\\n  override_command: True\\n  command: python3 -m http.server 8787\\n  pkg_extras: python*-setuptools python*-enum python*-netaddr ruby\\\\n    environment: \, pkg_extras: python*-setuptools python*-enum python*-netaddr ruby\\n    environment:\\n      <<: *env\\n, platforms: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - name: centos7\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    hostname: centos7\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    image: centos:7\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    override_command: True\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    command: python -m SimpleHTTPServer 8787\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    environment: &env\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      http_proxy: \\\, verifier: \\\\\\\\\\\\n  name: testinfra\\\\\\\\n  lint: \\\\\\\\\\\\\\\\n    name: flake8\\\\\\\\n    extra_args: [\",CWE-502,0
"pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML",1,"pkg_extras: python*-setuptools python*-enum python*-netaddr epel-release ruby PyYAML, pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML, pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML","CWE-1023, CWE-1024, CWE-1172, CWE-1185, CWE-1187, CWE-1194, CWE-1195",1,"---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8",0,"ansible: {}, driver: {} \nprovisioner: {}\nscenario: {}, lint: {}, log: {}, name: {}, provisioner: {}, scenario: {}, verifier: {}\\n","CWE-123, CWE-456, CWE-678, CWE-789, CWE-987",0
"pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML",1,pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML,CWE-937,,"---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8",0,"ANSIBLE_LIBRARY, ANSIBLE_STDOUT_CALLBACK, environment, flake8, lint, override_command, pkg_extras, test_sequence","CWE-1223, CWE-1234, CWE-1235, CWE-1236, CWE-1237, CWE-1238, CWE-1239",0
"name: ""postgresql@{{ postgres_version }}-main.service""",1,"name: \, name: \\, name: \\\\, name: \\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-25, CWE-275, CWE-276",1,"- name: enable postgres
  become: yes
  service:
    name: ""postgresql@{{ postgresql_version }}-main.service""
    state: started
    enabled: yes
  tags:
    - postgres-enable",0,"become: yes, enabled: yes, name: \\, service:\n    name: \, state: started, tags:\\n    - postgres-enable\\n  \\n","CWE-1000, CWE-1001, CWE-1002, CWE-1003, CWE-1004, CWE-1005, CWE-1006, CWE-1007, CWE-601, CWE-602",0
"when: checkgiinstall.stdout != ""1"" and oracle_install_version_gi == item.version and oracle_sw_copy|bool and oracle_sw_unpack|bool
  when: checkgiinstall.stdout != ""1"" and  oracle_install_version_gi == item.version and not oracle_sw_copy|bool and oracle_sw_unpack|bool",1,"and oracle_sw_unpack|bool\\\\\\\\\\\\\\\, and oracle_sw_unpack|bool\\nwhen: checkgiinstall.stdout!= \\\\, oracle_sw_unpack|bool\\\\nwhen: checkgiinstall.stdout!= \\\\\\\\\\\\\\\\, oracle_sw_unpack|bool\\nwhen: checkgiinstall.stdout!= \\, when: checkgiinstall.stdout!= \, when: checkgiinstall.stdout!= \\, when: checkgiinstall.stdout!= \\\\, when: checkgiinstall.stdout!= \\\\\\\\",CWE-732,1,"---

- name: Extract files to stage-area (GI)
  unarchive: src={{ oracle_stage }}/{{ item.filename }}  dest={{ oracle_stage }}/{{ item.version }} copy=no
  with_items: ""{{oracle_sw_image_gi}}""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - oragridswunpack
  when: checkgiinstall.stdout != ""1"" and ""{{ oracle_install_version_gi }}"" == ""{{ item.version }}"" and oracle_sw_copy|bool and oracle_sw_unpack|bool

- name: Extract files to stage-area (GI) (from remote location)
  unarchive: src={{ oracle_stage_remote }}/{{ item.filename }}  dest={{ oracle_stage }}/{{ item.version }} copy=no
  with_items: ""{{oracle_sw_image_gi}}""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
  - oragridswunpack
  when: checkgiinstall.stdout != ""1"" and ""{{ oracle_install_version_gi }}"" == ""{{ item.version }}"" and not oracle_sw_copy|bool and oracle_sw_unpack|bool

- name: Install cvuqdisk rpm
  yum: name=""{{ oracle_stage_install }}/{{ oracle_install_version_gi }}/grid/rpm/{{ cvuqdisk_rpm }}"" state=present
  when: configure_cluster
  tags: cvuqdisk
  ignore_errors: true

- name: Setup response file for install (GI)
  template: src=grid-install.rsp.{{ oracle_install_version_gi }}.j2 dest={{ oracle_rsp_stage }}/{{ oracle_grid_responsefile }} owner=""{{ grid_install_user }}"" group={{ oracle_group }} mode=775 backup=yes
  with_items: ""{{asm_diskgroups}}""
  tags:
    - responsefilegi
  when: master_node and checkgiinstall.stdout != ""1"" and item.diskgroup == oracle_asm_init_dg

- name: Install Grid Infrastructure
  shell: ""{{ oracle_stage_install }}/{{ oracle_install_version_gi }}/grid/runInstaller -responseFile {{ oracle_rsp_stage }}/{{ oracle_grid_responsefile }} -waitforcompletion -ignorePrereq -ignoreSysPrereqs -showProgress -silent""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - oragridinstall
  when: master_node and checkgiinstall.stdout != ""1"" #and oracle_sw_unpack
  register: giinstall

- debug: var=giinstall.stdout_lines
  when: master_node and checkgiinstall.stdout != ""1""

- name: Run oraInstroot script after installation
  shell: ""{{ oracle_inventory_loc }}/orainstRoot.sh""
  sudo: yes
  tags:
    - runroot
  when: checkgiinstall.stdout != ""1""

- name: Run root script after installation (Master Node)
  shell: ""{{ oracle_home_gi }}/root.sh""
  sudo: yes
  tags:
    - runroot
  when: master_node and checkgiinstall.stdout != ""1""
  register: rootmaster

- debug: var=rootmaster.stdout_lines
  when: master_node and checkgiinstall.stdout != ""1""

- name: Run root script after installation (Other Nodes)
  shell: ""sleep {{ item.0 * 60 }}; {{ oracle_home_gi }}/root.sh""
  sudo: yes
  with_indexed_items: ""{{groups[hostgroup]}}""
  tags:
    - runroot
  when: not master_node and checkgiinstall.stdout != ""1"" and inventory_hostname == item.1
  register: rootother

- debug: var=rootother.stdout_lines
  when: not master_node and checkgiinstall.stdout != ""1""

- name: Setup response file for configToolAllCommands
  template: src=configtoolallcommands.rsp.{{ oracle_install_version_gi }}.j2 dest={{ oracle_rsp_stage }}/configtoolallcommands.rsp owner=""{{ grid_install_user }}"" group={{ oracle_group }} mode=755 backup=yes
  tags:
    - responsefileconfigtool
  when: master_node and run_configtoolallcommand  and checkgiinstall.stdout != ""1""

- name: Run configToolAllCommands
  shell: ""{{ oracle_home_gi }}/cfgtoollogs/configToolAllCommands RESPONSE_FILE={{ oracle_rsp_stage }}/configtoolallcommands.rsp""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - runconfigtool
  when: master_node and run_configtoolallcommand and checkgiinstall.stdout != ""1""
  ignore_errors: true
  register: configtool",0,"Gi installation requires adminstrative credentials. This could be done with sudoers configuration (https://docs.ansible.com/ansible/cheatsheet_sudoers.html) and allow other users or groups to use the sudoers command. This is useful if the task uses'sudo: no' or'sudo_user: user' but requires root to run., The group owner of the inventory file should not be a system account. See https://docs.ansible.com/ansible/latest/plugins/inventory.html for further information. This could be avoided using the groupvar_groups variable. See https://docs.ansible.com/ansible/latest/plugins/inventory.html for further information., This playbook is vulnerable to privilege escalation. Privilege escalation is enabled by default and can be avoided using the ansible noescape variable. Please consult with https://docs.ansible.com/ansible/latest/plugins/connection_plugins.html#connection-plug-in-specific-parameter-defaults for further information., This script uses default values for ansible tasks. Default values could lead to configuration errors and insecure settings, e.g. in security. This could be avoided using ansible defaults with ansible defaults (https://docs.ansible.com/ansible/developing_inventory.html) or ansible's lookup system (https://docs.ansible.com/ansible/playbooks_lookups.html)., This task uses ansible_user with root privileges without a sudoers configuration. This could be avoided with using the sudo_user variable. See https://docs.ansible.com/ansible/latest/plugins/connection_plugins.html#connection-plug-in-specific-parameter-defaults for further information., This task uses sudo_user with a non-admin user. See https://docs.ansible.com/ansible/latest/plugins/connection_plugins.html#connection-plug-in-specific-parameter-defaults for further information. This could be avoided using sudoers., Use of ansible_sudo variable with root privileges without using sudo_user variable. See https://docs.ansible.com/ansible/latest/plugins/connection_plugins.html#connection-plug-in-specific-parameter-defaults for further information. This could be avoided using the sudo_user variable. See https://docs.ansible.com/ansible/latest/plugins/connection_plugins.html#connection-plug-in-specific-parameter-defaults for further information., You may want to enable host_key_checking to ensure you trust the remote servers. See https://docs.ansible.com/ansible/latest/plugins/ssh.html#ssh-plugins-configuration-settings for further information. This could be done with the host_key_checking variable. See https://docs.ansible.com/ansible/latest/plugins/ssh.html#ssh-plugins-configuration-settings for further information.","CWE-213, CWE-264, CWE-266, CWE-295, CWE-732",0
"grants={{ item.1.grants |default(omit)  }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, schema: {{ item.1.schema }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants | default (omit) }}
          object_privs={{ item.1.object_privs |default (omit)}}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, schema: {{ item.1.schema | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""",1,"grants={{ item.1.grants |default(omit) }}, object_privs={{ item.1.object_privs | default (omit) }}, object_privs={{ item.1.object_privs |default (omit) }}",CWE-732,1,"---
- name: Manage grants (cdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.oracle_db_name }}
          user={{ db_user }}
          password={{ db_password_cdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_databases }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants

- name: Manage grants (pdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.pdb_name }}
          user={{ db_user }}
          password={{ db_password_pdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_pdbs }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0 is defined and item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants",0,"become_user: \\\\\\\\\\\, become_user: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, mode={{ db_mode }}\\\\\\\\\\\\\\\\\\\\\\n      with_subelements:\\\\\\\\\\\\\\\\\\\\\\\\n          - \\\\\\\\\\\\\\\\\\\\\\\\, mode={{ db_mode }}\\n      with_subelements:\\n          - \, oracle_grants:\\n          schema={{ item.1.schema }}\\n          state={{ item.1.state }}\\n          grants={{ item.1.grants }}\\n          hostname={{ inventory_hostname }}\\n          service_name={{ item.0.pdb_name }}\\n          user={{ db_user }}\\n          password={{ db_password_pdb}}\\n          mode={{ db_mode }}\\n      with_subelements:\\n          - \\, oracle_grants:\n          schema={{ item.1.schema }}\n          state={{ item.1.state }}\n          grants={{ item.1.grants }}\n          hostname={{ inventory_hostname }}\n          service_name={{ item.0.oracle_db_name }}\n          user={{ db_user }}\n          password={{ db_password_cdb}}\n          mode={{ db_mode }}\n      with_subelements:\n          - \, oracle_user\\, run_once: \\\\\, run_once: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, user={{ db_user }}\\\\\\\\\\n          password={{ db_password_cdb}}\\\\\\\\\\n          mode={{ db_mode }}\\\\\\\\\\n      with_subelements:\\\\\\\\n          - \\\\\\\\\\","CWE-119, CWE-778",0
"ovirt_scheduling_policies_facts_internal_24:
      ovirt_api_facts_internal_25:
      ovirt_clusters_internal_24:
        ovirt_clusters_internal_24:",1,"ovirt_api_facts_internal_25, ovirt_clusters_internal_24",CWE-732,1,"---
- block:
  - name: Login to oVirt
    ovirt_auth:
      url: ""{{ engine_url }}""
      username: ""{{ engine_user }}""
      password: ""{{ engine_password }}""
      ca_file: ""{{ engine_cafile | default(omit) }}""
      insecure: ""{{ engine_insecure | default(true) }}""
    when: ovirt_auth is undefined
    register: loggedin
    tags:
      - always

  - name: Get hosts
    ovirt_hosts_facts:
      auth: ""{{ ovirt_auth }}""
      pattern: ""cluster={{ cluster_name | mandatory }} update_available=true {{ host_names | map('regex_replace', '(.*)', 'name=\\1') | list | join(' or ') }} {{ host_statuses | map('regex_replace', '(.*)', 'status=\\1') | list | join(' or ') }}""
  
  - name: Check if there are hosts to be updated
    debug:
      msg: ""No hosts to be updated""
    when: ovirt_hosts | length == 0
  
  - block:
    - name: Init failed_host_names and succeed_host_names list
      set_fact:
        failed_host_names: []
        succeed_host_names: []

    - name: Get cluster facts
      ovirt_clusters_facts:
        auth: ""{{ ovirt_auth }}""
        pattern: ""name={{ cluster_name }}""
  
    - name: Get name of the original scheduling policy
      ovirt_scheduling_policies_facts:
        auth: ""{{ ovirt_auth }}""
        id: ""{{ ovirt_clusters[0].scheduling_policy.id }}""
  
    - name: Remember the cluster scheduling policy
      set_fact:
        cluster_scheduling_policy: ""{{ ovirt_scheduling_policies[0].name }}""
  
    - name: Get list of VMs in cluster
      ovirt_vms_facts:
        auth: ""{{ ovirt_auth }}""
        pattern: ""cluster={{ cluster_name }}""
  
    - name: Set in cluster upgrade policy
      ovirt_clusters:
        auth: ""{{ ovirt_auth }}""
        name: ""{{ cluster_name }}""
        scheduling_policy: InClusterUpgrade
  
    - name: Shutdown VMs which can be stopped
      ovirt_vms:
        auth: ""{{ ovirt_auth }}""
        state: stopped
        name: ""{{ item }}""
        force: true
      with_items:
        - ""{{ stopped_vms | default([]) }}""
  
    - include: pinned_vms.yml
  
    - include: upgrade.yml
      with_items:
        - ""{{ ovirt_hosts }}""
      when: ""item.id not in host_ids or stop_pinned_to_host_vms""

    when: ovirt_hosts | length > 0
    always:
      - name: Set original cluster policy
        ovirt_clusters:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ cluster_name }}""
          scheduling_policy: ""{{ cluster_scheduling_policy }}""
  
      - name: Start again stopped VMs
        ovirt_vms:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ item }}""
        with_items:
          - ""{{ stopped_vms | default([]) }}""
  
      - name: Start again pin to host VMs
        ovirt_vms:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ item }}""
        with_items:
          - ""{{ pinned_vms_names | default([]) }}""
        when: ""stop_pinned_to_host_vms""
  
      - name: Print info about host which was updated
        debug:
          msg: ""Following hosts was successfully updated: {{ succeed_host_names }}""
        when: ""succeed_host_names | length > 0""

      - name: Fail the playbook, if some hosts wasn't updated
        fail:
          msg: ""The cluster upgrade failed. Hosts {{ failed_host_names }} wasn't updated.""
        when: ""failed_host_names | length > 0""

  always:
    - name: Logout from oVirt
      ovirt_auth:
        state: absent
        ovirt_auth: ""{{ ovirt_auth }}""
      when: not loggedin.skipped | default(false)
      tags:
        - always",0,"ansible-playbook myplaybook.yaml, name: Init failed_host_names and succeed_host_names list, name: Remember the cluster scheduling policy, name: Set original cluster policy, state: stopped, variable \, when: \, when: ovirt_hosts | length > 0, with_items:, with_items: - \","CWE-113, CWE-116",0
"args:
      warn: false
    command: ""engine-setup --accept-defaults --config-append={{ answer_file_path }} {{ offline }}""
    until: health_page is success",1,"args: {}warn: falsecommand: engine-setup --accept-defaults --config-append={{\t\t\t\tanswer_file_path\t\t\t\t}} {{ offline }}, command: engine-setup --accept-defaults --config-append={{\\\\\\\\t\\\\\\\\t\\\\\\\\t\\\\\\\\tanswer_file_path\\\\\\\\t\\\\\\\\t\\\\\\\\t\\\\\\\\t}} {{ offline }}, command: engine-setup --accept-defaults --config-append={{\\\\t\\\\t\\\\t\\\\tanswer_file_path\\\\t\\\\t\\\\t\\\\t}} {{ offline }}, command: engine-setup --accept-defaults --config-append={{\\t\\t\\t\\tanswer_file_path\\t\\t\\t\\t}} {{ offline }}, until: health_page is success","CWE-16, CWE-20, CWE-319, CWE-77",1,"---
- block:
  - name: Set answer file path
    set_fact:
      answer_file_path: ""/tmp/answerfile-{{ lookup('pipe', 'date +%Y%m%d%H%M%SZ') }}.txt""


  - name: Use the default answerfile
    template:
      src: answerfile_{{ ovirt_engine_setup_version }}_basic.txt.j2
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is undefined

  - name: Copy custom answer file
    template:
      src: ""{{ ovirt_engine_setup_answer_file_path }}""
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is defined

  - name: Update setup packages
    package:
      name: ""ovirt*setup*""
      state: latest
    when: ovirt_engine_setup_update_setup_packages

  - name: Update all packages
    package:
      name: ""*""
      state: latest
    when: ovirt_engine_setup_update_all_packages

  - name: Set accept defaults parameter if variable is set
    set_fact:
      accept_defaults: ""{{ '--accept-defaults' if ovirt_engine_setup_accept_defaults else '' }}""

  - name: Run engine-setup with answerfile
    command: ""engine-setup --config-append={{ answer_file_path }} {{ accept_defaults }}""
    tags:
      - skip_ansible_lint

  - name: Make sure `ovirt-engine` service is running
    service:
      name: ovirt-engine
      state: started

  - name: Check if Engine health page is up
    uri:
      url: ""http://{{ ansible_fqdn }}/ovirt-engine/services/health""
      status_code: 200
    register: health_page
    retries: 12
    delay: 10
    until: health_page|success

  always:
    - name: Clean temporary files
      file:
        path: ""{{ answer_file_path }}""
        state: 'absent'",0,"accept_defaults: {{ '--accept-defaults' if ovirt_engine_setup_accept_defaults else '' }}, name: Copy custom answer file, name: Run engine-setup with answerfile, name: Set accept defaults parameter if variable is set\\nset_fact:, name: Set answer file path, ovirt_engine_setup_accept_defaults\\\\naccept_defaults: {{ '--accept-defaults' if ovirt_engine_setup_accept_defaults else '' }}, ovirt_engine_setup_update_all_packages\\nwhen: ovirt_engine_setup_update_setup_packages\\n, state: 'absent', when: ovirt_engine_setup_update_all_packages\nwhen: ovirt_engine_setup_update_setup_packages\n","CWE-1031, CWE-863",0
,1,"ansible_user: null, become: true, become_method: sudo, become_user: root, gather_facts: yes, no_log: false, tags, vars_files","CWE-250, CWE-306, CWE-307",1,- /opt/appdata/themes/nzbget:/app/nzbget/webui:shared,0,"- /opt/appdata/themes/nzbget:/app/nzbget/webui:shared, shared, webui:shared","1028, 327",0
"regexp: nzb_backup_dir = """"",1,regexp: nzb_backup_dir =,CWE-264,0,"- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""nzb_backup_dir =""
    replace: ""nzb_backup_dir = /nzb""
  when: sabnzbd_ini.stat.exists == False

- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""admin_dir = admin""
    replace: ""admin_dir = /admin""
  when: sabnzbd_ini.stat.exists == False",0,"path: /opt/appdata/sabnzbd/sabnzbd.ini, regexp: \, replace: \, replace: \\, replace: \\\\, when: sabnzbd_ini.stat.exists == False","CWE-779, CWE-829",0
,1,"default_vault, var ansible_connection, vars","CWE-256, CWE-287, CWE-345",0,"- name: password
  pause:
    prompt: ""Please create a Universal Password (.htaccess / Wordpress & etc)""
  register: pw

- debug: msg=""Using following pw {{pw_input.user_input}}""

- name: Replace password with user input
  replace:
    path: /opt/appdata/plexguide/var.yml
    regexp: defaultpassword
    replace: ""{{pw_input.user_input}}""",0,"Replace password with user input, debug: msg, defaultpassword, path, path: /opt/appdata/plexguide/var.yml, prompt, pw_input.user_input, regexp: defaultpassword, replace","CWE-114, CWE-307, CWE-319, CWE-321, CWE-327, CWE-347, CWE-798",0
"when: updatecheck.stdout == ""18.09.2,""",1,"if updatecheck.stdout == \\, when: updatecheck.stdout == 18.09.2,\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\t\\\\\\\\n\\\\, when: updatecheck.stdout == 18.09.2,\\\\\\\\\\\\n\\\\t\\\\n, when: updatecheck.stdout == 18.09.2,\\\\\\n\\t, when: updatecheck.stdout == 18.09.2,\\\n\, when: updatecheck.stdout == 18.09.2,\\\n\\, when: updatecheck.stdout == \, when: updatecheck.stdout == \\, when: updatecheck.stdout == \\\\, when: updatecheck.stdout == \\\\\\\\",CWE-1031,0,"---
- name: ""Establish Facts""
  set_fact:
    switch=""on""
    updatecheck=""default""

- name: ""Docker Check""
  stat:
    path: ""/usr/bin/docker""
  register: check

- name: ""Docker Version Check - True""
  shell: ""docker --version | awk '{print $3}'""
  register: updatecheck

- debug:
    msg: "" {{ updatecheck }} ""

- name: ""Switch - On""
  set_fact:
    switch=""off""
  when: updatecheck.stdout == ""18.06.0-ce,""

- debug:
    msg: ""Switch - {{ switch }}""

- debug:
    msg: ""UpdateCheck - {{ updatecheck }}""

- name: Install required packages
  apt: ""name={{item}} state=present""
  with_items:
    - apt-transport-https
    - ca-certificates
    - software-properties-common
  when: switch == ""on""

- name: Add official gpg signing key
  apt_key:
    id: 0EBFCD88
    url: https://download.docker.com/linux/ubuntu/gpg
  when: switch == ""on""

- name: ""Stop All Containers""
  shell: ""docker stop $(docker ps -a -q)""
  ignore_errors: yes
  when:
    - check.stat.exists == True
    - switch == ""on""

- name: Official Repo
  apt_repository:
    repo: ""deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} edge""
  register: apt_docker_repositories
  when: switch == ""on""

- name: Update APT packages list
  apt:
    update_cache: yes
  when: apt_docker_repositories.changed and switch == ""on""

- name: Release docker-ce from hold
  dpkg_selections:
    name: docker-ce
    selection: install
  when: switch == ""on""

- name: Install docker-ce
  apt:
    name: docker-ce=18.06.0~ce~3-0~ubuntu
    state: present
    update_cache: yes
    force: yes
  when: switch == ""on""

- name: Put docker-ce into hold
  dpkg_selections:
    name: docker-ce
    selection: hold
  when: switch == ""on""

- name: Uninstall docker-py pip module
  pip:
    name: docker-py
    state: absent
  ignore_errors: yes
  when: switch == ""on""

- name: Install docker pip module
  pip:
    name: docker
    state: latest
  ignore_errors: yes
  when: switch == ""on""

- name: Check docker daemon.json exists
  stat:
    path: /etc/docker/daemon.json
  register: docker_daemon

- name: Stop docker to enable overlay2
  systemd: state=stopped name=docker enabled=yes
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: Import daemon.json
  copy: ""src=daemon.json dest=/etc/docker/daemon.json force=yes mode=0775""
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: Start docker (Please Wait)
  systemd: state=started name=docker enabled=yes
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: ""Wait for 20 seconds before commencing""
  wait_for:
    timeout: 20
  when: switch == ""on""

- name: Check override folder exists
  stat:
    path: /etc/systemd/system/docker.service.d
  register: docker_override

- name: Create override folder
  file: ""path=/etc/systemd/system/docker.service.d state=directory mode=0775""
  when:
    - docker_override.stat.exists == False
    - switch == ""on""
  tags: docker_standard

- name: Import override file
  copy: ""src=override.conf dest=/etc/systemd/system/docker.service.d/override.conf force=yes mode=0775""
  tags: docker_standard
  when: switch == ""on""

- name: create plexguide network
  docker_network:
    name: ""plexguide""
    state: present
  tags: docker_standard
  when: switch == ""on""

- name: ""Start All Containers""
  shell: ""docker start $(docker ps -a -q)""
  ignore_errors: yes
  when:
    - switch == ""on""
    - check.stat.exists == True",0,"apt: \\, apt: \\\\\\\\, pip: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: \, shell: \\, shell: \\\\, shell: \\\\\\\\\\\\\\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-276,0
"traefik.frontend.rule: ""Host:heimdall.{{domain.stdout}}""",1,"Host:heimdall.{{domain.stdout}}, traefik.frontend.rule: \, traefik.frontend.rule: \\, traefik.frontend.rule: \\\\, traefik.frontend.rule: \\\\\\\\\\\\, traefik.frontend.rule: \\\\\\\\\\\\\\\\\\\\\\\\, traefik.frontend.rule: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-20, CWE-23, CWE-732, CWE-89",1,"- name: Register Domain
  shell: ""cat /var/plexguide/server.domain""
  register: domain
  ignore_errors: True
      traefik.frontend.rule: ""Host:heimdall.{domain.stdout}}""",0,"traefik.frontend.rule: \, traefik.frontend.rule: \\, traefik.frontend.rule: \\\\, traefik.frontend.rule: \\\\\\\, traefik.frontend.rule: \\\\\\\\\\\\\\, traefik.frontend.rule: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, traefik.frontend.rule: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-500, CWE-502, CWE-530, CWE-532, CWE-537, CWE-538",0
,1,,,0,"---
  - name: Check MOVE Service
    stat:
      path: ""/etc/systemd/system/move.service""
    register: move

  - name: Stop If Move Service Running
    systemd: state=stopped name=move
    when: move.stat.exists
    
  - name: Install Move Service
    template:
      src: move.js2
      dest: /etc/systemd/system/move.service 
      force: yes
    when: move.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=move daemon_reload=yes enabled=no

  - name: Start Move
    systemd: state=started name=move enabled=yes
    when: move.stat.exists
    
  - name: Check RCLONE Service
    stat:
      path: ""/etc/systemd/system/rclone.service""
    register: rclone

  - name: Stop If RClone Service Running
    systemd: state=stopped name=rclone
    when: rclone.stat.exists
    
  - name: Install RCLONE Service
    template:
      src: rclone.js2
      dest: /etc/systemd/system/rclone.service 
      force: yes
    when: rclone.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=rclone daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=rclone enabled=yes
    when: rclone.stat.exists

  - name: Check UNIONFS Service
    stat:
      path: ""/etc/systemd/system/unionfs.service""
    register: unionfs

  - name: Stop If UNIONFS Service Running
    systemd: state=stopped name=unionfs
    when: unionfs.stat.exists
    
  - name: Install UNIONFS Service
    template:
      src: unionfs.js2
      dest: /etc/systemd/system/unionfs.service 
      force: yes
    when: unionfs.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=unionfs daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=unionfs enabled=yes
    when: unionfs.stat.exists",0,"  - name: Install MOVE Service\\\\\\\\n    template:\\n      src: move.js2\\\\\\\\n      dest: /etc/systemd/system/move.service \\\\\\\\\n      force: yes\\\\\\\\n    when: move.stat.exists == False\\\\\\\\n\\\\\\\\\\\\n  - name: Daemon-Reload\\\\\\\\n    systemd: state=stopped name=move daemon_reload=yes enabled=no\\\\\\\\n    when: move.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,   - name: Install RCLONE Service\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    template:\\\\n      src: rclone.js2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n      dest: /etc/systemd/system/rclone.service \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,   - name: Install UNIONFS Service\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, - name: Check MOVE Service\n      stat:\n        path: \, n: Start UNIONFS\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    systemd: state=started name=unionfs enabled=yes\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    when: unionfs.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: Check RCLONE Service\\n      stat:\\n        path: \\, name: Check UNIONFS Service\\\\n      stat:\\\\n        path: \\\\, when: move.stat.exists == False\\\\\\\\\\\\n  - name: Daemon-Reload\\\\\\\\\\\\n    systemd: state=stopped name=move daemon_reload=yes enabled=no\\\\\\\\\\\\n    when: move.stat.exists\\\\\\\\\\\\n\\\\\\\\\\\\n  - name: Start Move\\\\\\\\\\\\n    systemd: state=started name=move enabled=yes\\\\\\\\\\\\n    when: move.stat.exists\\\\\\\\\\\\n\\\\\\\\\\\\n\\\\\\\\\\\\n  - name: Check RCLONE Service\\\\n      stat:\\n        path: \\, when: rclone.stat.exists == False\\\\\\\\\\\\\\\\\\\\\\\\n  - name: Daemon-Reload\\\\\\\\\\\\\\\\\\\\\\\\n    systemd: state=stopped name=rclone daemon_reload=yes enabled=no\\\\\\\\\\\\\\\\\\\\\\\\n    when: rclone.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\n  - name: Start RClone\\\\\\\\\\\\\\\\\\\\\\\\n    systemd: state=started name=rclone enabled=yes\\\\\\\\\\\\\\\\\\\\\\\\n    when: rclone.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\n  - name: Check UNIONFS Service\\\\\\\\\\\\\\\\\\\\\\\\n      stat:\\\\, when: unionfs.stat.exists == False\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - name: Daemon-Reload\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    systemd: state=stopped name=unionfs daemon_reload=yes enabled=no\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    when: unionfs.stat.exists\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - name: Start RClone\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    systemd: state",CWE-78: Improper Neutralization of Special Elements used in a SQL Command ('SQL Injection'),0
"extport: ""7997""",1,"- name: 'task_name', become: 'true', become_method:'sudo', become_user: 'user', loop: 'list', loop: 'range', run_once: 'True', set_fact: 'var_name', when: 'expression', with: 'dictionary'",CWE-123,0,"---
- name: ""Establish Key Variables""
  set_fact:
    intport: ""8000""
    extport: ""7999""
    pgrole: ""{{role_name}}""
    image: ""coderaiser/cloudcmd""

- name: ""Key Variables Recall""
  include_role:
    name: ""pgmstart""
    tasks_from: ""keyvar.yml""

- name: Create Directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - ""/opt/appdata/cloudblitz/""

- name: Check config file exists
  stat:
    path: ""/opt/appdata/cloudblitz/.cloudcmd.json""
  register: cloud_json

- name: Install configblitz.json
  template:
    src: configblitz.json
    dest: /opt/appdata/cloudblitz/.cloudcmd.json
    force: yes
  when: cloud_json.stat.exists == False

- name: ""Set Default Volume - {{pgrole}}""
  set_fact:
    default_volumes:
      - /:/SERVER
      - /opt/appdata/cloudblitz:/root/

- name: ""Establish Key Variables - {{pgrole}}""
  set_fact:
    default_env:
      PUID: 1000
      PGID: 1000

- name: ""Set Default Labels - {{pgrole}}""
  set_fact:
    default_labels:
      traefik.enable: ""true""
      traefik.frontend.redirect.entryPoint: ""https""
      traefik.frontend.rule: ""Host:{{pgrole}}.{{domain.stdout}}""
      traefik.port: ""{{intport}}""

- include_role:
    name: ""pgmstart""",0,"default_env: PUID: 1000, PGID: 1000, default_labels: traefik.enable: true, traefik.frontend.redirect.entryPoint: https, traefik.frontend.rule: Host:{{pgrole}}.{{domain.stdout}}, traefik.port: 8000, default_volumes: - /:/SERVER, - /opt/appdata/cloudblitz:/root/, file: path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true, image: coderaiser/cloudcmd, intport: 8000, name: Create Directories, name: Establish Key Variables, name: Key Variables Recall, name: Set Default Labels",CWE-306,0
"- ""443:443""",1,450:23:00,CWE-123,1,"- name: Create nginx-proxy directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - /opt/nginx-proxy

      - ""443:443""""",0,"443:443, file:, mode=0775, name:, name: Create nginx-proxy directories, nginx-proxy, owner=1000 group=1000, path={{item}} state=directory, recurse=true, with_items:","CWE-250, CWE-257, CWE-259, CWE-275, CWE-384",1
,1,"tasks: {tasks: task_1,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, tasks: {tasks: task_1,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_2,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_3},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_1: {cmd: whoami},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, tasks: {tasks: task_1,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_2,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_3},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_1: {cmd: whoami},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_2: {cmd: whoami},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  task_3: {cmd: whoami,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  register: user},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  hosts: {all: localhost},\\\\, tasks: {tasks: task_1,\\\\\\\\\\\\\\\\\\\\n  task_2,\\\\\\\\\\\\\\\\\\\\n  task_3},\\\\\\\\\\\\\\\\\\\\n  task_1: {cmd: whoami},\\\\\\\\\\\\\\\\\\\\n  task_2: {cmd: whoami},\\\\\\\\\\\\\\\\\\\\n  task_3: {cmd: whoami,\\\\\\\\\\\\\\\\\\\\n  register: user},\\\\\\\\\\\\\\\\\\\\n  hosts: {all: localhost},\\\\\\\\\\\\\\\\\\\\n  gather_facts: true,\\\\\\\\\\\\\\\\\\\\n  become: true,\\\\\\\\\\\\\\\\\\\\n  become_user: user,\\\\\\\\\\\\\\\\\\\\n , tasks: {tasks: task_1,\\\\\\\\n  task_2,\\\\\\\\n  task_3},\\\\\\\\n  task_1: {cmd: whoami},\\\\\\\\n  task_2: {cmd: whoami},\\\\\\\\n  task_3: {cmd: whoami,\\\\\\\\n  register: user},\\\\\\\\n  hosts: {all: localhost},\\\\\\\\n  gather_facts: true,\\\\\\\\n  become: true,\\\\\\\\n  become_user: user,\\\\\\\\n  become_method: sudo, tasks: {tasks: task_1,\\\\n  task_2,\\\\n  task_3},\\\\n  task_1: {cmd: whoami},\\\\n  task_2: {cmd: whoami},\\\\n  task_3: {cmd: whoami,\\\\n  register: user},\\\\n  hosts: {all: localhost},\\\\n  gather_facts: true,\\\\n  become: true,\\\\n  become_user: user,\\\\n  become_method: sudo, tasks: {tasks: task_1,\\n  task_2,\\n  task_3},\\n  task_1: {cmd: whoami},\\n  task_2: {cmd: whoami},\\n  task_3: {cmd: whoami,\\n  register: user},\\n  hosts: {all: localhost},\\n  gather_facts: true,\\n  become: true,\\n  become_user: user,\\n  become_method: sudo, vars: {username: user},\\n  group: {group_name: group_name}, vars: {username: user},\n  group: {group_name: group_name}","CWE-306, CWE-307, CWE-308, CWE-312, CWE-328, CWE-334, CWE-352, CWE-367, CWE-380, CWE-396",0,"---

- name: fio mixed randread and sequential write benchmark on tikv_data_dir disk
  shell: ""cd {{ fio_deploy_dir }} && ./fio -ioengine=psync -bs=32k -fdatasync=1 -thread -rw=randrw -percentage_random=100,0 -size={{ benchmark_size }} -filename=fio_randread_write_test.txt -name='fio mixed randread and sequential write test' -iodepth=4 -runtime=60 -numjobs=4 -group_reporting --output-format=json --output=fio_randread_write_test.json""
  register: fio_randread_write

- name: clean fio mixed randread and sequential write benchmark temporary file
  file:
    path: ""{{ fio_deploy_dir }}/fio_randread_write_test.txt""
    state: absent

- name: get fio mixed test randread iops
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --read-iops""
  register: disk_mix_randread_iops
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test write iops
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --write-iops""
  register: disk_mix_write_iops
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test randread latency
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --read-lat""
  register: disk_mix_randread_lat
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test write latency
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --write-lat""
  register: disk_mix_write_lat
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed randread and sequential write summary
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --summary""
  register: disk_mix_randread_write_smmary
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: fio mixed randread and sequential write benchmark command
  debug:
    msg: ""fio mixed randread and sequential write benchmark command: {{ fio_randread_write.cmd }}.""
  run_once: true

- name: fio mixed randread and sequential write benchmark summary
  debug:
    msg: ""fio mixed randread and sequential write benchmark summary: {{ disk_mix_randread_write_smmary.stdout }}.""

- name: Preflight check - Does fio mixed randread and sequential write iops of tikv_data_dir disk meet requirement - randread
  fail:
    msg: 'fio mixed randread and sequential write test: randread iops of  tikv_data_dir disk is too low: {{ disk_mix_randread_iops.stdout }} < {{ min_ssd_mix_randread_iops }}, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_randread_iops.stdout|int < min_ssd_mix_randread_iops|int

- name: Preflight check - Does fio mixed randread and sequential write iops of tikv_data_dir disk meet requirement - sequential write
  fail:
    msg: 'fio mixed randread and sequential write test: sequential write iops of tikv_data_dir disk is too low: {{ disk_mix_write_iops.stdout }} < {{ min_ssd_mix_write_iops }}, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_write_iops.stdout|int < min_ssd_mix_write_iops|int

- name: Preflight check - Does fio mixed randread and sequential write latency of tikv_data_dir disk meet requirement - randread
  fail:
    msg: 'fio mixed randread and sequential write test: randread latency of  tikv_data_dir disk is too low: {{ disk_mix_randread_lat.stdout }} ns > {{ max_ssd_mix_randread_lat }} ns, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_randread_lat.stdout|int > max_ssd_mix_randread_lat|int

- name: Preflight check - Does fio mixed randread and sequential write latency of tikv_data_dir disk meet requirement - sequential write
  fail:
    msg: 'fio mixed randread and sequential write test: sequential write latency of tikv_data_dir disk is too low: {{ disk_mix_write_lat.stdout }} ns > {{ max_ssd_mix_write_lat }} ns, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_write_lat.stdout|int > max_ssd_mix_write_lat|int",0,"fio mixed randread and sequential write iops of tikv_data_dir disk is too low, fio mixed randread and sequential write test: randread iops of  tikv_data_dir disk is too low, fio mixed randread and sequential write test: randread latency of  tikv_data_dir disk is too low, fio mixed randread and sequential write test: sequential write iops of tikv_data_dir disk is too low, randread latency of  tikv_data_dir disk is too low, sequential write iops of tikv_data_dir disk is too low","CWE-776, CWE-778",0
enable-telemetry: true,1,"disable_telemetry: false, disable_telemetry: true, disable_teletmetry: false, enable-telemetry: true, enable_teletmetry: false, enable_teletmetry: true","CWE-269, CWE-275, CWE-276, CWE-290",1,"dashboard:
  public-path-prefix: ""/dashboard""
  internal-proxy: false
  disable-telemetry: false",0,"dashboard:, disable-telemetry: false, internal-proxy: false, public-path-prefix: \","CWE-209, CWE-259, CWE-275, CWE-732",0
"expr: increase(tidb_server_event_total{type=~""server_start|server_hang""}[15m])  > 0
      expr:  increase(tidb_server_event_total{type=~""server_start|server_hang""}[15m])  > 0
  - alert: tidb_tikvclient_backoff_total
    expr: increase( tidb_tikvclient_backoff_total[10m] )  > 10
      expr:  increase( tidb_tikvclient_backoff_total[10m] )  > 10",1,"increase( tidb_tikvclient_backoff_total[10m] )  > 10, increase(tidb_server_event_total{type=~",CWE-758,1,"- alert: TiDB_schema_error
    expr: increase(tidb_session_schema_lease_error_total{type=""outdated""}[15m]) > 0
      level: emergency
      expr:  increase(tidb_session_schema_lease_error_total{type=""outdated""}[15m]) > 0
      description: 'alert: instance: {{ $labels.instance }} values: {{ $value }}'
      summary: TiDB schema error
  - alert: TiDB_tikvclient_region_err_total
    expr: increase( tidb_tikvclient_region_err_total[10m] )  > 6000
      level: emergency
      expr:  increase( tidb_tikvclient_region_err_total[10m] )  > 6000
      summary: TiDB tikvclient_backoff_count error
  - alert: TiDB_domain_load_schema_total
    expr: increase( tidb_domain_load_schema_total{type=""failed""}[10m] )  > 10
      level: emergency
      expr:  increase( tidb_domain_load_schema_total{type=""failed""}[10m] )  > 10
      summary: TiDB domain_load_schema_total error
  - alert: TiDB_monitor_keep_alive
    expr: increase(tidb_monitor_keep_alive_total[10m]) < 100
      expr:  increase(tidb_monitor_keep_alive_total[10m]) < 100
      summary: TiDB monitor_keep_alive error
  - alert: TiDB_server_panic_total
    expr: increase(tidb_server_panic_total[10m]) > 0
      level: critical
      expr:  increase(tidb_server_panic_total[10m]) > 0
      summary: TiDB server panic total
  - alert: TiDB_memery_abnormal
    expr: go_memstats_heap_inuse_bytes{job=""tidb""} > 1e+10
      expr: go_memstats_heap_inuse_bytes{job=""tidb""} > 1e+10
      description: 'alert: values: {{ $value }}'
      summary: TiDB mem heap is over 1GiB
  - alert: TiDB_query_duration
    expr: histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance)) > 1
      level: warning
      expr:  histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance)) > 1
      summary: TiDB query duration 99th percentile is above 1s
  - alert: TiDB_server_event_error
    expr: increase(tidb_server_server_event{type=~""server_start|server_hang""}[15m])  > 0
      level: warning
      expr:  increase(tidb_server_server_event{type=~""server_start|server_hang""}[15m])  > 0
      summary: TiDB server event error
  - alert: TiDB_tikvclient_backoff_count
    expr: increase( tidb_tikvclient_backoff_count[10m] )  > 10
      level: warning
      expr:  increase( tidb_tikvclient_backoff_count[10m] )  > 10
      summary: TiDB tikvclient_backoff_count error",0,"TiDB mem heap is over 1GiB, TiDB server event error, expr:  increase(tidb_domain_load_schema_total{type=\\, expr:  increase(tidb_monitor_keep_alive_total[10m]) < 100\\n  summary: TiDB monitor_keep_alive error, expr:  increase(tidb_server_panic_total[10m]) > 0\n  summary: TiDB server panic total, expr:  increase(tidb_tikvclient_region_err_total[10m] )  > 6000\\\\n  summary: TiDB tikvclient_backoff_count error, expr: go_memstats_heap_inuse_bytes{job=\, level: warning\\n  expr:  increase(tidb_server_server_event{type=\\, summary: TiDB mem heap is over 1GiB, summary: TiDB server panic total",CWE-754,0
,1,"- name: example, name: example","CWE-119, CWE-190",0,"pulp_webserver_trusted_root_certificates_path: /usr/local/share/ca-certificates/
pulp_webserver_trusted_root_certificates_update_bin: update-ca-certificates
pulp_webserver_python_cryptography: python3-cryptography",0,"pulp_webserver_python_cryptography: python3-cryptography, pulp_webserver_trusted_root_certificates_path: /usr/local/share/ca-certificates/, pulp_webserver_trusted_root_certificates_update_bin: update-ca-certificates","CWE-1024, CWE-1025",0
"- name: Delegate a master control plane node
  block:
    - name: Lookup control node from file
      command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
      changed_when: false
      register: k3s_control_delegate_raw
    - name: Ensure control node is delegated to for obtaining a token
      set_fact:
        k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""
    - name: Ensure the control node address is registered in Ansible
      set_fact:
        k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""",1,"block:, command: 'grep 'P_True' if (k3s_controller_count | length > 1) else 'C_True' /tmp/inventory.txt', name: Ensure control node address is registered in Ansible, name: Ensure the control node address is registered in Ansible, name: Lookup control node from file, register: k3s_control_delegate_raw, set_fact: k3s_control_delegate, set_fact: k3s_control_delegate_raw, set_fact: k3s_control_node_address","CWE-120, CWE-269, CWE-275, CWE-276, CWE-284, CWE-285, CWE-732",1,"when: hostvars[item].k3s_control_node is defined
        and hostvars[item].k3s_control_node
  when: k3s_controller_count is defined
        and k3s_controller_count | length > 1

- name: Ensure ansible_host is mapped to inventory_hostname
  lineinfile:
    path: /tmp/inventory.txt
    line: >-
      {{ item }}
      @@@
      {{ hostvars[item].ansible_host | default(hostvars[item].ansible_fqdn) }}
      @@@
      C_{{ hostvars[item].k3s_control_node }}
      @@@
      P_{{ hostvars[item].k3s_primary_control_node | default(False) }}
    create: true
  loop: ""{{ play_hosts }}""
  when: hostvars[item].k3s_control_node is defined

- name: Lookup control node from file
  command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
  changed_when: false
  register: k3s_control_delegate_raw

- name: Ensure control node is delegated to for obtaining a token
  set_fact:
    k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""

- name: Ensure the control node address is registered in Ansible
  set_fact:
    k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""
  when: k3s_control_node_address is not defined",0,"loop: \\, loop: \\\, loop: \\\\, loop: \\\\\\\\\\\\, loop: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: hostvars[item].k3s_control_node is defined, when: k3s_control_delegate_raw.stdout.split(' @@ ')[0], when: k3s_control_node_address is not defined, when: k3s_controller_count is defined","CWE-213, CWE-214, CWE-215",
"zone: ""{{ dns_domain }}""
    zone: ""{{ dns_domain }}.""
    record: ""master-0.{{ env_id }}.{{ dns_domain }}.""
    zone: ""{{ dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ dns_domain }}.""",1,"zone: \, zone: \\\, zone: \\\\\\, zone: \\\\\\\, zone: \\\\\\\\\\\\, zone: \\\\\\\\\\\\\\\\\\\\\\\\, zone: \\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-22, CWE-25",1,"---
- name: Ensure Route53 zone is present
  route53_zone:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}""
    state: present

- name: Update Route53 with OCP master record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""master.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ master_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode

- name: Update Route53 with OCP infra wildcard record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ infra_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode",0,"command: create\\n    overwrite: yes, route53: {{ aws_access_key }}\n    aws_secret_key: {{ aws_secret_key }}\n    zone: {{ public_dns_domain }}.    \n    record: \, ttl: 300\\n  when: not ha_mode, type: A\\n    value: \, when: not ha_mode\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    when: not ha_mode\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    when: not ha_mode, when: not ha_mode\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    when: not ha_mode\\\\\\\\n    when: not ha_mode, when: not ha_mode\\\\\\\\\\\\\\\\\\n    when: not ha_mode, when: not ha_mode\\\\\\\\n  when: not ha_mode, when: not ha_mode\\\\n    when: not ha_mode, when: not ha_mode\\\\n  when: not ha_mode",CWE-200,0
"- role: rhsm
    - configure_rhsm
    - role: config-bonding
    - role: config-vlans
    - role: config-routes
  tags:
    - configure_infra_hosts_networking

    - role: update-host
  tags:
    - update_host
    - role: config-iscsi-client
    - configure_iscsi_client",1,"role: config-bonding, role: config-iscsi-client, role: config-routes, role: config-vlans, role: rhsm\\n- configure_rhsm, role: update-host\n  tags:","CWE-120, CWE-250, CWE-251, CWE-253, CWE-254, CWE-256, CWE-89",1,"- hosts: aws-provisioner
  roles:
  - aws/manage-networks",0,"- aws/manage-networks, hosts: aws-provisioner, roles:",CWE-1291,0
"body_format: json
    body:
      name: ""{{ atlassian.jira.permission_scheme.name }}""
      description: ""{{ atlassian.jira.permission_scheme.description }}""
      permissions: ""{{ permission_list }}""
  register: permission_scheme_output
    default_permission_scheme_id: ""{{ permission_scheme_output.json.id }}""",1,"body_format: json\nbody:\n  name: \, description: \\, name: \, permission_scheme_output.json.id \\\\\\\\, permission_scheme_output.json.id \\\\\\\\\\\\\\\\\\\\\\n\, permissions: \\, permissions: \\\\, register: permission_scheme_output\\\\n\\\\n\\\\ndefault_permission_scheme_id: \\\\\\\\, register: permission_scheme_output\\n\\n\\ndefault_permission_scheme_id: \\\\",CWE-120,1,"---
- name: Create Jira Permission Scheme
  uri:
    url: ""{{ atlassian.jira.url }}/rest/api/2/permissionscheme""
    method: POST
    user: ""{{ atlassian.jira.username }}""
    password: ""{{ atlassian.jira.password }}""
    return_content: yes
    force_basic_auth: yes
    body_format: json
    header:
      - Accept: 'application/json'
      - Content-Type: 'application/json'
    body: ""{{ lookup('template','permissionScheme.json.j2') }}""
    status_code: 201
  register: permissionScheme

- name: Set fact for Permission Scheme ID
  set_fact:
    PermissionScheme: ""{{ permissionScheme.json.id }}""",0,"body: body_format: force_basic_auth: header: return_content: status_code:, body_format:, force_basic_auth:, return_content: status_code:, uri: body_format: force_basic_auth: header: body:","CWE-1021, CWE-345, CWE-362",0
"---

    - aws/manage-networks",1,"module: dynamodb-table, module: ec2-instance, module: elb, module: iam-role, module: iam-user, module: kms-key, module: network-interface, module: rds, module: s3-bucket, module: sns",CWE-20,0,"- hosts: aws-provisioner
  roles:
  - aws/manage-networks",0,"- hosts: aws-provisioner, hosts: aws-provisioner, hosts: aws\\nprovisioner, hosts:\\naws-provisioner, roles: - aws\\\\nmanage-networks, roles: \\n- aws/manage-networks, roles:\\n- aws/manage-networks, roles:\\n- aws\\\\\\\\nmanage-networks, roles:\n- aws/manage-networks","CWE-264, CWE-312",0
"- name: ""Reset facts to ensure we start over""
  set_fact:
    inventory_id: """"
    project_id: """"
    job_template_id: """"

  - job_template_id is not defined or job_template_id == """"",1,"job_template_id == \, job_template_id == \\, job_template_id == \\\\\\, job_template_id == \\\\\\\\\\\\\\\\\\\\\\\\\\\\, job_template_id == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, job_template_id is not defined or job_template_id == \, job_template_id is not defined or job_template_id == \\, job_template_id is not defined or job_template_id == \\\, job_template_id is not defined or job_template_id == \\\\\\\\\\\\, job_template_id is not defined or job_template_id == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-103,1,"- name: ""Get the job template id based on job template name""
    job_template_id: ""{{ item.id }}""
  - item.name|trim == job_template.name|trim
  - ""{{ existing_job_templates_output.rest_output }}""
  block:
  - name: ""Create job template {{ job_template.name }}""
    uri:
      url: ""{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/job_templates/""
      user: ""{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}""
      password: ""{{ ansible_tower.admin_password }}""
      force_basic_auth: yes
      method: POST
      body: ""{{ lookup('template', 'job-template.j2') }}""
      body_format: 'json'
      headers:
        Content-Type: ""application/json""
        Accept: ""application/json""
      validate_certs: no
      status_code: 200,201,400
    register: job_template_creation_output

  - name: ""Get the created job template id""
    set_fact:
      job_template_id: ""{{ job_template_creation_output.json.id }}""
  when:
  - job_template_id is not defined

- name: ""Add credentials to job template: {{ job_template.name }}""
  include_tasks: process-job-template-credentials.yml
  loop: ""{{ job_template.credentials | default([ job_template.credential|trim ]) }}""
  loop_control:
    loop_var: job_template_credential
  when: (job_template.credentials is defined) or (job_template.credential is defined)",0,"body: \, body: \\, body: \\\, body: \\\\, body: \\\\\\, body: \\\\\\\\\\\, body: \\\\\\\\\\\\\\\\\\\\\\\\, body: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, body: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, body: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-284,0
- docker_install|default(False),1,"ansible_become|default(False), ansible_httpapi_connection, ansible_sftp_host, ansible_sftp_password, ansible_sftp_port, ansible_sftp_username, ansible_ssh_extra_args, ansible_ssh_host, ansible_ssh_user|default(ubuntu), docker_install|default(False)","CWE-256, CWE-257, CWE-272, CWE-384",1,"---

- name: ""Install, configure and enable Docker""
  include: docker.yml
  when: 
  - docker_install|default('no') == ""yes""",0,----,"1104, 1336, 252, 275, 276, 397, 399, 400",0
"dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc""
    dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o""
    dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins""
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc && \
          export GOPATH={{ ansible_env.HOME}}/{{ gopath }} && \
          make BUILDTAGS=""seccomp selinux"" && make install
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o && \
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins && \
    regexp: '^#storage_driver = ""overlay""'
    line: ""{{ item }}""
    insertbefore: '^#storage_option = \['
  with_items:
    - 'storage_option = ['
    - '""overlay2.override_kernel_check=true"",'
    - ']'

- name: change runtime (runc) path
  replace:
    regexp: '^runtime =.*$'
    replace: 'runtime = ""/usr/local/sbin/runc""'
    name: /etc/crio/crio.conf
    backup: yes
          Environment=\""KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio/crio.sock --container-runtime-endpoint /var/run/crio/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'",1,"-\\\\'storage_option = [\\\\\n -\\\\\, -\\\\\\\\\\n -\\\\\\\\\\n -\\\\\\\, Environment=\, cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc && \\nexport GOPATH={{ ansible_env.HOME}}/{{ gopath }} && \\nmake BUILDTAGS=seccomp selinux && make install\\n\\n cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o && \\n cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins && \\n, line: \\\\, name: /etc/crio/crio.conf \\\\\\\, regexp: '^#storage_driver = \\, replace: '^#storage_driver = \, replace: '^runtime = \\\\, with_items: \\\n  -\\'storage_option = \\\",CWE-1205,1,"---

- name: Check Golang version
  shell: /usr/local/go/bin/go version
  register: go_version
  failed_when: false
  changed_when: false

- name: install Golang upstream
  unarchive:
    src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""
    dest: ""/usr/local""
    remote_src: True
  when: '""go1.8.3"" not in go_version.stdout'

- name: Set custom Golang path in RHEL/CentOS/Fedora
  lineinfile:
    dest: /root/.bashrc
    line: 'export PATH=/usr/local/go/bin:$PATH'
    insertafter: 'EOF'
    regexp: 'export PATH=\/usr\/local\/go\/bin:\$PATH'
    state: present

- name: link golang
  file: src=/usr/local/go/bin/go dest=/usr/bin/go state=link

- name: Install required packages for cri-o
  yum:
    name: ""{{ item }}""
    state: latest
  with_items:
    - wget
    - git
    - make
    - gcc
    - tar
    - libseccomp-devel
    - golang
    - go-md2man
    - glib2-devel
    - glibc-static
    - container-selinux
    - btrfs-progs-devel
    - device-mapper-devel
    - glibc-devel
    - gpgme-devel
    - libassuan-devel
    - libgpg-error-devel
    - pkgconfig
    - json-glib-devel
    - skopeo-containers



- name: Make directories
  file:
    path: ""{{ item }}""
    state: directory
  with_items:
       - /usr/local/go
       - /etc/systemd/system/kubelet.service.d/
       - /var/lib/etcd
       - /etc/cni/net.d
       - /etc/containers

- name: clone runc
  git:
    repo: https://github.com/opencontainers/runc
    dest: /root/src/github.com/opencontainers/runc

- name: clone CRI-O
  git:
    repo: https://github.com/kubernetes-incubator/cri-o
    dest: /root/src/github.com/kubernetes-incubator/cri-o
    version: kube-1.6.x

- name: clone CNI
  git:
    repo: https://github.com/containernetworking/plugins
    dest: /root/src/github.com/containernetworking/plugins

- name: build runc
  shell: |
          cd /root/src/github.com/opencontainers/runc && \
          export GOPATH=/root && \
          make BUILDTAGS=""seccomp selinux"" && \
          make install

- name: link runc
  file: src=/usr/local/sbin/runc dest=/usr/bin/runc state=link

- name: build cri-o
  shell: |
          cd /root/src/github.com/kubernetes-incubator/cri-o && \
          make install.tools && \
          make && \
          make install && \
          make install.systemd && \
          make install.config

- name: build CNI stuff
  shell: |
          cd /root/src/github.com/containernetworking/plugins && \
          ./build.sh && \
          mkdir -p /opt/cni/bin && \
          cp bin/* /opt/cni/bin/

- name: run CRI-O with systemd cgroup manager
  replace:
    regexp: 'cgroupfs'
    replace: 'systemd'
    name: /etc/crio/crio.conf
    backup: yes

- name: run with overlay2
  replace:
    regexp: 'storage_driver = """"'
    replace: 'storage_driver = ""overlay2""'
    name: /etc/crio/crio.conf
    backup: yes

- name: add overlay2 storage opts on RHEL/CentOS
  lineinfile:
    dest: /etc/crio/crio.conf
    line: '""overlay2.override_kernel_check=1""'
    insertafter: 'storage_option = \['
    regexp: 'overlay2\.override_kernel_check=1'
    state: present
  
- name: enable and start CRI-O
  systemd:
    name: crio
    state: started
    enabled: yes
    daemon_reload: yes

- name: modprobe br_netfilter
  command: ""modprobe br_netfilter""
  ignore_errors: true

- name: tune sysctl
  lineinfile:
    line: ""net/bridge/bridge-nf-call-iptables = 1""
    dest: /etc/sysctl.conf
    insertafter: 'EOF'
    regexp: '\/net\/bridge\/bridge-nf-call-iptables = 1'
    state: present
  ignore_errors: true

- name: reload sysctl
  command: ""sysctl -p""
  ignore_errors: true

- name: systemd dropin for kubeadm
  shell: |
          sh -c 'echo ""[Service]
          Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'

- name: flush iptables
  command: ""iptables -F""",0,"Line: 103-105, Line: 119-121, Line: 28-36, Line: 41-44, Line: 47-48, Line: 75-76, Line: 81-83, Line: 89, Line: 94, Line: 99",CWE-601,0
crio_version: v1.11.1,1,crio_version: v1.11.1,CWE-732,1,"---

- name: Check Golang version
  shell: /usr/local/go/bin/go version
  register: go_version
  failed_when: false
  changed_when: false

- name: install Golang upstream
  unarchive:
    src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""
    dest: ""/usr/local""
    remote_src: True
  when: '""go1.8.3"" not in go_version.stdout'

- name: Set custom Golang path in RHEL/CentOS/Fedora
  lineinfile:
    dest: /root/.bashrc
    line: 'export PATH=/usr/local/go/bin:$PATH'
    insertafter: 'EOF'
    regexp: 'export PATH=\/usr\/local\/go\/bin:\$PATH'
    state: present

- name: link golang
  file: src=/usr/local/go/bin/go dest=/usr/bin/go state=link

- name: Install required packages for cri-o
  yum:
    name: ""{{ item }}""
    state: latest
  with_items:
    - wget
    - git
    - make
    - gcc
    - tar
    - libseccomp-devel
    - golang
    - go-md2man
    - glib2-devel
    - glibc-static
    - container-selinux
    - btrfs-progs-devel
    - device-mapper-devel
    - glibc-devel
    - gpgme-devel
    - libassuan-devel
    - libgpg-error-devel
    - pkgconfig
    - json-glib-devel
    - skopeo-containers



- name: Make directories
  file:
    path: ""{{ item }}""
    state: directory
  with_items:
       - /usr/local/go
       - /etc/systemd/system/kubelet.service.d/
       - /var/lib/etcd
       - /etc/cni/net.d
       - /etc/containers

- name: clone runc
  git:
    repo: https://github.com/opencontainers/runc
    dest: /root/src/github.com/opencontainers/runc

- name: clone CRI-O
  git:
    repo: https://github.com/kubernetes-incubator/cri-o
    dest: /root/src/github.com/kubernetes-incubator/cri-o
    version: kube-1.6.x

- name: clone CNI
  git:
    repo: https://github.com/containernetworking/plugins
    dest: /root/src/github.com/containernetworking/plugins

- name: build runc
  shell: |
          cd /root/src/github.com/opencontainers/runc && \
          export GOPATH=/root && \
          make BUILDTAGS=""seccomp selinux"" && \
          make install

- name: link runc
  file: src=/usr/local/sbin/runc dest=/usr/bin/runc state=link

- name: build cri-o
  shell: |
          cd /root/src/github.com/kubernetes-incubator/cri-o && \
          make install.tools && \
          make && \
          make install && \
          make install.systemd && \
          make install.config

- name: build CNI stuff
  shell: |
          cd /root/src/github.com/containernetworking/plugins && \
          ./build.sh && \
          mkdir -p /opt/cni/bin && \
          cp bin/* /opt/cni/bin/

- name: run CRI-O with systemd cgroup manager
  replace:
    regexp: 'cgroupfs'
    replace: 'systemd'
    name: /etc/crio/crio.conf
    backup: yes

- name: run with overlay2
  replace:
    regexp: 'storage_driver = """"'
    replace: 'storage_driver = ""overlay2""'
    name: /etc/crio/crio.conf
    backup: yes

- name: add overlay2 storage opts on RHEL/CentOS
  lineinfile:
    dest: /etc/crio/crio.conf
    line: '""overlay2.override_kernel_check=1""'
    insertafter: 'storage_option = \['
    regexp: 'overlay2\.override_kernel_check=1'
    state: present
  
- name: enable and start CRI-O
  systemd:
    name: crio
    state: started
    enabled: yes
    daemon_reload: yes

- name: modprobe br_netfilter
  command: ""modprobe br_netfilter""
  ignore_errors: true

- name: tune sysctl
  lineinfile:
    line: ""net/bridge/bridge-nf-call-iptables = 1""
    dest: /etc/sysctl.conf
    insertafter: 'EOF'
    regexp: '\/net\/bridge\/bridge-nf-call-iptables = 1'
    state: present
  ignore_errors: true

- name: reload sysctl
  command: ""sysctl -p""
  ignore_errors: true

- name: systemd dropin for kubeadm
  shell: |
          sh -c 'echo ""[Service]
          Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'

- name: flush iptables
  command: ""iptables -F""",0,"line: 'export PATH=/usr/local/go/bin:$PATH', line:'storage_option = \[',, regexp: 'cgroupfs', regexp:'storage_driver =","CWE-1200 (CWE-1200, CWE-127, CWE-129, CWE-601 (Missing Permissions to Execute Code from Untrusted Source)",0
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""",1,"OS::Cinder::Volume, OS::Cinder::VolumeAttachment, OS::Glance::Image, OS::Heat::CloudFormationStack, OS::Heat::Deployment, OS::Heat::ResourceGroup, OS::Heat::SoftwareConfig, OS::TripleO::Services::NovaPlacement","CWE-116, CWE-276, CWE-601, CWE-602, CWE-787, CWE-89",1,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",0,"OS::TripleO::Services::Cinder, OS::TripleO::Services::NeutronLbaasV2, OS::TripleO::Services::NovaPlacement, OS::TripleO::Services::PlacementApi",CWE-1206,0
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""",1,"NovaPlacement, OS::TripleO::Services::NovaPlacement",CWE-787,1,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",0,"OS::TripleO::Services::NovaPlacement, OS::TripleO::Services::NovaPlacement{% endif %}, OS::TripleO::Services::PlacementApi, install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16, install.version|default(undercloud_version)|openstack_release > 15, openstack_release > 10 and openstack_release < 16","CWE-1003, CWE-1028, CWE-1050, CWE-1123, CWE-1147, CWE-1199, CWE-1262, CWE-1433, CWE-190",0
"fp_ip_address_from: ""{{ opstools_external_ip.stdout }}""",1,"fp_ip_address_from: \t\t\, opstools_external_ip.stdout","CWE-103, CWE-190, CWE-327",1,"- name: run port forwarding role
  include_role:
      name: network/forward-port
  vars:
      fp_ip_address_from: ""{{ opstools_management_ip.stdout }}""
      fp_destination_port: 8081
  delegate_to: hypervisor
  when:
      - ""'hypervisor' in groups""
      - install.overcloud.opstools_forward|default(True)",0,"fp_destination_port: 8081\\\\\\\\\\\\\\\\n  delegate_to: hypervisor\\\\\\\\\\\\\\\\n  when:\\\\\\\\\\\\\\\\\\\\\\n      - 'hypervisor' in groups\\\\\\\\\\\\\\\\\\\\n      - install.overcloud.opstools_forward|default(True), fp_destination_port: 8081\\\\\\\\n  delegate_to: hypervisor\\\\\\\\n  when:\\n      - 'hypervisor' in groups\\\\\\\\n      - install.overcloud.opstools_forward|default(True), fp_destination_port: 8081\\\\n  delegate_to: hypervisor\\\\n  when:\\\\\\\\\\\\\\\\\\n      - 'hypervisor' in groups\\\\\\\\\\\\\\\\n      - install.overcloud.opstools_forward|default(True), fp_destination_port: 8081\\\\n  delegate_to: hypervisor\\\\n  when:\\\\\\\\\\n      - 'hypervisor' in groups\\\\\\\\\\\\n      - install.overcloud.opstools_forward|default(True), fp_destination_port: 8081\\\\n  delegate_to: hypervisor\\\\n  when:\\\\n      - 'hypervisor' in groups\\\\n      - install.overcloud.opstools_forward|default(True), fp_destination_port: 8081\\\\n  delegate_to: hypervisor\\\\n  when:\\n      - 'hypervisor' in groups\\\\n      - install.overcloud.opstools_forward|default(True), fp_destination_port: 8081\\n  delegate_to: hypervisor\\n  when:\\n      - 'hypervisor' in groups\\n      - install.overcloud.opstools_forward|default(True), fp_ip_address_from: \, name: run port forwarding role\\n  include_role:\\n      name: network/forward-port\\n  vars:\\n      fp_ip_address_from: \\, name: run port forwarding role\n  include_role:\n      name: network/forward-port\n  vars:\n      fp_ip_address_from: \","CWE-125, CWE-129, CWE-25",1
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""",1,OS::TripleO::Services::NovaPlacement,"CWE-1200, CWE-1201, CWE-1203, CWE-1207, CWE-1208, CWE-1209, CWE-1212, CWE-1214, CWE-1215, CWE-1216",1,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",0,"OS::TripleO::Services::NovaPlacement, OS::TripleO::Services::PlacementApi","CWE-116, CWE-120, CWE-798, CWE-949",1
"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""",1,OS::TripleO::Services::NovaPlacement,CWE-937,1,"- ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}""",0,"install.version|default(undercloud_version)|openstack_release < 16, install.version|default(undercloud_version)|openstack_release > 10, install.version|default(undercloud_version)|openstack_release > 15",CWE-749,1
"- name: get the vlan number where external network should be served
    - name: create new vlan interface in ovs system
    - name: get the IP address for the external network interface
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalInterfaceDefaultRoute | awk -F' ' '{print $2}'""
      register: stat_ip_result
    - name: configure external gateway's IP for this interface
      shell: ""sudo ip addr add {{ stat_ip_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""

    - debug: var={{ iface_ip_result }}


    - name: get cidr of the external network
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalNetCidr | awk -F' ' '{print $2}'""
      register: route_result

    - name: add new static route for external network
      shell: ""sudo ip route add {{ route_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""",1,"debug: var={{ iface_ip_result }}, shell:","CWE-122, CWE-127, CWE-787",1,"---
- set_fact:
      isolation_file: ""network-isolation{{ (installer.network.protocol == 'ipv6') | ternary('-v6','') }}.yaml""

- name: append the network environment template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/network/network-environment.yaml \'

- name: append the network isolation template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/{{ isolation_file }} \'

- block:
    - name: get the IP address on the external network default route
      shell: ""cat {{ template_base }}/network/network-environment.yaml | grep ExternalNetworkVlanID | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(\""'\"",'') }} tag={{ result.stdout | replace(\""'\"",'') }} -- set interface vlan{{ result.stdout | replace(\""'\"",'') }} type=internal;""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""

    - name: get the IP address on the external network default route
      shell: ""cat {{ installer.overcloud.template_base }}/network-environment.yaml | grep ExternalNetCidr | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ip addr add {{ result.stdout | replace(\""'\"",'') }}/{{ (installer.network.protocol == 'ipv6') | ternary('64','24') }} dev {{ ansible_interfaces | first }}""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""
  when: installer.network.backend == 'vlan'",0,"cat {{ installer.overcloud.template_base }}/network-environment.yaml | grep ExternalNetCidr | awk -F':' '{print $2}' | sed's/[^0-9.]//', sudo ip addr add {{ result.stdout | replace(\\, sudo ip addr add {{ result.stdout | replace(\\\\, sudo ip addr add {{ result.stdout | replace(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(\, sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(\\, sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(\\\\\\\\\\\\",CWE-758,1
"command: ""sosreport --batch -e openstack_ironic --batch -p openstack,openstack_undercloud,openstack_controller --all-logs --experimental -k docker.all=on --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""",1,sosreport,"CWE-119, CWE-120, CWE-121, CWE-122, CWE-123, CWE-124, CWE-125, CWE-126, CWE-127, CWE-128",1,"---
- name: Define sosrep folder
  set_fact:
      tmp_sos_dir: '/var/sosrep'

- name: Make sure that sosreport is installed on the host
  yum:
      name: sos
      state: latest

- name: Cleanup sosreport dir if exists
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent

- name: Create dir for sosreport
  file:
      name: ""{{ tmp_sos_dir }}""
      state: directory
      mode: 0644

- name: Collect sosreport logs
  command: ""sosreport --batch --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""
  register: sosreport_result
  failed_when: ""'sosreport has been generated' not in sosreport_result.stdout""

- name: get name of the generated file
  find:
      paths: ""{{ tmp_sos_dir }}""
      patterns: ""sosreport-{{ inventory_hostname }}-*.tar.gz""
      recurse: no
  register: sosreport_file

- name: fetch sosreport archive
  fetch:
      flat: yes
      src: ""{{ item.path }}""
      dest: ""{{ dest_dir }}/""
      validate_checksum: no
  ignore_errors: true
  with_items: ""{{ sosreport_file.files }}""

- name: Cleanup sosreport dir after run
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent",0,"sosreport --batch --tmp-dir /var/sosrep --name={{ inventory_hostname }} -z gzip, sosreport --batch --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip","CWE-20, CWE-601, CWE-77, CWE-89",1
,1,"hosts:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, hosts:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - 192.168.0.12\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, hosts:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - 192.168.0.11\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, hosts:\\\\\\\\\\\\\\\\\\\\\\\\\\n  - 192.168.0.10\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, hosts:\\\\\\\\\\n  - 192.168.0.9\\\\\\\\\\\\n, hosts:\\\\n  - 192.168.0.8\\\\n, hosts:\\n  - 192.168.0.6\\n, hosts:\\n  - 192.168.0.7\\n, hosts:\n  - 192.168.0.5\n",CWE-312,1,"- include_tasks: pre.yml
        delegate_to: ""{{  vbmc_inventory_host }}""

- block:
      - include_tasks: check.yml
        when: action == 'check'
      - include_tasks: cleanup.yml
        when: action == 'cleanup'
      - include_tasks: remove.yml
        when: action == 'remove'
  delegate_to: ""{{  vbmc_inventory_host }}""",0,"- include_tasks: pre.yml \n  delegate_to: \\, delegate_to: \\\, delegate_to: \\\\\, delegate_to: \\\\\\\\\\\, delegate_to: \\\\\\\\\\\\\\\\\\\\\\\\, delegate_to: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-1037, CWE-275",1
"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""",1,if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %},"CWE-1130, CWE-1292, CWE-1296, CWE-1297, CWE-1299, CWE-1300, CWE-1301, CWE-1302, CWE-1303, CWE-1304",1,"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhApi
        - OS::TripleO::Services::AodhEvaluator
        - OS::TripleO::Services::AodhNotifier
        - OS::TripleO::Services::AodhListener
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendSimpleCrypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendDogtag{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendKmip{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendPkcs11Crypto{% endif %}""
        - OS::TripleO::Services::CACerts
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerCollector{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerExpirer{% endif %}""
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::CephMds{% endif %}""
        - OS::TripleO::Services::CephMon
        - OS::TripleO::Services::CephExternal
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::CephMgr{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::CephRbdMirror{% endif %}""
        - OS::TripleO::Services::CephRgw
        - OS::TripleO::Services::CinderApi
        - OS::TripleO::Services::CinderScheduler
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 11 %}OS::TripleO::Services::Core{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - OS::TripleO::Services::Fluentd{% if install.version|default(undercloud_version) |openstack_release < 13 %}Client{% endif %}
        - OS::TripleO::Services::GlanceApi
        - ""{% if install.version|default(undercloud_version) |openstack_release < 11 %}OS::TripleO::Services::GlanceRegistry{% endif %}""
        - OS::TripleO::Services::GnocchiApi
        - OS::TripleO::Services::GnocchiMetricd
        - OS::TripleO::Services::GnocchiStatsd
        - OS::TripleO::Services::HeatApi
        - OS::TripleO::Services::HeatApiCfn
        - ""{% if install.version|default(undercloud_version) |openstack_release < 12 %}OS::TripleO::Services::HeatApiCloudwatch{% endif %}""
        - OS::TripleO::Services::HeatEngine
        - OS::TripleO::Services::Horizon
        - OS::TripleO::Services::IronicApi
        - OS::TripleO::Services::IronicConductor
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::IronicPxe{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - OS::TripleO::Services::Keepalived
        - OS::TripleO::Services::Kernel
        - OS::TripleO::Services::Keystone
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - OS::TripleO::Services::ManilaApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendIsilon{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendUnity{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendVMAX{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendVNX{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::ManilaBackendGeneric{% endif %}""
        - OS::TripleO::Services::ManilaBackendNetapp
        - OS::TripleO::Services::ManilaBackendCephFs
        - OS::TripleO::Services::ManilaScheduler
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::MySQLClient{% endif %}""
        - OS::TripleO::Services::Memcached
        - OS::TripleO::Services::NeutronApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::NeutronBgpVpnApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::NeutronSfcApi{% endif %}""
        - OS::TripleO::Services::NeutronCorePlugin
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::NeutronL2gwApi{% endif %}""
        - OS::TripleO::Services::NovaApi
        - OS::TripleO::Services::NovaConductor
        - OS::TripleO::Services::NovaConsoleauth
        - OS::TripleO::Services::NovaIronic
        - OS::TripleO::Services::NovaMetadata
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::NovaPlacement{% endif %}""
        - OS::TripleO::Services::NovaScheduler
        - OS::TripleO::Services::NovaVncProxy
        - OS::TripleO::Services::Ntp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaDeploymentConfig{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHealthManager{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHousekeeping{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaWorker{% endif %}""
        - OS::TripleO::Services::OpenDaylightApi
        - OS::TripleO::Services::OpenDaylightOvs
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNDBs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNController{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::PankoApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - OS::TripleO::Services::SaharaApi
        - OS::TripleO::Services::SaharaEngine
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - OS::TripleO::Services::SensuClient
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAgent{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAnalyzer{% endif %}""
        - OS::TripleO::Services::SwiftProxy
        - OS::TripleO::Services::SwiftStorage
        - OS::TripleO::Services::SwiftRingBuilder
        - OS::TripleO::Services::Snmp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::TripleoPackages
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""",0,"OS::TripleO::Services::AodhNotifier, OS::TripleO::Services::CeilometerAgentCentral, OS::TripleO::Services::CephRgw, OS::TripleO::Services::CinderApi, OS::TripleO::Services::CinderScheduler, OS::TripleO::Services::GlanceApi",CWE-306,1
"- name: append the ceph storage flavor name to the base overcloud deploy script
  when:
    - ""not install.storage.external""
    - ""not install.splitstack""
  when:
    - install.storage.config == 'internal'

- name: append the storage ceph template in splitstack deployment
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'
  when:
    - ""install.splitstack""
    - install.version|default(undercloud_version)|openstack_release < 12",1,"install.splitstack:, install.storage.config:, install.version:, line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \', lineinfile:, name:, name: append the ceph storage flavor name to the base overcloud deploy script, name: append the storage ceph template in splitstack deployment, openstack_release:, when:","CWE-327, CWE-502",1,"- block:
    - name: import ceph facts
      set_fact:
          ceph_facts: ""{{ lookup('file', '{{ inventory_dir }}/{{ hostvars[(groups.ceph|first)].inventory_hostname }}') }}""

    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-scale {{ storage_nodes }} \'
      when: ""templates.storage_add_scale | default(True)""

    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-flavor {% if groups[""ceph""] is defined %}ceph{% else %}baremetal{% endif %} \'
      when: ""templates.storage_add_scale | default(True)""

  when: ""not install.storage.external""
  vars:
      storage_nodes: ""{{ (install.storage.nodes|default(0)) or (groups['ceph']|default([])|length) or 1 }}""

- name: append the storage template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'

- name: prepare ceph storage template
  vars:
      ceph_compt_version: ""
          {%- if install.version|openstack_release < 8 -%}8
          {%- elif install.version|openstack_release >= 10 -%}10
          {%- else -%}{{ install.version|openstack_release }}{%- endif -%}""
  template:
      src: ""storage/ceph.yml.j2""
      dest: ""{{ template_base }}/ceph.yaml""
      mode: 0755

- name: append the storage ceph custom template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/ceph.yaml \'",0,"dest:, dest: '~/overcloud_deploy.sh',, dest: '~/overcloud_deploy.sh\\n    ',, line: '--ceph-storage-flavor {% if groups[\\\, line: '--ceph-storage-scale {{ storage_nodes }} \\n, line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \\', line: '-e {{ template_base }}/ceph.yaml \\\\', line: '{{ template_base }}/ceph.yaml \\\\n    ',, template: \\\n      src: \\\n          \\\n          {\\\\n              {% if install.version|openstack_release < 8 -%}8\\\\n              {% elif install.version|openstack_release >= 10 -%}10\\\\n              {\\\\n              % endif -%} {{ install.version|openstack_release }}{%- endif -%}, when: 'templates.storage_add_scale | default(True)',","CWE-1205, CWE-1206, CWE-1207, CWE-1208, CWE-1209, CWE-1210, CWE-1211, CWE-1212, CWE-1213, CWE-1214",1
"state: ""{{ ir_default_pip_versions[item] is defined | ternary('present', 'latest') }}""",1,"state: {{ ir_default_pip_versions[item] is defined | ternary('present', 'latest') }}","CWE-732, CWE-779",1,"- name: Include configuration vars
  include_vars: ""vars/config/{{ test.setup }}.yml""

- name: Clone tempest_conf
  git:
      repo: ""{{ tempest_conf.repo }}""
      version: ""{{ tempest_conf.revision | default(omit) }}""
      dest: ""{{ tempest_conf.dir }}""
  register: tempest_conf_repo

- name: Create tempest conf venv with latest pip, setuptools and pbr
  pip:
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      name: ""{{ item }}""
      state: latest
  with_items:
      - pip
      - setuptools
      - pbr

- name: Install tempest_conf
  pip:
      name: "".""
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      chdir: ""{{ tempest_conf.dir }}""

- name: Init tempest
  shell: |
      source .venv/bin/activate
      tempest init ""~/{{ test.dir }}""
  args:
      chdir: ""{{ tempest_conf.dir }}""
      creates: ""~/{{ test.dir }}/etc""

- name: Set facts for configuration run
  set_fact:
      config_command: ""discover-tempest-config""
      config_dir: ""{{ tempest_conf.dir }}""",0,"shell: \, shell: \\, shell: \\\\, shell: \\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\, shell: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-22,1
"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhListener
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendSimpleCrypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendDogtag{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendKmip{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendPkcs11Crypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::CephMgr{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CinderBackendVRTSHyperScale{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Clustercheck{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Congress{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ec2Api{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Etcd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::IronicPxe{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::ManilaBackendGeneric{% endif %}""
        - OS::TripleO::Services::ManilaBackendNetapp
        - OS::TripleO::Services::ManilaBackendCephFs
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::NeutronSfcApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaDeploymentConfig{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHealthManager{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHousekeeping{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaWorker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNDBs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNController{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAgent{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAnalyzer{% endif %}""
        - ""{% if roles_sshd %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::SwiftRingBuilder
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Tacker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Vpp{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""",1,"BarbicanBackendDogtag, BarbicanBackendKmip, BarbicanBackendPkcs11Crypto, BarbicanBackendSimpleCrypto, CephMgr, ContainersLogrotateCrond, LoginDefs, NeutronSfcApi, OctaviaApi, VirtStorageBackendCeph","CWE-120: Use of Uninitialized Resources, CWE-20: Improper Input Validation, CWE-285: Improper Authorization, CWE-732: Improper Loop Condition, CWE-863: Improper Check of a Loop Condition",1,"networks:
        - InternalApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""",0,"CWE-119: Improper Restriction of Operative System Tools or Interaction, CWE-22: Improper Control Flow Management, CWE-275: Improper Control of Resource Identification and Modification, CWE-319: Cleartext Transmission of Confidential Information, CWE-327: Use of a Broken or Risky Cryptographic Algorithm, CWE-329: Resource Management Errors, CWE-522: Insufficiently Random Salt, CWE-601: URL Redirection to Data Leak, CWE-79: Improper Neutralization of Special Elements used in a Command (Command Injection)","CWE-119, CWE-22, CWE-275, CWE-319, CWE-327, CWE-329, CWE-522, CWE-601, CWE-79",1
"OS::TripleO::Galera::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/galera_internal.yaml""",1,"OS::TripleO::Galera::Net::SoftwareConfig: \, OS::TripleO::Galera::Net::SoftwareConfig: \\, OS::TripleO::Galera::Net::SoftwareConfig: \\\\, OS::TripleO::Galera::Net::SoftwareConfig: \\\\\\\\, OS::TripleO::Galera::Net::SoftwareConfig: \\\\\\\\\\\\\, OS::TripleO::Galera::Net::SoftwareConfig: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, OS::TripleO::Galera::Net::SoftwareConfig: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-20, CWE-200",1,"galera_role:
    name: Galera
        OS::TripleO::Galera::Net::SoftwareConfig: ${deployment_dir}/network/nic-configs/galera_internal.yaml
        OS::TripleO::Galera::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api${ipv6_postfix_underscore}.yaml
    flavor: galera
    host_name_format: 'galera-%index%'",0,"OS::TripleO::Galera::Net::SoftwareConfig: ${deployment_dir}/network/nic-configs/galera_internal.yaml, OS::TripleO::Galera::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api${ipv6_postfix_underscore}.yaml, flavor: galera, host_name_format: 'galera-%index%', name: Galera","CWE-1035, CWE-1046, CWE-1102, CWE-113, CWE-1143, CWE-384",1
cp {{ base_image_path }}/{{ base_image }} {{ base_image_path }}/{{ node_image }},1,cp {{ base_image_path }}/{{ base_image }} {{ base_image_path }}/{{ node_image }},"CWE-117, CWE-119, CWE-120, CWE-125, CWE-19, CWE-20, CWE-79",1,"---
- name: create disk(s) from vm base image
  shell: |
      {% for num in range(1, item.value.amount + 1, 1) %}
      {% for disk_name, disk_values in item.value.disks.iteritems() %}
      {% set template_image = '{0}-{1}.qcow2'.format(item.value.name, disk_name) %}
      {% set node_image = '{0}-{1}-{2}.qcow2'.format(item.value.name, num - 1, disk_name) %}
      {% if not disk_values.import_url %}

      cp {{ base_image_path }}/{{ template_image }} {{ base_image_path }}/{{ node_image }}
      qemu-img create -f qcow2 -o preallocation={{ disk_values.preallocation }} {{ disk_values.path  }}/{{ node_image }} {{ disk_values.size }}
      {% if disk_name == 'disk1' -%}
      virt-resize --expand /dev/sda1 {{ base_image_path }}/{{ base_image }} {{ disk_values.path }}/{{ node_image }}
      virt-customize -a {{ disk_values.path }}/{{ node_image }} \
      {%- for index in range(item.value.interfaces | length - 1) %}
          --run-command 'cp /etc/sysconfig/network-scripts/ifcfg-eth{0,{{ index + 1 }}} && sed -i s/DEVICE=.*/DEVICE=eth{{ index + 1 }}/g /etc/sysconfig/network-scripts/ifcfg-eth{{ index +1 }}' {% if not loop.last %}\
          {% endif %}
      {% endfor %}
      {% endif %}
      {% endif %}
      {% endfor %}
      {% endfor %}
  with_dict: ""{{ provisioner.topology.nodes }}""
  register: ""vm_disks""
  async: 7200
  poll: 0

- name: Wait for our disks to be created
  async_status:
      jid: ""{{ item.ansible_job_id }}""
  register: disk_tasks
  until: disk_tasks.finished
  retries: 300
  with_items: ""{{ vm_disks.results }}""",0,the actual misconfigured code snippet,the related CWE ID,1
"- {role: ""installer/ospd/loadbalancer/"", when: 'loadbalancer' in groups}",1,when: 'loadbalancer' in groups},CWE-601,1,"- {role: ""installer/ospd/loadbalancer/"", when: provisioner.topology.nodes.loadbalancer is defined }",0,", - {role: \, ], installer/ospd/loadbalancer/, when: provisioner.topology.nodes.loadbalancer is defined \\, }",CWE-732,0
"stdout: ""{{ (network_template|selectattr('name_lower','equalto','external')|first).ip_subnet }}""",1,"first), selectattr('\\'equalto', 'external')|first), stdout: \","CWE-758, CWE-759",1,"when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the external vlan id from the network_environment_file
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      vlan_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'equalto', 'external')|first).vlan }}""
  when: use_network_data|bool
  when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the cidr of the external network from network_data
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      route_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'external')|first).ip_subnet }}""
  when: use_network_data|bool
  when: ansible_os_family == ""RedHat""",0,"name: Get the cidr of the external network from network_data, name: Get the external vlan id from the network_environment_file, when: not use_network_data|bool, when: use_network_data|bool\nwhen: not use_network_data|bool","CWE-1021, CWE-1022, CWE-1024, CWE-1035, CWE-601",0
"when: ""'virthost' in groups""",1,"when:  \\\\\\\\\\\\\\\\u0027virthost\\\\\\\\\\\\\\\\u0027 in groups, when:  \\\\\\\\u0027virthost\\\\\\\\u0027 in groups, when:  \\\\u0027virthost\\\\u0027 in groups, when:  \\u0027virthost\\u0027 in groups, when:  \u0027virthost\u0027 in groups, when: \\u0027virthost\\u0027 in groups",CWE-1031,0,"when: ""groups.virthost is defined""",0,"when: \, when: \\, when: \\\\, when: \\\\\\\\, when: \\\\\\\\\\\\\\\\\\\\, when: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-1030,0
"OS::TripleO::Telemetry::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/${nics_subfolder}/messaging_internal.yaml""
        OS::TripleO::Telemetry::Ports::InternalApiPort: ""{{ install.heat.templates.basedir }}/network/ports/internal_api.yaml""",1,"OS::TripleO::Telemetry::Net::SoftwareConfig:, OS::TripleO::Telemetry::Ports::InternalApiPort:, OS::TripleO::Telemetry::Ports::InternalApiPort:,, install.heat.templates.basedir /network/ports/internal_api.yaml, network/nic-configs/${nics_subfolder}/messaging_internal.yaml","CWE-103, CWE-189, CWE-190, CWE-275, CWE-287",1,"telemetry_role:
    name: Telemetry

    resource_registry:
        OS::TripleO::Messaging::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/messaging_internal.yaml""
        OS::TripleO::Messaging::Ports::InternalApiPort: ""{{ install.heat.templates.basedir }}/network/ports/internal_api.yaml""
    flavor: telemetry
    networks:
        - InternalApi
    host_name_format: 'telemetry-%index%'

    services:
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhApi
        - OS::TripleO::Services::AodhEvaluator
        - OS::TripleO::Services::AodhListener
        - OS::TripleO::Services::AodhNotifier
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::AuditD{% endif %}""
        - OS::TripleO::Services::CACerts
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerCollector{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerExpirer{% endif %}""
        - OS::TripleO::Services::CephClient
        - OS::TripleO::Services::CephExternal
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - OS::TripleO::Services::Collectd
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - OS::TripleO::Services::Fluentd{% if install.version|default(undercloud_version) |openstack_release < 13 %}Client{% endif %}
        - OS::TripleO::Services::GnocchiApi
        - OS::TripleO::Services::GnocchiMetricd
        - OS::TripleO::Services::GnocchiStatsd
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - OS::TripleO::Services::Kernel
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::MongoDb{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::MySQLClient{% endif %}""
        - OS::TripleO::Services::Ntp
        - OS::TripleO::Services::Pacemaker
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::PankoApi{% endif %}""
        - OS::TripleO::Services::Redis
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - OS::TripleO::Services::SensuClient
        - OS::TripleO::Services::Snmp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::TripleoPackages
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""",0,"OS::TripleO::Services::AodhEvaluator, OS::TripleO::Services::CeilometerApi, OS::TripleO::Services::CeilometerExpirer, OS::TripleO::Services::CephExternal, OS::TripleO::Services::Pacemaker, OS::TripleO::Services::RsyslogSidecar, host_name_format: 'telemetry-%index%', resources\n    resource_registry:, services\\\\n    - OS::TripleO::Services::AodhNotifier, services\\n    - OS::TripleO::Services::GnocchiMetricd",CWE-345,1
"shell: |
            test -e .venv/bin/activate && source .venv/bin/activate
            subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html
            test -e .venv/bin/activate && source .venv/bin/activate",1,"subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html, test -e.venv/bin/activate && source.venv/bin/activate","CWE-77, CWE-78",1,"test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ inventory_dir }}/tempest_results/tempest-results-{{ test_suite }}.*.subunit""
      test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ test_output_filename }}.subunit""
      - ""{{ test_output_filename }}.xml""
      - ""{{ test_output_filename }}.html""
            > {{ test_output_filename }}.subunit;
            testr run {{ ('--parallel --concurrency=' + test.threads|string) if test.threads|default('') else '' }} --subunit '{{ '|'.join(parts) }}' > {{ test_output_filename }}.subunit
      - name: generate results report in HTML format
        command: ""subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: generate results report in JunitXML format
        shell: |
            subunit2junitxml --output-to={{ test_output_filename }}.xml \
                < {{ test_output_filename }}.subunit | subunit2pyunit
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.junitxml is defined

        command: ""sed '/^<testsuite/s/name=\""\""/name=\""{{ test_suite }}\""/' -i {{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined
      - name: add the test suite name in the HTML report title
        command: ""sed 's#<title>\\(.*Test Report\\)</title>#<title>\\1 - {{ test_suite }}</title>#' \
                  -i {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: Fetch subunit results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.subunit""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.subunit""
            flat: yes
            fail_on_missing: yes

      - name: Fetch JUnit XML results file
            src: ""{{ test.dir }}/{{ test_output_filename }}.xml""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined

      - name: Fetch HTML results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.html""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.html""
            flat: yes
            fail_on_missing: no
        when:
            results_formats.html is defined",0,"sed '/^<testsuite/s/name=\, sed '/^<testsuite/s/name=\\, sed's#<title>\\(.*Test Report\\)</title>#<title>\\1 - {{ test_suite }}</title>#' -i {{ test_output_filename }}.html, sed's#<title>\\\\(.*Test Report\\\\)</title>#<title>\\\\1 - {{ test_suite }}</title>#' -i {{ test_output_filename }}.html, subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html, subunit2junitxml --output-to=\\\\, subunit2junitxml --output-to={{ test_output_filename }}.xml < {{ test_output_filename }}.subunit | subunit2pyunit, testr run {{ ('--parallel --concurrency=' + test.threads|string) if test.threads|default('') else '' }} --subunit '{{ '|'.join(parts) }}' > {{ test_output_filename }}.subunit",CWE-772,1
"path: ""{{ overcloud_deploy_script|default('~/overcloud_deploy.sh') }}""",1,"path: \, path: \\, path: \\\, path: \\\\, path: \\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-1201, CWE-1238, CWE-1260",1,"---
- name: Unsupported OS version on undercloud
  fail:
      msg: ""InfraRed supports updates for OpenStack version 11""
  when:
      - not (undercloud_version == ""11"")

- name: Checking overcloud_deploy_file
  stat:
      path: ""~/overcloud_deploy.sh""
  register: overcloud_deploy_file

- name: Deployment script not found
  fail:
      msg: ""Overcloud deployment script not found. Expected path: ~/overcloud_deploy.sh ""
  when: not overcloud_deploy_file.stat.exists or not overcloud_deploy_file.stat.executable

- name: Checking file with overcloud credentials
  stat:
      path: ""~/overcloudrc.v3""
  register: oc_credentials_file

- name: Overcloud RC not found
  fail:
      msg: ""File with OC credentials not found. Expected path: ~/overcloudrc.v3""
  when: not oc_credentials_file.stat.exists or not oc_credentials_file.stat.readable",0,"Checking file with overcloud credentials, Checking overcloud_deploy_file, Deployment script not found, Overcloud RC not found, Unsupported OS version on undercloud","CWE-257, CWE-258",0
"""OS::TripleO::CephStorage::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""",1,OS::TripleO::CephStorage::Net::SoftwareConfig,"CWE-190, CWE-285, CWE-287",1,"---
ceph_role:
    name: CephStorage

    resource_registry:
        ""OS::TripleO::Compute::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""

    flavor: ceph
    host_name_format: 'ceph-%index%'

    services:
        - OS::TripleO::Services::CACerts
        - OS::TripleO::Services::CephOSD
        - OS::TripleO::Services::Kernel
        - OS::TripleO::Services::Ntp
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoPackages
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::SensuClient
        - OS::TripleO::Services::FluentdClient
        - OS::TripleO::Services::VipHosts",0,"OS::TripleO::Services::CACerts, OS::TripleO::Services::FluentdClient, OS::TripleO::Services::Kernel, OS::TripleO::Services::Ntp, OS::TripleO::Services::SensuClient, OS::TripleO::Services::Timezone, OS::TripleO::Services::TripleoFirewall, OS::TripleO::Services::TripleoPackages, OS::TripleO::Services::VipHosts, name: CephStorage",CWE-732,1
"- ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""",1,"OS::TripleO::Services::Aide, OS::TripleO::Services::AuditD, OS::TripleO::Services::Collectd, OS::TripleO::Services::Ipsec, OS::TripleO::Services::LoginDefs, OS::TripleO::Services::RsyslogSidecar, OS::TripleO::Services::Tuned","CWE-256, CWE-268, CWE-269, CWE-275, CWE-276",1,"networks:
        - InternalApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""",0,"OS::TripleO::Services::CinderBackup, OS::TripleO::Services::Docker, OS::TripleO::Services::Galera, OS::TripleO::Services::Glance, OS::TripleO::Services::HAProxy, OS::TripleO::Services::Memcached, OS::TripleO::Services::NovaNeutronAgent, OS::TripleO::Services::Pacemaker, OS::TripleO::Services::PtpGratuitousTolerance, OS::TripleO::Services::TripleOKeystoneService",CWE-25,1
"- name: get controller flavor
  shell: ""source ~/stackrc; openstack flavor list -c Name -f value | grep 'controller-'""
  register: controller_flavor

  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local'""",1,"shell: source ~\/stackrc; ironic node-update {{ item.1 }} replace properties\/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local', shell: source ~\\/stackrc; ironic node-update {{ item.1 }} replace properties\\/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local', shell: source ~\\\\/stackrc; ironic node-update {{ item.1 }} replace properties\\\\/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local', shell: source ~\\\\\\\\\\\\/stackrc; ironic node-update {{ item.1 }} replace properties\\\\\\\\\\\\/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local', shell: source ~\\\\\\\\\\\\\\\\\\\\\\\\\\\\/stackrc; ironic node-update {{ item.1 }} replace properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local', shell: source ~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\/stackrc; ironic node-update {{ item.1 }} replace properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local', shell: source ~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\/stackrc; ironic node-update {{ item.1 }} replace properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\/capabilities='profile:{{ controller_flavor.stdout }},node:, shell: source ~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\/stackrc; ironic node-update {{ item.1 }} replace properties\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, shell: source ~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-778,1,"- name: set additional propertiesx
  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:baremetal,node:controller-{{ item.0 }},boot_option:local'""
  with_indexed_items: ""{{ groups['controller'] }}""",0,"boot_option:local\\\, capabilities='profile:baremetal\\\\, groups[\'controller\']\n    \n  }, item.0 }},\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, profile:baremetal,\\\\\\\\\\\\\\, profile:baremetal\\\\\\, source ~/stackrc; ironic node-update {\\\\{ item.1 \\\\} replace properties/capabilities=\\\\', source ~/stackrc; ironic node-update {\\{ item.1 \\} replace properties/capabilities='profile:baremetal,boot_option:local', with_indexed_items:, with_indexed_items: \","CWE-20, CWE-807, CWE-862, CWE-863",0
"- name: Override postgres params for CentOs or RedHat when ovirt >= 4.2
    - ansible_distribution in ('CentOS', 'RedHat')
    - ansible_distribution in ('CentOS', 'RedHat')",1,"- ansible_distribution in ('CentOS', 'RedHat'), ansible_distribution in ('CentOS', 'RedHat')",CWE-758,,"- name: Include postgres params
  include_vars: default.yml

- name: Override postgres params for CentOs or Red Hat when ovirt >= 4.2
  include_vars: postgres95.yml
  when:
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')

- name: install psycopg2 requirements to run ansible modules managing postgres.
  yum:
    name: ""python-psycopg2""
    state: ""present""

    name: ""{{ postgres_service_name }}""
    name: ""{{ postgres_server }}""
- name: scl enable
  shell: 'scl enable rh-postgresql95 bash'
  when:
    - postgresql_status|failed
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version < '4.2'
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
  args:
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version >= '4.2'
    name: ""{{ postgres_service_name }}""
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: ""{{ postgres_config_file }}""
    dest: ""{{ postgres_config_file }}""
    dest: '/usr/lib/systemd/system/{{ postgres_service_name }}.service'
    path: ""{{ postgres_config_file }}""
    name: ""{{ postgres_service_name }}""
- name: create DWH DB user
  become: true
  postgresql_user:
    name: ""{{ item.user }}""
    password: ""{{ item.password }}""
    - user: ""{{ ovirt_engine_db_user }}""
      password: ""{{ ovirt_engine_db_password }}""
    - user: ""{{ ovirt_engine_dwh_db_user }}""
      password: ""{{ ovirt_engine_dwh_db_password }}""
  when: ovirt_engine_dwh_remote_db == True
- name: create engine & DWH DBs
  become: true
  postgresql_db:
    name: ""{{ item.db_name }}""
    owner: ""{{ item.user }}""
    encoding: UTF-8
    lc_collate: en_US.UTF-8
    lc_ctype: en_US.UTF-8
    template: template0
    - db_name: ""{{ ovirt_engine_db_name }}""
      user: ""{{ ovirt_engine_db_user }}""
    - db_name: ""{{ ovirt_engine_dwh_db_name }}""
      user: ""{{ ovirt_engine_dwh_db_user }}""
    name: ""{{ postgres_service_name }}""",0,"become: true postgresql_user: - user: \, postgres_config_file, postgres_data_dir, postgres_db, postgres_service_name, scl enable, shell: '{{ postgres_setup_cmd }}'  creates: '{{ postgres_config_file }}', template: template0, when: ovirt_engine_version < '4.2', when: ovirt_engine_version >= '4.2'",CWE-1032,0
name: postgresql,1,"name: postgresql\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: postgresql\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- name: Create directory to store PG data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  file: path=/var/lib/postgresql\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  owner: postgres\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  group: postgres\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: postgresql\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n- name: Create directory to store PG data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  file: path=/var/lib/postgresql\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  owner: postgres\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  group: postgres\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  mode: '0700', name: postgresql\\\\\\\\\\\\\\\\n- name: Create directory to store PG data\\\\\\\\\\\\\\\\n  file: path=/var/lib/postgresql\\\\\\\\\\\\\\\\n  owner: postgres\\\\\\\\\\\\\\\\n  group: postgres\\\\\\\\\\\\\\\\n  mode: '0700', name: postgresql\\\\\\n- name: Create directory to store PG data\\\\\\n  file: path=/var/lib/postgresql\\\\\\n  owner: postgres\\\\\\n  group: postgres\\\\\\n  mode: '0700', name: postgresql\\\\n- name: Create directory to store PG data\\\\n  file: path=/var/lib/postgresql\\\\n  owner: postgres\\\\n  group: postgres\\\\n  mode: '0700', name: postgresql\\n- name: Create directory to store PG data\\n  file: path=/var/lib/postgresql\\n  owner: postgres\\n  group: postgres\\n  mode: '0700', name: postgresql\n- name: Create directory to store PG data\n  file: path=/var/lib/postgresql\n  owner: postgres\n  group: postgres\n  mode: '0700'","CWE-119, CWE-200",0,"- name: check state of database
  service:
    name: ovirt-engine
    state: running

- name: check state of engine
  service:
    name: ovirt-engine
    state: running

- name: restart of ovirt-engine service
  service:
    name: ovirt-engine
    state: restarted

- name: check health status of page
  uri:
    url: ""http://{{ovirt_engine_hostname}}/ovirt-engine/services/health""
    status_code: 200
  register: health_page
  retries: 12
  delay: 10
  until: health_page|success",0,"name: ovirt-engine\\\\\\\\n    state: running, name: ovirt-engine\\\\\\\\n    state: running\\\\\\\\n, name: ovirt-engine\\\\n    state: restarted\\\\n, service: ovirt-engine\\\\\\\\n    state: running\\\\\\\\n, service: ovirt-engine\\\\n    state: running, uri:\\\\n      status_code: 200\\\\n  register: health_page\\\\n  retries: 12\\\\n  delay: 10\\\\n  until: health_page|success\\\\n, uri:\\\\n      url: \\, uri:\\n      status_code: 200\\n  register: health_page\\n  retries: 12\\n  delay: 10\\n  until: health_page|success\\n, uri:\\n      url: \\, uri:\n      url: \","CWE-276, CWE-362, CWE-732",1
"- name: Set runner executor section
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^\s*[runners.{{ gitlab_runner.executor|default(""shell"") }}]'
    line: '  [runners.{{ gitlab_runner.executor|default(""shell"") }}]'
    state: present
    insertafter: '^\s*executor ='
    backrefs: no
  check_mode: no
  notify: restart_gitlab_runner


    line: '    image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*\[runners.docker\]'
    line: '    privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*\[runners.docker\]'
    line: '    volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'

    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'",1,"insertafter: '^\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\s*\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\[runners.ssh\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\]'..., insertafter: '^\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\s*\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\[runners.ssh\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\]'..., insertafter: '^\\\\\\\\\\\\\\\\\\\\s*\\\\\\\\\\\\\\\\\\\\[runners.ssh\\\\\\\\\\\\\\\\\\\\]'..., insertafter: '^\\\\\\\\s*\\\\\\\\[runners.ssh\\\\\\\\]'..., insertafter: '^\\\\s*\\\\[runners.ssh\\\\]'..., insertafter: '^\\s*\\[runners.ssh\\]'..., insertafter: '^\s*executor =', line:' [runners.{{ gitlab_runner.executor|default(\, state: present","CWE-1028, CWE-502",,"regexp: '^\s*limit ='
    line: '  limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    insertafter: '^\s*name ='
    backrefs: no
    regexp: '^\s*url ='
    line: '  url = {{ gitlab_runner_coordinator_url | to_json }}'
    insertafter: '^\s*limit ='
    backrefs: no
    regexp: '^\s*executor ='
    line: '  executor = {{ gitlab_runner.executor|default(""shell"") | to_json }}'
    insertafter: '^\s*url ='
    backrefs: no
    regexp: '^\s*image ='
    line: '  image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*privileged ='
    line: '  privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*volumes ='
    line: '  volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Type ='
    line: '  Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Path ='
    line: '  Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*ServerAddress ='
    line: '  ServerAddress = {{ gitlab_runner.cache_s3_server_address|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*AccessKey ='
    line: '  AccessKey = {{ gitlab_runner.cache_s3_access_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*SecretKey ='
    line: '  SecretKey = {{ gitlab_runner.cache_s3_secret_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Shared ='
    line: '  Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketName ='
    line: '  BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketLocation ='
    line: '  BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Insecure ='
    line: '  Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*user ='
    line: '  user = {{ gitlab_runner.ssh_user|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*host ='
    line: '  host = {{ gitlab_runner.ssh_host|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*port ='
    line: '  port = {{ gitlab_runner.ssh_port|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*password ='
    line: '  password = {{ gitlab_runner.ssh_password|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*identity_file ='
    line: '  identity_file = {{ gitlab_runner.ssh_identity_file|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no",0,"line':'AccessKey = {{ gitlab_runner.cache_s3_access_key|default(, line':'BucketName = {{ gitlab_runner.cache_s3_bucket_name|default(, line':'Path = {{ gitlab_runner.cache_path|default(, line':'SecretKey = {{ gitlab_runner.cache_s3_secret_key|default(, line':'ServerAddress = {{ gitlab_runner.cache_s3_server_address|default(, line':'Shared = {{ gitlab_runner.cache_shared|default(, line':'Type = {{ gitlab_runner.cache_type|default(, line':'image = {{ gitlab_runner.docker_image|default(, line':'password = {{ gitlab_runner.ssh_password|default(, line':'port = {{ gitlab_runner.ssh_port|default(","CWE-119, CWE-120",0
when: gitlab_runner_registration_token | length > 0  # Ensure value is set,1,gitlab_runner_registration_token | length > 0,CWE-190,1,"- name: Register GitLab Runner
  include: register-runner.yml
  when: gitlab_runner_registration_token != ''",0,when: gitlab_runner_registration_token!= '',CWE-937,1
"regexp: '^\s*limit ='
    line: '  limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    insertafter: '^\s*name ='
    backrefs: no
    regexp: '^\s*url ='
    line: '  url = {{ gitlab_runner_coordinator_url | to_json }}'
    insertafter: '^\s*limit ='
    backrefs: no
    regexp: '^\s*executor ='
    line: '  executor = {{ gitlab_runner.executor|default(""shell"") | to_json }}'
    insertafter: '^\s*url ='
    backrefs: no
    regexp: '^\s*image ='
    line: '  image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*privileged ='
    line: '  privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*volumes ='
    line: '  volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Type ='
    line: '  Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Path ='
    line: '  Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*ServerAddress ='
    line: '  ServerAddress = {{ gitlab_runner.cache_s3_server_address|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*AccessKey ='
    line: '  AccessKey = {{ gitlab_runner.cache_s3_access_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*SecretKey ='
    line: '  SecretKey = {{ gitlab_runner.cache_s3_secret_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Shared ='
    line: '  Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketName ='
    line: '  BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketLocation ='
    line: '  BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Insecure ='
    line: '  Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*user ='
    line: '  user = {{ gitlab_runner.ssh_user|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*host ='
    line: '  host = {{ gitlab_runner.ssh_host|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*port ='
    line: '  port = {{ gitlab_runner.ssh_port|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*password ='
    line: '  password = {{ gitlab_runner.ssh_password|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*identity_file ='
    line: '  identity_file = {{ gitlab_runner.ssh_identity_file|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no",1,"bucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default(, bucketName = {{ gitlab_runner.cache_s3_bucket_name|default(, insecure = {{ gitlab_runner.cache_s3_insecure|default(, keyFile = {{ gitlab_runner.ssh_key_file|default(, passphrase = {{ gitlab_runner.ssh_passphrase|default(, password = {{ gitlab_runner.ssh_password|default(, path = {{ gitlab_runner.cache_s3_path | default(, port = {{ gitlab_runner.ssh_port|default(, secretKey = {{ gitlab_runner.cache_s3_secret_key|default(, shared = {{ gitlab_runner.cache_shared|default(","CWE-1297, CWE-1298, CWE-1312, CWE-1313, CWE-1314, CWE-1315, CWE-1395, CWE-1398, CWE-1399, CWE-1411",,"---
- name: Create temporary file
  tempfile:
    state: file
    path: ""{{ temp_runner_config_dir.path }}""
    prefix: ""gitlab-runner.{{ gitlab_runner_index }}{{ runner_config_index }}.""
  register: temp_runner_config_keyword
  check_mode: no
  changed_when: false

- name: Isolate runner configuration
  copy:
    dest: ""{{ temp_runner_config_keyword.path }}""
    content: ""[[runners]]""
  check_mode: no
  changed_when: false

- name: Set concurrent limit option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)limit ='
    line: '\1limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set coordinator URL
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)url ='
    line: '\1url = {{ gitlab_runner_coordinator_url | to_json }}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set runner executor option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)executor ='
    line: '\1executor = {{ gitlab_runner.executor|default(""shell"") | to_json}}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set runner docker image option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)image ='
    line: '\1image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.docker_image is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set docker privileged option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)privileged ='
    line: '\1privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    state: ""{{ 'present' if gitlab_runner.docker_privileged is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set docker volumes option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)volumes ='
    line: '\1volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    state: ""{{ 'present' if gitlab_runner.docker_volumes is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache type option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Type ='
    line: '\1Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_type is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache path option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Path ='
    line: '\1Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_path is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache shared option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Shared ='
    line: '\1Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    state: ""{{ 'present' if gitlab_runner.cache_shared is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 bucket name option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)BucketName ='
    line: '\1BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_bucket_name is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 bucket location option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)BucketLocation ='
    line: '\1BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_bucket_location is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 insecure option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Insecure ='
    line: '\1Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_insecure is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner",0,"Path = {{ gitlab_runner.cache_path|default(\\\\, Shared = {{ gitlab_runner.cache_shared|default(\\\\\\\\, Type = {{ gitlab_runner.cache_type|default(\\, executor = {{ gitlab_runner.executor|default(\, image = {{ gitlab_runner.docker_image|default(\\, image = {{ gitlab_runner.docker_image|default(\\\\\\\\, limit = {{ gitlab_runner.concurrent_specific|default(0) }}, privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}, url = {{ gitlab_runner_coordinator_url | to_json }}, volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}",CWE-276,1
--cache-s3-secret-key '{{ gitlab_runner_cache_s3_secret_key }}',1,----,"CWE-345, CWE-999",1,"{% if gitlab_runner_cache_type is defined %}
    --cache-type '{{ gitlab_runner_cache_type }}'
    --cache-s3-server-address '{{ gitlab_runner_cache_s3_server_address }}'
    --cache-s3-access-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-secret-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-bucket-name '{{ gitlab_runner_cache_s3_bucket_name }}'
    --cache-s3-insecure '{{ gitlab_runner_cache_s3_insecure }}'
    --cache-cache-shared '{{ gitlab_runner_cache_cache_shared }}'
    {% endif %}",0,"--cache-cache-shared '{{ gitlab_runner_cache_cache_shared }}', cache-s3-access-key '{{ gitlab_runner_cache_s3_access_key }}', cache-s3-bucket-name '{{ gitlab_runner_cache_s3_bucket_name }}', cache-s3-insecure '{{ gitlab_runner_cache_s3_insecure }}', cache-s3-secret-key '{{ gitlab_runner_cache_s3_access_key }}', cache-s3-server-address '{{ gitlab_runner_cache_s3_server_address }}', cache-type '{{ gitlab_runner_cache_type }}'",CWE-937,0
insertafter: '^\s*url =',1,url =,CWE-918,,"- name: Set environment option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^\s*environment ='
    line: '  environment = {{ gitlab_runner.env_vars|default([]) | to_json }}'
    state: present
    insertafter: '^\s*url='
    backrefs: no
  check_mode: no
  notify: restart_gitlab_runner",0,"^\s*environment = {{ gitlab_runner.env_vars|default([]) | to_json }}, backrefs: no, check_mode: no, dest: {{ temp_runner_config.path }}', insertafter: '^\s*url=', insertbefore: '^\\s*lineinfile, line:' environment = {{ gitlab_runner.env_vars|default([]) | to_json }}', notify: restart_gitlab_runner, regexp: '^\\s*environment =', state: present","CWE-190, CWE-772, CWE-778",0
"dest: ""{{ gitlab_runner_config_file }}""",1,"\\, dest: \, gitlab_runner_config_file \\, gitlab_runner_config_file\\, gitlab_runner_config_file\\\, gitlab_runner_config_file\\\\, gitlab_runner_config_file\\\\\\n, gitlab_runner_config_file\n","CWE-120, CWE-732, CWE-89",0,"---
- name: (Windows) Create .gitlab-runner dir
  win_file:
    path: ""{{ gitlab_runner_config_file_location }}""
    state: directory

- name: (Windows) Ensure config.toml exists
  win_file:
    path: ""{{ gitlab_runner_config_file }}""
    state: touch
    modification_time: preserve
    access_time: preserve

- name: (Windows) Set concurrent option
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^(\s*)concurrent =.*'
    line: '$1concurrent = {{ gitlab_runner_concurrent }}'
    state: present
    backrefs: yes
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows

- name: (Windows) Add listen_address to config
  win_lineinfile:
    dest: /etc/gitlab-runner/config.toml
    regexp: '^listen_address =.*'
    line: 'listen_address = ""{{ gitlab_runner_listen_address }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_listen_address | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_windows

- name: (Windows) Add sentry dsn to config
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^sentry_dsn =.*'
    line: 'sentry_dsn = ""{{ gitlab_runner_sentry_dsn }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_sentry_dsn | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows",0,"concurrent, gitlab_runner_concurrent, listen_address, sentry_dsn","CWE-120, CWE-20, CWE-25",1
"name: ""{{ item }}""",1,"name: \, name: \\ \\\\\\\, name: \\\, name: \\\\, name: \\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-1038,,"name: ""{{ item }}""
    with_items: ""{{ rocket_chat_dep_packages }}""
      name: 
    with_items: ""{{ rocket_chat_dep_packages }}""",0,"name, with_items","CWE-113, CWE-116, CWE-119, CWE-120, CWE-121, CWE-122, CWE-125",0
file:  CentOS-Base,1,"group: \\, host: \\\\, key: \, name: \\, password: \\\\, password: \\\\\\, type: \, url: \\, username: \\\\, value: \\","CWE-256, CWE-257",1,"- name: Configure default CentOS online repos
    yum_repository:
      name: {{ item }}
      enabled: {{ rock_online_install }}
      file:  CentOS-Base.repo
    with_items:
      - base
      - updates
      - extras

    when: with_elasticsearch
    when: (with_elasticsearch and with_bro)
    when: (with_elasticsearch and with_bro) and bro_mapping.status == 404
  - name: Add broctl wrapper for admin use
    copy:
      src: broctl.sh
      dest: /usr/sbin/broctl
      mode: 0754
      owner: root
      group: root
    when: with_bro

    when: with_pulledpork and not rules_file.stat.exists
    when: with_kibana and rock_online_install
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana and (kibana_config.rock_config is undefined or kibana_config.rock_config != rock_dashboards_version)
    when: with_kibana and with_bro
    when: with_kibana and with_suricata and not with_bro
    when: with_kibana
    when: with_kibana
    seboolean: 
      name: httpd_can_network_connect
      state: yes 
      persistent: yes
        for intf in {{ rock_monifs | join(' ') }}; do
    - name: create kafka suricata topic
           --topic suricata-raw",0,"when: with_bro, when: with_kibana, when: with_kibana and with_bro, when: with_kibana and with_bro and (kibana_config.rock_config is undefined or kibana_config.rock_config!= rock_dashboards_version), when: with_kibana and with_suricata and not with_bro","CWE-352, CWE-354, CWE-732",1
"dest: ""{{ bro_sysconfig_dir }}/broctl.cfg""
      creates: ""{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""
      src: ""{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""
      dest: ""{{ bro_site_dir }}/scripts/rock""",1,"dest: \\, dest: \\\\, dest: \\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dest: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, src: \, src: \\, src: \\\\\\\\\\\\\\\\, src: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-758, CWE-778",1,"- { pkg: docket, test: ""{{with_docket}}"", state: installed }
        - { port: ""8443/tcp"", test: ""{{ with_docket }}"" }
  - name: Create /opt/bro dir for wandering users
      dest: ""/opt/bro""
      state: directory
  - name: Create note to wandering users
    copy:
      dest: ""/opt/bro/README.md""
      content: |
        Hey! Where's my Bro?
        =========================

        RockNSM has aligned the Bro package to be inline with Fedora packaging
        guidelines in an effort to push the package upstream for maintenance.
        Fedora and EPEL have a great community and we believe others can benefit
        from our hard work.

        Here's where you can find your stuff:

        Bro configuration files
        -----------------------
        /opt/bro/etc -> /etc/bro

        Bro site scripts
        -----------------------
        /opt/bro/share/bro/site -> /usr/share/bro/site

        Bro logs and spool dirs (same as previous ROCK iterations)
        -----------------------
        /opt/bro/logs -> /data/bro/logs
        /opt/bro/spool -> /data/bro/spool

      dest: ""{{ bro_sysconfig_dir }}/node.cfg""
      dest: {{ bro_sysconfig_dir }}/broctl.cfg""
      dest: ""{{ bro_sysconfig_dir }}/networks.cfg""
      path: /scripts
      dest: ""{{ bro_site_dir }}/scripts/README.txt""
      dest: ""{{ bro_site_dir }}/scripts/rock""
      dest: ""{{ bro_site_dir }}/scripts/""
      creates: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""
      src: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""
      dest: """"{{ bro_site_dir }}/scripts/rock""""
      path: ""{{ bro_site_dir }}/scripts/rock""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
  - name: Add bro aliases
      job: ""/usr/bin/broctl cron >/dev/null 2>&1""
    command: /usr/bin/broctl install
      name: bro
    notify: reload broctl
      export kibuser=$(getent passwd 1000 | awk -F: '{print $1}') && export kibpw=$(xkcdpass -a rock) && echo -e ""U: ${kibuser}\nP: ${kibpw}"" > /home/${kibuser}/KIBANA_CREDS.README && printf ""${kibuser}:$(echo ${kibpw} | openssl passwd -apr1 -stdin)\n"" | sudo tee -a /etc/nginx/htpasswd.users > /dev/null 2>&1
    seboolean:
      state: yes
    shell:
      service: name=broctl state=""{{ 'started' if enable_bro else 'stopped' }}""",0,"command: /usr/bin/broctl install, export kibuser=$(getent passwd 1000 | awk -F: '{print $1}') && export kibpw=$(xkcdpass -a rock) && echo -e \, export kibuser=$(getent passwd 1000 | awk -F: '{print $1}') && export kibpw=$(xkcdpass -a rock) && echo -e \\, path: /scripts, shell: service: name=broctl state=","CWE-120, CWE-257",0
"state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""",1,"state: \, state: \\, state: \\\\, state: \\\\\\\\, state: \\\\\\\\\\\\\\\\ \\\, state: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, state: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-257,,"state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""",0,"local_services | selectattr(\'name\', \'equalto\',\'stenographer\') | map(attribute=\'enabled\') | first | bool, local_services | selectattr(\\'name\\', \\'equalto\\',\\'stenographer\\') | map(attribute=\\'enabled\\') | first | bool, local_services | selectattr(\\\\'name\\\\', \\\\'equalto\\\\',\\\\'stenographer\\\\') | map(attribute=\\\\'enabled\\\\') | first | bool, local_services | selectattr(\\\\\\\\\\\\'name\\\\\\\\\\\\', \\\\\\\\\\\\\'equalto\\\\\\\\\\\\',\\\\\\\\\\\\'stenographer\\\\\\\\\\\\') | map(attribute=\\\\\\\\\\\\'enabled\\\\\\\\\\\\') | first | bool, local_services | selectattr(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'equalto\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'stenographer\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') | map(attribute=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'enabled\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') | first | bool, local_services | selectattr(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-601,0
"apt:
    pkg: fail2ban
    state: latest
    update_cache: true
    cache_valid_time: ""{{ apt_cache_valid_time }}""
  template:
    src: ""{{ item }}.j2""
    dest: /etc/fail2ban/{{ item }}
  service:
    name: fail2ban
    state: started
    enabled: yes",1,"apt:\\\\\\\\\\n    pkg: fail2ban\\\\\\\\r\\\\n    state: latest\\\\r\\\\n  update_cache: true\\\\r\\\\n  cache_valid_time: \\, apt:\\\\r\\\\\\\\n    pkg: fail2ban\\\\\\\\r\\\\\\\\n    state: latest\\\\\\\\r\\\\\\\\n  update_cache: true\\\\\\\\r\\\\\\\\n  cache_valid_time: \\\\, apt:\\r\\\\n    pkg: fail2ban\\\\r\\\\n    state: latest\\\\r\\\\n  update_cache: true\\\\r\\\\n  cache_valid_time: \\, cache_valid_time: \, enabled: yes\\r\\n    \, name: fail2ban\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\n    state: started\\\\\\\\\\\\\\\\\\\\\\\\n  enabled: yes\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\n    \\\\\\\\\\\\\\\\, name: fail2ban\\\\\\\\\\\\\\\\r\\\\\\\\n  state: started\\\\\\\\n  enabled: yes\\\\\\\\\\\\r\\\\n    \\\\\\, name: fail2ban\\\\\\\\\\\\n    state: started\\\\\\\\r\\\\n  enabled: yes\\\\\\\\\\\\n    \\\\, name: fail2ban\\r\\n  state: started\\r\\n  enabled: yes\\\\r\\n    \\, state: latest\\r\n    update_cache: true\r\n  cache_valid_time: \",CWE-937,0,"---
- name: ensure fail2ban is installed
  apt: pkg=fail2ban state=latest update_cache=true cache_valid_time={{ apt_cache_valid_time }}
  notify:
    - restart fail2ban

- name: ensure fail2ban is configured
  template: src={{ item }}.j2 dest=/etc/fail2ban/{{ item }}
  with_items:
    - jail.local
    - fail2ban.local
  notify:
    - restart fail2ban

- name: ensure fail2ban starts on a fresh reboot
  service: name=fail2ban state=started enabled=yes",0,"notify: - restart fail2ban, template: src=...","CWE-1024, CWE-1026, CWE-1030, CWE-1035, CWE-1038, CWE-1042, CWE-1058, CWE-1112, CWE-1115, CWE-1126",0
"local_action: command ansible {{ inventory_hostname }} -m ping{{ (inventory_file == None) | ternary('', ' -i ' + inventory_file | string) }} -u root",1,"ansible, ansible {{ inventory_file | string }}, ansible {{ inventory_file | string }} -m ping, ansible {{ inventory_file | string }} -u root, ansible {{ inventory_hostname }}, ansible {{ inventory_hostname }} -m ping, ansible {{ inventory_hostname }} -m ping{{ (inventory_file == None) | ternary('','-i'+ inventory_file | string) }}, ansible {{ inventory_hostname }} -u root",CWE-200,,"local_action: command ansible {{ inventory_hostname }} -m ping -i {{ inventory_file }} -u root
    ansible_ssh_user: ""{{ (root_status.rc == 0) | ternary('root', admin_user) }}""

- name: Announce which user was selected
  debug:
    msg: ""Note: Ansible will attempt connections as user = {{ ansible_ssh_user }}""",0,"Note: Ansible will attempt connections as user = {{ ansible_ssh_user }}, ansible_ssh_user: \, debug:, local_action: command ansible {{ inventory_hostname }} -m ping -i {{ inventory_file }} -u root, msg: Note: Ansible will attempt connections as user = {{ ansible_ssh_user }}","CWE-221, CWE-257",0
"project_subtree: ""{{ project.subtree }}""",1,project_subtree,"CWE-20, CWE-200, CWE-22, CWE-264, CWE-269, CWE-285, CWE-306, CWE-346, CWE-369, CWE-89",1,"project_subtree: ""{{ project.subtree | default(False) }}""",0,"project_subtree: \, project_subtree: \\, project_subtree: \\\\, project_subtree: \\\\\\, project_subtree: \\\\\\\\\\\\\\\\, project_subtree: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, project_subtree: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, project_subtree: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, project_subtree: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, project_subtree: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-20, CWE-732",0
"when: sensu_redis_server
  when: sensu_rabbitmq_server",1,"c:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, c:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\\\\\\\\\\\\\n  when: sensu_redis_server\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  when: sensu_rabbitmq_server\\\\\\\\\\\\\\\\\\\\\\\\\\\\n, c:\\\\\\\\\\\\r\\\\n  when: sensu_redis_server\\\\n  when: sensu_rabbitmq_server\\\\n, c:\\\\r\\n  when: sensu_redis_server\\n  when: sensu_rabbitmq_server\\n, c:\\r\\n  when: sensu_redis_server\\n  when: sensu_rabbitmq_server\\n, c:\r\n  when: sensu_redis_server\n  when: sensu_rabbitmq_server\n, sensu_rabbitmq_server, sensu_redis_server","CWE-276, CWE-278",,"---

  - name: Ensure the Sensu group is present
    group: name={{ sensu_group_name }}
             state=present
             
  - name: Ensure the Sensu user is present
    user: name={{ sensu_user_name }}
          group={{ sensu_group_name }}
          shell=/bin/false
          home={{ sensu_config_path }}
          createhome=yes
          state=present

  - name: Ensure the Sensu config directory is present
    file: dest={{ sensu_config_path }}/conf.d state=directory recurse=yes
          owner={{ sensu_user_name }} group={{ sensu_group_name }}

  - name: Ensure Sensu dependencies are installed
    pkgin: name=build-essential,ruby21-base state=present

  - name: Ensure Uchiwa (dashboard) dependencies are installed
    pkgin: name=go state=present
    when: sensu_include_dashboard

  - name: Ensure Sensu is installed
    gem: name=sensu state={{ sensu_gem_state }} user_install=no
    notify:
      - restart sensu-client service
    
  - name: Ensure Sensu 'plugins' gem is installed
    gem: name=sensu-plugin state={{ sensu_plugin_gem_state }} user_install=no

  - include: ssl.yml tags=ssl

  - include: rabbit.yml tags=rabbitmq
    when: rabbitmq_server

  - include: redis.yml tags=redis
    when: redis_server

  - include: server.yml tags=server
    when: sensu_master

  - include: dashboard.yml tags=dashboard
    when: sensu_include_dashboard
    
  - include: client.yml tags=client

  - include: plugins.yml tags=plugins
    when: sensu_include_plugins",0,"gem: name=sensu state={{ sensu_gem_state }} user_install=no, home={{ sensu_config_path }} createhome=yes, include: client.yml tags=client, include: plugins.yml tags=plugins, include: rabbit.yml tags=rabbitmq, include: server.yml tags=server, pkgin: name=build-essential,ruby21-base state=present, ruby: name={{ sensu_gem_name }} state={{ sensu_gem_state }} user_install=no, shell=/bin/false, user: name={{ sensu_user_name }}","CWE-124, CWE-264, CWE-272, CWE-276, CWE-398",0
enabled: yes,1,enabled: yes,"CWE-259, CWE-260, CWE-261, CWE-262, CWE-263, CWE-264, CWE-269, CWE-271, CWE-275, CWE-276",1,"- name: Deploy Tessen server configuratiuon
  template:
    dest: ""{{ sensu_config_path }}/conf.d/tessen.json""
    owner: ""{{ sensu_user_name }}""
    group: ""{{ sensu_group_name }}""
    src: sensu-tessen.json.j2
  notify: restart sensu-server service

  service:
    name: ""{{ sensu_server_service_name if not se_enterprise else sensu_enterprise_service_name }}""
    state: started
  enabled: yes
  service:
    name: sensu-api
    state: started
    enabled: yes",0,"enabled: yes\\\\n  service:, enabled: yes\\n  service:, name: sensu-\\\\napi, service:\\\\\\\\n    name: sensu-\\\\\\\\\\\\\\\\napi, service:\\\\n    name:, service:\\\\n    name: sensu-\\\\\\\\napi, service:\\n    name:, state: started\\\\\\\\n  enabled:, state: started\\\\\\n  enabled:, template:\n    dest:",CWE-306,0
istio_git_repo: https://github.com/istio/istio.git,1,istio_git_repo: https://github.com/istio/istio.git,"CWE-1135, CWE-1235, CWE-5676, CWE-5677",,"istio_git_repo: https://github.com/istio/istio.git
istio_git_branch: 0.6.0

istio_playbook_release_tag: 0.6.0
istio_playbook_auth: false
istio_playbook_jaeger: false
istio_playbook_delete_resources: false
istio_playbook_cluster_flavour: ocp
istio_playbook_dest: /home/istio
istio_playbook_namespace: istio-system
istio_playbook_addon: grafana,prometheus,servicegraph
istio_playbook_samples:",0,"istio_playbook_addon: grafana,prometheus,servicegraph, istio_playbook_auth: false, istio_playbook_cluster_flavour: ocp, istio_playbook_delete_resources: false, istio_playbook_jaeger: false, istio_playbook_namespace: istio-system, istio_playbook_release_tag: 0.6.0","CWE-22, CWE-276, CWE-287, CWE-312, CWE-320",0
value: sb-2.1.x,1,value: sb-2.1.x,"CWE-20, CWE-22",1,"apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: s2i-springboot-example
spec:
  taskRef:
    name: s2i-jdk8
  inputs:
    resources:
      - name: git-repo
        resourceSpec:
          type: git
          params:
            - name: revision
              value: master
            - name: url
              value: https://github.com/snowdrop/rest-http-example
  outputs:
    resources:
      - name: image
        resourceSpec:
          type: image
          params:
            - name: url
              value: quay.io/snowdrop/spring-boot-example",0,"params:, resources:",CWE-269,0
"- name: ""Setting service_name fact from config""
    splunk_service_name: ""{{ splunk.service_name }}""
    - ""'service_name' in splunk""
- name: Set Splunk service name
  block:
    - name: Setting SplunkForwarder service
      set_fact:
        splunk_service_name: ""SplunkForwarder.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role == ""splunk_universal_forwarder""
    - name: Setting Splunkd service
      set_fact:
        splunk_service_name: ""Splunkd.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role != ""splunk_universal_forwarder""
    - name: Setting splunk service
      set_fact:
        splunk_service_name: ""splunk""
      when:
        - ansible_system is match(""Linux"")
        - not splunk_systemd

    - name: Setting splunkforwarder Windows service
      set_fact:
        splunk_service_name: ""splunkforwarder""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role == ""splunk_universal_forwarder""

    - name: Setting splunkd Windows service
      set_fact:
        splunk_service_name: ""splunkd""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role != ""splunk_universal_forwarder""
    - splunk_service_name is not defined or not splunk_service_name
    - splunk.enable_service",1,"not splunk_service_name, splunk.enable_service, splunk_service_name in splunk","CWE-1038, CWE-120",,"---
- name: ""Setting service_name to SplunkForwarder.service""
  set_fact:
    splunk_service_name: ""SplunkForwarder.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd
    - splunk.build_location is search('forwarder')

- name: ""Setting service_name to Splunkd.service""
  set_fact:
    splunk_service_name: ""Splunkd.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd is False

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and not ansible_system is match(""Linux"")",0,"name: \, splunk_service_name: \, splunk_service_name: \\, splunk_service_name: \\\\, splunk_service_name: \\\\\\\\, splunk_service_name: \\\\\\\\\\\\\\\\\\, splunk_service_name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-120, CWE-352",0
"changed_when:
    - splunk_cluster_bundle_result.stdout.find('Applying') != -1
    - splunk_cluster_bundle_result.stdout.find('bundle') != -1",1,"changed_when:\\\\\\\\\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\\\\\\\\r  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\\\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\\r  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\\\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\\r\\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\\\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\\r\\\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\\\\\\\\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\\r\\\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\\\\\\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\\r\\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\r\\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1, changed_when:\r\n  - splunk_cluster_bundle_result.stdout.find('Applying')!= -1\r\n  - splunk_cluster_bundle_result.stdout.find('bundle')!= -1",CWE-749,0,"---
- name: Get indexer count
  set_fact:
    num_indexer_hosts: ""{{ groups['splunk_indexer'] | length }}""

- name: Get default replication factor
  set_fact:
    idxc_search_factor: ""{{ splunk.idxc.search_factor }}""
    idxc_replication_factor: ""{{ splunk.idxc.replication_factor }}""

- name: Lower indexer search/replication factor
  set_fact:
    idxc_search_factor: 1
    idxc_replication_factor: 1
  when: num_indexer_hosts|int < 3

- name: Set indexer discovery
  uri:
    url: ""https://127.0.0.1:{{ splunk.svc_port }}/servicesNS/nobody/system/configs/conf-server""
    method: POST
    user: admin
    password: ""{{ splunk.password }}""
    validate_certs: False
    body: ""name=indexer_discovery&pass4SymmKey={{ splunk.shc.secret }}""
    body_format: json
    headers:
      Content-Type: ""application/x-www-form-urlencoded""
    status_code: 201,409
    timeout: 10
  register: set_indexer_discovery
  changed_when: set_indexer_discovery.status == 201

- name: Set the current node as a Splunk indexer cluster master
  command: ""{{ splunk.exec }} edit cluster-config -mode master -replication_factor {{ splunk.idxc.replication_factor }} -search_factor {{ splunk.idxc.search_factor }} -secret '{{ splunk.idxc.secret }}' -cluster_label '{{ splunk.idxc.label }}' -auth 'admin:{{ splunk.password }}'""
  register: task_result
  until: task_result.rc == 0
  retries: ""{{ retry_num }}""
  delay: 3
  notify:
    - Restart the splunkd service

- name: Flush restart handlers
  meta: flush_handlers

- name: Apply the cluster bundle to the Splunk cluster master
  command: ""{{ splunk.exec }} apply cluster-bundle -auth admin:{{ splunk.password }} --skip-validation --answer-yes""
  register: splunk_cluster_bundle_result
  failed_when: >
    (""No new bundle will be pushed"" not in splunk_cluster_bundle_result.stderr)
    and (""Rolling restart of the peers"" not in splunk_cluster_bundle_result.stderr)
    and (""Applying bundle"" not in splunk_cluster_bundle_result.stdout)
  changed_when: splunk_cluster_bundle_result.stdout.find('Applying bundle') != -1

- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml",0,"When: num_indexer_hosts|int < 3, command: splunk.exec edit cluster-config -mode master -replication_factor splunk.idxc.replication_factor, command: splunk.exec edit cluster-config -mode master -search_factor splunk.idxc.search_factor, set_fact: idxc_replication_factor: 1, set_fact: idxc_search_factor: 1",CWE-295,0
"register: set_symmkey

- include_tasks: trigger_restart.yml
  when: set_symmkey is changed",1,"include_tasks, include_tasks: trigger_restart.yml, is changed, register, register: set_symmkey, task to Analyze:, trigger_restart.yml, when, when: set_symmkey, when: set_symmkey is changed","CWE-743, CWE-749, CWE-787",1,"- name: Set general pass4SymmKey
  ini_file: 
    dest: ""{{ splunk.home }}/etc/system/local/server.conf""
    section: ""general""
    option: ""pass4SymmKey""
    value: ""{{ splunk.pass4SymmKey }}""
  notify:
    - Restart the splunkd service",0,"- Restart the splunkd service\\\\n    \\\\, - name: Set general pass4SymmKey\n  ini_file:\n    dest: \, notify:\\\\n    - Restart the splunkd service\\n    \\, notify:\\n    - Restart the splunkd service\\n    \, option: \, option: \\\\\\\\, section: \\, section: \\\\, value: \",CWE-307,0
"- include_tasks: prepare_apps_bundle.yml
- include_tasks: initialize_cluster_master.yml",1,"include_tasks: initialize_cluster_master.yml, include_tasks: prepare_apps_bundle.yml","CWE-1024, CWE-1030",1,"- include_tasks: initialize_cluster_master.yml
- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml
- include_tasks: ../../../roles/splunk_common/tasks/provision_apps.yml
  when:
    - splunk.apps_location
- include_tasks: push_apps_to_indexers.yml
  when:
    - splunk.apps_location",0,"initialize_cluster_master.yml, push_apps_to_indexers.yml, splunk_common/tasks/provision_apps.yml","CWE-287, CWE-352",0
"path: ""{{ splunk.app_paths.deployment }}""",1,"path: \, path: \\, path: \\\\, path: \\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-263, CWE-269, CWE-270, CWE-271, CWE-272, CWE-275, CWE-284, CWE-285",1,"- name: Gather all deployment server apps
  find:
    path: ""{{ splunk.home }}/etc/deployment_apps""
    recurse: no
    file_type: directory
  register: deployment_apps

    section: ""serverClass:all:app:{{ item.path | basename }}""
  with_items: ""{{ deployment_apps.files }}""",0,"file_type: \\\\, file_type: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, path: \\, path: \\\\\\\\\\\\\\\\\\\\\\\\\\\\, recursel: \\\\, recursel: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, section: \\\\\\\\\\\\\\\\, with_items: \","CWE-117, CWE-126, CWE-326, CWE-327, CWE-949",0
- include_tasks: ../../../roles/splunk_common/tasks/check_for_required_restarts.yml,1,include_tasks,"CWE-1020, CWE-1022, CWE-1025, CWE-1026, CWE-1031, CWE-1032, CWE-1034, CWE-1035, CWE-1036, CWE-1039",1,- include_tasks: check_for_required_restarts.yml,0,include_tasks: check_for_required_restarts.yml,CWE-120,0
"- name: ""Wait for port {{ splunk.svc_port }} to become open""
    port: ""{{ splunk.svc_port }}""",1,"name: \, name: \\, name: \\\\, port: {{ splunk.svc_port }} \\, wait_for: \\\\","CWE-285, CWE-295, CWE-319, CWE-327",1,"- name: ""Wait for port 8089 to become open""
    port: 8089",0,"- name: \, - name: \\\\\\\\\, 8089\\\\\\\\\\\\n\\\\\\\\\\\\, 8089\\\\n\\\\, 8089\\n\, port: 8089\\\\\\\\\\\\\\\, port: 8089\\\\\\\\\\\\\\n\\\\, port: 8089\\\\n\\",CWE-103,0
"license: MIT
  min_ansible_version: 2.0.1",1,"license: MIT min_ansible_version: 2.0.1, min_ansible_version: 2.0.1",CWE-1235,0,"---
galaxy_info:
  author: Justin Leitgeb
  description: Base image and common roles
  company: Stack Builders
  license: Proprietary
  min_ansible_version: 1.2
  platforms:
    - name: Debian
      versions:
        - all

  galaxy_tags:
    - sb-base

dependencies:
  - role: kamaln7.swapfile
    become: yes
    become_method: sudo
    remote_user: administrator
    swapfile_size: ""{{ swap_file_size }}""
    tags:
      - bootstrap

  - role: ansible-role-unattended-upgrades
    become: yes
    become_method: sudo
    remote_user: administrator
    unattended_origins_patterns:
      - 'origin=Debian,archive=${distro_codename},label=Debian-Security'
    unattended_mail: '{{ uu_email_alerts }}'
    unattended_automatic_reboot: false
    tags:
      - bootstrap

  - role: nickjj.fail2ban
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - bootstrap

  - role: geerlingguy.ntp
    become: yes
    become_method: sudo
    remote_user: administrator
    ntp_timezone: UTC
    tags:
      - bootstrap

  - role: jdauphant.ssl-certs
    become: yes
    become_method: sudo
    remote_user: administrator
    ssl_certs_common_name: ""{{ inventory_hostname }}""

    tags:
      - nginx-http
      - nginx-https

  - role: jdauphant.nginx
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - nginx

  - role: ANXS.postgresql
    tags:
      - basic-postgres",0,"ansible-galaxy install\n, remote_user: administrator, remote_user: administrator\\n, tags:\\n  - bootstrap\\n\\n  - nginx\\n\\n  - nginx-http\\n\\n  - nginx-https","CWE-190, CWE-937",0
service: name={{openvpn_service}} state=restarted,1,service: name={{openvpn_service}} state=restarted,"CWE-275, CWE-276, CWE-279, CWE-280, CWE-281, CWE-283, CWE-284, CWE-285, CWE-286, CWE-287",1,"---

- name: openvpn restart
  service: name=openvpn state=restarted",0,service: name=openvpn state=restarted,"CWE-25, CWE-257, CWE-266, CWE-269",0
"- include_tasks: system/firewall-deps.yml
  when:
    openvpn_open_firewall | bool
    or openvpn_route_traffic | bool
    or openvpn_client_to_client_via_ip | bool

- include_tasks: system/open-firewall.yml
  when: openvpn_open_firewall | bool",1,"include_tasks: system/firewall-deps.yml\\\\\\\\\\\\nwhen: openvpn_open_firewall | bool or openvpn_route_traffic | bool or openvpn_client_to_client_via_ip | bool, include_tasks: system/firewall-deps.yml\\\\nwhen: openvpn_open_firewall | bool or openvpn_route_traffic | bool, include_tasks: system/firewall-deps.yml\\nwhen: openvpn_open_firewall | bool, include_tasks: system/firewall-deps.yml\\nwhen: openvpn_open_firewall | bool or openvpn_route_traffic | bool or openvpn_client_to_client_via_ip | bool, include_tasks: system/open-firewall.yml\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, include_tasks: system/open-firewall.yml\\\\\\\\\\\\\\\\\\\\\\\\\\\\nwhen: openvpn_open_firewall | bool, include_tasks: system/open-firewall.yml\\\\\\\\\\\\\\\\nwhen: openvpn_open_firewall | bool, include_tasks: system/open-firewall.yml\\\\nwhen: openvpn_open_firewall | bool, include_tasks: system/open-firewall.yml\nwhen: openvpn_open_firewall | bool","CWE-269, CWE-276",1,"- include_tasks: system/forwarding.yml

- include_tasks: system/firewall.yml

- include_tasks: system/routing.yml
  when: openvpn_route_traffic | bool",0,when: openvpn_route_traffic | bool,CWE-200,0
when: threatstack_hostname and threatstack_policy,1,"name: name\\n, name: name\n, when: threatstack_hostname and threatstack_policy\n","CWE-119, CWE-120, CWE-18, CWE-19, CWE-20, CWE-203, CWE-21, CWE-22, CWE-25, CWE-319",1,"- name: Cloudsight setup default
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }}
  when: not threatstack_hostname and not threatstack_policy
- name: Cloudsight setup policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --policy=""{{ threatstack_policy}}""
  register: setup_result
  when: threatstack_policy and not threatstack_hostname
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and not threatstack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname/policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and threastack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret",0,"When: threatstack_hostname is specified, but deployment key is not, that is not a safe practice.\\n\\nThis creates a window of opportunity for an attacker to compromise the Cloudsight setup., When: threatstack_policy is specified, but deployment key is not, that is not a safe practice.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, When: threatstack_policy is specified, but deployment key is not, that is not a safe practice.\n\nThis creates a window of opportunity for an attacker to compromise the Cloudsight setup., when: threatstack_hostname is not specified, but deployment key is present, that is not a safe practice.\\\\n\\\\nThis creates a window of opportunity for an attacker to compromise the Cloudsight setup., when: threatstack_hostname is specified, but deployment key is not present, that is not a safe practice.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: threatstack_hostname is specified, but deployment key is not present, that is not a safe practice.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nThis creates a window of opportunity for an attacker to compromise the Cloudsight setup., when: threatstack_hostname is specified, but deployment key is not present, that is not a safe practice.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nThis creates a window of opportunity for an attacker to compromise the Cloudsight setup., when: threatstack_hostname is specified, but deployment key is not present, that is not a safe practice.\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nThis creates a window of opportunity for an attacker to compromise the Cloudsight setup., when: threatstack_hostname is specified, but deployment key is not present, that is not a safe practice.\\\\\\\\n\\\\\\\\nThis creates a window of opportunity for an attacker to compromise the Cloudsight setup.",CWE-319,0
"secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""",1,"secret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\ ntenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\ nclient_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\ nsubscription_id: {{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\\ nsecret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\ n tenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\ n client_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\ n subscription, secret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\\\ ntenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\\\ nclient_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\\\ nsubscription_id: {{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\\\\ nsecret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\\\ n tenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\\\ n client_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\\\ n subscription, secret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\\\ntenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\\\nclient_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\\\nsubscription_id: {{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\\\\nsecret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\\\n tenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\\\n client_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\\\n subscription, secret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\ntenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\nclient_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\nsubscription_id: {{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\\nsecret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\\n tenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\\n client_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\\n subscription, secret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\ntenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\nclient_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\nsubscription_id: {{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\nsecret: {{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\n tenant: {{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\n client_id: {{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\n subscription",CWE-307,1,"---
- set_fact:
    resource_group: ""Algo_{{ region }}""

- name: Create a resource group
  azure_rm_resourcegroup:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    name: ""{{ resource_group }}""
    location: ""{{ region }}""
    tags:
        service: algo

- name: Create a virtual network
  azure_rm_virtualnetwork:
    resource_group: ""{{ resource_group }}""
    name: algo_net
    address_prefixes: ""10.10.0.0/16""
    tags:
        service: algo

- name: Create a subnet
  azure_rm_subnet:
    resource_group: ""{{ resource_group }}""
    name: algo_subnet
    address_prefix: ""10.10.0.0/24""
    virtual_network: algo_net
    tags:
        service: algo

- name: Create an instance
  azure_rm_virtualmachine:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    resource_group: ""{{ resource_group }}""
    admin_username: ubuntu
    virtual_network: algo_net
    name: ""{{ azure_server_name }}""
    ssh_password_enabled: false
    vm_size: Standard_D1
    tags:
      service: algo
    ssh_public_keys:
      - { path: ""/home/ubuntu/.ssh/authorized_keys"", key_data: ""{{ lookup('file', '{{ ssh_public_key }}') }}"" }
    image:
      offer: UbuntuServer
      publisher: Canonical
      sku: '16.04-LTS'
      version: latest
  register: azure_rm_virtualmachine

- set_fact:
    ip_address: ""{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].properties.ipConfigurations[0].properties.publicIPAddress.properties.ipAddress }}""

- name: Add the instance to an inventory group
  add_host:
    name: ""{{ ip_address }}""
    groups: vpn-host
    ansible_ssh_user: ubuntu
    ansible_python_interpreter: ""/usr/bin/python2.7""
    easyrsa_p12_export_password: ""{{ easyrsa_p12_export_password }}""
    cloud_provider: azure
    ipv6_support: no

- name: Wait for SSH to become available
  local_action: ""wait_for port=22 host={{ ip_address }} timeout=320""",0,"client_id: \\\, client_id: \\\\\\\\\\\\, image:\\n\\t\\\\\\\\, location: {{ region }}\\n, location: {{ region }}\n, name: \\\\\, subscription_id: \\\\\\, subscription_id: \\\\\\\\\\\\, tags:\n\t\\\\, virtual_network: algo_net\\n","CWE-287, CWE-319, CWE-320",0
"- name: Build python virtual environment
  import_tasks: venv.yml
- name: Include prompts
  import_tasks: prompts.yml
- block:
    - set_fact:
        algo_region: >-
          {% if region is defined %}{{ region }}
          {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
          {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

    - name: Security group created
      cs_securitygroup:
        name: ""{{ algo_server_name }}-security_group""
        description: AlgoVPN security group
      register: cs_security_group

    - name: Security rules created
      cs_securitygroup_rule:
        security_group: ""{{ cs_security_group.name }}""
        protocol: ""{{ item.proto }}""
        start_port: ""{{ item.start_port }}""
        end_port: ""{{ item.end_port }}""
        cidr: ""{{ item.range }}""
      with_items:
        - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

    - name: Keypair created
      cs_sshkeypair:
        name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
        public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
      register: cs_keypair

    - name: Set facts
      set_fact:
        image_id: ""{{ cloud_providers.cloudstack.image }}""
        size: ""{{ cloud_providers.cloudstack.size }}""
        disk: ""{{ cloud_providers.cloudstack.disk }}""
        keypair_name: ""{{ cs_keypair.name }}""

    - name: Server created
      cs_instance:
        name: ""{{ algo_server_name }}""
        root_disk_size: ""{{ disk }}""
        template: ""{{ image_id }}""
        ssh_key: ""{{ keypair_name }}""
        security_groups: ""{{ cs_security_group.name }}""
        zone: ""{{ algo_region }}""
        service_offering: ""{{ size }}""
      register: cs_server

    - set_fact:
        cloud_instance_ip: ""{{ cs_server.default_ip }}""
        ansible_ssh_user: ubuntu
  environment:
    CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
    CLOUDSTACK_REGION: ""{{ algo_cs_region }}""",1,"cloud_providers.cloudstack, default_zone | int - 1], import_tasks: venv.yml\n- name: Include prompts\nimport_tasks: prompts.yml, set_fact: \\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n, with_items: \\\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n","CWE-20, CWE-312, CWE-319, CWE-937",1,"---
- block:
    - name: Build python virtual environment
      import_tasks: venv.yml

    - name: Include prompts
      import_tasks: prompts.yml

    - block:
      - set_fact:
          algo_region: >-
            {% if region is defined %}{{ region }}
            {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
            {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

      - name: Security group created
        cs_securitygroup:
          name: ""{{ algo_server_name }}-security_group""
          description: AlgoVPN security group
        register: cs_security_group

      - name: Security rules created
        cs_securitygroup_rule:
          security_group: ""{{ cs_security_group.name }}""
          protocol: ""{{ item.proto }}""
          start_port: ""{{ item.start_port }}""
          end_port: ""{{ item.end_port }}""
          cidr: ""{{ item.range }}""
        with_items:
          - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

      - name: Keypair created
        cs_sshkeypair:
          name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
          public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
        register: cs_keypair

      - name: Set facts
        set_fact:
          image_id: ""{{ cloud_providers.cloudstack.image }}""
          size: ""{{ cloud_providers.cloudstack.size }}""
          disk: ""{{ cloud_providers.cloudstack.disk }}""
          keypair_name: ""{{ cs_keypair.name }}""

      - name: Server created
        cs_instance:
          name: ""{{ algo_server_name }}""
          root_disk_size: ""{{ disk }}""
          template: ""{{ image_id }}""
          ssh_key: ""{{ keypair_name }}""
          security_groups: ""{{ cs_security_group.name }}""
          zone: ""{{ algo_region }}""
          service_offering: ""{{ size }}""
        register: cs_server

      - set_fact:
          cloud_instance_ip: ""{{ cs_server.default_ip }}""
          ansible_ssh_user: ubuntu
      environment:
        PYTHONPATH: ""{{ cloudstack_venv }}/lib/python2.7/site-packages/""
        CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
        CLOUDSTACK_REGION: ""{{ algo_cs_region }}""

      rescue:
      - debug: var=fail_hint
        tags: always
      - fail:
        tags: always",0,"- name: Build python virtual environment\\\\n import_tasks: venv.yml\\\\n\\\\n, - name: Include prompts import_tasks: prompts.yml, - name: Security rules created cs_securitygroup_rule: security_group: \, - set_fact: algo_region: \\\\\\r\\\\\n, - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }\r\n, - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }\\r\\n, block: - set_fact: algo_region: \\\r\\n, rescue: - debug: var=fail_hint \\\\\\\\\n tags: always - fail: \\\\\\\\\\\\\n tags: always, rescue: - debug: var=fail_hint\\\\n tags: always - fail: \\\\n tags: always, rescue: - debug: var=fail_hint\\n tags: always - fail: \\n tags: always","CWE-119, CWE-257",0
"-out crl/{{ item }}.crt
      creates: crl/{{ item }}.crt",1,"-out crl/{{ item }}.crt, creates: crl/{{ item }}.crt","CWE-1033, CWE-1035, CWE-120, CWE-125, CWE-128",1,"- name: Get active users
  local_action: >
    shell grep ^V index.txt | grep -v ""{{ IP_subject_alt_name }}"" | awk '{print $5}' | sed 's/\/CN=//g'
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
  register: valid_certs

- name: Revoke non-existing users
  local_action: >
    shell openssl ca -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt &&
      openssl ca -gencrl -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt -out crl/{{ item }}.crt
      touch crl/{{ item }}_revoked
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
    creates: crl/{{ item }}_revoked
  environment:
    subjectAltName: ""DNS:{{ item }}""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""

- name: Copy the revoked certificates to the vpn server
  copy:
    src: configs/{{ IP_subject_alt_name }}/pki/crl/{{ item }}.crt
    dest: ""{{ config_prefix|default('/') }}etc/ipsec.d/crls/{{ item }}.crt""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""
  notify:
    - rereadcrls",0,"args: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, args: \\\\\\\\\\\\\\\\\\\\\\\\\r\\\\\\\\\\\\n chdir: \\\\\\\\\\\\\\\\, args: \\\\\\\\\r\\\\n chdir: \\\\, args: \\\r\\n chdir: \\, args: \r\n chdir: \, environment: \\r\\n subjectAltName: \\, when: item not in users\\\\\\\\r\\\\n  with_items: \\\\\\\\\\\\, with_items: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\",CWE-327,0
"name: ""{{ opensmtpd_extra_packages }}""",1,"name: \\, name: \\\, name: \\\\, name: \\\\\\\\, name: \\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, name: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\","CWE-769, CWE-778, CWE-779",1,"---

- name: Enable opensmtpd_service
  service:
    name: ""{{ opensmtpd_service }}""
    arguments: ""{{ opensmtpd_flags }}""
    enabled: yes

- name: Install opensmtpd_extra_packages
  openbsd_pkg:
    name: ""{{ item }}""
  with_items: ""{{ opensmtpd_extra_packages }}""",0,"arguments: \\, enabled: yes, item, name: \\, name: \\\\, service: {\n  name: \, with_items: \\","CWE-119, CWE-772, CWE-779",0
"mode: 0644
    group: 'root'
    owner: 'root'",1,"group: 'root', mode: 0644, mode: 0644\\n, mode: 0644\n, owner: 'root'","CWE-22, CWE-221",,mode: 0600,0,mode: 0600,CWE-1024,0
- irods-rule-engine-plugin-python-4.2.8.0-1,1,irods-rule-engine-plugin-python-4.2.8.0-1,CWE-22,,"- name: Ensure iRODS 4.2.7 packages are absent
      - irods-uu-microservices-4.2.7_0.8.1-1
      - irods-sudo-microservices-4.2.7_1.0.0-1
      - davrods-4.2.7_1.4.2-1
      - irods-server-4.2.8-1
      - irods-runtime-4.2.8-1
      - irods-database-plugin-postgres-4.2.8-1
      - irods-rule-engine-plugin-python-4.2.8-1",0,"- name: Ensure iRODS 4.2.7 packages are absent, davrods-4.2.7_1.4.2-1, irods-database-plugin-postgres-4.2.8-1, irods-rule-engine-plugin-python-4.2.8-1, irods-runtime-4.2.8-1, irods-server-4.2.8-1, irods-sudo-microservices-4.2.7_1.0.0-1, irods-uu-microservices-4.2.7_0.8.1-1","CWE-601, CWE-862",0
when: ruleset.install_scripts and install_rulesets,1,when: ruleset.install_scripts and install_rulesets,"CWE-1031, CWE-1033, CWE-1034, CWE-1044, CWE-1049, CWE-1052, CWE-1091, CWE-1247, CWE-1254, CWE-1304",,"when: ruleset.install_scripts == ""yes"" and install_rulesets == ""yes""",0,"and install_rulesets == \\, install_rulesets, install_rulesets == \\, ruleset.install_scripts, ruleset.install_scripts == \\, when: ruleset.install_scripts == \",CWE-1040,0
description: Install and configure viasite/zsh-config and oh-my-zsh,1,"- hosts: localhost\\n      roles:\\n        - {role: ansible-role-viasite-zsh-config}, become: yes, hosts: localhost, name: Change owner of home directory to local user, name: Change owner of home directory to local user\\\\n      sudo: yes, name: Create home directory for local user, name: Create home directory for local user\\n      sudo: yes, name: Create local user, roles:\\n  - {role: ansible-role-viasite-zsh-config}, vars:\n    ansible_host: localhost","CWE-190, CWE-275, CWE-276, CWE-295, CWE-319, CWE-327",,"---
galaxy_info:
  author: Stanislav Popov
  company: Viasite
  description: Install and configure popstas/zsh-config and oh-my-zsh
  license: MIT
  min_ansible_version: 1.8
  platforms:
    - name: Ubuntu
      versions:
        - trusty
        - xenial
  categories:
    - system
dependencies: []",0,"dependencies: [], galaxy_info: {min_ansible_version: 1.8}, platforms: {name: Ubuntu}","CWE-190, CWE-20",0
"- wazuh_manager_config.cluster.node_type == ""master"" or wazuh_manager_config.cluster.node_type == ""worker""",1,"ansible_playbook, if, master or worker, wazuh_manager_config.cluster, wazuh_manager_config.cluster.node_type, wazuh_manager_config.cluster.node_type == \, wazuh_manager_config.cluster.node_type == \\, wazuh_manager_config.cluster.node_type == \\\\","CWE-1037, CWE-601",,"when:       
      - wazuh_manager_config.cluster.node_type == ""master""
      - wazuh_manager_config.cluster.node_type == ""master""",0,"- wazuh_manager_config.cluster.node_type == \\\\, master\\\\\\\\\\\\n    \\\\, master\\n    \\, wazuh_manager_config.cluster.node_type == \, wazuh_manager_config.cluster.node_type == \\, wazuh_manager_config.cluster.node_type == \\\\\\\\, wazuh_manager_config.cluster.node_type == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, when: \\, when: \\\\\\\\\\\\\\\\, when: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n    \\\\\\\\\\\\\\\\\\\\","WAZUH0009, WAZUH0048",0
"baseurl: ""{{ wazuh_manager_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}-5""
    baseurl: ""{{ wazuh_manager_config.repo.yum }}""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}""",1,"baseurl: \, baseurl: \\, baseurl: \\\\, gpgkey: \\, gpgkey: \\\\, gpgkey: \\\\\\",CWE-1196,,"- name: RedHat/CentOS 5 | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}-5""
    - (ansible_facts['os_family']|lower == 'redhat')
    - (ansible_os_family = ansible_distribution_major_version|int <= 5)
  register: repo_v5_manager_installed
- name: RedHat/CentOS/Fedora | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}""
  changed_when: false
    - repo_v5_manager_installed is undefined",0,"- (ansible_facts['os_family']|lower =='redhat'), - name: RedHat/CentOS 5 | Install Wazuh repo, - name: RedHat/CentOS/Fedora | Install Wazuh repo, baseurl:, gpg:, gpgkey:, register: repo_v5_manager_installed, repo_v5_manager_installed is undefined","1006, 1189",0
"od_node_name: ""{{ elasticsearch_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""",1,"vars:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, vars:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - {kibana_node_name}, vars:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - {kibana_node_name}, vars:\\\\\\\\\\\\\\\\\\\\\\n  - {kibana_node_name}, vars:\\\\\\\\\\n  - {kibana_node_name}, vars:\\\\n  - {kibana_node_name}, vars:\\n  - {kibana_node_name}, vars:\n  - {elasticsearch_node_name}",CWE-190,1,"- name: Configure node name
    block:
      - name: Setting node name (Elasticsearch)
        set_fact:
          od_node_name: elasticsearch_node_name
        when:
          elasticsearch_node_name is defined and kibana_node_name is not defined

      - name: Setting node name (Kibana)
        set_fact:
          od_node_name: kibana_node_name
        when:
          kibana_node_name is defined

      - name: Setting node name (Filebeat)
        set_fact:
          od_node_name: filebeat_node_name
        when:
          filebeat_node_name is defined


      - ""{{ od_node_name }}.key""
      - ""{{ od_node_name }}.pem""
      - ""{{ od_node_name }}_http.key""
      - ""{{ od_node_name }}_http.pem""
      - ""{{ od_node_name }}_elasticsearch_config_snippet.yml""
      block: ""{{ lookup('file', '{{ local_certs_path }}/certs/{{ od_node_name }}_elasticsearch_config_snippet.yml') }}""
      -h {{ hostvars[od_node_name]['ip'] }}",0,tasks/main.yml,CWE-284,0
"- {
      role: geerlingguy.repo-epel,
      version: 1.2.3,
      tags: [
        ""dependency"",
        ""dependency.epel""
      ],
      when: ansible_os_family == 'RedHat'
  }
  - {
      role: srsp.oracle-java,
      version: 2.19.1,
      tags: [
        ""dependency"",
        ""dependency.java""
      ]
  }
  - {
      role: gantsign.maven,
      version: 4.0.0,
      tags: [
        ""dependency,"",
        ""dependency.maven""
      ]
  }
  - { role: andrewrothstein.terraform,
      version: v2.2.10,
      tags: [
        ""dependency"",
        ""dependency.terraform""
      ]
  }",1,ansible_os_family,"CWE-1043, CWE-829",1,"dependencies:
  - { role: geerlingguy.repo-epel, tags: [""dependency""] }
  - { role: srsp.oracle-java, tags: [""dependency""] }
  - { role: gantsign.maven, tags: [""dependency""] }
  - { role: andrewrothstein.terraform, tags: [""dependency""] }",0,"dependencies:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dependencies:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n  - { role: gantsign.maven, tags: [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\, dependencies:\\\\\\\\\\\\\\\\\\\\\\n  - { role: geerlingguy.repo-epel, tags: [\\\\\\\\\\\\\\\\\\\\\\\\, dependencies:\\\\\\\\n  - { role: gantsign.maven, tags: [\\\\\\\\\\\\, dependencies:\\\\n  - { role: andrewrothstein.terraform, tags: [\\\\, dependencies:\\n  - { role: gantsign.maven, tags: [\\, dependencies:\n  - { role: geerlingguy.repo-epel, tags: [\",CWE-20,0