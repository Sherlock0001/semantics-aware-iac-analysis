Commit_id,Added_lines_fixing_commit,Added_lines_bug_inducing_commit
5ee1e89cebf89a85e1ccb4fdc4c55d4b0e5c8f4a,"  command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""","- name: ""[NC] - Set Nextcloud settings in config.php""
  shell: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_config_settings }}""
- name: ""[NC] - Set Redis Server""
  command: php occ config:system:set {{ item.name }} --value=""{{ item.value }}""
    - ""{{ nextcloud_redis_settings }}""
  when: nextcloud_install_redis_server == True

    mode: 0750"
b6a5ef7c8339e29cefdefa0fa5e25ec2b9785df9,when: (not _nextcloud_conf.stat.exists) or (_nextcloud_configured.rc is defined and _nextcloud_configured.rc != 0),"  check_mode: no
  when: _nextcloud_configured.rc != 0"
bc3cc2c906ce22b390d257f4b760aeb0fea845ab,when: (nc_sudo_installed_result.rc is defined) and (nc_sudo_installed_result.rc != 0),"  register: nc_sudo_installed_result
  when: nc_sudo_installed_result is defined and nc_sudo_installed_result.rc != 0"
1563b184d00351eb67ecb42b6affbb3f6b34b31d,"  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
","  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
"
1563b184d00351eb67ecb42b6affbb3f6b34b31d,"  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state | default('present') }}  update_cache=yes""
","
##
 # BROWSER
 #
 
- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
#TODO sudo ln -s /usr/lib/chromium-browser/chromedriver /var/lib/chromedriver
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
#TODO sudo ln -s /usr/bin/chromedrive /var/lib/chromedriver  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes"
1563b184d00351eb67ecb42b6affbb3f6b34b31d,"  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
","  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes""
  action: ""{{ ansible_pkg_mgr }} name={{ item }} state={{ util_pkg_state|default('present') }}  update_cache=yes"""
1563b184d00351eb67ecb42b6affbb3f6b34b31d,"  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ util_pkg_state | default('present') }}""
","
##
 # BROWSER
 #
 
- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  with_items:
    - chromium-chromedriver # see selenium role
  become: yes
  changed_when: false

- name: jenkins-slave | Install requirement for zaproxy
  action: ""{{ ansible_pkg_mgr }} name={{ item }} update_cache=yes state={{ jenkins_pkg_state|default('present') }}""
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  with_items:
    - chromedriver # see selenium role
  become: yes
  changed_when: false
  
#TODO sudo ln -s /usr/lib/chromium-browser/chromedriver /var/lib/chromedriver
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/lib/chromium-browser/chromedriver state=link
  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu')
  ignore_errors: true   
  become: yes
  
#TODO sudo ln -s /usr/bin/chromedrive /var/lib/chromedriver  
- name: jenkins-slave | Create chromedriver directory link (legacy required)
  file: path=/var/lib/chromedriver src=/usr/bin/chromedriver state=link
  when: (ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat')
  ignore_errors: true   
  become: yes"
c3e886658c02e0ae1358a526e848efb305e87163,"    - (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare(8, '<'))","    - (ansible_distribution_version != 'buster/sid') and (ansible_distribution_version is version_compare
(8, '<'))"
88892c691caa20d9744a88d83a92f45d3e3e9510,"    owner: ""{{ consul_user }}""
    group: ""{{ consul_group }}""","  file:
    dest: ""{{ consul_tls_dir }}""
    state: directory
    owner: root
    group: root
    mode: 0755
    copy:
      src: ""{{ consul_src_files }}/{{ consul_ca_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_ca_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_crt }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_crt }}""
    copy:
      src: ""{{ consul_src_files }}/{{ consul_server_key }}""
      dest: ""{{ consul_tls_dir }}/{{ consul_server_key }}""
  template:
    src: config_server_tls.json.j2
    dest: ""{{ consul_config_path }}/server/config_server_tls.json"""
4db4dbc669a92509651ea709cab37f2138222363,"    url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ item }}""
    url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script""","    url: ""http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}/service/siesta/rest/v1/script/{{ item }}"""
3003b5a0d7edcc879dadd96bd18098f1902a3d91,"- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: ""modify auth user admin password {{admin_password}}""
  register: change_password
  until: change_password is not failed
  retries: 5
","- name: Change BIG-IP F5 mgmt password
  bigip_command:
    provider:
      ssh_keyfile: ""{{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}-private.pem""
      transport: cli
      user: admin
      server: ""{{ ansible_host }}""
    commands: modify auth user admin password admin"
de17464d2a3f6fce64b95b75cc88975aa1c6c4cc,"  - name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""{{ec2_info.arista.filter}}""
        architecture: ""x86_64""
    register: arista_amis","#### CISCO AMI
- name: BLOCK FOR CISCO AMI
  block:
  - name: find ami for cisco (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""cisco-CSR*BYOL*""
        architecture: ""x86_64""
    register: cisco_ami_list

  - name: save ami for cisco (NETWORKING MODE)
    set_fact:
      cisco_ami: >
        {{ cisco_ami_list.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""cisco""'

#### ARISTA AMI
- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for arista (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""*EOS*""
        architecture: ""x86_64""
    register: arista_amis

  - name: save ami for arista eos (NETWORKING MODE)
    set_fact:
      arista_ami: >
        {{ arista_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""arista""'

#### JUNIPER AMI
- name: BLOCK FOR ARISTA AMI
  block:
  - name: find ami for juniper vsrx (NETWORKING MODE)
    ec2_ami_facts:
      region: ""{{ ec2_region }}""
      owners: ""679593333241""
      filters:
        name: ""junos-vsrx3-x86-64-18.4R1.8--pm*""
        architecture: ""x86_64""
    register: juniper_amis

  - name: save ami for juniper (NETWORKING MODE)
    set_fact:
      juniper_ami: >
        {{ juniper_amis.images | selectattr('name', 'defined') | sort(attribute='creation_date') | last }}
  when: 'network_type == ""multivendor"" or network_type == ""juniper""'"
cd4135f3c47a318db17732fa4775faf7c72d9b0a,"    - name: fail on purpose now to let user know code server failed
      debug:
        msg: ""VS code integration has failed in provisioner/roles/code_server/tasks/main.yml""
      failed_when: true","---
- name: remove dns entries for each vs code instance
  include_tasks: teardown.yml
  when: teardown|bool

- name: check to see if SSL cert already applied
  become: no
  get_certificate:
    host: ""{{username}}-code.{{ec2_name_prefix|lower}}.{{workshop_dns_zone}}""
    port: 443
  delegate_to: localhost
  run_once: true
  register: check_cert
  ignore_errors: true
  when:
    - not teardown

- name: perform DNS and SSL certs for ansible control node
  block:
    - name: setup vscode for web browser access
      include_tasks: ""codeserver.yml""
  rescue:
    - debug:
        msg: 'VS code integration has failed'

    - name: make sure tower is on
      shell: ansible-tower-service start
      register: install_tower
      until: install_tower is not failed
      retries: 5

    - name: appends
      set_fact:
        coder_information: |
          - VS code integration has failed, please use direct SSH addresses
      run_once: true
      delegate_to: localhost
      delegate_facts: true
  when:
    - not teardown|bool
    - check_cert is failed"
758cc03405dc7aa418911a39ad82f73cad7853f5,"  command: systemctl daemon-reload
  tags: skip_ansible_lint","  systemd:
    daemon-reload: yes"
f5163a9d506c056bf13fc11f9a4929c7c1434b9a,"  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ mode=0755 remote_src=True
  notify: Restart Caddy","  unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} copy=no
  unarchive: src={{ caddy_home }}/caddy.tar.gz dest={{ caddy_home }} creates={{ caddy_home }}/caddy copy=no

- name: Copy Caddy Binary
  copy: src={{ caddy_home }}/caddy dest=/usr/bin/ remote_src=True"
e3d1b8496d75c72dbbcdfd84d7a7ed9a19a1a587,"  retries: 3
  delay: 2
  retries: 3
  delay: 2","  user:
    name: ""{{ caddy_user }}""
    system: yes
    createhome: yes
    home: ""{{ caddy_home }}""
  get_url:
    url: https://api.github.com/repos/mholt/caddy/git/refs/tags
    dest: ""{{ caddy_home }}/releases.txt""
    force: yes
  copy:
    content: ""{{ caddy_features }}""
    dest: ""{{ caddy_home }}/features.txt""
  get_url:
    url: ""{{ caddy_url | quote }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    force: yes
    timeout: 300
    retries: 3
    delay: 2
  get_url:
    url: ""{{ caddy_url}}""
    dest: ""{{ caddy_home }}/caddy.tar.gz""
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
    timeout: 300
    retries: 3
    delay: 2
  command: >
    gpg
      --keyserver-options timeout={{ caddy_pgp_recv_timeout }}
      --keyserver {{ caddy_pgp_key_server }}
      --recv-keys {{ caddy_pgp_key_id }}
  get_url:
    url: ""{{ caddy_sig_url }}""
    dest: ""{{ caddy_home }}/caddy.tar.gz.asc""
    timeout: 60
    force: yes
    force_basic_auth: ""{{ caddy_license != 'personal' }}""
  command: >
    gpg
      --verify {{ caddy_home }}/caddy.tar.gz.asc
      {{ caddy_home }}/caddy.tar.gz
  unarchive:
    src: ""{{ caddy_home }}/caddy.tar.gz""
    dest: ""{{ caddy_home }}""
    copy: no
    owner: ""{{ caddy_user }}""
  unarchive:
   src: ""{{ caddy_home }}/caddy.tar.gz""
   dest: ""{{ caddy_home }}""
   creates: ""{{ caddy_home }}/caddy""
   copy: no
   owner: ""{{ caddy_user }}""
  copy:
    src: ""{{ caddy_home }}/caddy""
    dest: ""{{ caddy_bin }}""
    mode: 0755
    remote_src: true
  file:
    path: ""{{ item }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0770
  file:
    path: ""{{ caddy_log_dir }}""
    state: directory
    owner: ""{{ caddy_user }}""
    mode: 0775
  copy:
    content: ""{{ caddy_config }}""
    dest: ""{{ caddy_conf_dir }}/Caddyfile""
    owner: ""{{ caddy_user }}""
  stat:
    path: /run/systemd/system
  template:
    src: ""{{ item }}""
    dest: /etc/init/caddy.conf
    mode: 0644
  template:
    src: caddy.service
    dest: /etc/systemd/system/caddy.service
    mode: 0644
  service:
    name: caddy
    state: started
    enabled: yes"
e4f403862aeae7a64c6708cb687088cbf014c41e,"  shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;'""","  when: item.hstore is defined and item.hstore

- name: PostgreSQL | Add uuid-ossp to the database with the requirement
  sudo: yes
  sudo_user: ""{{postgresql_admin_user}}""
  shell: ""psql {{item.name}} -c 'CREATE EXTENSION IF NOT EXISTS uuid-ossp;""
  with_items: postgresql_databases
  when: item.uuid_ossp is defined and item.uuid_ossp"
5691ee0fd602158c140c83c66eaa04e961b2bd86,"    name: ""postgresql-contrib-{{postgresql_version}}""","# file: postgresql/tasks/extensions/contrib.yml

- name: PostgreSQL | Extensions | Make sure the development headers are installed
  apt:
    name: libpq-dev
    state: present
  notify:
    - restart postgresql
"
22a6ddf73051eaea683771030fe9c824143acf73,"  run_once: True
  run_once: True
- name: 'Create folder {{structured_dir_name}} for structured YAML files'
  run_once: True
  run_once: True

- name: 'Create folder {{structured_cvp_name}} for CVP structured YAML files'
  file:
    path: '{{structured_cvp}}'
    state: directory
    mode: 0755
  delegate_to: localhost
  run_once: True
","---
# tasks file for build_directories

- name: 'Cleanup existing folders in {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: absent 
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{output_dir}}'
  file:
    path: '{{output_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{structured_dir_name}} for structued YAML files'
  file:
    path: '{{structured_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost
- name: 'Create folder {{eos_config_dir_name}} for EOS Configuration files'
  file:
    path: '{{eos_config_dir}}'
    state: directory
    mode: 0755
  delegate_to: localhost"
bdcbb7d3509cd422cf16d82b9c9883466a85dc36,- name: Create/invoke script virtualenv for create galaxy admin,"- name: Create/invoke script virtualenv
  pip: name={{ item }} virtualenv={{ galaxy_venv_dir }} virtualenv_command=""{{ pip_virtualenv_command | default( 'virtualenv' ) }}""
  with_items:
    - pyyaml
    - bioblend
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""

- name: Create Galaxy admin user
  command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python /usr/local/bin/create_galaxy_user.py --user {{ galaxy_admin }} --password {{ galaxy_admin_pw }} --key {{ galaxy_admin_api_key }}
  sudo: yes
  sudo_user: ""{{ galaxy_user_name }}""

#- name: Copy the bootstrap user management script
#  copy: src=manage_bootstrap_user.py dest={{ galaxy_server_dir }}/manage_bootstrap_user.py owner={{ galaxy_user_name }}

#- name: Create Galaxy bootstrap user
#  command: chdir={{ galaxy_server_dir }} {{ galaxy_venv_dir }}/bin/python manage_bootstrap_user.py -c {{ galaxy_config_file }} create -e {{ galaxy_admin }} -p """"

#- name: Remove the bootstrap user management script
#  file: dest={{ galaxy_server_dir }}/manage_bootstrap_user.py state=absent"
ad2b7b3268eea6c4fd804389525055f94ae076ca,"  shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  shell: ""cat /var/log/supervisor/galaxy*""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /home/galaxy/galaxy/*.log""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs

- name: get_logs
  shell: ""cat /etc/supervisor/conf.d/galaxy.conf""
- debug: var=galaxylogs

- name: get_logs
  shell: ""supervisorctl status""
  register: galaxylogs
  ignore_errors: yes

- debug: var=galaxylogs
","- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs

- name get_logs
  shell: cat /var/log/supervior/galaxy*
  register: galaxy_logs
  ignore_errors: yes

- debug var=galaxy_logs
"
6e731af7905578a4f00cbc615ca6369b42d2e444,"    - {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugins/interactive_environments/rstudio/config/allowed_images.yml""}","    - {src: ""rstudio.ini.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/rstudio.ini""}
    - {src: ""allowed_images.yml.j2"", dest: ""{{ galaxy_config_dir }}/plugin/interactive_environments/rstudio/config/allowed_images.yml""}"
c48c6dd160a85e96d841386cc61a0be5ff000433,"    - export NATIVE_SPEC=""--ntasks=`/usr/bin/nproc` --share""","supervisor_env_vars:
    - export IP_ADDRESS=`curl icanhazip.com`
    - export GALAXY_CONF_FTP_UPLOAD_SITE=""ftp://$IP_ADDRESS""
    - export MASQUERADE_ADDRESS=$IP_ADDRESS
    - export NTASK=""--ntasks=`/usr/bin/nproc` --share"""
d5b6394f44aa82451079d84f646e200caafbd2a8,"########## BOTH ########## 


- name: create data directory for cassandra on mounted storage
  file: path={{data_dir}}/cassandra state=directory owner={{cassandra_user}}  group={{cassandra_user}}

- name: create the cassandra yaml from template
  template: src=cassandra/cassandra.yaml dest={{cassandra_yaml}} owner={{cassandra_user}}  group={{cassandra_user}}
    - restart cassandra  




","
########## REDHAT ########## 

- name: setup yum repo
  copy: src=yum/datastax.repo dest=/etc/yum.repos.d/datastax.repo
  when: ansible_os_family == ""RedHat""   

- name: install packages (Redhat)
  yum: name={{item}} state=present
  with_items:
    - $datastax
    - $cassandra
  tags:
    - packages 
  notify: 
    - restart cassandra
    - configure cassandra
  when: ansible_os_family == ""RedHat"" 

- name: set cassandra to use the ByteOrderedPartitioner (RedHat)
  lineinfile: "" 
    dest=/etc/cassandra/conf/cassandra.yaml  
    regexp=^partitioner 
    line='partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner'""
  notify:
    - restart cassandra
  when: ansible_os_family == ""RedHat""     
 

########## DEBIAN ########## 

- name: install packages (Debian)
  apt: pkg=wget  state=present
  when: ansible_os_family == ""Debian"" 

- name: install packages (Debian)
  apt: pkg=software-properties-common  state=present
  when: ansible_os_family == ""Debian"" 

- name: install packages 2 (Debian)
  apt: pkg=python-software-properties  state=present
  when: ansible_os_family == ""Debian"" 

- name: apt-add-repository (Debian)
  command: apt-add-repository 'deb http://debian.datastax.com/community stable main'
  when: ansible_os_family == ""Debian"" 

- name: get datastax repo key (Debian)
  command: wget http://debian.datastax.com/debian/repo_key --output-document=/tmp/datastax.key
  when: ansible_os_family == ""Debian""   

- name: add datastax repo key  (Debian)
  command: apt-key add /tmp/datastax.key
  when: ansible_os_family == ""Debian""   

- name: install Cassandra (Debian)
  apt: pkg=cassandra=1.2.10 state=present update_cache=yes
  with_items:
    - $cassandra
  tags:
    - packages 
  notify: 
    - restart cassandra
    - configure cassandra
  when: ansible_os_family == ""Debian"" 


- name: set cassandra to use the ByteOrderedPartitioner  (Debian)
  lineinfile: "" 
    dest=/etc/cassandra/cassandra.yaml  
    regexp=^partitioner 
    line='partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner'""
  notify:
    - restart cassandra
  when: ansible_os_family == ""Debian""


- name: increase stack size (Debian only)
  shell: ""sed -e s/Xss180k/Xss256k/g /etc/cassandra/cassandra-env.sh --in-place""
  when: ansible_os_family == ""Debian""

- name: setup production settings for Cassandra (not overwrites files)
  copy: src={{item.src}} dest={{item.dest}}
  with_items:
    - { src: cassandra/90-nproc.conf, dest: /etc/security/limits.d/90-nproc.conf }
    - { src: cassandra/limits.conf, dest: /etc/security/limits.conf }
  notify: 
    - restart cassandra"
fedada359674dc76ed3b7f96edd77a5753e242fe,"  dir: '{{ data_dir }}/spatial-data'
  url: '{{ geoserver_url }}'
  geoserver_data_dir: ""{{ geoserver_data_dir | default('/data/spatial-data/geoserver_data_dir') }}""
biocacheServiceUrl: ""{{ biocache_service_url | default('https://biocache.ala.org.au/ws') }}""
biocacheUrl: ""{{ biocache_url | default('https://biocache.ala.org.au') }}""
spatialService.url: ""{{ spatial_service_url }}""
grails.serverURL: {{spatial_service_url}}","#
# au.org.ala.spatial.service config
#
data:
  dir: '{{data_dir}}/spatial-data'
geoserver:
  url: '{{geoserver_url}}'
  username: '{{ geoserver_username | default('admin') }}'
  password: '{{ geoserver_password | default('geoserver') }}'
  canDeploy: {{ can_deploy_to_geoserver | default('true') }}
# To use a remote geoserver instance, set geoserver.remote.geoserver_data_dir to the geoserver_data_dir path on the
# remote server. This will cause layer files to be copied to geoserver_data_dir/data/
#  remote:
  geoserver_data_dir: ""{{ geoserver_data_dir | default('{{data_dir}}/geoserver_data_dir') }}""

shpResolutions: {{ shp_resolutions | default([0.5, 0.25, 0.1, 0.05]) }}
grdResolutions: {{ grd_resolutions | default([0.5, 0.25, 0.1, 0.05, 0.01]) }}

biocacheServiceUrl: ""{{biocache_service_url | default('https://biocache.ala.org.au/ws')}}""
biocacheUrl: ""{{biocache_url | default('https://biocache.ala.org.au')}}""
openstreetmap:
  url: ""{{ openstreetmap_tile_url | default('https://tile.openstreetmap.org') }}""

slave.enable: {{ slave_enable | default(true) }}
service.enable: {{ service_enable | default(true) }}

serviceKey: {{ spatial_service_service_key  | default('') }}
batch_sampling_passwords: ""{{ batch_sampling_passwords | default('') }}""
batch_sampling_points_limit: {{ batch_sampling_points_limit | default(1000000) }}
batch_sampling_fields_limit: {{ batch_sampling_fields_limit | default(1000) }}

---
#
# au.org.ala.spatial.slave config
#
spatialService.url: ""{{spatial_service_url}}""
data.dir: ""{{ data_dir }}/spatial-data""
shp2pgsql.path: ""{{ shp2pgsql_path | default('/usr/bin/shp2pgsql') }}""
gdal.dir: ""{{ gdal_dir | default('/usr/bin/') }}""
gdm.dir: ""{{ gdm_dir | default('/data/spatial-data/modelling/gdm/DoGdm') }}""

aloc.xmx: ""{{ aloc_xmx | default('6G') }}""
aloc.threads: ""{{ aloc_threads | default(4) }}""
maxent.mx: ""{{ maxent_mx | default('1G') }}""
maxent.threads: ""{{ maxent_threads | default(4) }}""

sampling.threads: ""{{ sampling_threads | default(4) }}""

slaveKey: ""{{ spatial_service_slave_key | default('') }}""
serviceKey: ""{{ spatial_service_service_key | default('') }}""

# time between pushing status updates to the master for a task
statusTime: ""{{ status_time | default(3000) }}""
retryCount: ""{{ retry_count | default(10) }}""
retryTime: ""{{ retry_time | default(30000) }}""

#
#  CAS SETTINGS
#
#  NOTE: Some of these will be ignored if default_config exists
security:
  cas:
    casServerName: {{ auth_base_url }}
    uriFilterPattern: {{ uri_filter_pattern | default('/manageLayers,/manageLayers/.*,/admin,/admin/.*,/alaAdmin.*') }}
    uriExclusionFilterPattern: {{ uri_exclusion_filter_pattern | default('/assets.*,/images.*,/css.*,/js.*,/less.*,/tasks/status/.*') }}
    authenticateOnlyIfLoggedInFilterPattern: {{ authenticate_only_if_logged_in_filter_pattern | default('/master,/master/.*,/tasks,/tasks/.*') }}
    appServerName: {{ spatial_service_base_url }}
    casServerUrlPrefix: {{ auth_cas_url }}
    loginUrl: {{ auth_cas_url }}/login
    logoutUrl: {{ auth_cas_url }}/logout
    contextPath: {{ spatial_service_context_path }}
    bypass: {{ bypass_cas | default(true) }}
    disableCAS: {{ bypass_cas | default(true) }}
    gateway: {{ gateway_cas | default(false) }}

auth.admin_role: {{ auth_admin_role | default('ROLE_ADMIN') }}
app.http.header.userId: {{ app_http_header_userid | default('X-ALA-userId') }}

headerAndFooter.baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3') }}
ala.baseURL: {{ ala_base_url | default('https://www.ala.org.au') }}
bie.baseURL: {{ bie_base_url | default('https://bie.ala.org.au') }}
bie.searchPath: '/search'

records.url: {{ records_url | default('https://archives.ala.org.au/archives/exports/lat_lon_taxon.zip') }}

api_key: {{ spatial_service_api_key  | default('') }}
lists.url: {{ lists_url | default('https://lists.ala.org.au') }}
collections.url: {{ collections_url | default('https://collections.ala.org.au') }}
sandboxHubUrl: {{ sandbox_url | default('http://sandbox.ala.org.au/ala-hub') }}
sandboxBiocacheServiceUrl: {{sandbox_biocache_service_url | default('http://sandbox.ala.org.au/biocache-service') }}
phyloServiceUrl: {{ phylolink_url | default('https://phylolink.ala.org.au') }}

spatialHubUrl: {{ spatial_hub_url }}

gazField: {{ gaz_field | default('cl915') }}
userObjectsField: {{ user_objects_field | default('cl1083') }}

apiKeyCheckUrlTemplate: ""{{api_key_check_url_template | default('https://auth.ala.org.au/apikey/ws/check?apikey={0}') }}""
spatialService.remote: ""{{spatial_service_remote_url}}""

journalmap.api_key: {{ journalmap_api_key | default('') }}
journalmap.url: {{ journalmap_url | default('https://www.journalmap.org/') }}

# For side by side installation with layers-service, analysis-service
#legacy.workingdir: '/{{ data_dir }}/ala/data/alaspatial/'

#legacy.enabled: true

#legacy compatability type
#""link"" = link legacy files into new locations
#""copy"" = copy legacy files into new locations
#""move"" = move legacy files into new locations
#legacy.type=""link""

#legacy.ANALYSIS_LAYER_FILES_PATH: '{{ data_dir }}/ala/data/layers/analysis/'
#legacy.LAYER_FILES_PATH: '{{ data_dir }}/ala/data/layers/ready'
#legacy.ALASPATIAL_OUTPUT_PATH: '{{ data_dir }}/ala/runtime/output'

grails.plugin.elfinder.rootDir: '{{ data_dir }}/spatial-service'

i18n.override.dir: '{{ data_dir }}/spatial-service/config/i81n/'


#layers-store config

#Threads created for each batch intersection and each individual shape file
#layers_store.BATCH_THREAD_COUNT: 3

#Set LAYER_INDEX_URL to use REMOVE layer intersections.
#layers_store.LAYER_INDEX_URL: https://spatial.ala.org.au/layers-service

#Use local layer files for sampling or the /intersect/batch service provided by LAYER_INDEX_URL
#layers_store.LOCAL_SAMPLING: false
#layers_store.LOCAL_SAMPLING: true

# Set intersect config reload time in ms
#layers_store.CONFIG_RELOAD_WAIT: 12000000

#Comma separated shape file fields to preload, or 'all'
#layers_store.PRELOADED_SHAPE_FILES: all
#layers_store.PRELOADED_SHAPE_FILES: cl22,cl20

# Grid intersection buffer size in bytes.  Must be multiple of 64.
# Only applies to grids > 80MB.
# layers_store.GRID_BUFFER_SIZE=4096
#layers_store.GRID_BUFFER_SIZE: 40960

# Number of GridCacheReader objects to open.
#layers_store.GRID_CACHE_READER_COUNT: 5

# layers_store ingestion
#layers_store.CAN_INGEST_LAYERS: false
#layers_store.CAN_UPDATE_LAYER_DISTANCES: false
#layers_store.CAN_UPDATE_GRID_CACHE: false
#layers_store.CAN_GENERATE_ANALYSIS_FILES: false
#layers_store.CAN_INTERSECT_LAYERS: false
#layers_store.CAN_GENRATE_THUMBNAILS: false

#layers_store.FIELD_STYLES: true

layers_store.GEONETWORK_URL: '{{ geonetwork_url | default('') }}'

distributions.cache.dir: ""{{ data_dir }}/${appName}/mapCache/""
distributions.geoserver.image.url: ""/ALA/wms?service=WMS&version=1.1.0&request=GetMap&sld={{ distribution_image_sld_url | default('https://fish.ala.org.au/data/dist.sld')}}&layers=ALA:aus1,ALA:Distributions&styles=&bbox=109,-47,157,-7&srs=EPSG:4326&format=image/png&width=400&height=400&viewparams=s:""

dataSource:
    url: 'jdbc:postgresql://{{layers_db_host}}/{{layers_db_name}}'
    username: {{layers_db_username}}
    password: {{layers_db_password}}

grails.serverURL: {{spatial_service_base_url}}
grails.app.context: {{spatial_service_context_path}}

skin.orgNameLong: {{ orgNameLong | default('Atlas of Living Australia') }}
skin.orgNameShort: {{ orgNameShort | default('ALA') }}

grails.controllers.upload.maxFileSize: {{ max_request_size | default(524288000) }}
grails.controllers.upload.maxRequestSize: {{ max_request_size | default(524288000) }}"
49f341684a3481e23f26c85fb6ea3959458f3d55,"- name: Add entries for demo into hosts file
  lineinfile: dest=/etc/hosts regexp='^127\.0\.0\.1' line=""127.0.0.1 localhost {{ demo_hostname | default('') }} ala.vagrant.dev ala demo.vagrant1.ala.org.au vagrant1.ala.org.au"" owner=root group=root mode=0644

- name: Ensure data directory exists
    - demo","- include: ../../common/tasks/setfacts.yml

- name: ensure data directory exists
  file: path=/srv/{{ demo_hostname }}/www/html state=directory owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - demo

- name: Copy welcome page (Debian)
  template: src=index.html dest=/srv/{{ demo_hostname }}/www/index.html mode=0666
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/biocache-media"" 
  ignore_errors: yes
  tags:
    - demo

- name: Create symlink to data/biocache-media from /srv/[hostname]/www/html/biocache-media
  command: ""ln -sf /data/biocache-media  /srv/{{ demo_hostname }}/www/html/biocache-media"" 
  ignore_errors: yes
  tags: 
    - demo"
66ddfb6b3b641f4580add2b7d0086f96c70d254e,"        service-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}
        management-url: {{ bootadmin_client_base_url | default('${serverURL}/') }}{{ userdetails_context_path | default('userdetails') }}/status","    authCookieName: {{ auth_cookie_name | default('ALA-Auth') }}
      enabled: {{ oauth_providers_flickr_enabled | default('true') }}
    inaturalist:
      enabled: {{ oauth_providers_inaturalist_enabled | default('false') }}
      key: {{ oauth_providers_inaturalist_key | default('') }}
      secret: {{ oauth_providers_inaturalist_secret | default('') }}
      callback: ${grails.serverURL}/profile/inaturalistCallback
biocache.search.baseUrl: {{ biocache_base_url }}/occurrences/search
headerAndFooter:
  baseURL: {{ header_and_footer_baseurl | default('https://www.ala.org.au/commonui-bs3')}}
  version: {{ header_and_footer_version | default('1')}}
{% if bootadmin_enabled %}
        service-base-url: {{ bootadmin_client_base_url | default('${grails.serverURL}/') }}
#        management-url: ${spring.boot.admin.client.service-base-url}{{ userdetails_context_path | default('userdetails') }}/status
{% endif %}
{% if spring_session_redis_clustered %}
{% endif %}"
6bb9fbcc0b6f18a20eb288ee725f7847b6182e67,"    sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*
# Run again with 8u172 as the basis for servers that were on that version instead
- name: Fix Webupd8 Team failing to update and Oracle removing old download (part 2)
    sed -i 's|JAVA_VERSION=8u172|JAVA_VERSION=8u181|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|SHA256SUM_TGZ=""1845567095bfbfebd42ed0d09397939796d05456290fb20a83c476ba09f991d3""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_172|J_DIR=jdk1.8.0_181|' oracle-java8-installer.*","    sed -i 's|JAVA_VERSION=8u161|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u161-b12/2f38c3b165be4555a1fa6e98c45e0808/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""6dbc56a0e3310b69e91bb64db63a485bd7b6a8083f08e48047276380a0e2021e""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_161|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*
- name: Switch oracle jdk 8 from security (b171) to bug fix+security (b172)
    sed -i 's|JAVA_VERSION=8u171|JAVA_VERSION=8u172|' oracle-java8-installer.* &&
    sed -i 's|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/|PARTNER_URL=http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/|' oracle-java8-installer.* &&
    sed -i 's|SHA256SUM_TGZ=""b6dd2837efaaec4109b36cfbb94a774db100029f98b0d78be68c27bec0275982""|SHA256SUM_TGZ=""28a00b9400b6913563553e09e8024c286b506d8523334c93ddec6c9ec7e9d346""|' oracle-java8-installer.* &&
    sed -i 's|J_DIR=jdk1.8.0_171|J_DIR=jdk1.8.0_172|' oracle-java8-installer.*"
25293f1515c43819d3eb2be70e07d41bfe27143b,"    - ""{{data_dir}}/ala/runtime/files/""","- include: ../../common/tasks/setfacts.yml
  tags:
    - spatial-hub
    - config    

- include: ../../apache_vhost/tasks/main.yml context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - apache_vhost
    - spatial-hub
  when: not webserver_nginx

- name: add nginx vhost if configured
  include_role:
    name: nginx_vhost
  vars:
    hostname: ""{{ spatial_hub_hostname }}""
    context_path: ""{{ spatial_hub_context_path }}""
  tags:
    - nginx_vhost
    - deploy
    - spatial-hub
  when: webserver_nginx

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ spatial_hub_war_url }}' context_path='{{ spatial_hub_context_path }}' hostname='{{ spatial_hub_hostname }}'
  tags:
    - deploy
    - tomcat_vhost
    - spatial-hub

- name: ensure target directories exist [data subdirectories etc.]
  file: path={{item}} state=directory owner={{tomcat_user}} group={{tomcat_user}}
  with_items:
    - ""{{data_dir}}/ala/data/runtime/files/""
  tags:
    - spatial-hub

- name: copy all config.properties
  template: src=spatial-hub-config.properties dest={{data_dir}}/spatial-hub/config/spatial-hub-config.properties
  tags:
    - spatial-hub 
    - config

- name: copy all log4j.properties
  template: src=log4j.properties dest={{data_dir}}/spatial-hub/config/log4j.properties
  tags:
    - spatial-hub

- name: set data ownership
  file: path={{data_dir}}/ala/data/ owner={{tomcat_user}} group={{tomcat_user}} recurse=true
  notify: 
    - restart tomcat
  tags:
    - spatial-hub"
ff198450ca243b67075364ac5b0d092a3da82874,- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',"- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
#
# WAR file deployment and Apache/Tomcat virtual host configuration
#
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy"
ff198450ca243b67075364ac5b0d092a3da82874,- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_war_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}',"version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}"""
53a3a5eafcfe362464c600158cba69f78baf6ae2,{% if spring_session_redis_clustered is sameas true %},"    {% if spring_session_redis_clustered %}
    clustered:
      nodes: {{ spring_session_redis_host }}:{{ spring_session_redis_port | default('6379') }}
    {% endif %}"
d68bb6e4020ae332fd1e8988cfa216b7e76a25df,when: elasticsearch_proxy,when: elasticsearch_proxy | bool == True
d99802ffa8888444d92828f43c43ec295c0779a0,- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass_values }}',"  tags:
    - sandbox
#
# WAR file deployment and virtual host configuration
#

- include: ../../apache_vhost/tasks/main.yml context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}' additional_proxy_pass='{{ additional_proxy_pass }}'
  tags:
    - sandbox
    - deploy
    - apache_vhost

- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ sandbox_war_url }}' context_path='{{ sandbox_context_path }}' hostname='{{ sandbox_hostname }}'
  tags:
    - sandbox
    - deploy
    - tomcat_vhost

- name: Redirect to datacheck 
  template: src=index.html dest=/srv/{{ sandbox_hostname }}/www/index.html owner={{tomcat_user}} group={{tomcat_user}}
  tags:
    - sandbox
    - deploy
    - apache_vhost

#
# Properties and data file configuration
#
  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties
  tags:
    - sandbox
    - properties"
ff198450ca243b67075364ac5b0d092a3da82874,"webapi_war_url: ""{{maven_repo_ws_url}}""","- name: create DB
  mysql_db: name={{webapi_db_name}} state=present
    - db
- name: create DB user
  mysql_user: name={{webapi_db_username}} password={{webapi_db_password}} priv=*.*:ALL state=present
    - db
- name: ensure target directories exist [data subdirectories etc.]
  file: path=""{{ data_dir }}/webapi/config"" state=directory owner={{tomcat_user}} group={{tomcat_user}}
    - properties
- name: copy all config.properties
  template: src=webapi-config.properties.j2 dest={{data_dir}}/webapi/config/webapi-config.properties
    - properties
- name: set data ownership
  file: path={{data_dir}}/webapi owner={{tomcat_user}} group={{tomcat_user}} recurse=true
    - properties
#
# WAR file deployment and Apache/Tomcat virtual host configuration
#
- include: ../../apache_vhost/tasks/main.yml context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - apache_vhost
    - deploy
- include: ../../tomcat_deploy/tasks/main.yml war_url='{{ webapi_url }}' context_path='{{ webapi_context_path }}' hostname='{{ webapi_hostname }}'
    - tomcat_vhost
    - deploy"
ff198450ca243b67075364ac5b0d092a3da82874,"webapi_war_url: ""{{maven_repo_ws_url}}""","version: ""0.1""
artifactId: ""webapi""
classifier: ''
packaging: ""war""
webapi_url: ""{{maven_repo_ws_url}}"""
7a6c2725d715c208da4826bfe0f5ccdb61d63aec,"cassandra_user: cassandra
# package variables used for RedHat
datastax: dsc12-1.2.10-1
cassandra: cassandra12-1.2.10-1","# common variables across all roles
cassandra_user: cassandra"
e4552fe331d41b92dce8f9de7dc650b835f0a005,"  when: upgrade_check_script.stdout == ""False""","    mode: 0644
  import: others/master/postconfigure-upgrade.yaml
  when: upgrade_check_script.stdout == ""False"""
bddcb55f94af88897fe0f08eb279d7d9e0e6448f,"- name: bootstrap | configure node | kubead show me join command
- name: bootstrap | configure node | compose join command","---
- name: configure others | kubead show me join command
  command: kubeadm token create --print-join-command --ttl 5m
  delegate_to: ""{{ cluster_name }}-kube-master.service.automium.consul""
  register: kubeadm_join_command

- name: configure-bootstrap  | compose join command
  set_fact:
    join_command: ""{{ kubeadm_join_command.stdout }}""
"
58b74ff2e1fd5bef59c814fdbf62efcb22298494,"- name: Enable/disable services
  service:
    name: ""{{ item }}""
    enabled: ""{{ (enable_services | bool) | ternary('yes','no') }}""
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service

  when: (start_services | bool)
  tags:
    - service","---

- name: Package
  package:
    name: ""{{item}}""
    state: present
  loop: ""{{packages_to_install[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""

- name: Template >> /etc/dhcp/dhcpd.conf
  template:
    src: dhcpd.conf.j2
    dest: /etc/dhcp/dhcpd.conf
    owner: root
    group: root
    mode: 0644
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.networks.conf
  template:
    src: dhcpd.networks.conf.j2
    dest: /etc/dhcp/dhcpd.networks.conf
    owner: root
    group: root
    mode: 0644
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Template >> /etc/dhcp/dhcpd.{{item}}.conf
  template:
    src: dhcpd.subnet.conf.j2
    dest: /etc/dhcp/dhcpd.{{item}}.conf
    owner: root
    group: root
    mode: 0644
  with_items: ""{{networks}}""
  when:
    - j2_current_iceberg_network in item
    - networks[item].is_in_dhcp == true
#  notify: Restart dhcp_server services
  tags:
    - templates

- name: Start services
  service:
    name: ""{{item}}""
    state: started
    enabled: yes
  loop: ""{{services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version]}}""
"
1ac3a65f0d4ce2dd265dcd9ce808bbe6b7b487aa,"
  exporters:
#    node_exporter:
#      port: 9100
    bb_exporter:
      port: 9777
      collectors:
        cpu:
        ram:
        mounted:
          - /scratch
          - /home
        services:
          - slurmd.service
        
  # Define alerts related to selected exporters
    Exporter_down:
#      severity: critical
    bb_exporter_service:
      -  slurmd
","monitoring:
  alerts:
    - ExporterDown
    - OutOfDiskSpace"
c8fedbddb489233cb7c45d27781eef7f413d2036,"  loop: ""{{ log_client_services_to_start }}""","---

- name: Restart rsyslog services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service"
fc330d038c69dc639f5727521b7f78dea80852d9,advanced_dhcp_server_role_version: 1.0.4,"role_version: 1.0.2
packages_to_install:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcp
    _8:
      - dhcp-server
  centos:
    _7:
      - dhcp
    _8:
      - dhcp-server

services_to_start:
  ubuntu:
    _18:
      - isc-dhcp-server
  redhat:
    _7:
      - dhcpd
    _8:
      - dhcpd
  centos:
    _7:
      - dhcpd
    _8:
      - dhcpd

"
7d2b0cc649b10542b6e8907c9bc8a31484935787,dhcp_server_role_version: 1.0.7,dhcp_server_role_version: 1.0.6
1ac7eaf3661752773b2a9547470bed739f55caf0,dhcp_server_role_version: 1.0.4,"---
role_version: 1.0.4"
bbeef1a3e496a1413a06c214cc51dfbe8b26bfea,"- name: lineinfile █ Configure root color based on iceberg number
- name: copy █ Add disk usage small script for screenrc
","- name: Configure root color based on iceberg number
  lineinfile:
    path: /root/.bashrc
    line: 'PS1=""\[\e[01;{{31+(j2_current_iceberg_number|int)}}m\]\h:\w#\[\e[00;m\] ""'

- name: Add disk usage small script for screenrc
  copy:
    src: free_root_disk 
    dest: /usr/bin/free_root_disk
    mode: 0700

- name: Add screenrc configuration
  copy:
    src: screenrc
    dest: /root/.screenrc
    mode: 0644
"
526ad18fc9dae379943591a0be3d54f4b03321ec,- name: service █ Restart dhcp server,"---
- name: Restart dhcp services
  service:
    name: ""{{ item }}""
    state: restarted
  loop: ""{{ services_to_start[(ansible_distribution|lower|replace(' ','_'))]['_'+ansible_distribution_major_version] }}""
  tags:
    - service"
4aa44a7b09737aa0cf336432c4e6d8806e9dc46d,"    aws_access_key: ""{{ auth_var['aws_access_key_id'] | default(omit) }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] | default(omit) }}""","    aws_access_key: ""{{ auth_var['aws_access_key_id'] }}""
    aws_secret_key: ""{{ auth_var['aws_secret_access_key'] }}"""
959b49d332ed03521f4dbc8f94911293e3b54805,no_log: true,"- name: ""Provisioning resource group {{ res_grp }}""
  debug:
    msg: ""The current server obj is {{ res_grp }} \n groups vars are {{ r_grp_vars }} ""

- name: ""Including credentials of current resource {{ res_grp['resource_group_name'] }} ""
  include_vars: ""../vars/{{ res_grp['assoc_creds'] }}.yml""
  no_log: false 

- name: ""Checking res_grp ""
  debug:
    msg: "" res_grp {{ res_grp }}""

- name: ""Provision resource definitions""
  include: provision_res_defs.yml res_def={{ outer_item.0 }} res_grp_name={{ outer_item.1 }}
  with_nested:
    - ""{{ res_grp['res_defs'] }}""
    - [""{{ res_grp['resource_group_name'] }}""]
  loop_control:
    loop_var: outer_item
#- name: ""Register resource count""
#  shell: python -c ""print [x for x in range( 0, {{ server_var['count'] }} )]""
#  register: res_count

# Debug task 
#- name: ""Checking value of res_count ""
#  debug: 
#    msg: ""Value of res_count is {{ res_count.stdout }}""

# debug task 
#- name: ""illustration of looping resource for given count ""
#  debug:
#    msg: ""Credentials included for the resource are {{ item.0 }} {{ item.1 }} {{ item.2 }} {{ item.3 }} {{ item.4 }} {{ item.5 }} {{ item.6 }} {{ item.7 }} {{ item.8 }} {{ item.9 }} ""
#  with_nested:
#    - [""{{ endpoint }}""]
#    - [""{{ username }}""]
#    - [""{{ password }}""]
#    - [""{{ project }}""]
#    - [""{{ server_var['res_def']['image'] }}""]
#    - [""{{ server_var['res_def']['keypair']  }}""]
#    - [""{{ server_var['res_def']['flavor']  }}""]
#    - [""{{ server_var['res_def']['networks'][0] }}""]
#    - [""{{ server_var['resource_name'] }}""]
#    - ""{{ res_count.stdout }}""

#- name: ""Provision resource looping on count""
#  os_server:
#    state: ""{{ item.4 }}""
#    auth:
#      auth_url: ""{{ item.0 }}""
#      username: ""{{ item.1 }}""
#      password: ""{{ item.2 }}""
#      project_name: ""{{ item.3 }}""
#    name: ""{{ item.9 }}-{{ item.10 }}""
#    image: ""{{ item.5 }}""
#    key_name: ""{{ item.6  }}""
#    api_timeout: 300
#    flavor: ""{{ item.7 }}""
#    network: ""{{ item.8 }}""
#  with_nested:
#    - [""{{ endpoint }}""]
#    - [""{{ username }}""]
#    - [""{{ password }}""]
#    - [""{{ project }}""]
#    - [""{{ state }}""]
#    - [""{{ server_var['res_def']['image'] }}""]
#    - [""{{ server_var['res_def']['keypair']  }}""]
#    - [""{{ server_var['res_def']['flavor']  }}""]
#    - [""{{ server_var['res_def']['networks'][0] }}""]
#    - [""{{ server_var['resource_name'] }}""]
#    - ""{{ res_count.stdout }}"""
9addb5595e4b0ec16f7c61d8c4734d85d3cdc009,"  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init"" and cloud_config != {}
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init"" 
  when: node_exists['failed'] is defined and virt_type == ""cloud-init"" 
  when:  (node_exists['failed'] is defined) and  virt_type == 'virt-customize'
  ignore_errors: yes","- name: set cloud config default
  set_fact:
    cloud_config: ""{{ res_def['cloud_config'] | default({})  }}""

- name: set cloud_config virt_type
  set_fact:
    virt_type: ""{{ cloud_config['virt_type'] | default('cloud-init') }}""

- include_tasks: virt_customize.yml
  when: res_def['cloud_config']['virt_type'] == ""virt-customize""

  when: node_exists['failed'] is defined and uri_hostname != 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and uri_hostname == 'localhost' and virt_type == ""cloud-init""
 
  when: node_exists['failed'] is defined and res_def['network_bridge'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and res_def['networks'] is defined and virt_type == ""cloud-init""
  when: node_exists['failed'] is defined and virt_type == ""cloud-init""
  when:  node_exists['failed'] is defined and (res_def['cloud_config'] is not defined or virt_type == 'virt-customize')"
80b8b9a8079a949e1a0a5bbabb5bd811c329b570,"    dest: ""/tmp/{{ libvirt_resource_name }}{{ definition[4] }}{{ definition[2] }}""
    uri: ""{{ definition[0]['uri'] }}""","    uri: ""{{ definition[0] }}"""
41c97a0f5576ab1bb4985d6fab904df07789e868,"        name:
          - python2-dnf 
          - libvirt-devel
          - libguestfs-tools 
          - python-libguestfs","- name: Install dependencies
  block:
    - name: Install package dependencies
      package:
        name: ""{{ libvirt_pkg }}""
        state: latest
      with_items:
      - python2-dnf 
      - libvirt-devel
      - libguestfs-tools 
      - python-libguestfs
      become: true
      loop_control: 
        loop_var: libvirt_pkg
    - name: Install pypi dependencies of libvirt
      pip:
        name: ""{{ libvirt_pypi }}""
      with_items:
      - ""libvirt-python>=3.0.0""
      - ""lxml""
      loop_control: 
        loop_var: libvirt_pypi
  rescue:
    - fail:
        msg: 'Error installing the package dependencies! Please try adding password less priviledged sudo user or with --ask-sudo-pass'"
9138bf03103c485f2e845744a4931212a66340d3,,"---
- name: ""provision/deprovision os_subnet""
  os_subnet:
    allocation_pool_end:  ""{{ res_def['allocation_pool_end'] | default(omit) }}""
    allocation_pool_start:  ""{{ res_def['allocation_pool_start'] | default(omit) }}""
    api_timeout: ""{{ res_def['api_timeout'] | default(omit) }}""
    auth: ""{{ auth_var }}""
    cidr:  ""{{ res_def['cidr'] | default(omit) }}""
    dns_nameservers:  ""{{ res_def['dns_nameservers'] | default(omit) }}""
    enable_dhcp: ""{{ res_def['enable_dhcp'] | default(omit) }}""
    extra_specs: ""{{ res_def['extra_specs'] | default(omit) }}""
    gateway_ip:  ""{{ res_def['gateway_ip'] | default(omit) }}""
    host_routes:  ""{{ res_def['host_routes'] | default(omit) }}""
    ip_version:  ""{{ res_def['ip_version'] | default(omit) }}""
    ipv6_address_mode:  ""{{ res_def['ipv6_address_mode'] | default(omit) }}""
    ipv6_ra_mode:  ""{{ res_def['ipv6_ra_mode'] | default(omit) }}""
    network_name:  ""{{ res_def['network_name'] | default(omit) }}""
    no_gateway_ip:  ""{{ res_def['no_gateway_ip'] | default(omit) }}""
    use_default_subnetpool:  ""{{ res_def['use_default_subnetpool'] | default(omit) }}""
    interface: ""{{ res_def['interface'] | default(omit) }}""
    name: ""{{ os_resource_name }}""
    key: ""{{ res_def['key'] | default(omit) }}""
    project: ""{{ res_def['project'] | default(omit) }}""
    region_name: ""{{ res_def['region_name'] | default(omit) }}""
    state: ""{{ state }}""
    timeout: ""{{ res_def['timeout'] | default(600) }}""
    verify: no
    wait: yes
  register: res_def_output_auth
  no_log: ""{{ not debug_mode }}""
  when: 
   - auth_var != """"

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_os_network: ""{{ topology_outputs_os_network + [ res_def_output_auth ] }}""
  when:
    - auth_var != """"
      
- name: ""provision/deprovision os_subnet""
  os_subnet:
    allocation_pool_end:  ""{{ res_def['allocation_pool_end'] | default(omit) }}""
    allocation_pool_start:  ""{{ res_def['allocation_pool_start'] | default(omit) }}""
    api_timeout: ""{{ res_def['api_timeout'] | default(omit) }}""
    auth: ""{{ auth_var }}""
    cidr:  ""{{ res_def['cidr'] | default(omit) }}""
    dns_nameservers:  ""{{ res_def['dns_nameservers'] | default(omit) }}""
    enable_dhcp: ""{{ res_def['enable_dhcp'] | default(omit) }}""
    extra_specs: ""{{ res_def['extra_specs'] | default(omit) }}""
    gateway_ip:  ""{{ res_def['gateway_ip'] | default(omit) }}""
    host_routes:  ""{{ res_def['host_routes'] | default(omit) }}""
    ip_version:  ""{{ res_def['ip_version'] | default(omit) }}""
    ipv6_address_mode:  ""{{ res_def['ipv6_address_mode'] | default(omit) }}""
    ipv6_ra_mode:  ""{{ res_def['ipv6_ra_mode'] | default(omit) }}""
    network_name:  ""{{ res_def['network_name'] | default(omit) }}""
    no_gateway_ip:  ""{{ res_def['no_gateway_ip'] | default(omit) }}""
    use_default_subnetpool:  ""{{ res_def['use_default_subnetpool'] | default(omit) }}""
    interface: ""{{ res_def['interface'] | default(omit) }}""
    name: ""{{ os_resource_name }}""
    key: ""{{ res_def['key'] | default(omit) }}""
    project: ""{{ res_def['project'] | default(omit) }}""
    region_name: ""{{ res_def['region_name'] | default(omit) }}""
    state: ""{{ state }}""
    timeout: ""{{ res_def['timeout'] | default(600) }}""
    verify: no
    wait: yes
  register: res_def_output_auth
  no_log: ""{{ not debug_mode }}""
  when:
   - auth_var == """"

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_os_network: ""{{ topology_outputs_os_network + [ res_def_output_no_auth ] }}""
  when:
    - auth_var == """""
ff8211673256fb4f22ea2e2a560935b8fda73d72,"    admin_username: ""{{ res_def['vm_username'] | default('linchpin') }}""
","---
- name: ""Provisioning Azure VM when not async""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    ssh_public_keys: ""{{ssh_public_keys}}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    name: ""{{ nameOfvm | default(omit) }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    image: ""{{ image | default(omit) }}""
  register: res_def_output
  when: not _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure VM""
  azure_rm_virtualmachine:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    admin_username: ""{{ res_def['vm_username'] | default(linchpinUsername) }}""
    admin_password: ""{{ res_def['vm_password'] | default(linchpinPassword) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    vm_size: ""{{ res_def['vm_size'] | default('Standard_DS1_v2') }}""
    virtual_network_name: ""{{res_def['virtual_network_name']|default(vn_name)}}""
    name: ""{{  nameOfvm| default(omit) }}""
    image: ""{{ image | default(omit) }}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm + [res_def_output.ansible_facts.azure_vm] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vm: ""{{ topology_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vm: ""{{ async_outputs_azure_vm | add_res_data(lookup('vars', 'role_name'), res_def['role']) }}""
  when: _async"
00e4f4cc23b276d4421f1de264ef670e751a15c5,"    cloudconfig_users: ""{{ cloud_config['users'] | default([]) }}""","#  when:  node_exists['failed'] is defined and res_def['cloud_config'] is not defined

  register: pubkey_local
  register: pubkey_remote
- name: ""Create directories""
  file:
    path: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}""
    state: ""directory""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ res_def['name'] | default(res_def['res_name']) }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  when: node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-local""
  template:
    src: ""templates/cloud-config/user-data-fixed-local""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  when: res_def['cloud_config'] is not defined and node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-local""
    src: ""templates/cloud-config/user-data-local""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  vars:
    cloudconfig_users: ""{{ res_def['cloud_config']['users'] | default([]) }}""
  when: res_def['cloud_config'] is defined and node_exists['failed'] is defined and uri_hostname == 'localhost'

- name: ""Prepare cloud-config/user-data-remote""
  template:
    src: ""templates/cloud-config/user-data-remote""
    dest: ""/tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data""
  with_nested:
    - [""{{ res_grp_name }}""]
    - [""{{ libvirt_resource_name }}""]
    - ""{{ res_count.stdout }}""
  loop_control:
    loop_var: definition
  remote_user: ""{{ res_def['remote_user'] | default('root') }}""
  delegate_to: ""{{ uri_hostname }}""
  when: res_def['cloud_config'] is defined and node_exists['failed'] is defined and uri_hostname != 'localhost'

- name: ""Prepare cloud-config/user-data-remote""
  template:
    src: ""templates/cloud-config/user-data-fixed-remote""
- name: ""Prepare cloud-config/meta-data remote""
- name: ""Generate ci data cd image for cloud-init local""
- name: ""Generate ci data cd image for cloud-init remote host""
  command: mkisofs -o /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}.iso -V cidata -r -J --quiet /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data  /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/meta-data
  command: mkisofs -o /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}.iso -V cidata -r -J --quiet /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/user-data /tmp/vm-{{ definition[0] }}_{{ definition[1] }}_{{ definition[2] }}/meta-data
- name: ""Generate add admin script local""
- name: ""Generate add admin script remote""
- name: ""Remove cloud-init cdrom ""
- name: ""Start VM""
- name: ""Start relevant networks""
- name: ""mac_and_ip | extract mac address""
- name: ""mac_and_ip | wait for dhcp ip address""
- name: ""mac_and_ip | wait for dhcp ip address"""
09f67d8efca7c4bb86e5905c45f79388bc4d50a6,"    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_virtual_subnet') }}""","  no_log: ""{{ not debug_mode }}""

- name: """"Async::Provisioning Azure Virtual Subnet""
  azure_rm_subnet:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    name: ""{{ res_def['subnet_name'] | default(omit) }}""
    virtual_network_name: ""{{ res_def['virtual_network_name']}}""
    address_prefix: ""{{ res_def['address_prefix']|default('10.1.0.0/24')}}""
  register: res_def_output
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn_subnet: ""{{ topology_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn_subnet: ""{{ async_outputs_azure_vn_subnet | add_res_type( 'azure_vm') }}""
  when: _async"
039b82aec5403fc9ce5ab8a31efc76cc99a8b921,"      - ""{{ res_grp['resource_definitions'] }}""","---
- name: ""Unset the authvar from previous run""
  set_fact:
    auth_var: """"

- name: ""set cred profile""
  set_fact:
    cred_profile: ""{{ res_grp['credentials']['profile'] | default('default') }}""

- name: ""Get creds from auth driver""
  auth_driver:
    filename: ""{{ res_grp['credentials']['filename']  }}""
    cred_type: ""ovirt""
    cred_path: ""{{ creds_path | default(default_credentials_path) }}""
    driver: ""file""
  register: auth_var
  ignore_errors: true

- name: ""set auth_var""
  set_fact:
    auth_var: ""{{ auth_var['output'][cred_profile] }}""
  ignore_errors: true

- block:
  - name: Obtain SSO token with using username/password credentials
    ovirt_auth:
      url: ""{{ auth_var['ovirt_url'] }}""
      username: ""{{ auth_var['ovirt_username'] }}""
      ca_file: ""{{ auth_var['ovirt_ca_file'] | default(omit) }}""
      password: ""{{ auth_var['ovirt_password'] }}""
      insecure: ""{{ auth_var['ovirt_ca_file'] is not defined }}""

  - name: ""Provisioning resource definitions of current group""
    include: provision_res_defs.yml res_def={{ res_item.0 }} res_grp_name={{ res_item.1 }}
    with_nested:
      - ""{{ res_grp['resource_definitions']) }}""
      - [""{{ res_grp['resource_group_name'] }}""]
    loop_control:
      loop_var: res_item

  always:
    - name: Always revoke the SSO token
      ovirt_auth:
        state: absent
        ovirt_auth: ""{{ ovirt_auth }}"""
5f7d61ecd0deccc9459f0446b6f1ffc4d7052aae,"    address_prefixes: ""{{ res_def['address_prefixes']|default('10.1.0.0/16')}}""
","---
- name: ""Provisioning Azure Virtual Network when not async""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: ""{{ res_def['address_prefixes']|default(10.1.0.0/16)}}""
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  register: res_def_output

- name: ""Append outputitem to topology_outputs""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn + [res_def_output] }}""
  when: res_def_output['changed'] == true and not _async

- name: ""Async:: Provisioning Azure Virtual Network""
  azure_rm_virtualnetwork:
    client_id: ""{{ auth_var['client_id'] | default(omit) }}""
    tenant: ""{{ auth_var['tenant'] | default(omit) }}""
    secret: ""{{ auth_var['secret'] | default(omit) }}""
    subscription_id: ""{{ auth_var['subscription_id'] | default(omit) }}""
    resource_group: ""{{ res_def['resource_group'] | default(omit) }}""
    address_prefixes: 10.1.0.0/16
    name: ""{{res_def['virtual_network_name']|default(omit)}}""
  async: ""{{ async_timeout }}""
  poll: 0
  register: res_def_output
  when: _async
  no_log: ""{{ not debug_mode }}""

- name: ""Async:: Append outputitem to topology_outputs""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn + [res_def_output] }}""
  when: _async


- name: ""Add type to resource""
  set_fact:
    topology_outputs_azure_vn: ""{{ topology_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""


- name: ""Async:: Add type to resource""
  set_fact:
    async_outputs_azure_vn: ""{{ async_outputs_azure_vn | add_res_type( 'azure_virtual_network') }}""
  when: _async"
8951c9c3f5a0385b674f0202fbed442f9068879f,"  copy: src=fetch/{{ fsid }}/etc/ceph/radosgw.gateway.keyring dest=/etc/ceph/radosgw.gateway.keyring owner=root group=root mode=600
  file: path=/etc/ceph/radosgw.gateway.keyring mode=0600 owner=root group=root","- name: Copy RGW bootstrap key
  copy: src=fetch/{{ fsid }}/etc/ceph/keyring.radosgw.gateway dest=/etc/ceph/keyring.radosgw.gateway owner=root group=root mode=600
  when: cephx

- name: Set RGW bootstrap key permissions
  file: path=/etc/ceph/keyring.radosgw.gateway mode=0600 owner=root group=root
  when: cephx
"
b0891016a97a05e72bcf01c5e5b0b6c28581fa54,copy: >,"---
- name: create red hat storage package directories
  file: >
    path={{ item }}
    state=directory
  with_items:
    - ""{{ ceph_stable_rh_storage_mount_path }}""
    - ""{{ ceph_stable_rh_storage_repository_path }}""

- name: fetch the red hat storage iso from the ansible server
  fetch: >
    src={{ ceph_stable_rh_storage_iso_path }}
    dest={{ ceph_stable_rh_storage_iso_path }}
    flat=yes

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=mounted

- name: copy red hat storage iso content
  shell:
    cp -r {{ ceph_stable_rh_storage_mount_path }}/* {{ ceph_stable_rh_storage_repository_path }}
    creates={{ ceph_stable_rh_storage_repository_path }}/README

- name: mount red hat storage iso file
  mount: >
    name={{ ceph_stable_rh_storage_mount_path }}
    src={{ ceph_stable_rh_storage_iso_path }}
    fstype=iso9660
    state=unmounted"
a99e04a9b7bb1f6fec21f6f6cabc4c51de15c657,"  with_items: ""{{ groups.get(mon_group_name, []) }}""","---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names"
a99e04a9b7bb1f6fec21f6f6cabc4c51de15c657,"  with_items: ""{{ groups.get(mon_group_name, []) }}""","---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names"
e6f22b948ce1ae00dbada341652693f1de7f610a,failed_when: false,"---
# NOTE (leseb): the mds container needs the admin key
# so it can create the mds pools for cephfs
- name: set config and keys paths
  set_fact:
    ceph_config_keys:
      - /etc/ceph/ceph.conf
      - /etc/ceph/ceph.client.admin.keyring
      - /var/lib/ceph/bootstrap-mds/ceph.keyring

- name: stat for ceph config and keys
  local_action: stat path={{ item }}
  with_items: ceph_config_keys
  changed_when: false
  sudo: false
  ignore_errors: true
  register: statconfig

- name: try to fetch ceph config and keys
  copy: >
    src=fetch/docker_mon_files/{{ item.0 }}
    dest={{ item.0 }}
    owner=root
    group=root
    mode=644
  with_together:
    - ceph_config_keys
    - statconfig.results
  when: item.1.stat.exists == true"
1abed53f7d67155b853c78a2cc172b61798862b5,"- name: install nss-tools on redhat
    name: nss-tools
- name: install nss-tools on redhat
    name: nss-tools","- name: install libnss3-tools on redhat
  yum:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""yum""

- name: install libnss3-tools on redhat
  dnf:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == ""dnf""

- name: install libnss3-tools on debian
  apt:
    name: libnss3-tools
    state: present
  when: ansible_pkg_mgr == 'apt'
"
0ae0193144897676b56e1d4142565e759531fd35,"    name: '{{ ntp_service_name }}'
    name: '{{ chrony_daemon_name }}'","    enabled: yes

- name: disable ntpd
  failed_when: false
  service:
    name: ntpd
    state: stopped
    enabled: no

- name: disable chronyd
  failed_when: false
  service:
    name: chronyd
    enabled: no
    state: stopped

- name: disable timesyncd
  failed_when: false
  service:
    name: timesyncd
    enabled: no
    state: stopped"
dab3f6ee3fb83e0f844793661e1e2253ebb8ad7c,"    ceph_authtool_cmd: ""{{ container_binary + ' run --net=host --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""","    print(base64.b64encode(header + key).decode())""
  run_once: true # must run on a single mon only
  # add code to read the key or/and try to find it on other nodes
    secret: ""{{ monitor_keyring.stdout }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: copy the initial key in /etc/ceph (for containers)
  command: >
    cp /var/lib/ceph/tmp/{{ cluster }}.mon..keyring /etc/ceph/{{ cluster }}.mon.keyring
  changed_when: false
  when:
    - cephx
    - containerized_deployment
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    owner: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    group: ""{{ ceph_uid if containerized_deployment else 'ceph' }}""
    mode: ""0400""
  environment:
    CEPH_CONTAINER_IMAGE: ""{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}""
    CEPH_CONTAINER_BINARY: ""{{ container_binary }}""
- name: set_fact ceph-authtool container command
  set_fact:
    ceph_authtool_cmd: ""{{ container_binary + ' run --rm -v /var/lib/ceph:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-authtool ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' + ceph_client_docker_image_tag if containerized_deployment else 'ceph-authtool' }}""
  command: >
    {{ ceph_authtool_cmd }}
     /var/lib/ceph/tmp/{{ cluster }}.mon.keyring --import-keyring /etc/ceph/{{ cluster }}.client.admin.keyring
- name: set_fact ceph-mon container command
  set_fact:
    ceph_mon_cmd: ""{{ container_binary + ' run --rm --net=host -v /var/lib/ceph/:/var/lib/ceph:z -v /etc/ceph/:/etc/ceph/:z --entrypoint=ceph-mon ' + ceph_client_docker_registry + '/' + ceph_client_docker_image + ':' +ceph_client_docker_image_tag if containerized_deployment else 'ceph-mon' }}""

  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    --keyring /var/lib/ceph/tmp/{{ cluster }}.mon..keyring
  command: >
    {{ ceph_mon_cmd }}
    --cluster {{ cluster }}
    --setuser ceph
    --setgroup ceph
    --mkfs
    -i {{ monitor_name }}
    --fsid {{ fsid }}
    - not cephx"
4fe7f3784944980a5e0e1ae736920392cbca207b,"    default_release: ""{{ ceph_stable_release_uca | default('') }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else '' }}""","- name: install redhat ceph-mgr package
  package:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
  when:
    - ansible_os_family == 'RedHat'

- name: install ceph mgr for debian
  apt:
    name: ceph-mgr
    state: ""{{ (upgrade_ceph_packages|bool) | ternary('latest','present') }}""
    default_release: ""{{ ceph_stable_release_uca | default(omit) }}{{ ansible_distribution_release ~ '-backports' if ceph_origin == 'distro' and ceph_use_distro_backports else ''}}""
  when:
    - ansible_os_family == 'Debian'
"
6d55d57fa66da139e0f2a3cf084a0d3e281666ac,"- name: include multisite checks
- name: include master multisite tasks
- name: include secondary multisite tasks
- name: add zone to rgw stanza in ceph.conf","- name: Include multisite checks
  include: checks.yml
- name: Include master multisite tasks
  include: master.yml
  when: ""rgw_zonemaster is defined and rgw_zonemaster""
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False
- name: Include secondary multisite tasks
  include: secondary.yml
  when: ""rgw_zonesecondary is defined and rgw_zonesecondary""
  # Hard code this so we will skip the entire file instead of individual tasks (Default isn't Consistent)
  static: False
# Continue with common tasks
- name: Add zone to RGW stanza in ceph.conf
  lineinfile:
    dest: /etc/ceph/ceph.conf
    regexp: ""{{ ansible_host }}""
    insertafter: ""^[client.rgw.{{ ansible_host }}]""
    line: ""rgw_zone = {{ rgw_zone }}""
    state: present
  notify:
    - restart rgw"
a99e04a9b7bb1f6fec21f6f6cabc4c51de15c657,"  with_items: ""{{ groups.get(mds_group_name, []) }}""","---
- name: restart ceph mons
  service:
    name: ceph-mon@{{ monitor_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mon_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mon_group_name in group_names

- name: validate monitors
  include: validate-mon.yml
  when: mon_group_name in group_names"
a99e04a9b7bb1f6fec21f6f6cabc4c51de15c657,"  with_items: ""{{ groups.get(mds_group_name, []) }}""","---
- name: restart ceph mdss
  service:
    name: ceph-mds@{{ mds_name }}
    state: restarted
  # serial: 1 would be the proper solution here, but that can only be set on play level
  # upstream issue: https://github.com/ansible/ansible/issues/12170
  run_once: true
  with_items: ""{{ groups[mds_group_name] }}""
  delegate_to: ""{{ item }}""
  when:
    - socket.rc == 0
    - mds_group_name in group_names"
397c3fe4fd0a8133cb7c6669b488d31b0c24c4dc,ceph_mon_docker_tag: latest,ceph_osd_docker_tag: latest
962d1ad17f243c40739222fb1b5567239c1d9913,"  template: >
    src=s3gw.fcgi.j2","---
## Deploy RADOS Gateway
#

- name: Add Ceph extra
  apt_repository: >
    repo=""deb http://ceph.com/packages/ceph-extras/debian {{ ansible_lsb.codename }} main""
    state=present

- name: ""Install Apache, fastcgi and Rados Gateway""
  apt: >
    pkg={{ item }}
    state=present
  with_items:
    - apache2
    - libapache2-mod-fastcgi
    - radosgw

## Prepare Apache
#

- name: Install default httpd.conf
  template: src=httpd.conf dest=/etc/apache2/httpd.conf owner=root group=root

- name: Enable some apache mod rewrite and fastcgi
  command: ""{{ item }}""
  with_items:
    - a2enmod rewrite
    - a2enmod fastcgi

- name: Install Rados Gateway vhost
  template: >
    src=rgw.conf
    dest=/etc/apache2/sites-available/rgw.conf
    owner=root
    group=root

## Prepare RGW
#

- name: Create RGW directory
  file: >
    path=/var/lib/ceph/radosgw/{{ ansible_fqdn }}
    state=directory
    owner=root
    group=root
    mode=0644

- name: Enable Rados Gateway vhost and disable default site
  command: ""{{ item }}""
  with_items:
    - a2ensite rgw.conf
    - a2dissite default
  notify:
    - restart apache2

- name: Install s3gw.fcgi script
  copy: >
    src=s3gw.fcgi
    dest=/var/www/s3gw.fcgi
    mode=0555
    owner=root
    group=root

## If we don't perform this check Ansible will start multiple instance of radosgw
- name: Check if RGW is started
  command: /etc/init.d/radosgw status
  register: rgwstatus
  ignore_errors: True

- name: Start RGW
  command: /etc/init.d/radosgw start
  when: rgwstatus.rc != 0"
9ed45f7367bc03ccaa5c40e574e5a5036493c856,"    - (journal_collocation and raw_multi_journal)
      or (journal_collocation and osd_directory)
      or (journal_collocation and bluestore)
      or (raw_multi_journal and osd_directory)
      or (raw_multi_journal and bluestore)
      or (osd_directory and bluestore)","    - (journal_collocation and not raw_multi_journal)
      or (journal_collocation and not osd_directory)
      or (journal_collocation and not bluestore)
      or (raw_multi_journal and not osd_directory)
      or (raw_multi_journal and not bluestore)
      or (osd_directory and not bluestore)
      or bluestore"
f35a6656347d9a7e46e5cce51465d524de14b95d,name: python-pip,"- name: install pip on debian
  apt:
    name: pip
    state: present
  when: ansible_os_family == 'Debian'

- name: install pip on redhat
  yum:
    name: python-pip
    state: present
  when: ansible_os_family == 'RedHat'
"
733596582d0788a52795bc40b1a5cd94ddef0446,"    - (mon_socket is defined and mon_socket.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)","  when:
    # we test for both container and non-container
    - (mon_socket_stat is defined and mon_socket_stat.get('rc') != 0) or (ceph_mon_container_stat is defined and ceph_mon_container_stat.get('stdout_lines', [])|length == 0)

- name: include configure_ceph_command_aliases.yml
  include_tasks: configure_ceph_command_aliases.yml
  when:
    - containerized_deployment"
bfb1d6be12541bb0fff4db07a6389fc9ea24ac51,"    - import_role:
        name: ceph-infra
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-engine
      when: containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool","---
# This playbook is used to add a new MON to
# an existing cluster. It can run from any machine. Even if the fetch
# directory is not present it will be created.
#
# Ensure that all monitors are present in the mons
# group in your inventory so that the ceph configuration file
# is created correctly for the new OSD(s).
- hosts: mons
  gather_facts: false
  vars:
    delegate_facts_host: true
  pre_tasks:
    - name: gather facts
      setup:
      when: not delegate_facts_host | bool
    - import_role:
        name: ceph-defaults
    - name: gather and delegate facts
      setup:
      delegate_to: ""{{ item }}""
      delegate_facts: true
      with_items: ""{{ groups[mon_group_name] }}""
      run_once: true
      when: delegate_facts_host | bool
  tasks:
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-validate

- hosts: mons
  gather_facts: false
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-common
      when: not containerized_deployment | bool
    - import_role:
        name: ceph-container-common
      when: containerized_deployment | bool
    - import_role:
        name: ceph-config
    - import_role:
        name: ceph-infra
    - import_role:
        name: ceph-mon

# update config files on OSD nodes
- hosts: osds
  gather_facts: true
  become: true
  tasks:
    - import_role:
        name: ceph-defaults
    - import_role:
        name: ceph-facts
    - import_role:
        name: ceph-handler
    - import_role:
        name: ceph-config"
85fb03fc99e0b7cd14b27e369c8e952b79313d73,dest: /etc/default/ceph,"    dest: /etc/ceph/{{ cluster }}.conf

- name: configure cluster name
  lineinfile:
    dest: /etc/sysconfig/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""RedHat""

- name: configure cluster name
  lineinfile:
    dest: /etc/default/ceph/ceph
    insertafter: EOF
    line: ""CLUSTER={{ cluster }}""
  when:
    ansible_os_family == ""Debian"""
896676ee80226121785f44f50d1f01fff5aa2fd7,"    (ceph_health_raw.stdout | default('{}') | from_json)['state'] in ['leader', 'peon']","    (ceph_health_raw.stdout | default({}) | from_json)['state'] in ['leader', 'peon']"
f0207768f86d96390586f3fc9c610ea95613e987,"    src: ""{{ grafana_yum_repo_template }}""
    dest: ""/etc/yum.repos.d/{{ grafana_yum_repo_template | basename | regex_replace('\\.j2$', '') }}""","- name: Add Grafana repository file [RHEL/CentOS]
  template:
    src: grafana.yum.repo.j2
    dest: /etc/yum.repos.d/grafana.repo
    force: yes
    backup: yes
- name: Import Grafana GPG signing key [Debian/Ubuntu]
  apt_key:
    url: ""https://packagecloud.io/gpg.key""
    state: present
    validate_certs: false
  environment:
    http_proxy: ""{{ http_proxy | default('') }}""
    https_proxy: ""{{ https_proxy | default('') }}""
  when: ansible_pkg_mgr == ""apt""

- name: Add Grafana repository [Debian/Ubuntu]
  apt_repository:
    repo: deb https://packagecloud.io/grafana/stable/debian/ jessie main
    state: present
    update_cache: yes

- name: Install Grafana
  package:
    name: grafana
    state: present
  notify: restart grafana"
6f98eb20c955ef6d6c9e51d1d2a26e16cab18bb2,"    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check-config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check-rules %s""","- name: Set validator commands for prometheus 2.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '>=')

- name: Set validator commands for prometheus 1.x
  set_fact:
    prometheus_config_validator: ""{{ prometheus_root_dir }}/promtool check config %s""
    prometheus_rules_validator: ""{{ prometheus_root_dir }}/promtool check rules %s""
  when: prometheus_version | version_compare('2.0.0', '<')
"
c9e45e769c49b34bcc3843051299a5df4c0937fa,"  author: ""Sebastian Gumprich""
        - 6.5
  categories:
    - security
dependencies: []","---
galaxy_info:
  author: Sebastian Gumprich
  description: 'This Ansible role provides numerous security-related ssh configurations, providing all-round base protection.'
  company: Hardening Framework Team
  license: Apache License 2.0
  min_ansible_version: '1.9'
  platforms:
    - name: EL
      versions:
        - 6.4
	- 6.5
    - name: Oracle Linux
      versions:
        - 6.4
        - 6.5
    - name: Ubuntu
      versions:
        - 12.04
        - 14.04
    - name: Debian
      versions:
        - 6
        - 7
   categories:
    - system"
fe0b46ef2607c5e6889d16137c0c343af5106afc,- restart win zabbix agent,"    - restart win zabbix-agent
    - restart mac zabbix agent"
660bcf42daf85a8a7dec0ef9c5ed5af324a03389,,"    chain: INPUT
    source: ""{{ zabbix_agent_server }}"""
1e43ee9e1542c75e71cdb95a7efe2149f56898e8,"    dest: ""/etc/zabbix/scripts/""","
- name: ""Installing user-defined scripts""
  copy:
    src: ""scripts/{{ item }}""
    dest: ""/etc/zabbix/scripts/{{ item }}""
    owner: zabbix
    group: zabbix
    mode: 0644
  notify: restart zabbix-agent
  become: yes
  with_items: ""{{ zabbix_agent_userparameters }}""
  when: zabbix_agent_custom_scripts"
27d81e68ddfd967d12665e4ab081e9e239eb1c91,"  when: es_start_service and (es_enable_xpack and ""security"" in es_xpack_features) and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))","#perform security actions here now elasticsearch is started
- include: ./xpack/security/elasticsearch-security-native.yml
  when: es_start_service and (es_enable_xpack and '""security"" in es_xpack_features') and ((es_users is defined and es_users.native is defined) or (es_roles is defined and es_roles.native is defined))

#Templates done after restart - handled by flushing the handlers. e.g. suppose user removes security on a running node and doesn't specify es_api_basic_auth_username and es_api_basic_auth_password.  The templates will subsequently not be removed if we don't wait for the node to restart.
#We also do after the native realm to ensure any changes are applied here first and its denf up.
- include: elasticsearch-template.yml
  when: es_templates
  tags:
      - templates"
2f3f84b6f30d1fe2582c7b5cdbf8b7a35c9399f0,"  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^DATA_DIR"" insertafter=""^#DATA_DIR"" line=""DATA_DIR={{ es_data_dir }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_USER"" insertafter=""^#ES_USER"" line=""ES_USER={{ es_user }}""
  lineinfile: dest=/etc/sysconfig/elasticsearch regexp=""^ES_GROUP"" insertafter=""^#ES_GROUP"" line=""ES_GROUP={{ es_group }}""","- name: RedHat - configure memory
  lineinfile: dest=/etc/default/elasticsearch regexp=""^ES_HEAP_SIZE"" insertafter=""^#ES_HEAP_SIZE"" line=""ES_HEAP_SIZE={{ es_heap_size }}""
  when: es_heap_size is defined
  register: elasticsearch_configure"
bc7fc40b34932cc7205cca9dc9fa4d220d062f3c,"    mode: ""2750""","    owner: root
    mode: 2750
  copy: src={{ item }} dest={{ es_conf_dir }}/templates owner=root group={{ es_group }} mode=0660"
fc400862b1cc8a46db2d4ccbbd8cffa34b9223eb,#no_log: True,no_log: True
96cda04d603c48eb1b631bde06f2bd0b59d9e746,"  author: ""Florian Utz""","  author: """"
      - xenial"
e5de5ef5fb3dc984229129bfcc783b91db2cf402,"      set -o pipefail;
      set -o pipefail;","      - not ubuntu1804cis_skip_for_travis
      - not ubuntu1804cis_skip_for_travis
  shell: |
      set -o pipefail
      df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type d -perm -0002 2>/dev/null | xargs chmod a+t
  args:
      executable: /bin/bash
      - not ubuntu1804cis_allow_autofs
      - autofs_service_status.stdout == ""loaded""
  shell: |
      set -o pipefail
      dmesg | grep -E ""NX|XD"" | grep "" active""
  args:
      executable: /bin/bash"
c47ecf46c9b34c2d7a7d5c6c1232186c10977527,"  service:
      name: rsyslog
      enabled: yes
      - ubuntu1804cis_syslog == ""rsyslog""","#4.2.4 is here due to dependencies to 4.2.1.x
  command: ""systemctl enable rsyslog""
      - rsyslog_service_status.stdout != ""enabled"""
ec80f1994109661ea0e0e18de2b3ea726c0a5848,name: rsync,"- name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh, rlogin, rexec""
      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rsh""
        service:
          name: rsh.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rsh_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6

      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rlogin""
        service:
          name: rlogin.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rlogin_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6

      - name: ""SCORED | 2.1.6 | PATCH | Ensure rsh server is not enabled | rexec""
        service:
          name: rexec.socket
          state: stopped
          enabled: false
        when:
          - ubuntu1804cis_rsh_server == false
          - rexec_service_status.stdout == ""loaded""
          - ubuntu1804cis_rule_2_1_6
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.6

- name: ""SCORED | 2.1.7 | PATCH | Ensure talk server is not enabled""
  service:
    name: ntalk
    state: stopped
    enabled: false
  when:
    - ubuntu1804cis_ntalk_server == false
    - ntalk_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_1_7
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.7

- name: ""SCORED | 2.1.8 | PATCH | Ensure telnet server is not enabled""
  service:
    name: telnet
    state: stopped
    enabled: false
  when:
    - ubuntu1804cis_telnet_server == false
    - telnet_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_1_8
  tags:
    - level1
    - scored
    - patch
    - rule_2.1.8

- name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
  block:
      - name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
      - name: ""SCORED | 2.1.9 | PATCH | Ensure tftp server is not enabled""
            state: stopped
      - ubuntu1804cis_rule_2_1_9
      - rule_2.1.9
- name: ""SCORED | 2.1.10 | PATCH | Ensure xinetd is not enabled""
      - ubuntu1804cis_rule_2_1_10
      - rule_2.1.10

- name: ""SCORED | 2.1.11 | PATCH | Ensure openbsd-inetd is not installed""
  apt:
    name: openbsd-inetd
    state: absent
  when:
    - openbsd_inetd_service_status.stdout == ""ok installed""
    - ubuntu1804cis_rule_2_1_11
  tags:
    - level1
    - patch
    - scored
    - rule_2.1.11
- name: ""SCORED | 2.2.16 | PATCH | Ensure rsync service is not enabled ""
    name: rsyncd
    state: stopped
    enabled: false
    - not ubuntu1804cis_rsyncd_server
    - rsyncd_service_status.stdout == ""loaded""
    - ubuntu1804cis_rule_2_2_16
    - level1
    - scored
    - patch
    - rule_2.2.16
- name: ""SCORED | 2.2.17 | PATCH | Ensure NIS Server is not enabled""
      name: ypserv
      - not ubuntu1804cis_nis_server
      - ypserv_service_status.stdout == ""loaded"""
eb85f367b6fad40efb51e58dcd079963efde83d1,changed_when: tenant_assoc.status == 201,"---
# Handlers for rebuilding stripes
- name: Build stripes
  become: yes
  shell: ""yarn install && yarn build -- output""
  args:
    chdir: ""{{ stripes_conf_dir }}""
  listen: ""Rebuild stripes""
  #notify: ""Rebuild container""

- name: get package.json
  slurp: src={{ stripes_conf_dir }}/package.json
  register: platform_raw
  listen: ""Rebuild stripes""

- set_fact: platform={{ platform_raw.content|b64decode|from_json }}
  listen: ""Rebuild stripes""

- name: Generate module descriptors
  become: yes
  shell: node {{ stripes_conf_dir }}/node_modules/@folio/stripes-core/util/package2md.js {{ stripes_conf_dir }}/node_modules/{{ item.key }}/package.json > {{ stripes_conf_dir }}/module-descriptors/{{ item.key.split('/')[1] }}.json
  when: item.key.split('/')[0] == '@folio' and item.key != '@folio/stripes-components' and item.key != '@folio/stripes-core'
  with_dict: ""{{ platform.dependencies }}""
  listen: ""Rebuild stripes""

- name: Get module descriptor filenames
  shell: ls {{ stripes_conf_dir }}/module-descriptors
  register: mod_descrs
  changed_when: false
  listen: ""Register modules""

- name: Slurp module descriptors
  slurp: src={{ stripes_conf_dir }}/module-descriptors/{{ item }}
  with_items: ""{{ mod_descrs.stdout_lines }}""
  register: mod_descrs_raw
  listen: ""Register modules""

- set_fact: mod_descrs=[]
  listen: ""Register modules""

- set_fact:
    mod_descrs: ""{{ mod_descrs }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descrs_raw.results }}""
  listen: ""Register modules""

- name: Check module registration
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: mod_reg_status
  listen: ""Register modules""

- name: Register modules with Okapi
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules""
    method: POST
    body_format: json
    body: ""{{ item.1|to_json }}""
    status_code: 201
  when: mod_reg_status.results[item.0].status == 404
  register: mod_register
  changed_when: mod_register.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

- name: Check tenant-module association
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/item.id""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: tenant_modules
  listen: ""Register modules""

- name: Enable modules for tenant
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""
    method: POST
    body_format: json
    body: '{ ""id"" : ""{{ item.1.id }}"" }'
    status_code: 201
  when: tenant_modules.results[item.0].status == 404
  register: tenant_assoc
  changed_when: tenant_assoc == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

# Handlers for rebuilding the Docker container
# - name: copy Dockerfile to stripes conf dir
#   become: yes
#   copy: 
#     src: Dockerfile
#     dest: ""{{ stripes_conf_dir }}/Dockerfile""

# - name: create docker network for stripes
#   become: yes
#   docker_network: 
#     name: stripes-net

# - name: build and start stripes docker container
#   become: yes
#   docker_service:
#     project_name: stripes
#     definition: 
#       version: '2'
#       services: 
#         stripes:
#           build: ""{{ stripes_conf_dir }}""
#           image: stripes
#           environment: 
#             - STRIPES_HOST=0.0.0.0
#           ports: 
#             - ""{{ stripes_host_address }}:3000:3000""
#           networks:  
#             stripes-net:
#               aliases:
#                 - stripes-serv
#           restart: always
#       networks: 
#         stripes-net: 
#           external: true
#     state: present
#   register: stripes_container_status"
6998282888e55403ff589c3b654e004a974bc8fa,"  file: 
    path: /etc/nginx/sites-enabled/default
    state: absent
    notify: Restart nginx","- name: disable nginx default vhost
  become: yes
  file: path=/etc/nginx/sites-enabled/default
  state: absent
  notify: Restart nginx
"
72c360d009376395021cec26637cadabd571c7cc,"  register: mod_descrs_files
- name: Create mod_descr_list variable to order modules
  with_items: ""{{ mod_descrs_files.stdout_lines }}""
- name: Reset mod_descr_list variable
  set_fact: mod_descr_list=[]
- name: Build mod_descr_list for registration
  set_fact:
    mod_descr_list: ""{{ mod_descr_list }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""
  with_items: ""{{ mod_descr_list }}""
  with_indexed_items: ""{{ mod_descr_list }}""","---
- name: Build stripes
  become: yes
  shell: ""yarn install && yarn build -- output""
  args:
    chdir: ""{{ stripes_conf_dir }}""
  listen: ""Rebuild stripes""

- name: Record stripes rebuild variable
  set_fact:
    stripes_rebuild: true
  listen: ""Rebuild stripes""

- name: Get module descriptor filenames
  shell: ls {{ stripes_conf_dir }}/ModuleDescriptors
  register: mod_descrs
  changed_when: false
  listen: ""Register modules""

- name: Slurp module descriptors
  slurp: src={{ stripes_conf_dir }}/ModuleDescriptors/{{ item }}
  with_items: ""{{ mod_descrs.stdout_lines }}""
  register: mod_descrs_raw
  listen: ""Register modules""

- set_fact: mod_descrs=[]
  listen: ""Register modules""

- set_fact:
    mod_descrs: ""{{ mod_descrs }} + [ {{ item.content|b64decode|from_json }} ]""
  with_items: ""{{ mod_descrs_raw.results }}""
  listen: ""Register modules""

- name: Check module registration
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: mod_reg_status
  listen: ""Register modules""

- name: Register modules with Okapi
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/modules""
    method: POST
    body_format: json
    body: ""{{ item.1|to_json }}""
    status_code: 201
  when: mod_reg_status.results[item.0].status == 404
  register: mod_register
  changed_when: mod_register.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules""

- name: Check tenant-module association
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules/{{ item.id }}""
    status_code: 200, 404
  with_items: ""{{ mod_descrs }}""
  register: tenant_modules
  listen: ""Register modules""

- name: Enable modules for tenant
  uri:
    url: ""{{ stripes_okapi_url }}/_/proxy/tenants/{{ stripes_tenant }}/modules""
    method: POST
    body_format: json
    body: '{ ""id"" : ""{{ item.1.id }}"" }'
    status_code: 201
  when: tenant_modules.results[item.0].status == 404
  register: tenant_assoc
  changed_when: tenant_assoc.status == 201
  with_indexed_items: ""{{ mod_descrs }}""
  listen: ""Register modules"""
8dc2e7ce2a0821c3dba9feb972ba4fa22181c4c7,"- include: bootstrap_user.yml create_user=galaxy_tools_create_bootstrap_user
- include: bootstrap_user.yml delete_user=galaxy_tools_delete_bootstrap_user
  when: galaxy_tools_delete_bootstrap_user","- include: bootstrap_user.yml
  when: galaxy_tools_create_bootstrap_user and not galaxy_tools_api_key

  when: galaxy_tools_install_tools

- include: bootstrap_user.yml
  when: galaxy_tools_delete_bootstrap_user and not galaxy_tools_api_key"
d58bda8ff6f5532f7a3767a45db11f02ea0dabdf,- hosts: master,"---
- hosts: all
  remote_user: root
  gather_facts: no

  tasks:
  - name: Add devices to heketi nodes
    heketi: action=adddevice sshuser=""{{ ssh_user }}""  userkey=""{{ user_key }}"" server=""{{ servername }}""
            node=""{{ node }}"" devices=""{{ item.devices }}""
    with_items: hdict
    register: result

  - debug: msg=""{{ result.results[0]['msg'] }}"""
7872921989f8043059374ecf5abac9e5e3be657b,"    name: ""{{ graylog_mongodb_package_dependencies_python2 }}""
  when: ansible_python_version is version('3.0.0', '<')

- name: ""Package dependencies should be installed""
  yum:
    name: ""{{ graylog_mongodb_package_dependencies_python3 }}""
    state: present
  when: ansible_python_version is version('3.0.0', '>=')","
- name: Package dependencies should be installed
  yum: name={{ item }} state=installed
  with_items: ""{{ graylog_mongodb_package_dependencies | default([]) }}"""
82c71324d1461cc471641d834f1bbf105dbe97f8,min_ansible_version: 2.3,min_ansible_version: 2.2
8218e5c97297c7a7b64d3c39f53ab24227fc0fca,"  loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:","- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath"
8218e5c97297c7a7b64d3c39f53ab24227fc0fca,"  loop: ""{{ list_one | product(list_two) | list }}""
  vars:
    list_one:
    list_two:","  author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web
 "
8218e5c97297c7a7b64d3c39f53ab24227fc0fca,"  description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch","- name: LINEINFILE | Fix path
  lineinfile: >
    regexp='{{ item.0.regexp }}'
    line='{{ item.0.line }}'
    dest='{{ item.1 }}'
  with_nested:
    -
      - regexp: '^fastcgi_param  SCRIPT_FILENAME'
        line: 'fastcgi_param  SCRIPT_FILENAME    $realpath_root$fastcgi_script_name;'
      - regexp: '^fastcgi_param  DOCUMENT_ROOT'
        line: 'fastcgi_param  DOCUMENT_ROOT      $realpath_root;'
    - [ '/etc/nginx/fastcgi_params', '/etc/nginx/fastcgi.conf' ]
  when: nginx_fastcgi_fix_realpath"
8218e5c97297c7a7b64d3c39f53ab24227fc0fca,"  description: Nginx for Debian / FreeBSD
  min_ansible_version: 2.5
    - stretch","  author: Emilien Mantel
  description: Nginx for Debian 
  license: GPLv2 
  platforms:
  - name: Debian
    versions:
    - wheezy
    - jessie
  categories:
  - web
 "
8846e392b6f21a47304c850abc1f587a99ac01f5,"  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/alerts/AMQOnline_RouterMeshConnectivityHealth.asciidoc","# Custom SOP URLs for alerts
sops:
  BrokerMemory: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_BrokerMemory.asciidoc
  ComponentHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_ComponentHealth.asciidoc
  AuthenticationService: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_AuthenticationService.asciidoc
  RouterMeshUndeliveredHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshUndeliveredHealth.asciidoc
  RouterMeshConnectivityHealth: https://github.com/integr8ly/integreatly-help/blob/master/sops/AMQOnline_RouterMeshConnectivityHealth.asciidoc"
fb470c36c490804994e5308cb13ababaf5e7de37,"    name: crud
      name: spring-boot-rest-http-crud","apiVersion: template.openshift.io/v1
    iconClass: icon-node
    tags: nodejs, crud
    openshift.io/display-name: Fruit CRUD Application
    openshift.io/provider-display-name: Red Hat, Inc.
    openshift.io/documentation-url: https://github.com/integr8ly/walkthrough-applications.git
    description: Basic CRUD application for fruit
- kind: DeploymentConfig
  apiVersion: apps.openshift.io/v1
    name: crud-app
      app: crud-app
    revisionHistoryLimit: 10
    test: false
      app: crud-app
          app: crud-app
        - name: crud-app
          image: quay.io/integreatly/fruit-crud-app:1.0.1
          resources: {}
          terminationMessagePath: ""/dev/termination-log""
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
        restartPolicy: Always
        terminationGracePeriodSeconds: 30
        dnsPolicy: ClusterFirst
        securityContext: {}
        schedulerName: default-scheduler
- kind: Service
  apiVersion: v1
    name: crud-app
    ports:
    - protocol: TCP
      port: 8080
      app: crud-app
- kind: Route
  apiVersion: route.openshift.io/v1
    name: spring-boot-rest-http-crud
    to:
      kind: Service
      name: crud-app
    port:
      targetPort: 8080
    tls:
      termination: edge
    wildcardPolicy: None"
3fb75cffdbd86fbf9a34708e61abd5b268c3fc8f,"  shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-service""
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-service""","---
- name: ""Delete project namespace: {{ msbroker_namespace }}""
  shell: oc delete project {{ msbroker_namespace }}
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Delete CRDs
  shell: ""oc delete crd {{ item }}""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0
  with_items: syndesises.syndesis.io

- name: Clean up clusterservicebroker
  shell: ""oc delete clusterservicebrokers.servicecatalog.k8s.io managed-services-broker""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role
  shell: ""oc delete clusterroles.rbac.authorization.k8s.io managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0

- name: Clean up cluster role binding
  shell: ""oc delete clusterrolebindings.rbac.authorization.k8s.io default-cluster-account-managed-services""
  register: output
  failed_when: output.stderr != '' and 'not found' not in output.stderr
  changed_when: output.rc == 0"
2ef68caf5ce57939be0e7eeab826907196f306fe,,"ups_namespace: ""{{ eval_ups_namespace | default('unifiedpush') }}""
ups_app_namespaces: ""{{ eval_mdc_namespace | default('mobile-developer-console') }}""
ups_resources:
  - ""{{ ups_operator_resources }}/service_account.yaml""
  - ""{{ ups_operator_resources }}/role.yaml""
  - ""{{ ups_operator_resources }}/role_binding.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_androidvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_iosvariant_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_pushapplication_crd.yaml""
  - ""{{ ups_operator_resources }}/crds/push_v1alpha1_unifiedpushserver_crd.yaml""
ups_operator_deployment: ""{{ ups_operator_resources }}/operator.yaml""
ups_template_dir: /tmp
ups_server_name: unifiedpush
#backup
ups_backup: ""{{ backup_restore_install | default(false) }}""
ups_backup_name: ups-daily-at-midnight
ups_backup_schedule: ""{{ backup_schedule }}""
ups_backup_secret: ""s3-credentials""
ups_backup_secret_namespace: ""{{ backup_namespace }}""
ups_encryption_secret: ''
ups_encryption_secret_namespace: ""{{ backup_namespace }}""
ups_backup_rbac_template:
  - ""{{ backup_resources_location }}/rbac/role-binding-template.yaml""
ups_backup_rbac_resources:
  - ""{{ backup_resources_location }}/rbac/service-account.yaml""
  - ""{{ backup_resources_location }}/rbac/role.yaml""
#monitor
ups_svc_monitor_resources:
  - ""{{ ups_operator_resources }}/service_monitor.yaml"""
4911c19a5af814d26017516f8eaa831598daa616,shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\},"- name: ""include rhsso vars""
  include_vars: ../../rhsso/defaults/main.yml
    launcher_sso_openshift_idp_client_secret: ""{{ 99999 | random | to_uuid }}""
- name: Retrieve secret for launcher openshift sso client
  shell: oc get secret {{ launcher_sso_keycloak_client_id }}-client -n {{ rhsso_namespace }} -o template --template=\{\{.data.secret\}\} | base64 -d
  failed_when: openshift_client_secret_response.stderr != """"
  until: openshift_client_secret_response.stdout
  retries: 50
  delay: 3
  changed_when: openshift_client_secret_response.stdout"
5a2f06fca49924b7b023318a6aec5942cc7e233e,"- name: ""include launcher vars""
  include_vars: ../../launcher/defaults/main.yml

- name: Get Launcher SSO secure route
  shell: oc get route/{{ launcher_sso_prefix }} -o template --template \{\{.spec.host\}\} -n {{ launcher_namespace }}
  register: rhsso_secure_route
  retries: 60
  delay: 5
  until: rhsso_secure_route.rc == 0

- name: ""Generate secret for launcher client""
  set_fact:
    launcher_client_secret: ""{{ (ansible_date_time.epoch + launcher_namespace) | hash('sha512') }}""

- set_fact:
    launcher_sso_route: ""{{ rhsso_secure_route.stdout }}""

  shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_cluster_admin_username}}","- name: ""Create project namespace: {{ rhsso_namespace }}""
  shell: oc new-project {{ rhsso_namespace }}
  register: output
  failed_when: output.stderr != '' and 'already exists' not in output.stderr
  changed_when: output.rc == 0
- name: ""Ensure 1.2 tag is present for redhat sso in openshift namespace""
  shell: oc tag --source=docker registry.access.redhat.com/redhat-sso-7/sso72-openshift:1.2 openshift/redhat-sso72-openshift:1.2
  register: result
  until: result.stdout
  retries: 50
  delay: 1
  failed_when: not result.stdout
  changed_when: False
- name: ""Ensure 1.2 tag has an imported image in openshift namespace""
  shell: oc -n openshift import-image redhat-sso72-openshift:1.2
  register: result
  until: result.stdout
  retries: 50
  delay: 1
  failed_when: not result.stdout
  changed_when: False
- name: ""Create required objects""
  shell: ""oc create -f https://raw.githubusercontent.com/{{rhsso_operator_repo}}/keycloak-operator/{{rhsso_operator_commit_tag}}/deploy/{{ item }} -n {{ rhsso_namespace }}""
  with_items: ""{{ rhsso_operator_required_objects }}""
  register: rhsso_required_objects_result
  failed_when: rhsso_required_objects_result.stderr != '' and 'AlreadyExists' not in rhsso_required_objects_result.stderr

- name: ""Create operator deployment config template""
  template:
    src: ""operator-dc.yaml""
    dest: /tmp/operator-dc.yaml

- name: ""create operator deployment config""
  shell: ""oc create -f /tmp/operator-dc.yaml -n {{ rhsso_namespace }}""
  register: rhsso_dc
  failed_when: rhsso_dc.stderr != '' and 'AlreadyExists' not in rhsso_dc.stderr

- name: ""Create keycloak resource template""
  template:
    src: ""keycloak.json.j2""
    dest: ""/tmp/keycloak.json""

- name: ""Create keycloak resource""
  shell: oc create -f /tmp/keycloak.json -n {{ rhsso_namespace }}
  register: rhsso_keycloak
  failed_when: rhsso_keycloak.stderr != '' and 'AlreadyExists' not in rhsso_keycloak.stderr

- name: ""Generate secret for rhsso client""
  set_fact:
    rhsso_client_secret: ""{{ (ansible_date_time.epoch + rhsso_namespace) | hash('sha512') }}""

- name: ""include threescale vars""
  include_vars: ../../3scale/defaults/main.yml

- name: ""Generate secret for 3scale client""
  set_fact:
    threescale_client_secret: ""{{ (ansible_date_time.epoch + threescale_namespace) | hash('sha512') }}""

- name: ""Create keycloak realm resource template""
  template:
    src: ""keycloak-realm.json.j2""
    dest: ""/tmp/keycloak-realm.json""

- name: Seed evaluation users
  include: _inject_user.yml template=""/tmp/keycloak-realm.json"" email={{ rhsso_seed_users_email_format|format(item|int) }} username={{ rhsso_seed_users_name_format|format(item|int)}} password={{ rhsso_seed_users_password }}
  with_sequence: count={{ rhsso_seed_users_count }}

- name: ""Create keycloak realm resource""
  shell: oc create -f /tmp/keycloak-realm.json -n {{ rhsso_namespace }}
  register: rhsso_kcr
  failed_when: rhsso_kcr.stderr != '' and 'AlreadyExists' not in rhsso_kcr.stderr


- name: ""Verify rhsso realm is provisioned""
  shell: sleep 5; oc get keycloakrealm {{ rhsso_realm }} -o template --template \{\{.status.phase\}\}  -n {{ rhsso_namespace }}  |  grep  'reconcile'
  register: result
  until: result.stdout
  retries: 50
  delay: 10
  failed_when: not result.stdout
  changed_when: False

- name: configure logout
  import_tasks: logout.yml

- name: Add cluster admin role to evals admin
  shell: oc adm policy add-cluster-role-to-user cluster-admin {{rhsso_evals_admin_username}}"
3b11780ef0dc866d27231904fc81338facc8504f,"  shell: ""oc set env dc/tutorial-web-app \
  GITEA_TOKEN='{{ gitea_token }}' \
  GITEA_HOST='http://{{ gitea_ingress_host.stdout }}' \
  -n {{ webapp_namespace }} \
  --overwrite=true""
  when: check_webapp_installed_cmd.rc == 0 and gitea_ingress_host.stdout != ''

- name: Wait for pods
  shell: sleep 5; oc get pods --namespace {{ webapp_namespace }}  |  grep  ""Creating""
  register: result
  until: not result.stdout
  retries: 50
  delay: 10
  failed_when: result.stdout
  changed_when: False","# oc exec doesn't accept a namespace argument so make sure we are using the correct one
- name: Make sure we are using the gitea namespace
  shell: ""oc project {{ gitea_namespace }}""


- name: ""Wait for Gitea pods to be ready""
  shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[*].status.containerStatuses[?(@.ready==true)].ready}' | wc -w""
  register: gitea_result
  until: gitea_result.stdout.find(""1"") != -1
  retries: 30
- name: Get the name of the gitea pod
  shell: ""oc get pods --namespace={{ gitea_namespace }} --selector='deployment=gitea' -o jsonpath='{.items[0].metadata.name}'""
  register: gitea_pod_name

- name: Create the gitea admin user
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name={{ gitea_admin_username }} --password={{ gitea_admin_password }} --admin --email=admin@example.com --config /home/gitea/conf/app.ini""
  register: create_admin_user_cmd
  failed_when: create_admin_user_cmd.stderr != '' and 'already exists' not in create_admin_user_cmd.stderr
  changed_when: create_admin_user_cmd.rc == 0

- name: Fetch all old tokens
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens""
    method: GET
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
  register: gitea_old_access_tokens

- name: Delete all old access tokens
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens/{{ item.id }}""
    method: DELETE
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
    status_code: 204
  with_items: ""{{ gitea_old_access_tokens.json }}""

- name: Create a new admin token for the admin user
  uri:
    url: ""http://{{ gitea_namespace }}.{{ gitea_route_suffix }}/api/v1/users/{{ gitea_admin_username }}/tokens""
    method: POST
    headers:
      Accept: ""application/json""
      Content-Type: ""application/json""
    body: {""name"": ""{{ gitea_admin_token }}""}
    body_format: json
    force_basic_auth: yes
    user: ""{{ gitea_admin_username }}""
    password: ""{{ gitea_admin_password }}""
    return_content: yes
    status_code: 201
  register: admin_token_result

- name: Extract sha value from admin token result
  set_fact:
    gitea_token: ""{{ admin_token_result.json | json_query('sha1') }}""

- name: Print out the gitea admin token
  debug:
    msg: ""Gitea admin token is {{ gitea_token }}""

- name: Create gitea walkthrough users
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals{{ item }} --password=Password1 --email=evals{{ item }}@example.com --config /home/gitea/conf/app.ini""
  register: create_walkthrough_user_cmd
  failed_when: create_walkthrough_user_cmd.stderr != '' and 'already exists' not in create_walkthrough_user_cmd.stderr
  changed_when: create_walkthrough_user_cmd.rc == 0
  with_sequence: count={{ eval_seed_users_count }}

- name: Create a user for the evals admin
  shell: ""oc exec {{ gitea_pod_name.stdout }} -- /home/gitea/gitea admin create-user --name=evals-admin --password=Password1 --email=evals-admin@example.com --config /home/gitea/conf/app.ini""
  register: create_eval_admin_user_cmd
  failed_when: create_eval_admin_user_cmd.stderr != '' and 'already exists' not in create_eval_admin_user_cmd.stderr
  changed_when: create_eval_admin_user_cmd.rc == 0

- name: Get the gitea ingress host
  shell: oc get ingress --namespace=gitea --selector='app=gitea' -o jsonpath='{.items[0].spec.rules[0].host}'
  register: gitea_ingress_host

- name: Set gitea host as webapp env var
  shell: oc set env dc/tutorial-web-app GITEA_HOST=""{{ gitea_ingress_host.stdout }}"" -n {{ webapp_namespace }} --overwrite=true
  when: gitea_ingress_host.stdout != ''

# SET GITEA TOKEN ENV VAR IN THE WEBAPP
  shell: oc set env dc/tutorial-web-app GITEA_TOKEN=""{{ gitea_token }}"" -n {{ webapp_namespace }} --overwrite=true
  when: check_webapp_installed_cmd.rc == 0"
15c2bff6e792e084472c177e8d2bd7c469fedde8,shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10Mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} --param=BACKEND_IMAGE_TAG={{ launcher_backend_image_tag }} | oc create -n {{ launcher_namespace }} -f -,shell: oc process -n {{ launcher_namespace }} -f {{ launcher_template }} --param=CREATOR_BACKEND_MEMORY_REQUEST=10mi --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_USERNAME= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_PASSWORD= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_API_URL= --param=LAUNCHER_MISSIONCONTROL_OPENSHIFT_CONSOLE_URL= --param=LAUNCHER_KEYCLOAK_URL=https://{{ launcher_sso_route }}/auth --param=LAUNCHER_KEYCLOAK_REALM={{ launcher_sso_realm }} --param=LAUNCHER_KEYCLOAK_CLIENT_ID=launcher-public --param=LAUNCHER_BOOSTER_CATALOG_REPOSITORY={{ launcher_catalog_git_repo }} --param=LAUNCHER_BOOSTER_CATALOG_REF={{ launcher_catalog_git_ref }} --param=FRONTEND_IMAGE_TAG={{ launcher_frontend_image_tag }} | oc create -n {{ launcher_namespace }} -f -
945b2590ed4950b179ec83c77854c263de69a39d,,"---
- name: Get RH-SSO secure route
  local_action: command oc get route/secure-sso -o template --template \{\{.spec.host\}\} -n {{ rhsso_namespace }}
  register: rhsso_secure_route

- set_fact:
    rhsso_route: ""{{ rhsso_secure_route.stdout }}""

- name: Retrieve RH-SSO Client Config
  local_action: command cat /tmp/client-config.json
  register: client_config_raw

- set_fact:
    client_config: ""{{ client_config_raw.stdout }}""

- name: Add RH-SSO identity provider to master config
  blockinfile:
    path: ""{{ openshift_master_config }}""
    insertafter: ""identityProviders""
    backup: yes
    block: |2
        - name: rh_sso
          challenge: false
          login: true
          mappingInfo: add
          provider:
            apiVersion: v1
            kind: OpenIDIdentityProvider
            clientID: {{ rhsso_client_id }}
            clientSecret: {{ client_config.json.value }}
            urls:
              authorize: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/auth
              token: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/token
              userInfo: https://{{ rhsso_route }}/auth/realms/{{ rhsso_realm }}/protocol/openid-connect/userinfo
            claims:
              id:
              - sub
              preferredUsername:
               - preferred_username
              name:
              - name
              email:
              - email
  register: master_config_update
  become: yes

- name: restart openshift master api service
  service:
    name: atomic-openshift-master-api
    state: restarted
  become: yes
  when: master_config_update.changed

- name: restart openshift master controller service
  service:
    name: atomic-openshift-master-controllers
    state: restarted
  become: yes
  when: master_config_update.changed

- name: Delete local client config file
  file: path=/tmp/client-config.json state=absent"
02f4eab690026e5f8981f0d0dabfb64cd994c80d,"- name: Add labels to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}"", ""integreatly-middleware-service"":""true""}}}'
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0
- name: Add labels to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\"", \""integreatly-middleware-service\"":\""true\""}}}'""
  register: namespace_patch
  failed_when: namespace_patch.stderr != '' and 'not patched' not in namespace_patch.stderr
  changed_when: namespace_patch.rc == 0","- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """""
6997cd9c161bbca3b425b3cabaafc5b7c492b115,,"      when:
        - user_rhsso | default(true) | bool
        - mdc | default(true) | bool"
02f4eab690026e5f8981f0d0dabfb64cd994c80d,"      shell: /usr/local/bin/master-restart controllers

# Delete users and identities created by rhsso
-
  name: ""Get all users created by rhsso""
  shell: oc get users | grep 'rh_sso' | awk '{print $1}'
  register: users
  failed_when: false

-
  name: ""Delete users""
  shell:  ""oc delete users {{ users.stdout | replace('\n', ' ') }}""
  when: users.stdout != ''
  failed_when: false","- name: Add monitoring label to namespace
  shell: oc patch ns {{ che_namespace }} --patch '{""metadata"":{""labels"":{""{{ monitoring_label_name }}"":""{{ monitoring_label_value }}""}}}'

  when: che_infra_namespace is defined and che_infra_namespace != """"

- name: Add monitoring label to namespace
  shell: ""oc patch ns {{ che_infra_namespace }} --patch '{\""metadata\"":{\""labels\"":{\""{{ monitoring_label_name }}\"":\""{{ monitoring_label_value }}\""}}}'""
  when: che_infra_namespace is defined and che_infra_namespace != """""
717e6670c829be38f97ed6f3d4d31111fd6d642b,"  loop: ""{{ real_users }}""","# handlers file for robot-pkgs

- name: USB controllers warning
  command: >-
    zenity --warning --text {{ usb_warning_msg }}
  become: yes
  become_user: ""{{ item.user }}""
  with_items: ""{{ real_users }}""
  ignore_errors: yes"
89bee831638af63c5444a9c4684b75f6c676b08d,"node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address }}""","---
# node_interface: ""{{ ansible_default_ipv4.interface }}""
node_interface: ""enp0s8""
node_ip_address: ""{{ hostvars[inventory_hostname]['ansible_' + node_interface].ipv4.address"
420112ec1fc3d245cbbf3f8c8433470c39e3d19c,"- name: Install flannel
- name: Prepare and write flannel configuration to etcd
  include: config.yml
- name: Enable flannel on node
  service: name=flanneld enabled=yes
- name: Start flannel on node
  service: name=flanneld state=started
  register: flannel_started
  notify:
     - Restart docker engine","---
- name: Install flannel service (RHEL/CentOS)
  when: ansible_os_family == ""RedHat""
  yum:
    name: flannel
    state: latest
    update_cache: yes

- name: Update flannel config
  template: src=""flanneld.j2"" dest={{ flannel_dir }}/flanneld
  register: change_flannel

- name: Set facts about etcdctl command
  set_fact:
    peers: ""{% for hostname in groups['etcd'] %}http://{{ hostname }}:2379{% if not loop.last %},{% endif %}{% endfor %}""
    conf_file: ""/tmp/config.json""
    conf_loc: ""/{{ flannel_key }}/config""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Create flannel config file to go in etcd
  template: src=flannel-config.json dest={{ conf_file }}
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Load the flannel config file into etcd
  when: etcd_peer_url_scheme == 'http'
  shell: ""/usr/bin/etcdctl --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Load the flannel config file into secure etcd
  when: etcd_peer_url_scheme == 'https'
  shell: ""/usr/bin/etcdctl -cert-file={{ etcd_peer_cert_file }} --ca-file={{ etcd_peer_ca_file }} --key-file={ etcd_peer_key_file }} --no-sync --peers={{ peers }} set {{ conf_loc }} < {{ conf_file }}""
  run_once: true
  delegate_to: ""{{ groups['etcd'][0] }}""

- name: Copy etcd certificate from ansible host
  when: etcd_peer_url_scheme == 'https'
  copy: src={{ master_cert_dir }} dest={{ kube_config_dir }}
  register: etcd_cert

- name: Start and enable flannel on node
  when: change_flannel|succeeded
  service: name=flanneld enabled=no state=started

- name: Reload flanneld
  when: change_flannel|changed or etcd_cert|changed
  service: name=flanneld state=restarted"
2242e0a7969b040fba4018ee39338285a53cf759,,"    directories:
    directories:
      - kubedns/kubedns-autoscale-dp.yml
    directories:
    directories:
    directories:
        kind: namespace
    directories:
      - logging/elasticsearch
      - logging/elasticsearch/elasticsearch-sa.yml
      - logging/elasticsearch/elasticsearch-rbac.yml
      - logging/elasticsearch/elasticsearch-svc.yml
      - logging/elasticsearch/elasticsearch-sts.yml
      - logging/kibana/kibana-anonymous-rbac.yml
    directories:
      - monitoring/gpu-exporter
      - monitoring/prometheus-adapter
        kind: namespace
        namespace: """"
      - monitoring/grafana/grafana-res-definitions.yml
      - monitoring/grafana/grafana-gpu-cluster-definitions.yml
      - monitoring/grafana/grafana-gpu-node-definitions.yml
      - monitoring/grafana/grafana-gpu-pod-definitions.yml
      - monitoring/grafana/grafana-cluster-definitions.yml
      - monitoring/gpu-exporter/gpu-exporter-svc.yml
      - monitoring/gpu-exporter/gpu-exporter-ds.yml
      - monitoring/prometheus-adapter/prometheus-adapter-sa.yml
      - monitoring/prometheus-adapter/prometheus-adapter-rbac.yml
      - monitoring/prometheus-adapter/prometheus-adapter-svc.yml
      - monitoring/prometheus-adapter/prometheus-adapter-cm.yml
      - monitoring/prometheus-adapter/prometheus-adapter-apiservice.yml
      - monitoring/prometheus-adapter/prometheus-adapter-dp.yml
      - monitoring/servicemonitor/kube-state-metrics-sm.yml
      - monitoring/servicemonitor/gpu-exporter-sm.yml
      - monitoring/servicemonitor/grafana-sm.yml"
f61832bc76c8713b997efa2725c7ec7bd06632c3,"  when: ""'pve-no-subscription' in proxmox_repository_line""","- block:

  - name: Remove automatically installed PVE Enterprise repo configuration
    apt_repository:
      repo: ""deb https://enterprise.proxmox.com/debian jessie pve-enterprise""
      filename: pve-enterprise
      state: absent

  - name: Remove subscription popup dialog in web UI
    replace:
      dest: /usr/share/pve-manager/ext6/pvemanagerlib.js
      regexp: ""^          if .data.status !== 'Active'. {""
      replace: ""          if (false) {""
      backup: yes

  when: 'pve-no-subscription' in proxmox_repository_line
  when: proxmox_check_for_kernel_update

  when:
    - proxmox_reboot_on_kernel_update
    - __proxmox_kernel | changed

- name: Remove old Debian kernels
  apt:
    name: ""{{ item }}""
    state: absent
  with_items:
    - linux-image-amd64
    - linux-image-3.16.0-4-amd64
  when: proxmox_remove_old_kernels

- name: LDAP fix for authenticated search
  lineinfile:
    line: ""    $ldap->bind('{{ proxmox_ldap_bind_user }}', password => '{{ proxmox_ldap_bind_password }}');""
    insertbefore: ""ldap->search\\(""
    dest: /usr/share/perl5/PVE/Auth/LDAP.pm
  notify:
    - restart pvedaemon
  when:
    - proxmox_ldap_bind_user is defined
    - proxmox_ldap_bind_password is defined"
892b2386950bf02321ff63298bd7f2c6864faab3,"    - name: Mkdir for java installation
      win_file:
        path: '{{ java_path }}\{{ java_folder }}'
        state: directory
    - name: Create temporary directory
      win_tempfile:
        state: directory
      register: temp_dir_path
    - name: Unarchive to temporary directory
      win_unzip:
        src: '{{ java_artifact }}'
        dest: '{{ temp_dir_path }}'
    - name: Find java_folder in temp
      win_find:
        paths: '{{ temp_dir_path }}'
        recurse: false
        file_type: directory
      register: java_temp_folder
    - name: Copy from temporary directory
      win_copy:
        src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
        dest: '{{ java_path }}\{{ java_folder }}'
        remote_src: true

    - name: Check choco
      win_chocolatey:
        name: chocolatey
        state: present

    # https://help.sap.com/viewer/65de2977205c403bbc107264b8eccf4b/Cloud/en-US/76137f42711e1014839a8273b0e91070.html
    - name: 'Install vcredist package prior to using SAP JVM'
      win_chocolatey:
        name: vcredist2013
      register: choco_install
      retries: 15
      delay: 5
      until: choco_install is succeeded","---
- name: Check that the java_folder exists
  win_stat:
    path: '{{ java_path }}\{{ java_folder }}/bin'
  register: java_folder_bin

- name: Install java from tarball
  block:
  - name: Mkdir for java installation
    win_file:
      path: '{{ java_path }}\{{ java_folder }}'
      state: directory

  - name: Create temporary directory
    win_tempfile:
      state: directory
    register: temp_dir_path

  - name: Unarchive to temporary directory
    win_unzip:
      src: '{{ java_artifact }}'
      dest: '{{ temp_dir_path }}'

  - name: Find java_folder in temp
    win_find:
      paths: '{{ temp_dir_path }}'
      recurse: false
      file_type: directory
    register: java_temp_folder

  - name: Copy from temporary directory
    win_copy:
      src: '{{ java_temp_folder.files | map(attribute=""path"") | list | last }}\'
      dest: '{{ java_path }}\{{ java_folder }}'
      remote_src: true
  when: not java_folder_bin.stat.exists"
003ef19731dd9631e9959ce422ea9e4a6c5becd7,"        | regex_findall('(https://download[\.\w]+/java/GA/jdk'
 java_major_version|string + '[.\d]+/[\d\w]+/'
 java_major_version|string + '/GPL/openjdk-'
 java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')","---
- name: Set java minor version
  set_fact:
    minor: ""{{ java_minor_version | default('.*', True) }}""

- name: 'Fetch root page {{ openjdk_root_page }}'
  uri:
    url: '{{ openjdk_root_page }}/{{ java_major_version }}/'
    return_content: True
  register: root_page

- name: Find release url
  set_fact:
    release_url: >-
      {{ root_page['content']
        | regex_findall('(https://download\.oracle\.com/java/GA/jdk'
          + java_major_version|string + '[.\d]+/[\d\w]+/'
          + java_major_version|string + '/GPL/openjdk-'
          + java_major_version|string + '[\d._]+linux-x64_bin[\w\d.]+)')
      }}

- name: Exit if OpenJDK version is not General-Availability Release
  fail:
    msg: 'OpenJDK version {{ java_major_version }} not GA Release'
  when: release_url[1] is not defined

- name: 'Get artifact checksum {{ release_url[1] }}'
  uri:
    url: '{{ release_url[1] }}'
    return_content: True
  register: artifact_checksum

- name: Show artifact checksum
  debug:
    var: artifact_checksum.content

- name: 'Download artifact from {{ release_url[0] }}'
  get_url:
    url: '{{ release_url[0] }}'
    dest: '{{ download_path }}'
    checksum: 'sha256:{{ artifact_checksum.content }}'
  register: file_downloaded
  retries: 20
  delay: 5
  until: file_downloaded is succeeded

- name: Set downloaded artifact variable
  set_fact:
    java_artifact: '{{ file_downloaded.dest }}'

- name: Split artifact name
  set_fact:
    parts: >-
      {{ java_artifact
        | regex_findall('^(.*j[dkre]{2})-([0-9]+)[u.]([0-9.]+)[-_]([a-z]+)-(x64|i586)')
        | first | list }}

- name: Set variables based on split
  set_fact:
    java_package: '{{ parts[0][-3:] }}'
    java_major_version: '{{ parts[1] }}'
    java_minor_version: '{{ parts[2] }}'
    java_os: '{{ parts[3] }}'
    java_arch: '{{ parts[4] }}'"
7de193eefab687fb6921fc89dc0255c2ba032e10,"  - { src: php.ini.j2, dest: /etc/php5/conf.d/90-php-docker.ini }
  - { src: php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}","- name: install PHP5 packages

- name: Place PHP configuration files in place.
  template: src={{ item.src }} dest={{ item.dest }} owner=root group=root mode=644
  with_items:
  - { src: php/php.ini.j2, dest: /etc/php5/apache2/90-php-docker.ini }
  - { src: php/php.ini.j2, dest: /etc/php5/cli/90-php-docker.ini }
  - { src: php/xdebug.ini.j2, dest: /etc/php5/conf.d/20-xdebug.ini}
  notify: restart apache"
40f545b6172381f12e116c9df671393f65a8ec97,"  become: yes
  become: yes
  become: yes
  become: yes","- name: ensure rabbitmq is installed
  apt: pkg=rabbitmq-server state=installed
  sudo: yes

- name: activate rabbitmq_management plugin
  shell: ""/usr/sbin/rabbitmq-plugins enable rabbitmq_management""
  sudo: yes

- name: restart rabbitmq
  service: name=rabbitmq-server state=restarted
  sudo: yes

- name: get rabbitmqadmin script
  get_url: url=http://localhost:15672/cli/rabbitmqadmin dest=/usr/local/bin/rabbitmqadmin mode=755
  sudo: yes"
be34133b5039e8a6374c2d505f484b5fd016da0d,"    pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int < 16 else 'python3-venv' }}""","  when: ansible_lsb.major_release|int >= 8

# The Python 3 virtualenv package is named ""python3.4-venv"" on Ubuntu before
# Xenial
- name: install python 3 virtualenv package
  apt:
    pkg: ""{{ 'python3.4-venv' if ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int >= 16 else 'python3-venv' }}""
    state: latest
  become: yes"
f98e726d62df7e5cc9ab92b767d48646f3a6b80b,"  file: dest=""{{ solr_config_dir }}"" state=directory group=solr mode=""g+rwX"" recurse=yes","- name: create solr group
  group: name=solr state=present
  become: yes

  user: name=solr group=solr groups=""www-data"" comment=""Solr Daemon"" home=""{{ solr_install_dir }}""
- name: solr config directory permission
  file: dest=""{{ solr_config_dir }}"" state=directory owner=vagrant group=solr mode=""g+rwX"" recurse=yes
  become: yes

- name: solr install directory permission"
014898fa8f12c2973ba8e7ca7d9a9094e20f26c9,"  shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20 ] ; then exit 1 ; fi ; exit 0""
  args:
    executable: /bin/bash","- name: ""Wait for the running state of ToscaSubmitter""
  shell: ""i=0 ; while [ `docker ps --filter status=running --filter name=toscasubmitter | wc -l` -eq 1 ] && [ $i -lt 20 ] ; do echo -e 'ToscaSubmitter is not running yet!' ;  sleep 1 ; ((i++)) ; done ; if [ $i -eq 20] ; then echo '20 seconds expired, unsuccessfully. An error occurred.' ; exit 1 ; fi""
  register: output
  changed_when: output.stdout != """"
  when: f.stat.exists
"
8c9600107db815e82a96918b528db48fb5bb3696,"  service:
    name: iptables
    state: restarted
    enabled: yes
  service:
    name: micado
    state: restarted
    enabled: yes","
- name: Enable packet filtering
  shell: systemctl enable --now iptables"
c0eb1b6878d58894d188d4956ead5ea3a68986c8,"- name: Create app and data folders
  file:
    name: ""{{ item }}""
    state: directory
  with_items:
    - ""{{ bamboo_master_application_folder }}""
    - ""{{ bamboo_master_data_folder }}""

    home: ""{{ bamboo_master_data_folder }}""
- name: Set permissions for app and data folders","---
- name: ""Install JDK""
  yum:
    name: ""java-{{ openjdk_version }}-openjdk""
    state: installed

- name: Add local user
  user:
    name: ""{{ bamboo_user }}""

- name: Create app and data folders
  file:
    name: ""{{ item }}""
    state: directory
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  with_items:
    - ""{{ bamboo_home_directory }}""
    - ""{{ bamboo_root_directory }}""

- name: Download and unpack bamboo
  unarchive:
    src: ""https://www.atlassian.com/software/bamboo/downloads/binary/atlassian-bamboo-{{ bamboo_version }}.tar.gz""
    dest: ""{{ bamboo_root_directory }}""
    remote_src: True
    # keep our modified, newer files instead of overriding from the tarball
    keep_newer: yes
  changed_when: False

- name: Configure bamboo server (server.xml)
  template:
    src: server.xml.j2
    dest: ""{{ bamboo_application_directory }}/conf/server.xml""
  notify: restart bamboo

- name: Create current version file
  file:
    name: ""{{ bamboo_root_directory }}/current""
    state: touch
    mode: 0644
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  changed_when: False
    
- name: Update current version file
  template:
    src: current.j2
    dest: ""{{ bamboo_root_directory }}/current""
  notify: restart bamboo
  register: current_version

- name: Stop bamboo if newer version is going to be installed
  systemd:
    name: bamboo
    state: stopped
  when: current_version.changed

- name: Copy logrotate script for syslog
  copy:
    src: syslog
    dest: /etc/logrotate.d/syslog

- name: Add cronjob for cleanup Bamboo logs
  cron:
    name: cleanupbamboologs
    special_time: daily
    state: present
    job: ""/usr/bin/find {{ bamboo_application_directory }}/logs/ -name *.log -type f -mtime +7 -exec rm  {} \\;""

- name: Add cronjob for cleanup Bamboo build-dir
  cron:
    name: cleanupbamboobuilddir
    hour: 03
    minute: 15
    weekday: 0 # sunday
    state: present
    job: ""/usr/bin/find {{ bamboo_home_directory }}/xml-data/build-dir/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \\;""

- name: Add cronjob for cleanup maven repository cache
  cron:
    name: cleanupbamboomavenrepo
    hour: 03
    minute: 15
    weekday: 0 # sunday
    state: present
    job: ""/usr/bin/find {{ bamboo_home_directory }}/.m2/repository/ -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \\;""

- name: Install bamboo systemd unit script
  template:
    src: bamboo.service.j2
    dest: /etc/systemd/system/bamboo.service
    mode: 0744
  notify: restart bamboo

- name: Set bamboo.home property variable
  template:
    src: bamboo-init.properties
    dest: ""{{ bamboo_application_directory }}/atlassian-bamboo/WEB-INF/classes/bamboo-init.properties""
  notify: restart bamboo

- name: Set JVM settings
  replace:
    dest: ""{{ bamboo_application_directory }}/bin/setenv.sh""
    regexp: ""{{ item.regexp }}""
    replace: ""{{ item.replace }}""
  with_items:
    - regexp: 'JVM_MINIMUM_MEMORY=""[0-9]+[bkmg]'
      replace: 'JVM_MINIMUM_MEMORY=""{{ bamboo_jvm_heap_min }}'
    - regexp: 'JVM_MAXIMUM_MEMORY=""[0-9]+[bkmg]'
      replace: 'JVM_MAXIMUM_MEMORY=""{{ bamboo_jvm_heap_max }}'
  notify: restart bamboo

- name: Fix permissions on application folder
  file:
    path: ""{{ item }}""
    recurse: yes
    owner: ""{{ bamboo_user }}""
    group: ""{{ bamboo_user }}""
  changed_when: False
  with_items:
    - ""{{ bamboo_application_directory }}"""
9ba22fefe08f63915250ce4c579b0bf2a19c3b2a,"# TODO: Find out why this doesn't work
  command: ""sudo /opt/influxdb/influxd config -config {{influxdb_generated_config}}""
  register: influxdb_merged_config
  tags:
    - influxdb

- name: Write merged config
  copy:
    content: ""{{influxdb_merged_config.stdout}}""
    dest: ""{{influxdb_config_file}}""
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
  tags:
    - influxdb

- name: Ensure directories have correct permissions
  command: ""sudo chown -R {{influxdb_group}}:{{influxdb_user}} {{item}}""
  with_items:
    - ""{{influxdb_meta_dir}}""
    - ""{{influxdb_data_dir}}""
    - ""{{influxdb_hh_dir}}""
    - ""{{influxdb_wal_dir}}""","  when: ansible_distribution in ['Ubuntu', 'Debian']
- name: Create data dir
    path: ""{{influxdb_data_dir}}""
    state: directory
    group: ""{{influxdb_group}}""
    owner: ""{{influxdb_user}}""
- name: Create meta dir
    path: ""{{influxdb_meta_dir}}""
- name: Create hh dir
    path: ""{{influxdb_hh_dir}}""
- name: Create config directory
    dest: ""{{influxdb_generated_config}}""
- name: Run config update
  command: ""sudo su -c \""{{influxdb_opt_dir}}/influxd config -config {{influxdb_generated_config}} > {{influxdb_config_file}}\"" {{influxdb_user}}"""
d4be01565d4c0060626b4324728c482a6402bb43,"  file:
    path: ""{{ degoss_test_dir }}""
    state: directory
  loop: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
    clean: ""{{ degoss_clean | bool }}""
    clean_on_failure: ""{{ degoss_clean_on_failure | bool }}""
    debug: ""{{ degoss_debug | bool }}""","- include: versions/latest.yml
  when: goss_version == ""latest""

- include: versions/pinned.yml
  when: goss_version != ""latest""

# set play facts
- name: establish download url
  set_fact:
    goss_download_url: ""{{ goss_github_repo_url }}/releases/download/v{{ goss_real_version }}/goss-linux-amd64""

# create goss directories
- name: create goss directories
  file: path={{ item }} state=directory
  with_items:
    - ""{{ degoss_tmp_root }}""
    - ""{{ degoss_test_root }}""
    - ""{{ degoss_goss_install_dir }}""
  changed_when: degoss_changed_when

# download goss
- name: install
  get_url:
    url: ""{{ goss_download_url }}""
    dest: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    mode: 0755
  changed_when: degoss_changed_when

# symlink
- name: link
  file:
    state: link
    src: ""{{ degoss_goss_bin }}-{{ goss_real_version }}""
    dest: ""{{ degoss_goss_bin }}""
    force: true
  changed_when: degoss_changed_when

# deploy test files including the main and additional test files
- name: deploy test files
  copy: src={{ goss_file }} dest={{ degoss_test_root }}
  with_items: ""{{ [goss_file] + goss_addtl_files + goss_addtl_dirs }}""
  changed_when: degoss_changed_when

# run the tests
- name: run tests
  goss: executable={{ degoss_goss_bin }} path=""{{ goss_file }}"" format=""{{ goss_output_format }}""
  # never report failure, allowing us to clean up and then report failure later
  failed_when: false
  register: goss_output

# clean everything up
- name: clean
  file: path={{ degoss_tmp_root }} state=absent
  when: degoss_no_clean is undefined and not degoss_no_clean
  changed_when: degoss_changed_when

# our output callback plugin will catch the tag of this and format output accordingly
- name: report errors
  fail: msg=""Goss Tests Failed.""
  when: goss_output.goss_failed
  tags: [format_goss_output]"
f628157e430b7c7b4d6be934887a2f0eb43b3f8e,-signer {{ codesign_cert_filename }} -inkey {{ codesign_key_filename }} -certfile {{ cert_file_filename }} -outform DER \,"---
  - name: Gather list of source files
    command: ls {{ netbootxyz_root }}
    register: source_files

  - name: Create directories for signatures
    file:
      path: ""{{ item }}""
      state: directory
    with_items:
      - ""{{ sigs_dir }}""

  - name: Generate signatures for source files
    shell: |
      openssl cms -sign -binary -noattr -in {{ netbootxyz_root }}/{{ item }} \ 
      -signer {{ codesign_cert_location }} -inkey {{ codesign_key_location }} -certfile {{ cert_file_location }} -outform DER \
      -out {{ sigs_dir }}/{{ item }}.sig
    args:
      chdir: ""{{ cert_dir }}""
      warn: false
    with_items:
      - ""{{ source_files.stdout_lines }}""
    tags:
    - skip_ansible_lint"
352b360cf1bc2b11b8e0b2bd2b2535f211b4e724,"    src: ""{{ slurm_build_dir }}/etc/{{ item }}""
","---
- name: install build dependencies
  apt:
    name: ""{{ item }}""
  with_items: ""{{ slurm_build_deps }}""
  when: ansible_distribution == 'Ubuntu'

- name: remove slurm packages
  apt:
    name: ""{{ item }}""
    state: absent
  with_items:
    - slurm-wlm
    - slurmctld
    - slurmdbd
    - slurmd
  when: ansible_distribution == 'Ubuntu'

- name: install build dependencies
  yum:
    name: ""{{ item }}""
    state: present
  with_items: ""{{ slurm_build_deps }}""
  when: ansible_os_family == 'RedHat'

- name: remove slurm packages
  yum:
    name: ""{{ item }}""
    state: present
  with_items:
    - slurm
  when: ansible_os_family == 'RedHat'

- name: make build directory
  file:
    path: ""{{ slurm_build_dir }}""
    state: directory

- name: download source
  unarchive:
    src: ""{{ slurm_src_url }}""
    remote_src: yes
    dest: ""{{ slurm_build_dir }}""
    extra_opts:
      - --strip-components=1

- name: uninstall old version
  command: make -j uninstall
  args:
    chdir: ""{{ slurm_build_dir }}""
  ignore_errors: yes
  tags:
    - uninstall

- name: clean src dir
  command: make distclean
  args:
    chdir: ""{{ slurm_build_dir }}""
  ignore_errors: yes

- name: configure
  command: ""./configure --prefix={{ slurm_install_prefix }} --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build
  shell: ""make -j$(nproc) > build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build contrib
  shell: ""make -j$(nproc) contrib >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: install
  shell: ""make -j$(nproc) install >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: install contrib
  shell: ""make -j$(nproc) install-contrib >> build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}""

- name: build pam_slurm_adopt
  shell: ""make -j$(nproc) >> ../../build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}/contribs/pam_slurm_adopt""

- name: install pam_slurm_adopt
  shell: ""make -j$(nproc) install >> ../../build.log 2>&1""
  args:
    chdir: ""{{ slurm_build_dir }}/contribs/pam_slurm_adopt""

- name: copy service files
  copy:
    src: ""{{ item }}""
    dest: ""/etc/systemd/system/{{ item }}""
    remote_src: yes
  args:
    chdir: ""{{ slurm_build_dir }}""
  with_items:
    - slurmctld.service
    - slurmdbd.service
  when: is_controller

- name: copy service files
  copy:
    src: ""{{ item }}""
    dest: ""/etc/systemd/system/{{ item }}""
    remote_src: yes
  args:
    chdir: ""{{ slurm_build_dir }}""
  with_items:
    - slurmd.service
  when: is_compute

- name: restart munge
  service:
    name: munge
    state: restarted

- name: restart slurmd
  service:
    name: slurmd
    state: restarted
    enabled: yes
  when: is_compute

- name: restart slurmdbd
  service:
    name: slurmdbd
    state: restarted
    enabled: yes
  when: is_controller

- name: restart slurmctld
  service:
    name: slurmctld
    state: restarted
    enabled: yes
  when: is_controller"
96174a3f1cf671dfdc04d179722e51072345a9b0,"      - ""{{ ansible_os_family|lower }}.yml""","- name: gather os specific variables
  include_vars: ""{{ item }}""
  with_first_found:
    - files:
      - ""{{ ansible_distribution|lower }}-{{ ansible_distribution_major_version|lower }}.yml""
      - ""{{ ansible_distribution|lower }}.yml""
      paths:
      - ../vars
      skip: true
  tags: vars
"
14668fe5198abf2c14b40ac7d6505567031b1883,"  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'""
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}'""","- name: add options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Added' in check_installation_options.stdout""
  when: item.1.command == 'add'
  tags: [configuration, wordpress, wordpress-options]

- name: update options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }} {{ item.1.value }}""
  register: check_installation_options
  changed_when: ""'unchanged' not in check_installation_options.stdout""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'update'
  tags: [configuration, wordpress, wordpress-options]

- name: delete options
  command: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} {{ item.1.name }}""
  register: check_installation_options
  failed_when: False
  changed_when: ""'Could not delete' not in check_installation_options.stderr""
  with_subelements:
    - wordpress_installs
    - options
  when: item.1.command == 'delete'"
79e29502fb4e0ff1c30c0b5396bfa1b48fb84a8e,"  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1.name }} --activate""
  when: check_installation_plugins is defined and item.item.1.name and item.rc != 0
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}""
  when: item.1.name

- name: activate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin activate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-activate-plugin]

- name: deactivate (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin deactivate {{ item.1.name }}""
  register: check_activate_plugin
  changed_when: ""'Success: Plugin' in check_activate_plugin.stdout""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1.name and not item.1.activate | default(true)
  tags: [configuration, wordpress, wordpress-plugins, wordpress-deactivate-plugin]
  ","---
# tasks file for wordpress, plugins
- name: identify installation (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  register: check_installation_plugins
  failed_when: False
  changed_when: False
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-is-installed-plugin]

- name: install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1 }} --activate""
  with_items: check_installation_plugins.results
  when: check_installation_plugins is defined and item.item.1 and item.rc != 0
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin]

- name: check install (plugin)
  shell: ""wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}""
  with_subelements:
    - wordpress_installs
    - plugins
  when: item.1
  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin-check]"
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom"""
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_heapsize: ""128m"""
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ dashboard_min_heapsize }}""
springapp_max_heapsize: ""{{ dashboard_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m"""
6f79b938f075631908c18b51f3068a70ef59162b,"  user: name=authz-server home={{ authz_server_dir }} shell=/sbin/nologin
- include: install-release.yml
- include: install-snapshot.yml
- include: install-local.yml","---
- name: Create user
  user: name=authz-server home={{ authz_server_dir }}
  tags: authz-server

- name: Create logging directory
  file: path=/var/log/{{ java_app.name }} state=directory owner=authz-server group=authz-server mode=0755
  tags: authz-server

- name: Get the md5 sum of current jar
  stat: path={{ authz_server_dir }}/{{ authz_server_jar }}
  register: current_jar_stat
  tags: authz-server

- name: Create directory where we download application packages (e.g. jar/war files)
  file: path=~/app-downloads state=directory
  tags: authz-server

- name: Deploy | Download release
  get_url:
    url: ""{{ maven_repo }}/org/openconext/authz-server/{{ authz_server_version }}/authz-server-{{ authz_server_version }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp == '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Download snapshot
  get_url:
    url: ""{{ maven_snapshot_repo }}/org/openconext/authz-server/{{ authz_server_version }}-SNAPSHOT/authz-server-{{ authz_server_version }}-{{ authz_server_snapshot_timestamp }}.jar""
    dest: ""~/app-downloads/{{ authz_server_jar }}""
    force: yes
  when: authz_server_snapshot_timestamp != '' and authz_server_local_jar == ''
  tags: authz-server

- name: Deploy | Upload from local
  copy: src={{ authz_server_local_jar }} dest=~/app-downloads/{{ authz_server_jar }}
  when: authz_server_local_jar != ''
  tags: authz-server

- name: Get the md5 sum of the candidate
  stat: path=~/app-downloads/{{ authz_server_jar }}
  register: candidate_jar_stat
  tags: authz-server

- name: Replace the jar if it has changed
  shell: cp ~/app-downloads/{{ authz_server_jar }} {{ authz_server_dir }}/{{ authz_server_jar }}
  when: current_jar_stat.stat.exists == false or candidate_jar_stat.stat.md5 != current_jar_stat.stat.md5
  notify: restart authz-server
  tags: authz-server

- name: Copy logging config
  template: src=logback.xml.j2 dest={{ authz_server_dir }}/logback.xml owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy application config
  template: src=application.properties.j2 dest={{ authz_server_dir }}/application.properties owner=authz-server group=authz-server mode=0740
  notify: restart authz-server
  tags: authz-server

- name: Copy start script
  template: src=templates/spring-boot.j2 dest=/etc/init.d/{{ java_app.name }} mode=0755
  notify: restart authz-server
  tags: authz-server"
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom"""
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_heapsize: ""128m"""
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ authz_admin_min_heapsize }}""
springapp_max_heapsize: ""{{ authz_admin_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m"""
ba6c8beb212911feaab26718a2dd48b9a521e6a0,service: name=authz-admin state=restarted sleep=45,service: name=authz-admin state=restarted sleep=45`
05eb002134217b4cb50cd207e61a87d62d8af446,name: 'urn:schac:attribute-def:schacPersonalUniqueCode',"janus_service_registry_core:
    admin:
        name: Surfconext
        email: info@surfconext.nl
    auth: default-sp
    useridattr: NameID
#    auth: login-admin
#    useridattr: user
    user:
        autocreate: true
    dashboard:
        inbox:
            paginate_by: 20
    push:
        remote:
            test:
                name: ""OpenConext EngineBlock""
                url: ""https://{{ engine_api_janus_user }}:{{ engine_api_janus_password | vault }}@{{ engine_api_domain }}/api/connections""
    entity:
        prettyname: 'name:en'
        useblacklist: false
        usewhitelist: true
        validateEntityId: false
    enable:
        saml20-sp: true
        saml20-idp: true
        shib13-sp: false
        shib13-idp: false
    workflowstates:
        testaccepted:
            name:
                en: Test
                da: Test
                es: 'testaccepted - es'
            description:
                en: 'All test should be performed in this state'
                da: 'I denne tilstand skal al test foretages'
                es: 'Desc 1 es'
            abbr: 'TA'
        prodaccepted:
            name:
                en: Production
                da: Produktion
                es: 'prodaccepted - es'
            description:
                en: 'The connection is on the Production system'
                da: 'Forbindelsen er på Produktions systemet'
                es: 'Desc 5 es'
            textColor: green
            abbr: 'PA'
    workflowstate:
        default: testaccepted
    attributes:
        eduPersonTargetedID:
            name: 'urn:mace:dir:attribute-def:eduPersonTargetedID'
        eduPersonPrincipalName:
            name: 'urn:mace:dir:attribute-def:eduPersonPrincipalName'
        displayName:
            name: 'urn:mace:dir:attribute-def:displayName'
        'cn (common name)':
            name: 'urn:mace:dir:attribute-def:cn'
        givenName:
            name: 'urn:mace:dir:attribute-def:givenName'
        'sn (surname)':
            name: 'urn:mace:dir:attribute-def:sn'
        mail:
            name: 'urn:mace:dir:attribute-def:mail'
        schacHomeOrganization:
            name: 'urn:mace:terena.org:attribute-def:schacHomeOrganization'
        schacHomeOrganizationType:
            name: 'urn:mace:terena.org:attribute-def:schacHomeOrganizationType'
        schacPersonalUniqueCode:
            name: 'urn:mace:terena.org:attribute-def:schacPersonalUniqueCode'
        eduPersonAffiliation:
            name: 'urn:mace:dir:attribute-def:eduPersonAffiliation'
            specify_values: true
        eduPersonScopedAffiliation:
            name: 'urn:mace:dir:attribute-def:eduPersonScopedAffiliation'
            specify_values: true
        eduPersonEntitlement:
            name: 'urn:mace:dir:attribute-def:eduPersonEntitlement'
            specify_values: true
        isMemberOf:
            name: 'urn:mace:dir:attribute-def:isMemberOf'
            specify_values: true
        uid:
            name: 'urn:mace:dir:attribute-def:uid'
        preferredLanguage:
            name: 'urn:mace:dir:attribute-def:preferredLanguage'
        'nlEduPersonOrgUnit (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlEduPersonOrgUnit'
            specify_values: true
        'nlEduPersonStudyBranch (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlEduPersonStudyBranch'
            specify_values: true
        'nlStudielinkNummer (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlStudielinkNummer'
        'nlDigitalAuthorIdentifier (deprecated)':
            name: 'urn:mace:surffederatie.nl:attribute-def:nlDigitalAuthorIdentifier'
        'collabPersonId (deprecated)':
            name: 'urn:oid:1.3.6.1.4.1.1076.20.40.40.1'
    md:
        mapping:
            'UIInfo:Logo:0:height': 'logo:0:height'
            'UIInfo:Logo:0:width': 'logo:0:width'
            'UIInfo:Logo:0:url': 'logo:0:url'
            'UIInfo:Keywords:en': 'keywords:en'
            'UIInfo:Keywords:nl': 'keywords:nl'
            'UIInfo:Description:en': 'description:en'
            'UIInfo:Description:nl': 'description:nl'
    usertypes:
        - admin
        - readonly
        - technical
    messenger:
        external:
            mail:
                class: 'janus:SimpleMail'
                name: Mail
                option:
                    headers: ""MIME-Version: 1.0\r\nContent-type: text/html; charset=iso-8859-1\r\nFrom: JANUS <no-reply@example.org>\r\nReply-To: JANUS Admin <admin@example.org>\r\nX-Mailer: PHP/5.4.6-1ubuntu1.8""
    mdexport:
        default_options:
            entitiesDescriptorName: Federation
            maxCache: 86400
            maxDuration: 432000
            sign:
                enable: false
                privatekey: server.pem
                privatekey_pass: null
                certificate: server.crt
    encryption:
        enable: false
    access:
        createnewentity:
            workflow_states:
                all:
                    - all
                    - '-readonly'
        changeentityid:
            workflow_states:
                all:
                    - admin
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
            default: true
        allentities:
            workflow_states:
                all:
                    - admin
                    - readonly
        exportallentities:
            workflow_states:
                all:
                    - admin
        arpeditor:
            workflow_states:
                all:
                    - admin
        admintab:
            workflow_states:
                all:
                    - admin
        adminusertab:
            workflow_states:
                all:
                    - admin
        federationtab:
            workflow_states:
                all:
                    - ''
        showsubscriptions:
            workflow_states:
                all:
                    - ''
        addsubscriptions:
            workflow_states:
                all:
                    - ''
        editsubscriptions:
            workflow_states:
                all:
                    - ''
        deletesubscriptions:
            workflow_states:
                all:
                    - ''
        experimental:
            workflow_states:
                all:
                    - ''
        changeentitytype:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        exportmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        blockremoteentity:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        changeworkflow:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        changemanipulation:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        changearp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        editarp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        addarp:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        addmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        validatemetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        deletemetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        modifymetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        importmetadata:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
        entityhistory:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
        disableconsent:
            default: true
            workflow_states:
                testaccepted:
                    - all
                    - ' -readonly'
                prodaccepted:
                    - admin
    metadata_refresh_cron_tags:
        - daily
    validate_entity_certificate_cron_tags:
        - daily
    validate_entity_endpoints_cron_tags:
        - daily
    ca_bundle_file: /etc/pki/tls/certs/ca-bundle.crt
    metadatafields:
        saml20-idp:
            'name:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'displayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'description:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            certData:
                type: text
                default: ''
                default_allow: false
                required: true
            certData2:
                type: text
                default: ''
                default_allow: false
                required: false
            certData3:
                type: text
                default: ''
                default_allow: false
                required: false
            SingleLogoutService_Binding:
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            SingleLogoutService_Location:
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'contacts:#:contactType':
                type: select
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                select_values:
                    - technical
                    - support
                    - administrative
                    - billing
                    - other
                default: ''
                default_allow: false
            'contacts:#:givenName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:surName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:emailAddress':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                validate: isemail
                type: text
                default: ''
                default_allow: false
            'contacts:#:telephoneNumber':
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationDisplayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationURL:#':
                validate: isurl
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'logo:0:url':
                required: true
                default: 'https://.png'
                default_allow: false
                type: text
            'logo:0:width':
                required: true
                default: '120'
                type: text
                default_allow: true
            'logo:0:height':
                required: true
                default: '60'
                type: text
                default_allow: true
            redirect.sign:
                type: boolean
                required: true
                default: false
                default_allow: true
            'coin:publish_in_edugain':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:publish_in_edugain_date':
                validate: isdatetime
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:additional_logging':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:institution_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'SingleSignOnService:0:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: true
                default_allow: true
            'SingleSignOnService:0:Location':
                required: true
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'SingleSignOnService:1:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            'SingleSignOnService:1:Location':
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'coin:guest_qualifier':
                type: select
                required: true
                select_values:
                    - All
                    - Some
                    - None
                default: All
                default_allow: true
            'coin:schachomeorganization':
                type: text
                default: ''
                default_allow: false
                required: false
            NameIDFormat:
                type: select
                required: false
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default_allow: true
            'keywords:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'coin:disable_scoping':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:hidden':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'shibmd:scope:#:allowed':
                supported:
                    - 0
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                type: text
                default: ''
                default_allow: false
                required: false
            'shibmd:scope:#:regexp':
                type: boolean
                supported:
                    - 0
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                default: ''
                default_allow: false
                required: false
        saml20-sp:
            'name:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            'displayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'description:#':
                required: true
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
            certData:
                type: text
                default: ''
                default_allow: false
                required: false
            certData2:
                type: text
                default: ''
                default_allow: false
                required: false
            certData3:
                type: text
                default: ''
                default_allow: false
                required: false
            SingleLogoutService_Binding:
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                required: false
                default_allow: true
            SingleLogoutService_Location:
                required: false
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'contacts:#:contactType':
                type: select
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                select_values:
                    - technical
                    - support
                    - administrative
                    - billing
                    - other
                default: ''
                default_allow: false
            'contacts:#:givenName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:surName':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
            'contacts:#:emailAddress':
                required: true
                supported:
                    - 0
                    - 1
                    - 2
                validate: isemail
                type: text
                default: ''
                default_allow: false
            'contacts:#:telephoneNumber':
                supported:
                    - 0
                    - 1
                    - 2
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationDisplayName:#':
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'OrganizationURL:#':
                validate: isurl
                supported:
                    - en
                    - nl
                type: text
                default: ''
                default_allow: false
                required: false
            'logo:0:url':
                required: true
                default: 'https://.png'
                default_allow: false
                type: text
            'logo:0:width':
                required: true
                default: '120'
                type: text
                default_allow: true
            'logo:0:height':
                required: true
                default: '60'
                type: text
                default_allow: true
            redirect.sign:
                type: boolean
                required: true
                default: false
                default_allow: true
            'coin:publish_in_edugain':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:publish_in_edugain_date':
                validate: isdatetime
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:additional_logging':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:institution_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:trusted_proxy':
                type: boolean
                default: false
                default_allow: true
                required: false
            'AssertionConsumerService:0:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                required: true
                default_allow: true
            'AssertionConsumerService:#:Binding':
                type: select
                select_values:
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:SOAP'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:PAOS'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Artifact'
                    - 'urn:oasis:names:tc:SAML:2.0:bindings:URI'
                default: 'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST'
                required: false
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                default_allow: true
            'AssertionConsumerService:0:Location':
                required: true
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:#:Location':
                required: false
                validate: isurl
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:0:index':
                required: false
                type: text
                default: ''
                default_allow: false
            'AssertionConsumerService:#:index':
                required: false
                supported:
                    - 1
                    - 2
                    - 3
                    - 4
                    - 5
                    - 6
                    - 7
                    - 8
                    - 9
                type: text
                default: ''
                default_allow: false
            NameIDFormat:
                type: select
                required: true
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                default_allow: true
            'NameIDFormats:#':
                supported:
                    - 0
                    - 1
                    - 2
                type: select
                required: false
                select_values:
                    - 'urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:persistent'
                    - 'urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified'
                default: 'urn:oasis:names:tc:SAML:2.0:nameid-format:transient'
                default_allow: true
            'coin:no_consent_required':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:eula':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'url:#':
                required: true
                supported:
                    - en
                    - nl
                validate: isurl
                type: text
                default: ''
                default_allow: false
            'coin:gadgetbaseurl':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:secret':
                validate: lengteq20
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:two_legged_allowed':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consumer_key':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consumer_secret':
                validate: lengteq20
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:key_type':
                type: select
                select_values:
                    - HMAC_SHA1
                    - RSA_PRIVATE
                default: HMAC_SHA1
                default_allow: true
                required: false
            'coin:oauth:app_title':
                default: 'Application Title'
                default_allow: false
                type: text
                required: false
            'coin:oauth:app_description':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:app_thumbnail':
                validate: isurl
                default: 'https://www.surfnet.nl/thumb.png'
                default_allow: false
                type: text
                required: false
            'coin:oauth:app_icon':
                validate: isurl
                default: 'https://www.surfnet.nl/icon.gif'
                default_allow: false
                type: text
                required: false
            'coin:oauth:callback_url':
                validate: isurl
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:oauth:consent_not_required':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:ss:idp_visible_only':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:application_url':
                default: 'Application URL'
                default_allow: false
                type: text
                required: false
            'coin:implicit_vo_id':
                type: text
                default: ''
                default_allow: false
                required: false
            'coin:transparant_issuer':
                type: boolean
                default: ''
                default_allow: false
                required: false
            'coin:do_not_add_attribute_aliases':
                type: boolean
                default: false
                default_allow: true
                required: false
            'coin:display_unconnected_idps_wayf':
                type: boolean
                default: false
                default_allow: true
                required: false
    workflow:
        testaccepted:
            prodaccepted:
                role:
                    - admin
        prodaccepted:
            testaccepted:
                role:
                    - admin"
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_artifact_id: dashboard-server
springapp_artifact_type: jar
springapp_artifact_group_dir: org.openconext
springapp_version: ""{{ dashboard_server_version }}""
springapp_snapshot_timestamp: ""{{ dashboard_server_snapshot_timestamp }}""
springapp_dir: ""{{ dashboard_dir }}""
springapp_user: dashboard
springapp_service_name: dashboard
springapp_jar: ""{{ dashboard_jar }}""
springapp_tcpport: 9394
springapp_local_jar: ""{{ dashboard_local_jar }}""
springapp_heapsize: ""512m""
springapp_random_source: ""file:///dev/urandom"""
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_heapsize: ""128m"""
e7b19a669882460ece49442b9af51d5e07cb2e33,"springapp_min_heapsize: ""{{ attribute_mapper_min_heapsize }}""
springapp_max_heapsize: ""{{ attribute_mapper_max_heapsize }}""","springapp_artifact_id: attribute-mapper
springapp_artifact_type: jar
springapp_artifact_group_dir: /org/openconext
springapp_version: ""{{ attribute_mapper_version }}""
springapp_snapshot_timestamp: ""{{ attribute_mapper_snapshot_timestamp }}""
springapp_dir: ""{{ attribute_mapper_dir }}""
springapp_user: attribute-mapper
springapp_service_name: attribute-mapper
springapp_jar: ""{{ attribute_mapper_jar }}""
springapp_tcpport: 9292
springapp_local_jar: ""{{ attribute_mapper_local_jar }}""
springapp_debug: ""{{ attribute_mapper_debug }}""
springapp_debug_port: 1292
springapp_heapsize: ""128m"""
eab7d962e2bb70a8253c752bb6a2c8833d75c483,"- name: copy load engineblock sql
  template: src=files/{{ env_name }}/{{ engine_initial_sql }}.j2 dest=/tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''

- name: run load engineblock sql
  shell: mysql -u {{ engine_database_user }} -p{{ engine_database_password | vault }} -h localhost -D {{ engine_database_name }} < /tmp/{{ engine_initial_sql }}
  when: engine_initial_sql != ''
  register: mysql_output
  changed_when: False # script should be idempotent

  register: migrate_output
  changed_when: ""'no update needed' not in migrate_output.stderr""","
- name: Run EngineBlock migrations
  command: ./bin/migrate
  args:
    chdir: ""{{ engine_release_dir }}""
  changed_when: False # TODO How to check when migrate is up to date?"
e9a6268db9dce2545a928837b2c071453606c7c0,"    dest: ""{{ shared_path }}/""                  # Dest on host specified in {{ rsync_to }}
    set_remote_user: false
    ansible_ssh_extra_args: ""-l {{ unicorn_user }} -o ControlMaster=auto -o ControlPersist=60s -o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout=600""","---

- name: rsync asset files with ssh forwarding
  synchronize:
    src: ""{{ shared_path }}/{{ migrate_dir }}""  # Source on target host specified in --limit
    dest: ""{{ shared_path }}/{{ migrate_dir }}"" # Dest on host specified in {{ rsync_to }}
    mode: pull
    rsync_opts:
      - ""--chown={{ unicorn_user }}:{{ unicorn_user }}""
  vars:
    ansible_ssh_extra_args: '-o ForwardAgent=yes -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ConnectTimeout 600'
  loop:
    - assets
    - spree
    - system
  loop_control:
    loop_var: migrate_dir
  delegate_to: ""{{ groups[rsync_to][0] }}"""
fec255ee3c221596c4e1fd98efa8b0c78c1975fe,"# Remove empty rendered volume templates (e.g. inactivated for an env_type)
- name: Ignore empty rendered volume templates
  set_fact:
    path: ""{{ item }}""
  register: filtered_volumes
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined and lookup('template', item) | length > 1
  tags: deploy

- name: Update volume templates list for this app
  set_fact:
    volumes: ""{{ filtered_volumes | json_query('results[*].ansible_facts.path') | list }}""
  when: app.volumes is defined
  
  with_items: ""{{ volumes }}""","---
# Create volumes for an app

- name: Print app name
  debug: msg=""App name {{ app.name }}""
  tags: route

- name: Make sure application volumes exist
  openshift_raw:
    force: true
    definition: ""{{ lookup('template', item) | from_yaml }}""
    state: present
  with_items: ""{{ app.volumes }}""
  when: app.volumes is defined
  tags: volume"
0928e61bba2229bf7d25e8f2981fe24902b61723,when: not registries_vault.stat.exists,"---
# Create secrets to login to private docker registries

- name: Retrieve registries vault file
  stat:
    path: ""group_vars/customer/{{ customer }}/{{ env_type }}/secrets/registries.vault.yml""
  register: registries_vault

- block:
    - name: Check if registries vault exists
      debug:
        msg: ""No registries vault is associated with the project""
    - meta: end_play
  when: registries_vault.stat.exists

- name: Import registries variable
  include_vars:
    file: ""{{ registries_vault.stat.path }}""

- include_tasks: tasks/create_docker_registry_secret.yml
  loop: ""{{ registries }}""
  loop_control:
    loop_var: registry"
7032aee7df1c2c6e96795067285efa3b2a6de32e,"    line: ""OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""","---
- name: ""Setting Docker Facts""
  set_fact:
    docker_storage_block_device: ""{{ docker_storage_block_device | default(default_docker_storage_block_device) }}""
    docker_storage_volume_group: ""{{ docker_storage_volume_group | default(default_docker_storage_volume_group) }}""

- name: ""Install Docker""
  yum: 
    name: docker
    state: latest
  notify:
    - enable docker

- name: ""Confige Docker""
  lineinfile: 
    dest: /etc/sysconfig/docker 
    regexp: '^OPTIONS=.*$' 
    line: ""^OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'""

- name: ""Check for existing Docker Storage device""
  command: pvs
  register: pvs

- name: ""Set Docker Storage fact if already configured""
  set_fact:
    docker_storage_setup: true
  when: pvs.stdout | search('{{ docker_storage_block_device }}.*{{ docker_storage_volume_group }}')

- name: ""Configure Docker Storage Setup""
  template:
    src: docker-storage-setup.j2
    dest: /etc/sysconfig/docker-storage-setup
  when: docker_storage_setup is undefined
  
- name: ""Run Docker Storage Setup""
  command: docker-storage-setup
  when: docker_storage_setup is undefined
  notify:
  - restart docker

- name: ""Extend the Volume Group for Docker Storage""
  command: lvextend -l 90%VG /dev/{{ docker_storage_volume_group }}/docker-pool
  when: docker_storage_setup is undefined
  notify:
  - restart docker"
37731dd99593335ef775386d7df98fb1c21d9764,"                           | intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host_type_master'])) }}""
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host_type_node'])) }}""","  vars:
    env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersection(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersection(groups['tag_clusterid_' ~ cluster_id]
                                     | intersection( groups['tag_host_type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersection(groups['tag_clusterid_' ~ cluster_id]
                                   | intersection(groups['tag_host_type_node'])) }}""
    with_items: ""{{ env_cluster_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ master_hosts }}""
        region: ""{{ hostvars[item].ec2_region }}""
    with_items: ""{{ node_hosts }}"""
242d517958dffa59d43fd5f80e64775edd3b5016,"    env_cluster_hosts: ""{{ groups['tag_environment_' ~ env_id]
                           | intersect(groups['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups['tag_environment_' ~ env_id]
                      | intersect(groups['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups['tag_environment_' ~ env_id]
                    | intersect(groups['tag_clusterid_' ~ cluster_id]
                                | intersect(groups['tag_host-type_node'])) }}""","    env_cluster_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                           | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]) }}""
    master_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                      | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                  | intersect( groups.tags.['tag_host-type_master'])) }}""
    node_hosts: ""{{ groups.tags.['tag_environment_' ~ env_id]
                    | intersect(groups.tags.['tag_clusterid_' ~ cluster_id]
                                | intersect(groups.tags.['tag_host-type_node'])) }}"""
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""nova-api""
    - ""nova-compute""
    - ""nova-compute-ironic""
    - ""nova-conductor""
    - ""nova-consoleauth""
    - ""nova-novncproxy""
    - ""nova-scheduler""
    - ""nova-spicehtml5proxy""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists"
6f58b12d46cfbbba28b3eadf82b91d27bcea6856,"    - { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - { name: tempest, group: tempest }
    - [{ name: tempest, group: tempest }]
    - [{ name: tempest, group: tempest }]","---
- name: Ensuring the containers up
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- include: config.yml

- name: Check the configs
  command: docker exec {{ item.name }} /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

# NOTE(jeffrey4l): when config_strategy == 'COPY_ALWAYS'
# and container env['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE',
# just remove the container and start again
- name: Containers config strategy
  kolla_docker:
    name: ""{{ item.name }}""
    action: ""get_container_env""
  register: container_envs
  when: inventory_hostname in groups[item.group]
  with_items:
    - { name: tempest, group: tempest}

- name: Remove the containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE"" or item[1]['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""{{ item[0]['name'] }}""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'
    - item[1]['KOLLA_CONFIG_STRATEGY'] != 'COPY_ONCE'
    - item[2]['rc'] == 1
    - inventory_hostname in groups[item[0]['group']]
  with_together:
    - [{ name: tempest, group: tempest}]
    - container_envs.results
    - check_results.results"
84edfd09b67ef28e40b3ff24b96afa0ded5c532e,"    monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_id }}""","---
- name: Wait for Monasca Grafana to load
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/login""
    status_code: 200
  register: result
  until: result.get('status') == 200
  retries: 10
  delay: 2
  run_once: true

- name: Define Monasca Grafana control plane organisation name
  set_fact:
    monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_name }}""

- name: List Monasca Grafana organisations
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_orgs

- name: Create default control plane organisation if it doesn't exist
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body_format: json
    body:
      name: '{{ monasca_grafana_control_plane_org }}'
    force_basic_auth: true
  when: monasca_grafana_control_plane_org not in monasca_grafana_orgs.json|map(attribute='name')|unique

- name: Lookup Monasca Grafana control plane organisation ID
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/name/{{ monasca_grafana_control_plane_org }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_conf_org

- name: Add {{ monasca_grafana_admin_username }} user to control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/{{ monasca_grafana_conf_org.json.id }}/users'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
    force_basic_auth: true
    body_format: json
    status_code: 200, 409
  register: monasca_grafana_add_user_response
  run_once: True
  changed_when: monasca_grafana_add_user_response.status == 200
  failed_when: monasca_grafana_add_user_response.status not in [200, 409] or
               monasca_grafana_add_user_response.status == 409 and (""User is already"" not in  monasca_grafana_add_user_response.json.message|default(""""))

- name: Switch Monasca Grafana to the control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/user/using/{{ monasca_grafana_conf_org.json.id }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    force_basic_auth: true

- name: Enable Monasca Grafana datasource for control plane organisation
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/datasources""
    method: POST
    user: ""{{ monasca_grafana_admin_username }}""
    password: ""{{ monasca_grafana_admin_password }}""
    body: ""{{ item.value.data | to_json }}""
    body_format: json
    force_basic_auth: true
    status_code: 200, 409
  register: monasca_grafana_datasource_response
  run_once: True
  changed_when: monasca_grafana_datasource_response.status == 200
  failed_when: monasca_grafana_datasource_response.status not in [200, 409] or
               monasca_grafana_datasource_response.status == 409 and (""Data source with same name already exists"" not in  monasca_grafana_datasource_response.json.message|default(""""))
  with_dict: ""{{ monasca_grafana_data_sources }}""
  when: item.value.enabled | bool"
793a37e2ff5cc8ec3acb521c06872ae833fb8380,"- name: Restart keystone-ssh container
    service_name: ""keystone-ssh""
- name: Restart keystone-fernet container
    service_name: ""keystone-fernet""
- name: Restart keystone container
    service_name: ""keystone""","---
- name: Restart keystone container
  vars:
    service_name: ""keystone""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_conf: ""{{ keystone_confs.results|selectattr('item.key', 'equalto', service_name)|first  }}""
    policy_json: ""{{ keystone_policy_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_conf.changed | bool
      or keystone_domains.changed | bool
      or policy_json.changed | bool
      or keystone_wsgi.changed | bool
      or keystone_paste_ini.changed | bool
      or keystone_container.changed | bool

- name: Restart keystone-fernet container
  vars:
    service_name: ""keystone-fernet""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_conf: ""{{ keystone_confs.results|selectattr('item.key', 'equalto', service_name)|first  }}""
    policy_json: ""{{ keystone_policy_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_fernet_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_conf.changed | bool
      or policy_json.changed | bool
      or keystone_fernet_confs.changed | bool
      or keystone_fernet_container.changed | bool

- name: Restart keystone-ssh container
  vars:
    service_name: ""keystone-ssh""
    service: ""{{ keystone_services[service_name] }}""
    config_json: ""{{ keystone_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    keystone_ssh_container: ""{{ check_keystone_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes|reject('equalto', '')|list }}""
  when:
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or keystone_ssh_confs.changed | bool
      or keystone_ssh_container.changed | bool"
a1901d4264aa63b195ff1ccbcded5e63bf31c1ce,"      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-server'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      ( inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      and not enable_nova_fake | bool
      ) or
      ( inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
      and enable_nova_fake | bool
      )
      True if orchestration_engine == 'KUBERNETES' else
      ( inventory_hostname in groups['compute'] or
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      and not enable_nova_fake | bool
      ) or
      ( inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      and enable_nova_fake | bool
      )
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['compute']
      or (enable_manila | bool and inventory_hostname in groups['manila-share'])
      or inventory_hostname in groups['neutron-dhcp-agent']
      or inventory_hostname in groups['neutron-l3-agent']
      or inventory_hostname in groups['neutron-metadata-agent']
      or inventory_hostname in groups['neutron-vpnaas-agent']
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-dhcp-agent'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['neutron-l3-agent']
      or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-lbaas-agent'] }}""
      True if orchestration_engine == 'KUBERNETES' else
      inventory_hostname in groups['neutron-metadata-agent']
      or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
    host_in_groups: ""{{ True if orchestration_engine == 'KUBERNETES' else inventory_hostname in groups['neutron-vpnaas-agent'] }}""","neutron_services:
  openvswitch-db-server:
    container_name: ""openvswitch_db""
    image: ""{{ openvswitch_db_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    volumes:
      - ""{{ node_config_directory }}/openvswitch-db-server/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
      - ""openvswitch_db:/var/lib/openvswitch/""
  openvswitch-vswitchd:
    container_name: ""openvswitch_vswitchd""
    image: ""{{ openvswitch_vswitchd_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    privileged: True
    volumes:
      - ""{{ node_config_directory }}/openvswitch-vswitchd/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-server:
    container_name: ""neutron_server""
    image: ""{{ neutron_server_image_full }}""
    enabled: true
    group: ""neutron-server""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-server'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-server/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""kolla_logs:/var/log/kolla/""
  neutron-openvswitch-agent:
    container_name: ""neutron_openvswitch_agent""
    image: ""{{ neutron_openvswitch_agent_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'openvswitch' }}""
    privileged: True
    host_in_groups: >-
      {{
        ( inventory_hostname in groups['compute']
          or (enable_manila | bool and inventory_hostname in groups['manila-share'])
          or inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          or inventory_hostname in groups['neutron-vpnaas-agent']
          and not enable_nova_fake | bool
        ) or
        ( inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          or inventory_hostname in groups['neutron-vpnaas-agent']
          and enable_nova_fake | bool
        )
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-openvswitch-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-sfc-agent:
    container_name: ""neutron_sfc_agent""
    image: ""{{ neutron_sfc_agent_image_full }}""
    enabled: ""{{ neutron_plugin_agent == 'sfc' }}""
    privileged: True
    host_in_groups: >-
      {{
        ( inventory_hostname in groups['compute']
          or (enable_manila | bool and inventory_hostname in groups['manila-share'])
          or inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          and not enable_nova_fake | bool
        ) or
        ( inventory_hostname in groups['neutron-dhcp-agent']
          or inventory_hostname in groups['neutron-l3-agent']
          or inventory_hostname in groups['neutron-metadata-agent']
          and enable_nova_fake | bool
        )
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-sfc-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-linuxbridge-agent:
    container_name: ""neutron_linuxbridge_agent""
    image: ""{{  neutron_linuxbridge_agent_image_full }}""
    privileged: True
    enabled: ""{{ neutron_plugin_agent == 'linuxbridge' }}""
    environment:
      KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""
      NEUTRON_BRIDGE: ""br-ex""
      NEUTRON_INTERFACE: ""{{ neutron_external_interface }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['compute']
        or (enable_manila | bool and inventory_hostname in groups['manila-share'])
        or inventory_hostname in groups['neutron-dhcp-agent']
        or inventory_hostname in groups['neutron-l3-agent']
        or inventory_hostname in groups['neutron-metadata-agent']
        or inventory_hostname in groups['neutron-vpnaas-agent']
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-linuxbridge-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/lib/modules:/lib/modules:ro""
      - ""/run:/run:shared""
      - ""kolla_logs:/var/log/kolla/""
  neutron-dhcp-agent:
    container_name: ""neutron_dhcp_agent""
    image: ""{{ neutron_dhcp_agent_image_full }}""
    privileged: True
    enabled: True
    group: ""neutron-dhcp-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-dhcp-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-dhcp-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run/:/run/:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-l3-agent:
    container_name: ""neutron_l3_agent""
    image: ""{{ neutron_l3_agent_image_full }}""
    privileged: True
    enabled: ""{{ not enable_neutron_vpnaas | bool }}""
    host_in_groups: >-
      {{
        inventory_hostname in groups['neutron-l3-agent']
        or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-l3-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-lbaas-agent:
    container_name: ""neutron_lbaas_agent""
    image: ""{{ neutron_lbaas_agent_image_full }}""
    privileged: True
    enabled: ""{{ enable_neutron_lbaas | bool }}""
    group: ""neutron-lbaas-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-lbaas-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-lbaas-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-metadata-agent:
    container_name: ""neutron_metadata_agent""
    image: ""{{ neutron_metadata_agent_image_full }}""
    privileged: True
    enabled: true
    host_in_groups: >-
      {{
        inventory_hostname in groups['neutron-metadata-agent']
        or (inventory_hostname in groups['compute'] and enable_neutron_dvr | bool)
      }}
    volumes:
      - ""{{ node_config_directory }}/neutron-metadata-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run/netns/:/run/netns/:shared""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""
  neutron-vpnaas-agent:
    container_name: ""neutron_vpnaas_agent""
    image: ""{{ neutron_vpnaas_agent_image_full }}""
    privileged: True
    enabled: ""{{ enable_neutron_vpnaas | bool }}""
    group: ""neutron-vpnaas-agent""
    host_in_groups: ""{{ inventory_hostname in groups['neutron-vpnaas-agent'] }}""
    volumes:
      - ""{{ node_config_directory }}/neutron-vpnaas-agent/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""/run:/run:shared""
      - ""/run/netns/:/run/netns/:shared""
      - ""/lib/modules:/lib/modules:ro""
      - ""neutron_metadata_socket:/var/lib/neutron/kolla/""
      - ""kolla_logs:/var/log/kolla/""

"
790bf336d5e082c5c079bfb681f9509188eae72f,"  when: inventory_hostname in groups['glance-api']
  when: inventory_hostname in groups['glance-api']","- include: ceph.yml
  when: enable_ceph | bool

  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']
  when: inventory_hostname in groups['glance-api'] or
        inventory_hostname in groups['glance-registry']"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""heat-api""
    - ""heat-api-cfn""
    - ""heat-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists"
e05491bcd6d75cc204c3d15e3422ad37532c923a,- include: deploy.yml,"- name: Ensuring the containers up
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_state""
  register: container_state
  failed_when: container_state.Running == false

- include: config.yml

- name: Check the configs
  command: docker exec telegraf /usr/local/bin/kolla_set_configs --check
  changed_when: false
  failed_when: false
  register: check_results

# NOTE(jeffrey4l): when config_strategy == 'COPY_ALWAYS'
# and container env['KOLLA_CONFIG_STRATEGY'] == 'COPY_ONCE',
# just remove the container and start again
- name: Containers config strategy
  kolla_docker:
    name: ""telegraf""
    action: ""get_container_env""
  register: container_envs

- name: Remove the containers
  kolla_docker:
    name: ""telegraf""
    action: ""remove_container""
  register: remove_containers
  when:
    - config_strategy == ""COPY_ONCE""

- include: start.yml
  when: remove_containers.changed

- name: Restart containers
  kolla_docker:
    name: ""telegraf""
    action: ""restart_container""
  when:
    - config_strategy == 'COPY_ALWAYS'"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ watcher_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ watcher_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ watcher_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ trove_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ trove_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ trove_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
1b5353593c8f17594b7d39015b6093ef2ae65b14,"    update_cache: yes
    update_cache: yes","- name: Update yum cache
  yum:
    update_cache: yes
  become: True
  when: ansible_os_family == 'RedHat'

# Upgrading docker engine may cause containers to stop. Take a snapshot of the
# running containers prior to a potential upgrade of Docker.

- name: Check which containers are running
  command: docker ps -f 'status=running' -q
  become: true
  # If Docker is not installed this command may exit non-zero.
  failed_when: false
  changed_when: false
  register: running_containers

  register: apt_install_result
  register: yum_install_result

# If any packages were updated, and any containers were running, wait for the
# daemon to come up and start all previously running containers.

- block:
    - name: Wait for Docker to start
      command: docker info
      become: true
      changed_when: false
      register: result
      until: result is success
      retries: 6
      delay: 10

    - name: Ensure containers are running after Docker upgrade
      command: ""docker start {{ running_containers.stdout }}""
      become: true
  when:
    - install_result is changed
    - running_containers.rc == 0
    - running_containers.stdout != ''
  vars:
    install_result: ""{{ yum_install_result if ansible_os_family == 'RedHat' else apt_install_result }}""
  when:
    - ansible_distribution|lower == ""ubuntu""
    - item != """"
  when:
    - ansible_os_family == 'RedHat'
    - item != """""
5db9eab04223dd3e974e39490a84c506805b8644,"  vars:
    monasca_orgs_body:
      name: '{{ monasca_grafana_control_plane_org }}'
      body: ""{{ monasca_orgs_body | to_json }}""
  vars:
    monasca_user_body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
      body: ""{{ monasca_user_body | to_json }}""","---
- name: Wait for Monasca Grafana to load
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/login""
    status_code: 200
  register: result
  until: result.get('status') == 200
  retries: 10
  delay: 2
  run_once: true

- name: Define Monasca Grafana control plane organisation name
  set_fact:
    monasca_grafana_control_plane_org: ""{{ monasca_control_plane_project }}@{{ default_project_domain_name }}""

- name: List Monasca Grafana organisations
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_orgs

- name: Create default control plane organisation if it doesn't exist
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body_format: json
    body:
      name: '{{ monasca_grafana_control_plane_org }}'
    force_basic_auth: true
  when: monasca_grafana_control_plane_org not in monasca_grafana_orgs.json|map(attribute='name')|unique

- name: Lookup Monasca Grafana control plane organisation ID
  uri:
    method: GET
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/name/{{ monasca_grafana_control_plane_org }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    return_content: true
    force_basic_auth: true
  register: monasca_grafana_conf_org

- name: Add {{ monasca_grafana_admin_username }} user to control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/orgs/{{ monasca_grafana_conf_org.json.id }}/users'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    body:
      loginOrEmail: '{{ monasca_grafana_admin_username }}'
      role: Admin
    force_basic_auth: true
    body_format: json
    status_code: 200, 409
  register: monasca_grafana_add_user_response
  run_once: True
  changed_when: monasca_grafana_add_user_response.status == 200
  failed_when: monasca_grafana_add_user_response.status not in [200, 409] or
               monasca_grafana_add_user_response.status == 409 and (""User is already"" not in  monasca_grafana_add_user_response.json.message|default(""""))

- name: Switch Monasca Grafana to the control plane organisation
  uri:
    method: POST
    url: '{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/user/using/{{ monasca_grafana_conf_org.json.id }}'
    user: '{{ monasca_grafana_admin_username }}'
    password: '{{ monasca_grafana_admin_password }}'
    force_basic_auth: true

- name: Enable Monasca Grafana datasource for control plane organisation
  uri:
    url: ""{{ internal_protocol }}://{{ kolla_internal_vip_address }}:{{ monasca_grafana_server_port }}/api/datasources""
    method: POST
    user: ""{{ monasca_grafana_admin_username }}""
    password: ""{{ monasca_grafana_admin_password }}""
    body: ""{{ item.value.data | to_json }}""
    body_format: json
    force_basic_auth: true
    status_code: 200, 409
  register: monasca_grafana_datasource_response
  run_once: True
  changed_when: monasca_grafana_datasource_response.status == 200
  failed_when: monasca_grafana_datasource_response.status not in [200, 409] or
               monasca_grafana_datasource_response.status == 409 and (""Data source with same name already exists"" not in  monasca_grafana_datasource_response.json.message|default(""""))
  with_dict: ""{{ monasca_grafana_data_sources }}""
  when: item.value.enabled | bool"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""senlin-api""
    - ""senlin-engine""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ octavia_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ wather_database_name }}""
      password: ""{{ wather_database_password }}""
      host: ""%""
      priv: ""{{ wather_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ octavia_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ trove_database_name }}""
      password: ""{{ trove_database_password }}""
      host: ""%""
      priv: ""{{ trove_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
36c530f9ed1fa1b2a4ad2647b5c80c23e1659fad,"      name: ""{{ octavia_database_user }}""","  kolla_toolbox:
    module_name: mysql_db
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
  kolla_toolbox:
    module_name: mysql_user
    module_args:
      login_host: ""{{ database_address }}""
      login_port: ""{{ database_port }}""
      login_user: ""{{ database_user }}""
      login_password: ""{{ database_password }}""
      name: ""{{ octavia_database_name }}""
      password: ""{{ octavia_database_password }}""
      host: ""%""
      priv: ""{{ octavia_database_name }}.*:ALL""
      append_privs: ""yes""
  when: database.changed"
1e2a1a8fe12cd34abe904bbaca652884529f6f9a,,"---
- name: Restart rabbitmq container
  vars:
    service_name: ""rabbitmq""
    service: ""{{ rabbitmq_services[service_name] }}""
    config_json: ""{{ rabbitmq_config_jsons.results|selectattr('item.key', 'equalto', service_name)|first }}""
    rabbitmq_container: ""{{ check_rabbitmq_containers.results|selectattr('item.key', 'equalto', service_name)|first }}""
  kolla_docker:
    action: ""recreate_or_restart_container""
    common_options: ""{{ docker_common_options }}""
    name: ""{{ service.container_name }}""
    image: ""{{ service.image }}""
    volumes: ""{{ service.volumes }}""
  when:
    - action != ""config""
    - inventory_hostname in groups[service.group]
    - service.enabled | bool
    - config_json.changed | bool
      or rabbitmq_confs.changed | bool
      or rabbitmq_container.changed | bool"
192dcd1e1b9baf7f3177a694c2b1ce8bd62d9159,"# NOTE(mgoddard): Currently (just prior to Stein release), sending SIGHUP to
# nova compute services leaves them in a broken state in which they cannot
# start new instances. The following error is seen in the logs:
# ""In shutdown, no new events can be scheduled""
# To work around this we restart the nova-compute services.
# Speaking to the nova team, this seems to be an issue in oslo.service,
# with a fix proposed here: https://review.openstack.org/#/c/641907.
# This issue also seems to affect the proxy services, which exit non-zero in
# reponse to a SIGHUP, so restart those too.
# TODO(mgoddard): Remove this workaround when this bug has been fixed.
- name: Send SIGHUP to nova services
  become: true
  command: docker exec -t {{ item.value.container_name }} kill -1 1
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - not item.key.startswith('nova-compute')
    - not item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""
- name: Restart nova compute and proxy services
  become: true
  kolla_docker:
    action: restart_container
    common_options: ""{{ docker_common_options }}""
    name: ""{{ item.value.container_name }}""
  when:
    - inventory_hostname in groups[item.value.group]
    - item.value.enabled | bool
    - item.key in nova_services_require_nova_conf
    - item.key.startswith('nova-compute')
      or item.key.endswith('proxy')
  with_dict: ""{{ nova_services }}""","---
# This play calls sighup on every service to refresh upgrade levels
- name: Sighup nova-api
  command: docker exec -t nova_api kill -1 1
  when: inventory_hostname in groups['nova-api']

- name: Sighup nova-conductor
  command: docker exec -t nova_conductor kill -1 1
  when: inventory_hostname in groups['nova-conductor']

- name: Sighup nova-consoleauth
  command: docker exec -t nova_consoleauth kill -1 1
  when: inventory_hostname in groups['nova-consoleauth']

- name: Sighup nova-novncproxy
  command: docker exec -t nova_novncproxy kill -1 1
  when:
    - inventory_hostname in groups['nova-novncproxy']
    - nova_console == 'novnc'

- name: Sighup nova-scheduler
  command: docker exec -t nova_scheduler kill -1 1
  when: inventory_hostname in groups['nova-scheduler']

- name: Sighup nova-spicehtml5proxy
  command: docker exec -t nova_spicehtml5proxy kill -1 1
  when:
    - inventory_hostname in groups['nova-spicehtml5proxy']
    - nova_console == 'spice'

- name: Sighup nova-compute
  command: docker exec -t nova_compute kill -1 1
  when: inventory_hostname in groups['compute']"
6a737b19686c821c32778bb847c6548d51eef002,restart_policy: no,"---
- name: Running trove bootstrap container
  kolla_docker:
    action: ""start_container""
    common_options: ""{{ docker_common_options }}""
    detach: False
    environment:
      KOLLA_BOOTSTRAP:
      KOLLA_CONFIG_STRATEGY: ""{{ config_strategy }}""
    image: ""{{ trove_api_image_full }}""
    labels:
      BOOTSTRAP:
    name: ""bootstrap_trove""
    restart_policy: ""never""
    volumes:
      - ""{{ node_config_directory }}/trove-api/:{{ container_config_directory }}/:ro""
      - ""/etc/localtime:/etc/localtime:ro""
      - ""kolla_logs:/var/log/kolla/""
      - ""trove:/var/lib/trove/""
  run_once: True
  delegate_to: ""{{ groups['trove-api'][0] }}"""
00ce432d999615396677434a06fa2a74c390e6bf,"      - ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/aodh.conf""","---
- name: Ensuring config directories exist
  file:
    path: ""{{ node_config_directory }}/{{ item }}""
    state: ""directory""
    recurse: yes
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over config.json files for services
  template:
    src: ""{{ item }}.json.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/config.json""
  with_items:
    - ""aodh-api""
    - ""aodh-listener""
    - ""aodh-evaluator""
    - ""aodh-notifier""

- name: Copying over aodh.conf
  merge_configs:
    vars:
      service_name: ""{{ item }}""
    sources:
      - ""{{ role_path }}/templates/aodh.conf.j2""
      - ""{{ node_custom_config }}/global.conf""
      - ""{{ node_custom_config }}/database.conf""
      - ""{{ node_custom_config }}/messaging.conf""
      - ""{{ node_custom_config }}/aodh.conf""
      - ""{{ node_custom_config }}/aodh/{{ item }}.conf""
      - ""{{ node_custom_config }}/aodh/{{ inventory_hostname }}/{{ item }}.conf""
    dest: ""{{ node_config_directory }}/{{ item }}/aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier""

- name: Copying over wsgi-aodh files for services
  template:
    src: ""wsgi-aodh.conf.j2""
    dest: ""{{ node_config_directory }}/{{ item }}/wsgi-aodh.conf""
  with_items:
    - ""aodh-api""
    - ""aodh-evaluator""
    - ""aodh-listener""
    - ""aodh-notifier"""
9ae2ca63237f41589492e951258b680c9477b17e,"cloudkitty_processor_dimensions: ""{{ default_container_dimensions }}""","    dimensions: ""{{ cloudkitty_api_dimensions }}""
    dimensions: ""{{ cloudkitty_processor_dimensions }}""
cloudkitty_processor_diensions: ""{{ default_container_dimensions }}""
cloudkitty_api_dimensions: ""{{ default_container_dimensions }}""

"
508f3863cc31351d878277a42f90aa0c6b4e6671,"  command: docker exec -t kolla_toolbox /usr/bin/ansible localhost
  command: docker exec -t kolla_toolbox /usr/bin/ansible localhost","- name: Creating Nova-api database
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_db
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'""
  register: database_api
  changed_when: ""{{ database_api.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""

- name: Reading json from variable
  set_fact:
    database_api_created: ""{{ (database_api.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""

- name: Creating Nova-api database user and setting permissions
  command: docker exec -t kolla_ansible /usr/bin/ansible localhost
    -m mysql_user
    -a ""login_host='{{ database_address }}'
        login_user='{{ database_user }}'
        login_password='{{ database_password }}'
        name='{{ nova_api_database_name }}'
        password='{{ nova_api_database_password }}'
        host='%'
        priv='{{ nova_api_database_name }}.*:ALL'
        append_privs='yes'""
  register: database_api_user_create
  changed_when: ""{{ database_api_user_create.stdout.find('localhost | SUCCESS => ') != -1 and
                    (database_api_user_create.stdout.split('localhost | SUCCESS => ')[1]|from_json).changed }}""
  failed_when: database_api_user_create.stdout.split()[2] != 'SUCCESS'
  run_once: True
  delegate_to: ""{{ groups['nova-api'][0] }}""
"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/nova/policy.json""
  register: nova_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/nova/policy.json""
    dest: ""{{ node_config_directory }}/nova/policy.json""
  when:
    nova_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/heat/policy.json""
  register: heat_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/heat/policy.json""
    dest: ""{{ node_config_directory }}/heat/policy.json""
  when:
    heat_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/senlin/policy.json""
  register: senlin_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/senlin/policy.json""
    dest: ""{{ node_config_directory }}/senlin/policy.json""
  when:
    senlin_policy.stat.exists"
b5d1e4b457e6e1f396a39f975cd79e0beca65493,"    dest: ""{{ node_config_directory }}/{{ item }}/policy.json""
  with_items:
    - ""cinder-api""
    - ""cinder-backup""
    - ""cinder-scheduler""
    - ""cinder-volume""","
- name: Check if policies shall be overwritten
  local_action: stat path=""{{ node_custom_config }}/cinder/policy.json""
  register: cinder_policy

- name: Copying over existing policy.json
  template:
    src: ""{{ node_custom_config }}/cinder/policy.json""
    dest: ""{{ node_config_directory }}/cinder/policy.json""
  when:
    cinder_policy.stat.exists"
2346e5ced407e579fdfe4207500cffe4a69504e3,"    - { src: ""os_aggregate_storage.j2"", dest: ""/etc/logrotate.d/{{ rsyslog_client_log_rotate_file }}"" }","---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Stop rsyslog
  service:
    name: ""rsyslog""
    state: ""stopped""
  failed_when: false
  tags:
    - rsyslog-client-config

- name: Rsyslog Setup
  copy:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""
  with_items:
    - { src: ""50-default.conf"", dest: ""/etc/rsyslog.d/50-default.conf"" }
  tags:
    - rsyslog-client-config

- name: Find all log files
  shell: |
    find -L '{{ rsyslog_client_log_dir }}' -type f -name '*.log'
  register: log_files
  when: >
    rsyslog_client_log_dir is defined
  tags:
    - rsyslog-client-config

- name: Set fact for combined log files list and found logs
  set_fact:
    rsyslog_client_log_files: ""{{ log_files.stdout_lines | union(rsyslog_client_log_files) }}""
  when: >
    rsyslog_client_log_files is defined and
    log_files | success
  tags:
    - rsyslog-client-config

- name: Set fact for found log files list
  set_fact:
    rsyslog_client_log_files: ""{{ log_files.stdout_lines }}""
  when: >
    rsyslog_client_log_files is not defined and
    log_files | success
  tags:
    - rsyslog-client-config

- name: Check rsyslog_client_log_files is defined
  fail:
    msg: >
      There were no log files defined in `rsyslog_client_log_files`. Please use that
      variable to set the files that you want rsyslog to begin shipping logs for. This
      variable is a list that requires the full path to all log files. You can also set
      `rsyslog_client_log_dir` which will find all ""*.log"" files using the path
      provided.
  when: >
    rsyslog_client_log_files is not defined

- name: Write rsyslog config for found log files
  template:
    src: ""{{ item.src }}""
    dest: ""{{ item.dest }}""
    owner: ""root""
    group: ""root""
  with_items:
    - { src: ""99-rsyslog.conf.j2"", dest: ""/etc/rsyslog.d/{{ rsyslog_client_config_name }}"" }
    - { src: ""os_aggregate_storage.j2"", dest: ""/etc/logrotate.d/os_aggregate_storage"" }
    - { src: ""rsyslog.conf.j2"", dest: ""/etc/rsyslog.conf"" }
  tags:
    - rsyslog-client-config

- name: Start rsyslog
  service:
    name: ""rsyslog""
    state: ""started""
  tags:
    - rsyslog-client-config"
41f371dd110ee2deb7de3647bb99c82ae64dde1a,"    vhost: ""{{ notify_vhost }}""","---
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Usage:
#  To use this common task to create to create the user and vhost if
#  needed for the messaging backend configured for Notify communications.
#  To used this common task, the variables ""notify_user"", ""notify_password""
#  and ""notify_vhost"" must be set.

- name: Ensure Notify Rabbitmq vhost
  rabbitmq_vhost:
    name: ""{{ notify_vhost }}""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  tags:
    - common-rabbitmq
  when:
    - oslomsg_notify_transport == ""rabbit""

- name: Ensure Notify Rabbitmq user
  rabbitmq_user:
    user: ""{{ notify_user }}""
    password: ""{{ notify_password }}""
    vhost: ""{{ vhost }}""
    configure_priv: "".*""
    read_priv: "".*""
    write_priv: "".*""
    state: ""present""
  delegate_to: ""{{ groups[oslomsg_notify_host_group][0] }}""
  no_log: true
  tags:
    - common-rabbitmq
  when:
    - oslomsge_notify_transport == ""rabbit""
"
9186a6063af87df772fa5ecb183621d33653e97d,until: install_packages is success,"- name: Prepare MQ/DB services
  hosts: nova_conductor
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:

    - name: Configure rabbitmq vhost/user (nova)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure rabbitmq vhost/user (nova/telemetry)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure MySQL user (nova)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-placement)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api cell0)
      include: common-tasks/mysql-db-user.yml
      run_once: yes



- name: Install nova-conductor services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_conductor""
    nova_serial: ""{{ nova_conductor_serial | default(['1', '100%']) }}""



- name: Install nova-scheduler/nova-consoleauth services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_scheduler:nova_consoleauth:!nova_conductor""
    nova_serial: ""{{ nova_scheduler_serial | default(['1', '100%']) }}""



- name: Install nova API services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_os_compute:nova_api_placement:!nova_conductor:!nova_scheduler:!nova_consoleauth""
    nova_serial: ""{{ nova_api_serial | default(['1', '100%']) }}""



- name: Install nova console/metadata services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_metadata:nova_console:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement""
    nova_serial: ""{{ nova_console_serial | default(['1', '100%']) }}""



- name: Install nova compute
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_compute:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement:!nova_api_metadata:!nova_console""
    nova_serial: ""{{ nova_compute_serial | default('20%') }}""



# These facts are set against the deployment host to ensure that
# they are fast to access. This is done in preference to setting
# them against each target as the hostvars extraction will take
# a long time if executed against a large inventory.
- name: Refresh local facts after all software changes are made
  hosts: nova_all
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: ""!all""

    # This variable contains the values of the local fact set for the cinder
    # venv tag for all hosts in the 'cinder_all' host group.
    - name: Gather software version list
      set_fact:
        nova_all_software_versions: ""{{ (groups['nova_all'] | map('extract', hostvars, ['ansible_local', 'openstack_ansible', 'nova', 'venv_tag'])) | list }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean value which is True when
    # nova_all_software_versions contains a list of defined
    # values. If they are not defined, it means that not all
    # hosts have their software deployed yet.
    - name: Set software deployed fact
      set_fact:
        nova_all_software_deployed: ""{{ (nova_all_software_versions | select('defined')) | list == nova_all_software_versions }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean when all the values in
    # nova_all_software_versions are the same and the software
    # has been deployed to all hosts in the group.
    - name: Set software updated fact
      set_fact:
        nova_all_software_updated: ""{{ ((nova_all_software_versions | unique) | length == 1) and (nova_all_software_deployed | bool) }}""
      delegate_to: localhost
      run_once: yes


# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore skip the reload
# for that service.
- name: Reload all nova services to ensure new RPC object version is used
  hosts: ""nova_all:!nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_serial | default('100%') }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service reload
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova""
        service_action: ""reloaded""
        service_negate: ""{{ ['nova-placement-api.service'] + nova_service_negate | default([]) }}""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore restart it instead.
- name: Restart the nova placement API service to ensure new RPC object version is used
  hosts: ""nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_api_serial | default(['1', '100%']) }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service restart
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova-placement-api""
        service_action: ""restarted""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Perform online database migrations
  hosts: nova_conductor
  gather_facts: no
  user: root
  tasks:
    - name: Perform online data migrations
      command: ""{{ nova_bin }}/nova-manage db online_data_migrations""
      become: yes
      become_user: ""{{ nova_system_user_name }}""
      when:
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_online_data_migrations'] | bool""
      changed_when: false
      run_once: yes
      register: data_migrations

    - name: Disable the online migrations requirement
      ini_file:
        dest: ""/etc/ansible/facts.d/openstack_ansible.fact""
        section: nova
        option: need_online_data_migrations
        value: False
      when:
        - data_migrations | succeeded"
a8f828c5b31a762605b11183df88d9311eb28946,"              domain create {{ stack_user_domain_name }} --description ""Owns users and projects created by heat""","- name: Create heat domain
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              domain create {{ stack_domain }} --description ""Owns users and projects created by heat""
  ignore_errors: true

- name: Create heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              user create --domain {{ stack_user_domain_name }} --password {{ stack_domain_admin_password }} {{ stack_domain_admin }}
  ignore_errors: true

- name: Retrieve heat domain id
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
                    domain show {{ stack_user_domain_name }} | grep -oE -m 1 ""[0-9a-f]{32}""

- name: Assign admin role to heat domain admin user
  shell: |
    . /root/openrc
    openstack --os-identity-api-version=3 --os-auth-url={{ auth_identity_uri_v3 }} \
              role add --user {{ stack_domain_admin }} --domain {{ stack_user_domain_id }} admin"
773245e447e8d3abfdc09f2ebe5cf8466c5b54cb,"gnocchi_git_install_branch: b3b49c87e866475c149343fd77408bef259c5534 # HEAD of ""master"" as of 02.02.2017","gnocchi_git_install_branch: 908ca555f6a14547082e38e4f114926ec384a1bd # HEAD of ""master"" as of 24.01.2017"
9186a6063af87df772fa5ecb183621d33653e97d,- data_migrations  is succeeded,"- name: Prepare MQ/DB services
  hosts: nova_conductor
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:

    - name: Configure rabbitmq vhost/user (nova)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure rabbitmq vhost/user (nova/telemetry)
      include: common-tasks/rabbitmq-vhost-user.yml
      run_once: yes

    - name: Configure MySQL user (nova)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-placement)
      include: common-tasks/mysql-db-user.yml
      run_once: yes

    - name: Configure MySQL user (nova-api cell0)
      include: common-tasks/mysql-db-user.yml
      run_once: yes



- name: Install nova-conductor services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_conductor""
    nova_serial: ""{{ nova_conductor_serial | default(['1', '100%']) }}""



- name: Install nova-scheduler/nova-consoleauth services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_scheduler:nova_consoleauth:!nova_conductor""
    nova_serial: ""{{ nova_scheduler_serial | default(['1', '100%']) }}""



- name: Install nova API services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_os_compute:nova_api_placement:!nova_conductor:!nova_scheduler:!nova_consoleauth""
    nova_serial: ""{{ nova_api_serial | default(['1', '100%']) }}""



- name: Install nova console/metadata services
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_api_metadata:nova_console:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement""
    nova_serial: ""{{ nova_console_serial | default(['1', '100%']) }}""



- name: Install nova compute
  include: common-playbooks/nova.yml
  vars:
    nova_hosts: ""nova_compute:!nova_conductor:!nova_scheduler:!nova_consoleauth:!nova_api_os_compute:!nova_api_placement:!nova_api_metadata:!nova_console""
    nova_serial: ""{{ nova_compute_serial | default('20%') }}""



# These facts are set against the deployment host to ensure that
# they are fast to access. This is done in preference to setting
# them against each target as the hostvars extraction will take
# a long time if executed against a large inventory.
- name: Refresh local facts after all software changes are made
  hosts: nova_all
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: refresh local facts
      setup:
        filter: ansible_local
        gather_subset: ""!all""

    # This variable contains the values of the local fact set for the cinder
    # venv tag for all hosts in the 'cinder_all' host group.
    - name: Gather software version list
      set_fact:
        nova_all_software_versions: ""{{ (groups['nova_all'] | map('extract', hostvars, ['ansible_local', 'openstack_ansible', 'nova', 'venv_tag'])) | list }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean value which is True when
    # nova_all_software_versions contains a list of defined
    # values. If they are not defined, it means that not all
    # hosts have their software deployed yet.
    - name: Set software deployed fact
      set_fact:
        nova_all_software_deployed: ""{{ (nova_all_software_versions | select('defined')) | list == nova_all_software_versions }}""
      delegate_to: localhost
      run_once: yes

    # This variable outputs a boolean when all the values in
    # nova_all_software_versions are the same and the software
    # has been deployed to all hosts in the group.
    - name: Set software updated fact
      set_fact:
        nova_all_software_updated: ""{{ ((nova_all_software_versions | unique) | length == 1) and (nova_all_software_deployed | bool) }}""
      delegate_to: localhost
      run_once: yes


# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore skip the reload
# for that service.
- name: Reload all nova services to ensure new RPC object version is used
  hosts: ""nova_all:!nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_serial | default('100%') }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service reload
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova""
        service_action: ""reloaded""
        service_negate: ""{{ ['nova-placement-api.service'] + nova_service_negate | default([]) }}""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



# Note that the placement API service does not understand how to reload,
# so it fails when you try to make it do so. We therefore restart it instead.
- name: Restart the nova placement API service to ensure new RPC object version is used
  hosts: ""nova_api_placement""
  gather_facts: no
  serial: ""{{ nova_api_serial | default(['1', '100%']) }}""
  max_fail_percentage: 20
  user: root
  environment: ""{{ deployment_environment_variables | default({}) }}""
  tags:
    - nova
  tasks:
    - name: Execute nova service restart
      include: common-tasks/restart-service.yml
      vars:
        service_name: ""nova-placement-api""
        service_action: ""restarted""
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_service_restart'] | bool""



- name: Perform online database migrations
  hosts: nova_conductor
  gather_facts: no
  user: root
  tasks:
    - name: Perform online data migrations
      command: ""{{ nova_bin }}/nova-manage db online_data_migrations""
      become: yes
      become_user: ""{{ nova_system_user_name }}""
      when:
        - ""nova_all_software_updated | bool""
        - ""ansible_local['openstack_ansible']['nova']['need_online_data_migrations'] | bool""
      changed_when: false
      run_once: yes
      register: data_migrations

    - name: Disable the online migrations requirement
      ini_file:
        dest: ""/etc/ansible/facts.d/openstack_ansible.fact""
        section: nova
        option: need_online_data_migrations
        value: False
      when:
        - data_migrations | succeeded"
d8073fdb38a2038e59d9009a4f064bd2d8b3c902,"    pip_install_options_fact: ""{{ pip_install_options|default('') }} --constraint /opt/developer-pip-constraints.txt --constraint /opt/requirements/upper-constraints.txt""
  tags:
    - neutron-install
    - neutron-pip-packages

- name: Set pip_install_options_fact when not in developer mode
  set_fact:
    pip_install_options_fact: ""{{ pip_install_options|default('') }}""
  when:
    - not neutron_developer_mode | bool
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""
    extra_args: ""{{ pip_install_options_fact }}""","    extra_args: ""{{ pip_install_options|default('') }}"""
1ee6c091f2c9a0ee1da58d720cde3a4038d580b6,"- name: Create tmpfiles.d entry
    src: ""neutron-systemd-tmpfiles.j2""
    dest: ""/etc/tmpfiles.d/{{ program_name }}.conf""","---
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: Create neutron TEMP dirs
  file:
    path: ""{{ item.path }}/{{ program_name }}""
    state: directory
    owner: ""{{ system_user }}""
    group: ""{{ system_group }}""
    mode: ""2755""
  with_items:
    - { path: ""/var/run"" }
    - { path: ""/var/lock"" }

- name: Create tempfile.d entry
  template:
    src: ""neutron-systemd-tempfiles.j2""
    dest: ""/etc/tmpfiles.d/neutron.conf""
    mode: ""0644""
    owner: ""root""
    group: ""root""

- name: Place the systemd init script
  template:
    src: ""neutron-systemd-init.j2""
    dest: ""/etc/systemd/system/{{ program_name }}.service""
    mode: ""0644""
    owner: ""root""
    group: ""root""
  register: systemd_init

- name: Reload the systemd daemon
  command: ""systemctl daemon-reload""
  when: systemd_init | changed
  notify:
    - Restart neutron services"
1d4c3ad6ece7648d7329c5a6bfd42ed9b53b3022,"    - { path: ""{{ nova_lock_path }}"" }","---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

- name: create the system group
  group:
    name: ""{{ nova_system_group_name }}""
    state: ""present""
    system: ""yes""
  tags:
    - nova-group

- name: Create the nova system user
  user:
    name: ""{{ nova_system_user_name }}""
    group: ""{{ nova_system_group_name }}""
    comment: ""{{ nova_system_comment }}""
    shell: ""{{ nova_system_shell }}""
    system: ""yes""
    createhome: ""yes""
    home: ""{{ nova_system_home_folder }}""
  tags:
    - nova-user

- name: Create nova dir
  file:
    path: ""{{ item.path }}""
    state: directory
    owner: ""{{ item.owner|default(nova_system_user_name) }}""
    group: ""{{ item.group|default(nova_system_group_name) }}""
    mode: ""{{ item.mode|default('0755') }}""
  with_items:
    - { path: ""/etc/nova"" }
    - { path: ""/etc/nova/rootwrap.d"" }
    - { path: ""/etc/sudoers.d"", mode: ""0750"", owner: ""root"", group: ""root"" }
    - { path: ""/var/cache/nova"" }
    - { path: ""{{ nova_system_home_folder }}"" }
    - { path: ""{{ nova_system_home_folder }}/.ssh"", mode: ""0700"" }
    - { path: ""{{ nova_system_home_folder }}/cache/api"" }
    - { path: ""{{ nova_system_home_folder }}/instances"" }
    - { path: ""/var/lock/nova"" }
    - { path: ""/var/run/nova"" }
  tags:
    - nova-dirs

- name: Test for log directory or link
  shell: |
    if [ -h ""/var/log/nova""  ]; then
      chown -h {{ nova_system_user_name }}:{{ nova_system_group_name }} ""/var/log/nova""
      chown -R {{ nova_system_user_name }}:{{ nova_system_group_name }} ""$(readlink /var/log/nova)""
    else
      exit 1
    fi
  register: log_dir
  failed_when: false
  changed_when: log_dir.rc != 0
  tags:
    - nova-dirs
    - nova-logs

- name: Create nova log dir
  file:
    path: ""{{ item.path }}""
    state: directory
    owner: ""{{ item.owner|default(nova_system_user_name) }}""
    group: ""{{ item.group|default(nova_system_group_name) }}""
    mode: ""{{ item.mode|default('0755') }}""
  with_items:
    - { path: ""/var/log/nova"" }
  when: log_dir.rc != 0
  tags:
    - nova-dirs
    - nova-logs

- name: Drop sudoers file
  template:
    src: ""sudoers.j2""
    dest: ""/etc/sudoers.d/{{ nova_system_user_name }}_sudoers""
    mode: ""0440""
    owner: ""root""
    group: ""root""
  tags:
    - sudoers
    - nova-sudoers"
9f53e04687d5cab829e5dcf9c2049fddd5f44ed7,"  until: _stop  is success
  until: _start  is success","
  service:
    enabled: yes
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
- name: Stop services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""stopped""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _stop
  until: _stop | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
# Note (odyssey4me):
# The policy.json file is currently read continually by the services
# and is not only read on service start. We therefore cannot template
# directly to the file read by the service because the new policies
# may not be valid until the service restarts. This is particularly
# important during a major upgrade. We therefore only put the policy
# file in place after the service has been stopped.
#
- name: Copy new policy file into place
  copy:
    src: ""/etc/nova/policy.json-{{ nova_venv_tag }}""
    dest: ""/etc/nova/policy.json""
    owner: ""root""
    group: ""{{ nova_system_group_name }}""
    mode: ""0640""
    remote_src: yes
  listen: ""Restart nova services""
- name: Start services
  service:
    name: ""{{ item.service_name }}""
    enabled: yes
    state: ""started""
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  with_items: ""{{ filtered_nova_services }}""
  register: _start
  until: _start | success
  retries: 5
  delay: 2
  listen: ""Restart nova services""
- name: Wait for the nova-compute service to initialize
  command: ""openstack --os-cloud default compute service list --service nova-compute --format value --column Host""
  register: _compute_host_list
  retries: 10
  delay: 5
  until: ""ansible_nodename in _compute_host_list.stdout_lines""
  when:
    - ""'nova_compute' in group_names""
    - ""nova_discover_hosts_in_cells_interval | int < 1""
  listen: ""Restart nova services""
  service:
    daemon_reload: ""{{ (ansible_service_mgr == 'systemd') | ternary('yes', omit) }}""
  register: _restart
  until: _restart | success
  when:
    - inventory_hostname in groups['nova_api_placement']"
6be1f9dd1fdc04c7c610e14e3337af587e2717df,-mtime +{{ image_cache_expire_days|int - 1 }} {% endif %}|,"- name: Clean image cache directory
  shell: >-
    find {{ image_cache_dir }} -type f
    {% if not image_cache_dir_cleanup|bool %}
    -mtime +{{ image_cache_expire_days - 1 }} {% endif %}|
    xargs --no-run-if-empty -t rm -rf"
34248506123ba9c5348a20f7c575144434e6fbdb,name: 'subnode-{{ item.0 + 1 }}',"---

- when: inventory == 'all'
  block:
    #required for liberty based deployments
    - name: copy get-overcloud-nodes.py to undercloud
      template:
        src: 'get-overcloud-nodes.py.j2'
        dest: '{{ working_dir }}/get-overcloud-nodes.py'
        mode: 0755

    #required for liberty based deployments
    - name: fetch overcloud node names and IPs
      shell: >
          source {{ working_dir }}/stackrc;
          python {{ working_dir }}/get-overcloud-nodes.py
      register: registered_overcloud_nodes

    - name: list the overcloud nodes
      debug: var=registered_overcloud_nodes.stdout

    - name: fetch the undercloud ssh key
      fetch:
        src: '{{ working_dir }}/.ssh/id_rsa'
        dest: '{{ overcloud_key }}'
        flat: yes
        mode: 0400

    # add host to the ansible group formed from its type
    # novacompute nodes are added as compute for backwards compatibility
    - name: add overcloud node to ansible
      with_dict: '{{ registered_overcloud_nodes.stdout | default({}) }}'
      add_host:
        name: '{{ item.key }}'
        groups: ""overcloud,{{ item.key | regex_replace('overcloud-(?:nova)?([a-zA-Z0-9_]+)-[0-9]+$', '\\1') }}""
        ansible_host: '{{ item.key }}'
        ansible_fqdn: '{{ item.value }}'
        ansible_user: ""{{ overcloud_user | default('heat-admin') }}""
        ansible_private_key_file: ""{{ overcloud_key }}""
        ansible_ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'

- when: inventory == 'multinode'
  block:
    - name: Get subnodes
      command: cat /etc/nodepool/sub_nodes_private
      register: nodes

    - name: Add subnode to ansible inventory
      with_indexed_items: '{{ nodes.stdout_lines | default([]) }}'
      add_host:
        name: 'subnode-{{ item.0 + 2 }}'
        groups: ""overcloud""
        ansible_host: '{{ item.1 }}'
        ansible_fqdn: '{{ item.1 }}'
        ansible_user: ""{{ lookup('env','USER') }}""
        ansible_private_key_file: ""/etc/nodepool/id_rsa""

#required for regeneration of ssh.config.ansible
- name: set_fact for undercloud ip
  set_fact: undercloud_ip={{ hostvars['undercloud'].undercloud_ip }}
  when: hostvars['undercloud'] is defined and hostvars['undercloud'].undercloud_ip is defined

# Add the supplemental to the in-memory inventory.
- name: Add supplemental node vm to inventory
  add_host:
    name: supplemental
    groups: supplemental
    ansible_host: supplemental
    ansible_fqdn: supplemental
    ansible_user: '{{ supplemental_user }}'
    ansible_private_key_file: '{{ local_working_dir }}/id_rsa_supplemental'
    ansible_ssh_extra_args: '-F ""{{local_working_dir}}/ssh.config.ansible""'
    supplemental_node_ip: ""{{ supplemental_node_ip }}""
  when: supplemental_node_ip is defined

- name: set_fact for supplemental ip
  set_fact: supplemental_node_ip={{ hostvars['supplemental'].supplemental_node_ip }}
  when: hostvars['supplemental'] is defined and hostvars['supplemental'].supplemental_node_ip is defined

#readd the undercloud to reset the ansible_ssh parameters set in quickstart
- name: Add undercloud vm to inventory
  add_host:
    name: undercloud
    groups: undercloud
    ansible_host: undercloud
    ansible_fqdn: undercloud
    ansible_user: '{{ undercloud_user }}'
    ansible_private_key_file: '{{ undercloud_key }}'
    ansible_ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.local.ansible""'
    undercloud_ip: ""{{ undercloud_ip }}""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is not defined and undercloud_ip is defined

#required for regeneration of ssh.config.ansible
- name: set undercloud ssh proxy command
  set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ hostvars[groups['virthost'][0]].ansible_private_key_file }}
      {{ ssh_user }}@{{ hostvars[groups['virthost'][0]].ansible_host }}
      -W {{ undercloud_ip }}:22""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is defined and undercloud_ip is defined

#required for regeneration of ssh.config.ansible
- name: set undercloud ssh proxy command
  set_fact: undercloud_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ hostvars['localhost'].ansible_user_dir }}/.quickstart/id_rsa_virt_power
      {{ ssh_user }}@{{ hostvars['localhost'].ansible_default_ipv4.address }}
      -W {{ undercloud_ip }}:22""
  when: hostvars[groups['virthost'][0]].ansible_private_key_file is not defined and undercloud_ip is defined

- name: set supplemental ssh proxy command
  set_fact: supplemental_ssh_proxy_command=""ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no
      -o ConnectTimeout=60 -i {{ local_working_dir }}/id_rsa_virt_power
      {{ ssh_user }}@{{ hostvars[groups['virthost'][0]].ansible_host }}
      -W {{ supplemental_node_ip }}:22""
  when: supplemental_node_ip is defined

- name: create inventory from template
  delegate_to: localhost
  template:
    src: 'inventory.j2'
    dest: '{{ local_working_dir }}/hosts'

- name: regenerate ssh config
  delegate_to: localhost
  template:
    src: 'ssh_config.j2'
    dest: '{{ local_working_dir }}/ssh.config.ansible'
    mode: 0644
  when: undercloud_ip is defined

- name: regenerate ssh config for ssh connections from the virthost
  delegate_to: localhost
  template:
    src: 'ssh_config_localhost.j2'
    dest: '{{ local_working_dir }}/ssh.config.local.ansible'
    mode: 0644
  when: undercloud_ip is defined

# just setup the ssh.config.ansible and hosts file for the virthost
- name: check for existence of identity key
  delegate_to: localhost
  stat: path=""{{ local_working_dir }}/id_rsa_virt_power""
  when: undercloud_ip is not defined
  register: result_stat_id_rsa_virt_power

- name: set fact used in ssh_config_no_undercloud.j2 to determine if IdentityFile should be included
  set_fact:
    id_rsa_virt_power_exists: true
  when: undercloud_ip is not defined and result_stat_id_rsa_virt_power.stat.exists == True

- name: regenerate ssh config, if no undercloud has been launched.
  delegate_to: localhost
  template:
    src: 'ssh_config_no_undercloud.j2'
    dest: '{{ local_working_dir }}/ssh.config.ansible'
    mode: 0644
  when: undercloud_ip is not defined"
0cec56d80b5b75e89b4c02befc8a10f4e69e1f0a,"    - teardown-environment
    - teardown-provision","
# This teardown role will destroy all vms defined in the overcloud_nodes
# key, and the undercloud
- name:  Teardown undercloud and overcloud vms
  hosts: virthost
  gather_facts: yes
  roles:
    - libvirt/teardown
  tags:
    - teardown-all
    - teardown-virthost
    - teardown-nodes

# This teardown role will destroy libvirt networks
- name: Tear down environment
  hosts: virthost
  roles:
    - environment/teardown
  tags:
    - teardown-all
    - teardown-virthost

# Finally, we conditionally remove basic setup (users,
# groups, directories)to start from scratch
- name: Teardown user setup on virt host
  hosts: virthost
  roles:
    - provision/teardown
  tags:
    - teardown-all
"
70f3ba101b6acc94940aae0a1c615c01754d6ba2,"- when: networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length > 0","# If virtualport_type is defined for any networks, include OVS dependencies
- when: ""{{ networks|selectattr('virtualport_type', 'defined')|map(attribute='name')|list|length is greaterthan 0}}""
  block:

  # Install OVS dependencies
  - name: Install OVS dependencies
    include_role:
      name: 'parts/ovs'

  # Create any OVS Bridges that have been defined
  - name: Create OVS Bridges
    openvswitch_bridge:
      bridge: ""{{ item.bridge }}""
      state: present
    when: item.virtualport_type is defined and item.virtualport_type == ""openvswitch""
    with_items: ""{{ networks }}""
    become: true
"
355859beafaaf7c87dcb405e8543f3ba0895c8c8,"  when: undercloud_install_cli_options == '--use-heat' or containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_t_h_t_undercloud }}""
        dest: ""./hieradata-overrides-t-h-t-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-t-h-t-undercloud.yaml""
  when:
    - not undercloud_install_cli_options
    - not containerized_undercloud|bool
  block:
    - name: Generate undercloud hieradata overrides file from template
      template:
        src: ""{{ hieradata_override_file_classic_undercloud }}""
        dest: ""./hieradata-overrides-classic-undercloud.yaml""
        mode: 0600

    - name: Set fact for undercloud hieradata overrides file
      set_fact:
        undercloud_hieradata_override: ""./hieradata-overrides-classic-undercloud.yaml""

- name: Create undercloud configuration
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""","# Creat the scripts that will be used to deploy the undercloud
# environment.
- name: Create undercloud configuration
  template:
    src: ""{{ undercloud_config_file }}""
    dest: ""./undercloud.conf""
    mode: 0600

- name: Create undercloud hieradata overrides
  template:
    src: ""{{ undercloud_hieradata_override_file }}""
    dest: ""./quickstart-hieradata-overrides.yaml""
    mode: 0600

- name: Create undercloud install script
  template:
    src: ""{{ undercloud_install_script }}""
    dest: ""{{ working_dir }}/undercloud-install.sh""
    mode: 0755"
7a82d31a1d95d253be6bc69a7f52045552f04726,nodepool_cirros_checksum: md5:d56d54f110654dfd29b0e8ed56e6cda8,"nodepool_cirros_url: http://download.cirros-cloud.net/0.3.6/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_dest: /opt/cache/files/cirros-0.3.6-x86_64-disk.img
nodepool_cirros_checksum: md5:a30f156a83471adf89ebb70dc96d0c82"
a4b1340a1c8e61ca6a864a908fc2083b68918a7b,"     tempest_init: ""tempest init {{ tempest_dir }}""
     tempestconf: ""/usr/bin/discover-tempest-config""

- name: Create /var/log/containers/tempest
  file:
     path: /var/log/containers/tempest
     state: directory
  become: true

- name: Create /var/lib/tempestdata
  file:
     path: /var/lib/tempestdata
     state: directory
  become: true","      tempest_init: ""tempest init {{ tempest_dir }}""
      tempestconf: ""/usr/bin/discover-tempest-config"""
c494242b92e42906b9a6b1dabf8ed2d002749c4b,"undercloud_key: ""{{ local_working_dir }}/id_rsa_undercloud""","---
# defaults for all ovb-stack related tasks
local_working_dir: ""{{ lookup('env', 'HOME') }}/.quickstart""
working_dir: /home/stack

release: mitaka

node:
    prefix:
        - ""{{ 1000 |random }}""
        - ""{{ lookup('env', 'USER') }}""
        - ""{{ lookup('env', 'BUILD_NUMBER') }}""
tmp:
    node_prefix: '{{ node.prefix | reject(""none"") | join(""-"") }}-'

os_username: admin
os_password: password
os_tenant_name: admin
os_auth_url: 'http://10.0.1.10:5000/v2.0'
cloud_name: qeos7

stack_name: 'oooq-{{ prefix }}stack'
rc_file: /home/stack/overcloudrc
node_name: 'undercloud'
ssh_extra_args: '-F ""{{ local_working_dir }}/ssh.config.ansible""'
undercoud_key: ""{{ local_working_dir }}/id_rsa_undercloud""
node_groups:
    - 'undercloud'
    - 'tester'
templates_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal/templates""
ovb_dir: ""{{ local_working_dir }}/openstack-virtual-baremetal""
heat_template: ""{{ templates_dir }}/quintupleo.yaml""
environment_list:
    - ""{{ templates_dir }}/resource-registry.yaml""
    - ""{{ local_working_dir }}/{{ prefix }}env.yaml""

existing_key_location: '{{ local_working_dir }}'
remove_image_from_host_cloud: false

bmc_flavor: m1.medium
bmc_image: 'bmc-base'
bmc_prefix: '{{ prefix }}bmc'

baremetal_flavor: m1.large
baremetal_image: 'ipxe-boot'
baremetal_prefix: '{{ prefix }}baremetal'

key_name: '{{ prefix }}key'
private_net: '{{ prefix }}private'
node_count: 2
public_net: '{{ prefix }}public'
provision_net: '{{ prefix }}provision'

# QuintupleO-specific params ignored by virtual-baremetal.yaml
undercloud_name: '{{ prefix }}undercloud'
undercloud_image: '{{ prefix }}undercloud.qcow2'
undercloud_flavor: m1.xlarge
external_net:  '10.2.1.0/22'

network_isolation_type: multi-nic

setup_undercloud_connectivity_log: ""{{ working_dir }}/setup_undercloud_connectivity.log""

mtu: 1350
mtu_interface:
  - eth1
pvt_nameserver: 8.8.8.8

external_interface: eth2
external_interface_ip: 10.0.0.1
external_interface_netmask: 255.255.255.0

registered_releases:
  - mitaka
  - newton
  - master
  - rhos-9
"
3021af5ebfb52a7df055094178fdc2e788b8f6c8,"  fail: msg=""Overcloud nova list does not show expected number of {{ node_to_scale }} services""
  when: post_scale_node_count.stdout|int != {{ final_scale_value|int }}","    source {{ working_dir }}/stackrc;
    {{ working_dir }}/scale-deployment.sh &> overcloud_deployment_scale_console.log;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    source {{ working_dir }}/stackrc;
    nova list | grep {{ node_to_scale }} | cut -f2- -d':' | wc -l
  fail: msg=Overcloud nova list does not show expected number of {{ node_to_scale }} services
  when: post_scale_node_count.stdout != {{ final_scale_value }}"
9991eb2d64961535494ab6b38f9bb8e8dbaad12d,"    pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8"
9991eb2d64961535494ab6b38f9bb8e8dbaad12d,"    pkg_extras: python-setuptools haproxy PyYAML
    pkg_extras: python*-setuptools haproxy PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8"
9991eb2d64961535494ab6b38f9bb8e8dbaad12d,"    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    pkg_extras: python-setuptools haproxy
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    pkg_extras: python*-setuptools haproxy
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: true

verifier:
  name: testinfra
  lint:
    name: flake8"
9991eb2d64961535494ab6b38f9bb8e8dbaad12d,"    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby PyYAML
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby PyYAML","---
driver:
  name: docker

log: true

platforms:
  - name: centos7
    hostname: centos7
    image: centos:7
    override_command: True
    command: python -m SimpleHTTPServer 8787
    pkg_extras: python-setuptools python-enum34 python-netaddr epel-release ruby
    easy_install:
      - pip
    environment: &env
      http_proxy: ""{{ lookup('env', 'http_proxy') }}""
      https_proxy: ""{{ lookup('env', 'https_proxy') }}""

  - name: fedora28
    hostname: fedora28
    image: fedora:28
    override_command: True
    command: python3 -m http.server 8787
    pkg_extras: python*-setuptools python*-enum python*-netaddr ruby
    environment:
      <<: *env

provisioner:
  name: ansible
  log: true
  env:
    ANSIBLE_STDOUT_CALLBACK: yaml
    ANSIBLE_LIBRARY: ""../../../../library""

scenario:
  test_sequence:
    - destroy
    - create
    - prepare
    - converge
    - verify
    - destroy

lint:
  enabled: false

verifier:
  name: testinfra
  lint:
    name: flake8"
7093b73abba57c85f8adf2bac3b5c619e8d8d3d5,"    name: ""postgresql@{{ postgres_version }}-main.service""","
- name: enable postgres
  become: yes
  service:
    name: ""postgresql@{{ postgresql_version }}-main.service""
    state: started
    enabled: yes
  tags:
    - postgres-enable"
17720e1d5879d17e1e8e699da648ef203b4c8f83,"  when: checkgiinstall.stdout != ""1"" and oracle_install_version_gi == item.version and oracle_sw_copy|bool and oracle_sw_unpack|bool
  when: checkgiinstall.stdout != ""1"" and  oracle_install_version_gi == item.version and not oracle_sw_copy|bool and oracle_sw_unpack|bool","---

- name: Extract files to stage-area (GI)
  unarchive: src={{ oracle_stage }}/{{ item.filename }}  dest={{ oracle_stage }}/{{ item.version }} copy=no
  with_items: ""{{oracle_sw_image_gi}}""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - oragridswunpack
  when: checkgiinstall.stdout != ""1"" and ""{{ oracle_install_version_gi }}"" == ""{{ item.version }}"" and oracle_sw_copy|bool and oracle_sw_unpack|bool

- name: Extract files to stage-area (GI) (from remote location)
  unarchive: src={{ oracle_stage_remote }}/{{ item.filename }}  dest={{ oracle_stage }}/{{ item.version }} copy=no
  with_items: ""{{oracle_sw_image_gi}}""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
  - oragridswunpack
  when: checkgiinstall.stdout != ""1"" and ""{{ oracle_install_version_gi }}"" == ""{{ item.version }}"" and not oracle_sw_copy|bool and oracle_sw_unpack|bool

- name: Install cvuqdisk rpm
  yum: name=""{{ oracle_stage_install }}/{{ oracle_install_version_gi }}/grid/rpm/{{ cvuqdisk_rpm }}"" state=present
  when: configure_cluster
  tags: cvuqdisk
  ignore_errors: true

- name: Setup response file for install (GI)
  template: src=grid-install.rsp.{{ oracle_install_version_gi }}.j2 dest={{ oracle_rsp_stage }}/{{ oracle_grid_responsefile }} owner=""{{ grid_install_user }}"" group={{ oracle_group }} mode=775 backup=yes
  with_items: ""{{asm_diskgroups}}""
  tags:
    - responsefilegi
  when: master_node and checkgiinstall.stdout != ""1"" and item.diskgroup == oracle_asm_init_dg

- name: Install Grid Infrastructure
  shell: ""{{ oracle_stage_install }}/{{ oracle_install_version_gi }}/grid/runInstaller -responseFile {{ oracle_rsp_stage }}/{{ oracle_grid_responsefile }} -waitforcompletion -ignorePrereq -ignoreSysPrereqs -showProgress -silent""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - oragridinstall
  when: master_node and checkgiinstall.stdout != ""1"" #and oracle_sw_unpack
  register: giinstall

- debug: var=giinstall.stdout_lines
  when: master_node and checkgiinstall.stdout != ""1""

- name: Run oraInstroot script after installation
  shell: ""{{ oracle_inventory_loc }}/orainstRoot.sh""
  sudo: yes
  tags:
    - runroot
  when: checkgiinstall.stdout != ""1""

- name: Run root script after installation (Master Node)
  shell: ""{{ oracle_home_gi }}/root.sh""
  sudo: yes
  tags:
    - runroot
  when: master_node and checkgiinstall.stdout != ""1""
  register: rootmaster

- debug: var=rootmaster.stdout_lines
  when: master_node and checkgiinstall.stdout != ""1""

- name: Run root script after installation (Other Nodes)
  shell: ""sleep {{ item.0 * 60 }}; {{ oracle_home_gi }}/root.sh""
  sudo: yes
  with_indexed_items: ""{{groups[hostgroup]}}""
  tags:
    - runroot
  when: not master_node and checkgiinstall.stdout != ""1"" and inventory_hostname == item.1
  register: rootother

- debug: var=rootother.stdout_lines
  when: not master_node and checkgiinstall.stdout != ""1""

- name: Setup response file for configToolAllCommands
  template: src=configtoolallcommands.rsp.{{ oracle_install_version_gi }}.j2 dest={{ oracle_rsp_stage }}/configtoolallcommands.rsp owner=""{{ grid_install_user }}"" group={{ oracle_group }} mode=755 backup=yes
  tags:
    - responsefileconfigtool
  when: master_node and run_configtoolallcommand  and checkgiinstall.stdout != ""1""

- name: Run configToolAllCommands
  shell: ""{{ oracle_home_gi }}/cfgtoollogs/configToolAllCommands RESPONSE_FILE={{ oracle_rsp_stage }}/configtoolallcommands.rsp""
  sudo: yes
  sudo_user: ""{{ grid_install_user }}""
  tags:
    - runconfigtool
  when: master_node and run_configtoolallcommand and checkgiinstall.stdout != ""1""
  ignore_errors: true
  register: configtool"
2579628514c026234e7aaa1419be95ebf7edf473,"          grants={{ item.1.grants |default(omit)  }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, role: {{ item.1.name | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants |default(omit) }}
          object_privs={{ item.1.object_privs | default (omit) }}
  when: oracle_databases is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ db_service_name }}, schema: {{ item.1.schema }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""
          grants={{ item.1.grants | default (omit) }}
          object_privs={{ item.1.object_privs |default (omit)}}
  when: oracle_pdbs is defined and item.0 is defined and item.0.state|lower == 'present' and (item.1.grants is defined or item.1.object_privs is defined)
      label: ""port: {{ listener_port_template }}, service: {{ item.0.pdb_name }}, schema: {{ item.1.schema | default('none') }}, grants: {{ item.1.grants | default(omit) }}, state: {{ item.1.state }}""","---
# tasks file for manage-db-users
- name: Manage grants (cdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.oracle_db_name }}
          user={{ db_user }}
          password={{ db_password_cdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_databases }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants

- name: Manage grants (pdb)
  oracle_grants:
          schema={{ item.1.schema }}
          state={{ item.1.state }}
          grants={{ item.1.grants }}
          hostname={{ inventory_hostname }}
          service_name={{ item.0.pdb_name }}
          user={{ db_user }}
          password={{ db_password_pdb}}
          mode={{ db_mode }}
  with_subelements:
      - ""{{ oracle_pdbs }}""
      - users
  environment: ""{{oracle_env}}""
  when: item.0 is defined and item.0.state|lower == 'present' and item.1.grants is defined
  run_once: ""{{ configure_cluster }}""
  become_user: ""{{ oracle_user }}""
  tags: users,grants"
1a4c6f4bb0da458c0241762964458f4834e62bdd,"      # TODO: Change when Ansible 2.4 is released:
      # ovirt_scheduling_policies_facts:
      ovirt_scheduling_policies_facts_internal_24:
      # TODO: Change when Ansible 2.5 is released:
      # ovirt_api_facts:
      ovirt_api_facts_internal_25:
      # TODO: Change when Ansible 2.4 is released:
      # ovirt_clusters:
      ovirt_clusters_internal_24:
        # TODO: Change when Ansible 2.4 is released:
        # ovirt_clusters:
        ovirt_clusters_internal_24:","---
- block:
  - name: Login to oVirt
    ovirt_auth:
      url: ""{{ engine_url }}""
      username: ""{{ engine_user }}""
      password: ""{{ engine_password }}""
      ca_file: ""{{ engine_cafile | default(omit) }}""
      insecure: ""{{ engine_insecure | default(true) }}""
    when: ovirt_auth is undefined
    register: loggedin
    tags:
      - always

  - name: Get hosts
    ovirt_hosts_facts:
      auth: ""{{ ovirt_auth }}""
      pattern: ""cluster={{ cluster_name | mandatory }} update_available=true {{ host_names | map('regex_replace', '(.*)', 'name=\\1') | list | join(' or ') }} {{ host_statuses | map('regex_replace', '(.*)', 'status=\\1') | list | join(' or ') }}""
  
  - name: Check if there are hosts to be updated
    debug:
      msg: ""No hosts to be updated""
    when: ovirt_hosts | length == 0
  
  - block:
    - name: Init failed_host_names and succeed_host_names list
      set_fact:
        failed_host_names: []
        succeed_host_names: []

    - name: Get cluster facts
      ovirt_clusters_facts:
        auth: ""{{ ovirt_auth }}""
        pattern: ""name={{ cluster_name }}""
  
    - name: Get name of the original scheduling policy
      ovirt_scheduling_policies_facts:
        auth: ""{{ ovirt_auth }}""
        id: ""{{ ovirt_clusters[0].scheduling_policy.id }}""
  
    - name: Remember the cluster scheduling policy
      set_fact:
        cluster_scheduling_policy: ""{{ ovirt_scheduling_policies[0].name }}""
  
    - name: Get list of VMs in cluster
      ovirt_vms_facts:
        auth: ""{{ ovirt_auth }}""
        pattern: ""cluster={{ cluster_name }}""
  
    - name: Set in cluster upgrade policy
      ovirt_clusters:
        auth: ""{{ ovirt_auth }}""
        name: ""{{ cluster_name }}""
        scheduling_policy: InClusterUpgrade
  
    - name: Shutdown VMs which can be stopped
      ovirt_vms:
        auth: ""{{ ovirt_auth }}""
        state: stopped
        name: ""{{ item }}""
        force: true
      with_items:
        - ""{{ stopped_vms | default([]) }}""
  
    - include: pinned_vms.yml
  
    # Update only those hosts that aren't in list of hosts were VMs are pinned
    # or if stop_pinned_to_host_vms is enabled, which means we stop pinned VMs
    - include: upgrade.yml
      with_items:
        - ""{{ ovirt_hosts }}""
      when: ""item.id not in host_ids or stop_pinned_to_host_vms""

    when: ovirt_hosts | length > 0
    always:
      - name: Set original cluster policy
        ovirt_clusters:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ cluster_name }}""
          scheduling_policy: ""{{ cluster_scheduling_policy }}""
  
      - name: Start again stopped VMs
        ovirt_vms:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ item }}""
        with_items:
          - ""{{ stopped_vms | default([]) }}""
  
      - name: Start again pin to host VMs
        ovirt_vms:
          auth: ""{{ ovirt_auth }}""
          name: ""{{ item }}""
        with_items:
          - ""{{ pinned_vms_names | default([]) }}""
        when: ""stop_pinned_to_host_vms""
  
      - name: Print info about host which was updated
        debug:
          msg: ""Following hosts was successfully updated: {{ succeed_host_names }}""
        when: ""succeed_host_names | length > 0""

      - name: Fail the playbook, if some hosts wasn't updated
        fail:
          msg: ""The cluster upgrade failed. Hosts {{ failed_host_names }} wasn't updated.""
        when: ""failed_host_names | length > 0""

  always:
    - name: Logout from oVirt
      ovirt_auth:
        state: absent
        ovirt_auth: ""{{ ovirt_auth }}""
      when: not loggedin.skipped | default(false)
      tags:
        - always"
e0c7356fc6b4bfa86f16a33599172b6fb164f80e,"    args:
      warn: false
    command: ""engine-setup --accept-defaults --config-append={{ answer_file_path }} {{ offline }}""
    until: health_page is success","---
- block:
  - name: Set answer file path
    set_fact:
      answer_file_path: ""/tmp/answerfile-{{ lookup('pipe', 'date +%Y%m%d%H%M%SZ') }}.txt""


  - name: Use the default answerfile
    template:
      src: answerfile_{{ ovirt_engine_setup_version }}_basic.txt.j2
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is undefined

  - name: Copy custom answer file
    template:
      src: ""{{ ovirt_engine_setup_answer_file_path }}""
      dest: ""{{ answer_file_path }}""
      mode: 0600
      owner: root
      group: root
    when: ovirt_engine_setup_answer_file_path is defined

  - name: Update setup packages
    package:
      name: ""ovirt*setup*""
      state: latest
    when: ovirt_engine_setup_update_setup_packages

  - name: Update all packages
    package:
      name: ""*""
      state: latest
    when: ovirt_engine_setup_update_all_packages

  - name: Set accept defaults parameter if variable is set
    set_fact:
      accept_defaults: ""{{ '--accept-defaults' if ovirt_engine_setup_accept_defaults else '' }}""

  - name: Run engine-setup with answerfile
    command: ""engine-setup --config-append={{ answer_file_path }} {{ accept_defaults }}""
    tags:
      - skip_ansible_lint

  - name: Make sure `ovirt-engine` service is running
    service:
      name: ovirt-engine
      state: started

  - name: Check if Engine health page is up
    uri:
      url: ""http://{{ ansible_fqdn }}/ovirt-engine/services/health""
      status_code: 200
    register: health_page
    retries: 12
    delay: 10
    until: health_page|success

  always:
    - name: Clean temporary files
      file:
        path: ""{{ answer_file_path }}""
        state: 'absent'"
770791a7b0a5bee2264d336c7135bb139d97492a,,- /opt/appdata/themes/nzbget:/app/nzbget/webui:shared
e35c713f56f396c691d32ffb53b91b526889c97f,"    regexp: nzb_backup_dir = """"","- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""nzb_backup_dir =""
    replace: ""nzb_backup_dir = /nzb""
  when: sabnzbd_ini.stat.exists == False

- replace:
    path: /opt/appdata/sabnzbd/sabnzbd.ini
    regexp: ""admin_dir = admin""
    replace: ""admin_dir = /admin""
  when: sabnzbd_ini.stat.exists == False
"
d3d0710616669db046dddbd49decfc34f28c9c47,"#- debug: msg=""Using following password {{password_input.user_input}}""
#- name: Replace password with user input
#  replace:
#    path: /opt/appdata/plexguide/var.yml
#    regexp: defaultpassword
#    replace: ""{{password_input.user_input}}""","- name: password
  pause:
    prompt: ""Please create a Universal Password (.htaccess / Wordpress & etc)""
  register: pw

- debug: msg=""Using following pw {{pw_input.user_input}}""

- name: Replace password with user input
  replace:
    path: /opt/appdata/plexguide/var.yml
    regexp: defaultpassword
    replace: ""{{pw_input.user_input}}"""
6d6c4f28f82e438ae95a0c180ec39071bc498c36,"  when: updatecheck.stdout == ""18.09.2,""","#!/bin/bash
#
# [PlexGuide Ansible Role]
#
# Maintainer & Modded By: Admin9705 & Deiteq
# URL:                    https://plexguide.com
#
# Licensed under GNU General Public License v3.0 GPL-3 (in short)
#
#   You may copy, distribute and modify the software as long as you track
#   changes/dates in source files. Any modifications to our software
#   including (via compiler) GPL-licensed code must also be made available
#   under the GPL along with build & install instructions.
#
#################################################################################
# Original Ansible Role: l3uddz, Desimaniac - cloudbox.rocks
#################################################################################
---
- name: ""Establish Facts""
  set_fact:
    switch=""on""
    updatecheck=""default""

- name: ""Docker Check""
  stat:
    path: ""/usr/bin/docker""
  register: check

- name: ""Docker Version Check - True""
  shell: ""docker --version | awk '{print $3}'""
  register: updatecheck

- debug:
    msg: "" {{ updatecheck }} ""

- name: ""Switch - On""
  set_fact:
    switch=""off""
  when: updatecheck.stdout == ""18.06.0-ce,""

- debug:
    msg: ""Switch - {{ switch }}""

- debug:
    msg: ""UpdateCheck - {{ updatecheck }}""

- name: Install required packages
  apt: ""name={{item}} state=present""
  with_items:
    - apt-transport-https
    - ca-certificates
    - software-properties-common
  when: switch == ""on""

- name: Add official gpg signing key
  apt_key:
    id: 0EBFCD88
    url: https://download.docker.com/linux/ubuntu/gpg
  when: switch == ""on""

- name: ""Stop All Containers""
  shell: ""docker stop $(docker ps -a -q)""
  ignore_errors: yes
  when:
    - check.stat.exists == True
    - switch == ""on""

- name: Official Repo
  apt_repository:
    repo: ""deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} edge""
  register: apt_docker_repositories
  when: switch == ""on""

- name: Update APT packages list
  apt:
    update_cache: yes
  when: apt_docker_repositories.changed and switch == ""on""

- name: Release docker-ce from hold
  dpkg_selections:
    name: docker-ce
    selection: install
  when: switch == ""on""

- name: Install docker-ce
  apt:
    name: docker-ce=18.06.0~ce~3-0~ubuntu
    state: present
    update_cache: yes
    force: yes
  when: switch == ""on""

- name: Put docker-ce into hold
  dpkg_selections:
    name: docker-ce
    selection: hold
  when: switch == ""on""

- name: Uninstall docker-py pip module
  pip:
    name: docker-py
    state: absent
  ignore_errors: yes
  when: switch == ""on""

- name: Install docker pip module
  pip:
    name: docker
    state: latest
  ignore_errors: yes
  when: switch == ""on""

- name: Check docker daemon.json exists
  stat:
    path: /etc/docker/daemon.json
  register: docker_daemon

- name: Stop docker to enable overlay2
  systemd: state=stopped name=docker enabled=yes
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: Import daemon.json
  copy: ""src=daemon.json dest=/etc/docker/daemon.json force=yes mode=0775""
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: Start docker (Please Wait)
  systemd: state=started name=docker enabled=yes
  when:
    - docker_daemon.stat.exists == False
    - switch == ""on""

- name: ""Wait for 20 seconds before commencing""
  wait_for:
    timeout: 20
  when: switch == ""on""

- name: Check override folder exists
  stat:
    path: /etc/systemd/system/docker.service.d
  register: docker_override

- name: Create override folder
  file: ""path=/etc/systemd/system/docker.service.d state=directory mode=0775""
  when:
    - docker_override.stat.exists == False
    - switch == ""on""
  tags: docker_standard

- name: Import override file
  copy: ""src=override.conf dest=/etc/systemd/system/docker.service.d/override.conf force=yes mode=0775""
  tags: docker_standard
  when: switch == ""on""

- name: create plexguide network
  docker_network:
    name: ""plexguide""
    state: present
  tags: docker_standard
  when: switch == ""on""

- name: ""Start All Containers""
  shell: ""docker start $(docker ps -a -q)""
  ignore_errors: yes
  when:
    - switch == ""on""
    - check.stat.exists == True"
6683db239e045c47963cc160167297ec51dc70f5,"      traefik.frontend.rule: ""Host:heimdall.{{domain.stdout}}""","############################ Replace Variables
- name: Register Domain
  shell: ""cat /var/plexguide/server.domain""
  register: domain
  ignore_errors: True
############################  Replace Variables
      traefik.frontend.rule: ""Host:heimdall.{domain.stdout}}"""
8a278ebd4bc1e5540b3e27427f2cda10af62e5ed,,"---
########### Move Service
  - name: Check MOVE Service
    stat:
      path: ""/etc/systemd/system/move.service""
    register: move

  - name: Stop If Move Service Running
    systemd: state=stopped name=move
    when: move.stat.exists
    
  - name: Install Move Service
    template:
      src: move.js2
      dest: /etc/systemd/system/move.service 
      force: yes
    when: move.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=move daemon_reload=yes enabled=no

  - name: Start Move
    systemd: state=started name=move enabled=yes
    when: move.stat.exists
    
########### rclone
  - name: Check RCLONE Service
    stat:
      path: ""/etc/systemd/system/rclone.service""
    register: rclone

  - name: Stop If RClone Service Running
    systemd: state=stopped name=rclone
    when: rclone.stat.exists
    
  - name: Install RCLONE Service
    template:
      src: rclone.js2
      dest: /etc/systemd/system/rclone.service 
      force: yes
    when: rclone.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=rclone daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=rclone enabled=yes
    when: rclone.stat.exists

########### UNIONFS
  - name: Check UNIONFS Service
    stat:
      path: ""/etc/systemd/system/unionfs.service""
    register: unionfs

  - name: Stop If UNIONFS Service Running
    systemd: state=stopped name=unionfs
    when: unionfs.stat.exists
    
  - name: Install UNIONFS Service
    template:
      src: unionfs.js2
      dest: /etc/systemd/system/unionfs.service 
      force: yes
    when: unionfs.stat.exists == False

  - name: Daemon-Reload
    systemd: state=stopped name=unionfs daemon_reload=yes enabled=no

  - name: Start RClone
    systemd: state=started name=unionfs enabled=yes
    when: unionfs.stat.exists"
005a199c031f3bd253b7e855935b6de366401002,"    extport: ""7997""","#!/bin/bash
#
# Version:  Ansible-1
# GitHub:   https://github.com/Admin9705/PlexGuide.com-The-Awesome-Plex-Server
# Author:   Admin9705 & Deiteq
# URL:      https://plexguide.com
#
# PlexGuide Copyright (C) 2018 PlexGuide.com
# Licensed under GNU General Public License v3.0 GPL-3 (in short)
#
#   You may copy, distribute and modify the software as long as you track
#   changes/dates in source files. Any modifications to our software
#   including (via compiler) GPL-licensed code must also be made available
#   under the GPL along with build & install instructions.
#
############################################################# (KEY START)
---
- name: ""Establish Key Variables""
  set_fact:
    intport: ""8000""
    extport: ""7999""
    pgrole: ""{{role_name}}""
    image: ""coderaiser/cloudcmd""

- name: ""Key Variables Recall""
  include_role:
    name: ""pgmstart""
    tasks_from: ""keyvar.yml""
############################################################# (KEY END)
#
############################################################# (BASICS START)

- name: Create Directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - ""/opt/appdata/cloudblitz/""

- name: Check config file exists
  stat:
    path: ""/opt/appdata/cloudblitz/.cloudcmd.json""
  register: cloud_json

- name: Install configblitz.json
  template:
    src: configblitz.json
    dest: /opt/appdata/cloudblitz/.cloudcmd.json
    force: yes
  when: cloud_json.stat.exists == False

############################################################# (BASICS END)
- name: ""Set Default Volume - {{pgrole}}""
  set_fact:
    default_volumes:
      - /:/SERVER
      - /opt/appdata/cloudblitz:/root/

- name: ""Establish Key Variables - {{pgrole}}""
  set_fact:
    default_env:
      PUID: 1000
      PGID: 1000

- name: ""Set Default Labels - {{pgrole}}""
  set_fact:
    default_labels:
      traefik.enable: ""true""
      traefik.frontend.redirect.entryPoint: ""https""
      traefik.frontend.rule: ""Host:{{pgrole}}.{{domain.stdout}}""
      traefik.port: ""{{intport}}""

######################## Deploy PGMSTART
- include_role:
    name: ""pgmstart"""
8b255069cc38d96f7f56c54042b65983de53d6a2,"      - ""443:443""","
- name: Create nginx-proxy directories
  file: ""path={{item}} state=directory mode=0775 owner=1000 group=1000 recurse=true""
  with_items:
    - /opt/nginx-proxy

      - ""443:443"""""
b056aa9d0f8eb3782215878e37035ed83c83e128,,"---

- name: fio mixed randread and sequential write benchmark on tikv_data_dir disk
  shell: ""cd {{ fio_deploy_dir }} && ./fio -ioengine=psync -bs=32k -fdatasync=1 -thread -rw=randrw -percentage_random=100,0 -size={{ benchmark_size }} -filename=fio_randread_write_test.txt -name='fio mixed randread and sequential write test' -iodepth=4 -runtime=60 -numjobs=4 -group_reporting --output-format=json --output=fio_randread_write_test.json""
  register: fio_randread_write

- name: clean fio mixed randread and sequential write benchmark temporary file
  file:
    path: ""{{ fio_deploy_dir }}/fio_randread_write_test.txt""
    state: absent

- name: get fio mixed test randread iops
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --read-iops""
  register: disk_mix_randread_iops
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test write iops
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --write-iops""
  register: disk_mix_write_iops
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test randread latency
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --read-lat""
  register: disk_mix_randread_lat
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed test write latency
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --write-lat""
  register: disk_mix_write_lat
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: get fio mixed randread and sequential write summary
  shell: ""python parse_fio_output.py --target='fio_randread_write_test.json' --summary""
  register: disk_mix_randread_write_smmary
  args:
    chdir: ""{{ fio_deploy_dir }}/""

- name: fio mixed randread and sequential write benchmark command
  debug:
    msg: ""fio mixed randread and sequential write benchmark command: {{ fio_randread_write.cmd }}.""
  run_once: true

- name: fio mixed randread and sequential write benchmark summary
  debug:
    msg: ""fio mixed randread and sequential write benchmark summary: {{ disk_mix_randread_write_smmary.stdout }}.""

- name: Preflight check - Does fio mixed randread and sequential write iops of tikv_data_dir disk meet requirement - randread
  fail:
    msg: 'fio mixed randread and sequential write test: randread iops of  tikv_data_dir disk is too low: {{ disk_mix_randread_iops.stdout }} < {{ min_ssd_mix_randread_iops }}, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_randread_iops.stdout|int < min_ssd_mix_randread_iops|int

- name: Preflight check - Does fio mixed randread and sequential write iops of tikv_data_dir disk meet requirement - sequential write
  fail:
    msg: 'fio mixed randread and sequential write test: sequential write iops of tikv_data_dir disk is too low: {{ disk_mix_write_iops.stdout }} < {{ min_ssd_mix_write_iops }}, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_write_iops.stdout|int < min_ssd_mix_write_iops|int

- name: Preflight check - Does fio mixed randread and sequential write latency of tikv_data_dir disk meet requirement - randread
  fail:
    msg: 'fio mixed randread and sequential write test: randread latency of  tikv_data_dir disk is too low: {{ disk_mix_randread_lat.stdout }} ns > {{ max_ssd_mix_randread_lat }} ns, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_randread_lat.stdout|int > max_ssd_mix_randread_lat|int

- name: Preflight check - Does fio mixed randread and sequential write latency of tikv_data_dir disk meet requirement - sequential write
  fail:
    msg: 'fio mixed randread and sequential write test: sequential write latency of tikv_data_dir disk is too low: {{ disk_mix_write_lat.stdout }} ns > {{ max_ssd_mix_write_lat }} ns, it is strongly recommended to use SSD disks for TiKV and PD, or there might be performance issues.'
  when: disk_mix_write_lat.stdout|int > max_ssd_mix_write_lat|int"
c0df6c32cfa04b0ca3622683e2c9607a93605766,enable-telemetry: true,"
dashboard:
  public-path-prefix: ""/dashboard""
  internal-proxy: false
  disable-telemetry: false"
1619145ad8c2039ba245d0d7b2e3ebaac06abe31,"    expr: increase(tidb_server_event_total{type=~""server_start|server_hang""}[15m])  > 0
      expr:  increase(tidb_server_event_total{type=~""server_start|server_hang""}[15m])  > 0
  - alert: tidb_tikvclient_backoff_total
    expr: increase( tidb_tikvclient_backoff_total[10m] )  > 10
      expr:  increase( tidb_tikvclient_backoff_total[10m] )  > 10","  - alert: TiDB_schema_error
    expr: increase(tidb_session_schema_lease_error_total{type=""outdated""}[15m]) > 0
      level: emergency
      expr:  increase(tidb_session_schema_lease_error_total{type=""outdated""}[15m]) > 0
      description: 'alert: instance: {{ $labels.instance }} values: {{ $value }}'
      summary: TiDB schema error
  - alert: TiDB_tikvclient_region_err_total
    expr: increase( tidb_tikvclient_region_err_total[10m] )  > 6000
      level: emergency
      expr:  increase( tidb_tikvclient_region_err_total[10m] )  > 6000
      summary: TiDB tikvclient_backoff_count error
  - alert: TiDB_domain_load_schema_total
    expr: increase( tidb_domain_load_schema_total{type=""failed""}[10m] )  > 10
      level: emergency
      expr:  increase( tidb_domain_load_schema_total{type=""failed""}[10m] )  > 10
      summary: TiDB domain_load_schema_total error
  - alert: TiDB_monitor_keep_alive
    expr: increase(tidb_monitor_keep_alive_total[10m]) < 100
      expr:  increase(tidb_monitor_keep_alive_total[10m]) < 100
      summary: TiDB monitor_keep_alive error
  - alert: TiDB_server_panic_total
    expr: increase(tidb_server_panic_total[10m]) > 0
      level: critical
      expr:  increase(tidb_server_panic_total[10m]) > 0
      summary: TiDB server panic total
  - alert: TiDB_memery_abnormal
    expr: go_memstats_heap_inuse_bytes{job=""tidb""} > 1e+10
      expr: go_memstats_heap_inuse_bytes{job=""tidb""} > 1e+10
      description: 'alert: values: {{ $value }}'
      summary: TiDB mem heap is over 1GiB
  - alert: TiDB_query_duration
    expr: histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance)) > 1
      level: warning
      expr:  histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance)) > 1
      summary: TiDB query duration 99th percentile is above 1s
  - alert: TiDB_server_event_error
    expr: increase(tidb_server_server_event{type=~""server_start|server_hang""}[15m])  > 0
      level: warning
      expr:  increase(tidb_server_server_event{type=~""server_start|server_hang""}[15m])  > 0
      summary: TiDB server event error
  - alert: TiDB_tikvclient_backoff_count
    expr: increase( tidb_tikvclient_backoff_count[10m] )  > 10
      level: warning
      expr:  increase( tidb_tikvclient_backoff_count[10m] )  > 10
      summary: TiDB tikvclient_backoff_count error"
f4fd4608e4e93864d2c9e09c161be8b7010260fd,,"pulp_webserver_trusted_root_certificates_path: /usr/local/share/ca-certificates/
pulp_webserver_trusted_root_certificates_update_bin: update-ca-certificates
pulp_webserver_python_cryptography: python3-cryptography"
9524b07df0c773a767fc6f79ed3dd3bd2a279984,"- name: Delegate a master control plane node
  block:
    - name: Lookup control node from file
      command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
      changed_when: false
      register: k3s_control_delegate_raw
    - name: Ensure control node is delegated to for obtaining a token
      set_fact:
        k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""
    - name: Ensure the control node address is registered in Ansible
      set_fact:
        k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""","  when: hostvars[item].k3s_control_node is defined
        and hostvars[item].k3s_control_node
  when: k3s_controller_count is defined
        and k3s_controller_count | length > 1

- name: Ensure ansible_host is mapped to inventory_hostname
  lineinfile:
    path: /tmp/inventory.txt
    line: >-
      {{ item }}
      @@@
      {{ hostvars[item].ansible_host | default(hostvars[item].ansible_fqdn) }}
      @@@
      C_{{ hostvars[item].k3s_control_node }}
      @@@
      P_{{ hostvars[item].k3s_primary_control_node | default(False) }}
    create: true
  loop: ""{{ play_hosts }}""
  when: hostvars[item].k3s_control_node is defined

- name: Lookup control node from file
  command: ""grep '{{ 'P_True' if (k3s_controller_count | length > 1) else 'C_True' }}' /tmp/inventory.txt""
  changed_when: false
  register: k3s_control_delegate_raw

- name: Ensure control node is delegated to for obtaining a token
  set_fact:
    k3s_control_delegate: ""{{ k3s_control_delegate_raw.stdout.split(' @@@ ')[0] }}""

- name: Ensure the control node address is registered in Ansible
  set_fact:
    k3s_control_node_address: ""{{ hostvars[k3s_control_delegate].ansible_host | default(hostvars[k3s_control_delegate].ansible_fqdn) }}""
  when: k3s_control_node_address is not defined"
154b10ea64a17841f8609c3e048d3da9dc69ca2b,"    zone: ""{{ dns_domain }}""
    zone: ""{{ dns_domain }}.""
    record: ""master-0.{{ env_id }}.{{ dns_domain }}.""
    zone: ""{{ dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ dns_domain }}.""","# Create/update Route 53 zone with new records fro Infra and Master
---
- name: Ensure Route53 zone is present
  route53_zone:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}""
    state: present

- name: Update Route53 with OCP master record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""master.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ master_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode

- name: Update Route53 with OCP infra wildcard record
  route53:
    aws_access_key: ""{{ aws_access_key }}""
    aws_secret_key: ""{{ aws_secret_key }}""
    zone: ""{{ public_dns_domain }}.""
    record: ""*.apps.{{ env_id }}.{{ public_dns_domain }}.""
    type: A
    value: ""{{ infra_eip.public_ip }}""
    command: create
    overwrite: yes
    ttl: 300
  when: not ha_mode"
dce83c121d6e417c1e108712a8b2b753986d365b,"    - role: rhsm
    - configure_rhsm
    - role: config-bonding
    - role: config-vlans
    - role: config-routes
  tags:
    - configure_infra_hosts_networking

    - role: update-host
  tags:
    - update_host
    - role: config-iscsi-client
    - configure_iscsi_client
","- hosts: aws-provisioner
  roles:
  - aws/manage-networks"
abc45ed164c9b2ac57eea0edda8515673a9942ea,"    body_format: json
    body:
      name: ""{{ atlassian.jira.permission_scheme.name }}""
      description: ""{{ atlassian.jira.permission_scheme.description }}""
      permissions: ""{{ permission_list }}""
  register: permission_scheme_output
    default_permission_scheme_id: ""{{ permission_scheme_output.json.id }}""","---
- name: Create Jira Permission Scheme
  uri:
    url: ""{{ atlassian.jira.url }}/rest/api/2/permissionscheme""
    method: POST
    user: ""{{ atlassian.jira.username }}""
    password: ""{{ atlassian.jira.password }}""
    return_content: yes
    force_basic_auth: yes
    body_format: json
    header:
      - Accept: 'application/json'
      - Content-Type: 'application/json'
    body: ""{{ lookup('template','permissionScheme.json.j2') }}""
    status_code: 201
  register: permissionScheme

- name: Set fact for Permission Scheme ID
  set_fact:
    PermissionScheme: ""{{ permissionScheme.json.id }}"""
dce83c121d6e417c1e108712a8b2b753986d365b,"---

    - aws/manage-networks","- hosts: aws-provisioner
  roles:
  - aws/manage-networks"
16218c6366bef9ce3850a6294cabb598fb3afa35,"- name: ""Reset facts to ensure we start over""
  set_fact:
    inventory_id: """"
    project_id: """"
    job_template_id: """"

  - job_template_id is not defined or job_template_id == """"","- name: ""Get the job template id based on job template name""
    job_template_id: ""{{ item.id }}""
  - item.name|trim == job_template.name|trim
  - ""{{ existing_job_templates_output.rest_output }}""
  block:
  - name: ""Create job template {{ job_template.name }}""
    uri:
      url: ""{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/job_templates/""
      user: ""{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}""
      password: ""{{ ansible_tower.admin_password }}""
      force_basic_auth: yes
      method: POST
      body: ""{{ lookup('template', 'job-template.j2') }}""
      body_format: 'json'
      headers:
        Content-Type: ""application/json""
        Accept: ""application/json""
      validate_certs: no
      status_code: 200,201,400
    register: job_template_creation_output

  - name: ""Get the created job template id""
    set_fact:
      job_template_id: ""{{ job_template_creation_output.json.id }}""
  when:
  - job_template_id is not defined

- name: ""Add credentials to job template: {{ job_template.name }}""
  include_tasks: process-job-template-credentials.yml
  loop: ""{{ job_template.credentials | default([ job_template.credential|trim ]) }}""
  loop_control:
    loop_var: job_template_credential
  when: (job_template.credentials is defined) or (job_template.credential is defined)"
c687b18cb05b2dfc349b032304f029842f7ea4ae,- docker_install|default(False),"---

- name: ""Install, configure and enable Docker""
  include: docker.yml
  when: 
  - docker_install|default('no') == ""yes""
"
30b17a34f7bcaa12c2d50ccd5f569c25b75e06d6,"    dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc""
    dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o""
    dest: ""{{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins""
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/opencontainers/runc && \
          export GOPATH={{ ansible_env.HOME}}/{{ gopath }} && \
          make BUILDTAGS=""seccomp selinux"" && make install
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/kubernetes-incubator/cri-o && \
          cd {{ ansible_env.HOME }}/{{ gopath }}/src/github.com/containernetworking/plugins && \
    regexp: '^#storage_driver = ""overlay""'
    line: ""{{ item }}""
    insertbefore: '^#storage_option = \['
  with_items:
    - 'storage_option = ['
    - '""overlay2.override_kernel_check=true"",'
    - ']'

- name: change runtime (runc) path
  replace:
    regexp: '^runtime =.*$'
    replace: 'runtime = ""/usr/local/sbin/runc""'
    name: /etc/crio/crio.conf
    backup: yes
          Environment=\""KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio/crio.sock --container-runtime-endpoint /var/run/crio/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'","---
# Some (lots!) borrowed from https://github.com/cri-o/cri-o-ansible

- name: Check Golang version
  shell: /usr/local/go/bin/go version
  register: go_version
  failed_when: false
  changed_when: false

- name: install Golang upstream
  unarchive:
    src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""
    dest: ""/usr/local""
    remote_src: True
  when: '""go1.8.3"" not in go_version.stdout'

- name: Set custom Golang path in RHEL/CentOS/Fedora
  lineinfile:
    dest: /root/.bashrc
    line: 'export PATH=/usr/local/go/bin:$PATH'
    insertafter: 'EOF'
    regexp: 'export PATH=\/usr\/local\/go\/bin:\$PATH'
    state: present
  # when: ansible_distribution in ['Fedora', 'RedHat', 'CentOS']

- name: link golang
  file: src=/usr/local/go/bin/go dest=/usr/bin/go state=link

- name: Install required packages for cri-o
  yum:
    name: ""{{ item }}""
    state: latest
  with_items:
    - wget
    - git
    - make
    - gcc
    - tar
    - libseccomp-devel
    - golang
    - go-md2man
    - glib2-devel
    - glibc-static
    - container-selinux
    - btrfs-progs-devel
    - device-mapper-devel
    - glibc-devel
    - gpgme-devel
    - libassuan-devel
    - libgpg-error-devel
    - pkgconfig
    - json-glib-devel
    - skopeo-containers



- name: Make directories
  file:
    path: ""{{ item }}""
    state: directory
  with_items:
       - /usr/local/go
       - /etc/systemd/system/kubelet.service.d/
       - /var/lib/etcd
       - /etc/cni/net.d
       - /etc/containers

- name: clone runc
  git:
    repo: https://github.com/opencontainers/runc
    dest: /root/src/github.com/opencontainers/runc

- name: clone CRI-O
  git:
    repo: https://github.com/kubernetes-incubator/cri-o
    dest: /root/src/github.com/kubernetes-incubator/cri-o
    version: kube-1.6.x

- name: clone CNI
  git:
    repo: https://github.com/containernetworking/plugins
    dest: /root/src/github.com/containernetworking/plugins

- name: build runc
  shell: |
          cd /root/src/github.com/opencontainers/runc && \
          export GOPATH=/root && \
          make BUILDTAGS=""seccomp selinux"" && \
          make install

- name: link runc
  file: src=/usr/local/sbin/runc dest=/usr/bin/runc state=link

- name: build cri-o
  shell: |
          cd /root/src/github.com/kubernetes-incubator/cri-o && \
          make install.tools && \
          make && \
          make install && \
          make install.systemd && \
          make install.config

- name: build CNI stuff
  shell: |
          cd /root/src/github.com/containernetworking/plugins && \
          ./build.sh && \
          mkdir -p /opt/cni/bin && \
          cp bin/* /opt/cni/bin/

- name: run CRI-O with systemd cgroup manager
  replace:
    regexp: 'cgroupfs'
    replace: 'systemd'
    name: /etc/crio/crio.conf
    backup: yes

- name: run with overlay2
  replace:
    regexp: 'storage_driver = """"'
    replace: 'storage_driver = ""overlay2""'
    name: /etc/crio/crio.conf
    backup: yes

- name: add overlay2 storage opts on RHEL/CentOS
  lineinfile:
    dest: /etc/crio/crio.conf
    line: '""overlay2.override_kernel_check=1""'
    insertafter: 'storage_option = \['
    regexp: 'overlay2\.override_kernel_check=1'
    state: present
  
- name: enable and start CRI-O
  systemd:
    name: crio
    state: started
    enabled: yes
    daemon_reload: yes

- name: modprobe br_netfilter
  command: ""modprobe br_netfilter""
  ignore_errors: true

- name: tune sysctl
  lineinfile:
    line: ""net/bridge/bridge-nf-call-iptables = 1""
    dest: /etc/sysctl.conf
    insertafter: 'EOF'
    regexp: '\/net\/bridge\/bridge-nf-call-iptables = 1'
    state: present
  ignore_errors: true

- name: reload sysctl
  command: ""sysctl -p""
  ignore_errors: true

- name: systemd dropin for kubeadm
  shell: |
          sh -c 'echo ""[Service]
          Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'

- name: flush iptables
  command: ""iptables -F""

# - name: Permanently disable selinux
#   lineinfile:
#     dest: /etc/selinux/config
#     line: 'SELINUX=permissive'
#     regexp: '^SELINUX='
#   when: (ansible_distribution == 'Fedora' or ansible_distribution == 'RedHat' or ansible_distribution == 'CentOS')

# - name: disable selinux
#   command: ""setenforce 0""
#   when: (ansible_distribution == 'Fedora' or ansible_distribution == 'RedHat' or ansible_distribution == 'CentOS')"
30b17a34f7bcaa12c2d50ccd5f569c25b75e06d6,crio_version: v1.11.1,"---
# Some (lots!) borrowed from https://github.com/cri-o/cri-o-ansible

- name: Check Golang version
  shell: /usr/local/go/bin/go version
  register: go_version
  failed_when: false
  changed_when: false

- name: install Golang upstream
  unarchive:
    src: ""https://golang.org/dl/go1.8.3.linux-amd64.tar.gz""
    dest: ""/usr/local""
    remote_src: True
  when: '""go1.8.3"" not in go_version.stdout'

- name: Set custom Golang path in RHEL/CentOS/Fedora
  lineinfile:
    dest: /root/.bashrc
    line: 'export PATH=/usr/local/go/bin:$PATH'
    insertafter: 'EOF'
    regexp: 'export PATH=\/usr\/local\/go\/bin:\$PATH'
    state: present
  # when: ansible_distribution in ['Fedora', 'RedHat', 'CentOS']

- name: link golang
  file: src=/usr/local/go/bin/go dest=/usr/bin/go state=link

- name: Install required packages for cri-o
  yum:
    name: ""{{ item }}""
    state: latest
  with_items:
    - wget
    - git
    - make
    - gcc
    - tar
    - libseccomp-devel
    - golang
    - go-md2man
    - glib2-devel
    - glibc-static
    - container-selinux
    - btrfs-progs-devel
    - device-mapper-devel
    - glibc-devel
    - gpgme-devel
    - libassuan-devel
    - libgpg-error-devel
    - pkgconfig
    - json-glib-devel
    - skopeo-containers



- name: Make directories
  file:
    path: ""{{ item }}""
    state: directory
  with_items:
       - /usr/local/go
       - /etc/systemd/system/kubelet.service.d/
       - /var/lib/etcd
       - /etc/cni/net.d
       - /etc/containers

- name: clone runc
  git:
    repo: https://github.com/opencontainers/runc
    dest: /root/src/github.com/opencontainers/runc

- name: clone CRI-O
  git:
    repo: https://github.com/kubernetes-incubator/cri-o
    dest: /root/src/github.com/kubernetes-incubator/cri-o
    version: kube-1.6.x

- name: clone CNI
  git:
    repo: https://github.com/containernetworking/plugins
    dest: /root/src/github.com/containernetworking/plugins

- name: build runc
  shell: |
          cd /root/src/github.com/opencontainers/runc && \
          export GOPATH=/root && \
          make BUILDTAGS=""seccomp selinux"" && \
          make install

- name: link runc
  file: src=/usr/local/sbin/runc dest=/usr/bin/runc state=link

- name: build cri-o
  shell: |
          cd /root/src/github.com/kubernetes-incubator/cri-o && \
          make install.tools && \
          make && \
          make install && \
          make install.systemd && \
          make install.config

- name: build CNI stuff
  shell: |
          cd /root/src/github.com/containernetworking/plugins && \
          ./build.sh && \
          mkdir -p /opt/cni/bin && \
          cp bin/* /opt/cni/bin/

- name: run CRI-O with systemd cgroup manager
  replace:
    regexp: 'cgroupfs'
    replace: 'systemd'
    name: /etc/crio/crio.conf
    backup: yes

- name: run with overlay2
  replace:
    regexp: 'storage_driver = """"'
    replace: 'storage_driver = ""overlay2""'
    name: /etc/crio/crio.conf
    backup: yes

- name: add overlay2 storage opts on RHEL/CentOS
  lineinfile:
    dest: /etc/crio/crio.conf
    line: '""overlay2.override_kernel_check=1""'
    insertafter: 'storage_option = \['
    regexp: 'overlay2\.override_kernel_check=1'
    state: present
  
- name: enable and start CRI-O
  systemd:
    name: crio
    state: started
    enabled: yes
    daemon_reload: yes

- name: modprobe br_netfilter
  command: ""modprobe br_netfilter""
  ignore_errors: true

- name: tune sysctl
  lineinfile:
    line: ""net/bridge/bridge-nf-call-iptables = 1""
    dest: /etc/sysctl.conf
    insertafter: 'EOF'
    regexp: '\/net\/bridge\/bridge-nf-call-iptables = 1'
    state: present
  ignore_errors: true

- name: reload sysctl
  command: ""sysctl -p""
  ignore_errors: true

- name: systemd dropin for kubeadm
  shell: |
          sh -c 'echo ""[Service]
          Environment=\""KUBELET_EXTRA_ARGS=--enable-cri=true --container-runtime=remote --runtime-request-timeout=15m --image-service-endpoint /var/run/crio.sock --container-runtime-endpoint /var/run/crio.sock\"""" > /etc/systemd/system/kubelet.service.d/0-crio.conf'

- name: flush iptables
  command: ""iptables -F""

# - name: Permanently disable selinux
#   lineinfile:
#     dest: /etc/selinux/config
#     line: 'SELINUX=permissive'
#     regexp: '^SELINUX='
#   when: (ansible_distribution == 'Fedora' or ansible_distribution == 'RedHat' or ansible_distribution == 'CentOS')

# - name: disable selinux
#   command: ""setenforce 0""
#   when: (ansible_distribution == 'Fedora' or ansible_distribution == 'RedHat' or ansible_distribution == 'CentOS')"
473ee5c295caba864e8fed715200fc4fae9943dd,"        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}"""
473ee5c295caba864e8fed715200fc4fae9943dd,"        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}"""
2f22d41b63d74d82cac6766885c5231d8d4c4ca6,"      fp_ip_address_from: ""{{ opstools_external_ip.stdout }}""","- name: run port forwarding role
  include_role:
      name: network/forward-port
  vars:
      fp_ip_address_from: ""{{ opstools_management_ip.stdout }}""
      fp_destination_port: 8081
  delegate_to: hypervisor
  when:
      - ""'hypervisor' in groups""
      - install.overcloud.opstools_forward|default(True)
"
473ee5c295caba864e8fed715200fc4fae9943dd,"        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}"""
473ee5c295caba864e8fed715200fc4fae9943dd,"        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16) %}OS::TripleO::Services::NovaPlacement{% endif %}""","        - ""{% if (install.version|default(undercloud_version)|openstack_release > 10 and install.version|default(undercloud_version)|openstack_release < 16)OS::TripleO::Services::NovaPlacement{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 15 %}OS::TripleO::Services::PlacementApi{% endif %}"""
1cb229a90e02ad40bc51e94c7372fb1fce4a8161,"    - name: get the vlan number where external network should be served
    - name: create new vlan interface in ovs system
    - name: get the IP address for the external network interface
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalInterfaceDefaultRoute | awk -F' ' '{print $2}'""
      register: stat_ip_result
    - name: configure external gateway's IP for this interface
      shell: ""sudo ip addr add {{ stat_ip_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""

    - debug: var={{ iface_ip_result }}

#      failed_when: ""iface_ip_result.stderr != '' and 'RTNETLINK answers: File exists' not in iface_ip_result.stderr""

    - name: get cidr of the external network
      shell: ""cat {{ template_base }}/network/{{ network_environment_file }} | grep ExternalNetCidr | awk -F' ' '{print $2}'""
      register: route_result

    - name: add new static route for external network
      shell: ""sudo ip route add {{ route_result.stdout | replace(\""'\"",'') }} dev vlan{{ vlan_result.stdout | replace(\""'\"",'') }}""","---
- set_fact:
      isolation_file: ""network-isolation{{ (installer.network.protocol == 'ipv6') | ternary('-v6','') }}.yaml""

- name: append the network environment template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/network/network-environment.yaml \'

- name: append the network isolation template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/{{ isolation_file }} \'

- block:
    - name: get the IP address on the external network default route
      shell: ""cat {{ template_base }}/network/network-environment.yaml | grep ExternalNetworkVlanID | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ovs-vsctl add-port br-ctlplane vlan{{ result.stdout | replace(\""'\"",'') }} tag={{ result.stdout | replace(\""'\"",'') }} -- set interface vlan{{ result.stdout | replace(\""'\"",'') }} type=internal;""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""

    - name: get the IP address on the external network default route
      shell: ""cat {{ installer.overcloud.template_base }}/network-environment.yaml | grep ExternalNetCidr | awk -F':' '{print $2}' | sed 's/[^0-9.]//'""
      register: result

    - name: add the ip address to the device when vlan is used
      shell: ""sudo ip addr add {{ result.stdout | replace(\""'\"",'') }}/{{ (installer.network.protocol == 'ipv6') | ternary('64','24') }} dev {{ ansible_interfaces | first }}""
      register: vlan_result
      failed_when: ""vlan_result.stderr != '' and 'RTNETLINK answers: File exists' not in vlan_result.stderr""
  when: installer.network.backend == 'vlan'"
87739f41ff7656a468b30d603d1b3ae16aea58e3,"  command: ""sosreport --batch -e openstack_ironic --batch -p openstack,openstack_undercloud,openstack_controller --all-logs --experimental -k docker.all=on --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""","---
- name: Define sosrep folder
  set_fact:
      tmp_sos_dir: '/var/sosrep'

- name: Make sure that sosreport is installed on the host
  yum:
      name: sos
      state: latest

- name: Cleanup sosreport dir if exists
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent

- name: Create dir for sosreport
  file:
      name: ""{{ tmp_sos_dir }}""
      state: directory
      mode: 0644

- name: Collect sosreport logs
  command: ""sosreport --batch --tmp-dir {{ tmp_sos_dir }} --name={{ inventory_hostname }} -z gzip""
  register: sosreport_result
  failed_when: ""'sosreport has been generated' not in sosreport_result.stdout""

- name: get name of the generated file
  find:
      paths: ""{{ tmp_sos_dir }}""
      patterns: ""sosreport-{{ inventory_hostname }}-*.tar.gz""
      recurse: no
  register: sosreport_file

- name: fetch sosreport archive
  fetch:
      flat: yes
      src: ""{{ item.path }}""
      dest: ""{{ dest_dir }}/""
      validate_checksum: no
  ignore_errors: true
  with_items: ""{{ sosreport_file.files }}""

- name: Cleanup sosreport dir after run
  file:
      name: ""{{ tmp_sos_dir }}""
      state: absent"
b78375fbbed55c8ac9f88472d55fad3f8e4ce923,,"      - include_tasks: pre.yml
        delegate_to: ""{{  vbmc_inventory_host }}""

- block:
      - include_tasks: check.yml
        when: action == 'check'
      - include_tasks: cleanup.yml
        when: action == 'cleanup'
      - include_tasks: remove.yml
        when: action == 'remove'
  delegate_to: ""{{  vbmc_inventory_host }}"""
fd1fe0aee5f7f365d13a3eaa1672ee95ffe59640,"        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""","        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhApi
        - OS::TripleO::Services::AodhEvaluator
        - OS::TripleO::Services::AodhNotifier
        - OS::TripleO::Services::AodhListener
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendSimpleCrypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendDogtag{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendKmip{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendPkcs11Crypto{% endif %}""
        - OS::TripleO::Services::CACerts
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerCollector{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerExpirer{% endif %}""
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::CephMds{% endif %}""
        - OS::TripleO::Services::CephMon
        - OS::TripleO::Services::CephExternal
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::CephMgr{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::CephRbdMirror{% endif %}""
        - OS::TripleO::Services::CephRgw
        - OS::TripleO::Services::CinderApi
        - OS::TripleO::Services::CinderScheduler
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 11 %}OS::TripleO::Services::Core{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - OS::TripleO::Services::Fluentd{% if install.version|default(undercloud_version) |openstack_release < 13 %}Client{% endif %}
        - OS::TripleO::Services::GlanceApi
        - ""{% if install.version|default(undercloud_version) |openstack_release < 11 %}OS::TripleO::Services::GlanceRegistry{% endif %}""
        - OS::TripleO::Services::GnocchiApi
        - OS::TripleO::Services::GnocchiMetricd
        - OS::TripleO::Services::GnocchiStatsd
        - OS::TripleO::Services::HeatApi
        - OS::TripleO::Services::HeatApiCfn
        - ""{% if install.version|default(undercloud_version) |openstack_release < 12 %}OS::TripleO::Services::HeatApiCloudwatch{% endif %}""
        - OS::TripleO::Services::HeatEngine
        - OS::TripleO::Services::Horizon
        - OS::TripleO::Services::IronicApi
        - OS::TripleO::Services::IronicConductor
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::IronicPxe{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - OS::TripleO::Services::Keepalived
        - OS::TripleO::Services::Kernel
        - OS::TripleO::Services::Keystone
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - OS::TripleO::Services::ManilaApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendIsilon{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendUnity{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendVMAX{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ManilaBackendVNX{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::ManilaBackendGeneric{% endif %}""
        - OS::TripleO::Services::ManilaBackendNetapp
        - OS::TripleO::Services::ManilaBackendCephFs
        - OS::TripleO::Services::ManilaScheduler
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::MySQLClient{% endif %}""
        - OS::TripleO::Services::Memcached
        - OS::TripleO::Services::NeutronApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::NeutronBgpVpnApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::NeutronSfcApi{% endif %}""
        - OS::TripleO::Services::NeutronCorePlugin
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::NeutronL2gwApi{% endif %}""
        - OS::TripleO::Services::NovaApi
        - OS::TripleO::Services::NovaConductor
        - OS::TripleO::Services::NovaConsoleauth
        - OS::TripleO::Services::NovaIronic
        - OS::TripleO::Services::NovaMetadata
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::NovaPlacement{% endif %}""
        - OS::TripleO::Services::NovaScheduler
        - OS::TripleO::Services::NovaVncProxy
        - OS::TripleO::Services::Ntp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaDeploymentConfig{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHealthManager{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHousekeeping{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaWorker{% endif %}""
        - OS::TripleO::Services::OpenDaylightApi
        - OS::TripleO::Services::OpenDaylightOvs
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNDBs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNController{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::PankoApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - OS::TripleO::Services::SaharaApi
        - OS::TripleO::Services::SaharaEngine
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - OS::TripleO::Services::SensuClient
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAgent{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAnalyzer{% endif %}""
        - OS::TripleO::Services::SwiftProxy
        - OS::TripleO::Services::SwiftStorage
        - OS::TripleO::Services::SwiftRingBuilder
        - OS::TripleO::Services::Snmp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::TripleoPackages
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}"""
749796ca4b2f94082f4e478c688e4536d27f3ebd,"      - name: append the ceph storage flavor name to the base overcloud deploy script
  when:
    - ""not install.storage.external""
    - ""not install.splitstack""
  when:
    - install.storage.config == 'internal'

- name: append the storage ceph template in splitstack deployment
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'
  when:
    - ""install.splitstack""
    - install.version|default(undercloud_version)|openstack_release < 12","- block:
    - name: import ceph facts
      set_fact:
          ceph_facts: ""{{ lookup('file', '{{ inventory_dir }}/{{ hostvars[(groups.ceph|first)].inventory_hostname }}') }}""

    # If you reached this stage, it means a user explicitely entered the storage flag therefore he wants at least 1 node
    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-scale {{ storage_nodes }} \'
      when: ""templates.storage_add_scale | default(True)""

    - name: append the ceph storage template line to the base overcloud deploy script
      lineinfile:
          dest: ""~/overcloud_deploy.sh""
          line: '--ceph-storage-flavor {% if groups[""ceph""] is defined %}ceph{% else %}baremetal{% endif %} \'
      when: ""templates.storage_add_scale | default(True)""

  when: ""not install.storage.external""
  vars:
      storage_nodes: ""{{ (install.storage.nodes|default(0)) or (groups['ceph']|default([])|length) or 1 }}""

- name: append the storage template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e /usr/share/openstack-tripleo-heat-templates/environments/storage-environment.yaml \'

- name: prepare ceph storage template
  vars:
      # we have different keyrings mapped to each ospd version capabilities
      ceph_compt_version: ""
          {%- if install.version|openstack_release < 8 -%}8
          {%- elif install.version|openstack_release >= 10 -%}10
          {%- else -%}{{ install.version|openstack_release }}{%- endif -%}""
  template:
      src: ""storage/ceph.yml.j2""
      dest: ""{{ template_base }}/ceph.yaml""
      mode: 0755

- name: append the storage ceph custom template line to the base overcloud deploy script
  lineinfile:
      dest: ""~/overcloud_deploy.sh""
      line: '-e {{ template_base }}/ceph.yaml \'"
ed3c50b85cf905a7450d9abfee7f017ede020c3f,"      state: ""{{ ir_default_pip_versions[item] is defined | ternary('present', 'latest') }}""","- name: Include configuration vars
  include_vars: ""vars/config/{{ test.setup }}.yml""

- name: Clone tempest_conf
  git:
      repo: ""{{ tempest_conf.repo }}""
      version: ""{{ tempest_conf.revision | default(omit) }}""
      dest: ""{{ tempest_conf.dir }}""
  register: tempest_conf_repo

- name: Create tempest conf venv with latest pip, setuptools and pbr
  pip:
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      name: ""{{ item }}""
      state: latest
  with_items:
      - pip
      - setuptools
      - pbr

- name: Install tempest_conf
  pip:
      name: "".""
      virtualenv: ""{{ tempest_conf.dir }}/.venv""
      chdir: ""{{ tempest_conf.dir }}""

- name: Init tempest
  shell: |
      source .venv/bin/activate
      tempest init ""~/{{ test.dir }}""
  args:
      chdir: ""{{ tempest_conf.dir }}""
      creates: ""~/{{ test.dir }}/etc""

- name: Set facts for configuration run
  set_fact:
      config_command: ""discover-tempest-config""
      config_dir: ""{{ tempest_conf.dir }}"""
9c9b2889e8ff8e51e635a17318b89d65b2167838,"        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhListener
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendSimpleCrypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendDogtag{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendKmip{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::BarbicanBackendPkcs11Crypto{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::CephMgr{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CinderBackendVRTSHyperScale{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Clustercheck{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Congress{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ec2Api{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Etcd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::IronicPxe{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::ManilaBackendGeneric{% endif %}""
        - OS::TripleO::Services::ManilaBackendNetapp
        - OS::TripleO::Services::ManilaBackendCephFs
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::NeutronSfcApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaDeploymentConfig{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHealthManager{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaHousekeeping{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OctaviaWorker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNDBs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::OVNController{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAgent{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::SkydiveAnalyzer{% endif %}""
        - ""{% if roles_sshd %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::SwiftRingBuilder
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Tacker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Vpp{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Zaqar{% endif %}""","    networks:
        - InternalApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}"""
befc4c24dba4ba00bc9e7f7dd57c17f05c563851,"        OS::TripleO::Galera::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/galera_internal.yaml""","galera_role:
    name: Galera
        OS::TripleO::Galera::Net::SoftwareConfig: ${deployment_dir}/network/nic-configs/galera_internal.yaml
        OS::TripleO::Galera::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api${ipv6_postfix_underscore}.yaml
    flavor: galera
    host_name_format: 'galera-%index%'"
554909f0cb50ba737dad17e0d3bfea1e5c7060d1,cp {{ base_image_path }}/{{ base_image }} {{ base_image_path }}/{{ node_image }},"---
# For disks usually it will be only 1 disk so the async will run on nodes rather than disks creation
- name: create disk(s) from vm base image
  shell: |
      {% for num in range(1, item.value.amount + 1, 1) %}
      {% for disk_name, disk_values in item.value.disks.iteritems() %}
      {% set template_image = '{0}-{1}.qcow2'.format(item.value.name, disk_name) %}
      {% set node_image = '{0}-{1}-{2}.qcow2'.format(item.value.name, num - 1, disk_name) %}
      {% if not disk_values.import_url %}

      cp {{ base_image_path }}/{{ template_image }} {{ base_image_path }}/{{ node_image }}
      qemu-img create -f qcow2 -o preallocation={{ disk_values.preallocation }} {{ disk_values.path  }}/{{ node_image }} {{ disk_values.size }}
      {% if disk_name == 'disk1' -%}
      virt-resize --expand /dev/sda1 {{ base_image_path }}/{{ base_image }} {{ disk_values.path }}/{{ node_image }}
      virt-customize -a {{ disk_values.path }}/{{ node_image }} \
      {%- for index in range(item.value.interfaces | length - 1) %}
          --run-command 'cp /etc/sysconfig/network-scripts/ifcfg-eth{0,{{ index + 1 }}} && sed -i s/DEVICE=.*/DEVICE=eth{{ index + 1 }}/g /etc/sysconfig/network-scripts/ifcfg-eth{{ index +1 }}' {% if not loop.last %}\
          {% endif %}
      {% endfor %}
      {% endif %}
      {% endif %}
      {% endfor %}
      {% endfor %}
  with_dict: ""{{ provisioner.topology.nodes }}""
  register: ""vm_disks""
  async: 7200
  poll: 0

- name: Wait for our disks to be created
  async_status:
      jid: ""{{ item.ansible_job_id }}""
  register: disk_tasks
  until: disk_tasks.finished
  retries: 300
  with_items: ""{{ vm_disks.results }}"""
c84cc33b39d161cef24d4559cecb6dbace980447,"      - {role: ""installer/ospd/loadbalancer/"", when: 'loadbalancer' in groups}","      - {role: ""installer/ospd/loadbalancer/"", when: provisioner.topology.nodes.loadbalancer is defined }"
271183a30a7f77851a0dd21190be0fd2e5675607,"        stdout: ""{{ (network_template|selectattr('name_lower','equalto','external')|first).ip_subnet }}""","  when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the external vlan id from the network_environment_file
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      vlan_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'equalto', 'external')|first).vlan }}""
  when: use_network_data|bool
  when: not use_network_data|bool

- block:
  - name: read deployment network configuration from network_data
    command: ""cat {{ template_base }}/network/network_data.yaml""
    register: network_template_out
    changed_when: false

  - name: Get the cidr of the external network from network_data
    vars:
      network_template: ""{{ network_template_out.stdout | from_yaml }}""
    set_fact:
      route_result:
        stdout: ""{{ (network_template|selectattr('name_lower', 'external')|first).ip_subnet }}""
  when: use_network_data|bool
  when: ansible_os_family == ""RedHat"""
03b3cf17f03ab0b3e68eaa5b9ed869a2f0dcc5e1,"  when: ""'virthost' in groups""","  when: ""groups.virthost is defined"""
236cda5daba9875a30e153acb6aa10876ffbaad6,"        OS::TripleO::Telemetry::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/${nics_subfolder}/messaging_internal.yaml""
        OS::TripleO::Telemetry::Ports::InternalApiPort: ""{{ install.heat.templates.basedir }}/network/ports/internal_api.yaml""","telemetry_role:
    name: Telemetry

    resource_registry:
        OS::TripleO::Messaging::Net::SoftwareConfig: ""${deployment_dir}/network/nic-configs/messaging_internal.yaml""
        OS::TripleO::Messaging::Ports::InternalApiPort: ""{{ install.heat.templates.basedir }}/network/ports/internal_api.yaml""
    flavor: telemetry
    networks:
        - InternalApi
    host_name_format: 'telemetry-%index%'

    services:
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - OS::TripleO::Services::AodhApi
        - OS::TripleO::Services::AodhEvaluator
        - OS::TripleO::Services::AodhListener
        - OS::TripleO::Services::AodhNotifier
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::AuditD{% endif %}""
        - OS::TripleO::Services::CACerts
        - OS::TripleO::Services::CeilometerAgentCentral
        - OS::TripleO::Services::CeilometerAgentNotification
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerApi{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerCollector{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::CeilometerExpirer{% endif %}""
        - OS::TripleO::Services::CephClient
        - OS::TripleO::Services::CephExternal
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - OS::TripleO::Services::Collectd
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - OS::TripleO::Services::Fluentd{% if install.version|default(undercloud_version) |openstack_release < 13 %}Client{% endif %}
        - OS::TripleO::Services::GnocchiApi
        - OS::TripleO::Services::GnocchiMetricd
        - OS::TripleO::Services::GnocchiStatsd
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - OS::TripleO::Services::Kernel
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release < 13 %}OS::TripleO::Services::MongoDb{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::MySQLClient{% endif %}""
        - OS::TripleO::Services::Ntp
        - OS::TripleO::Services::Pacemaker
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::PankoApi{% endif %}""
        - OS::TripleO::Services::Redis
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - OS::TripleO::Services::SensuClient
        - OS::TripleO::Services::Snmp
        - ""{% if install.version|default(undercloud_version) |openstack_release > 10 %}OS::TripleO::Services::Sshd{% endif %}""
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::TripleoPackages
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}"""
d70152564e9db73dabd49db44d035426edd95df5,"        shell: |
            test -e .venv/bin/activate && source .venv/bin/activate
            subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html
            test -e .venv/bin/activate && source .venv/bin/activate","      test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ inventory_dir }}/tempest_results/tempest-results-{{ test_suite }}.*.subunit""
      test_output_filename: ""tempest-results-{{ test_suite }}.{{ max_seq_number | int + 1 }}""
      - ""{{ test_output_filename }}.subunit""
      - ""{{ test_output_filename }}.xml""
      - ""{{ test_output_filename }}.html""
            > {{ test_output_filename }}.subunit;
            testr run {{ ('--parallel --concurrency=' + test.threads|string) if test.threads|default('') else '' }} --subunit '{{ '|'.join(parts) }}' > {{ test_output_filename }}.subunit
      - name: generate results report in HTML format
        command: ""subunit2html {{ test_output_filename }}.subunit {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: generate results report in JunitXML format
        shell: |
            subunit2junitxml --output-to={{ test_output_filename }}.xml \
                < {{ test_output_filename }}.subunit | subunit2pyunit
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.junitxml is defined

        command: ""sed '/^<testsuite/s/name=\""\""/name=\""{{ test_suite }}\""/' -i {{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined
      - name: add the test suite name in the HTML report title
        command: ""sed 's#<title>\\(.*Test Report\\)</title>#<title>\\1 - {{ test_suite }}</title>#' \
                  -i {{ test_output_filename }}.html""
        args:
            chdir: ""{{ test.dir }}""
        when:
            results_formats.html is defined

      - name: Fetch subunit results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.subunit""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.subunit""
            flat: yes
            fail_on_missing: yes

      - name: Fetch JUnit XML results file
            src: ""{{ test.dir }}/{{ test_output_filename }}.xml""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.xml""
        when:
            results_formats.junitxml is defined

      - name: Fetch HTML results file
        fetch:
            src: ""{{ test.dir }}/{{ test_output_filename }}.html""
            dest: ""{{ inventory_dir }}/tempest_results/{{ test_output_filename }}.html""
            flat: yes
            fail_on_missing: no
        when:
            results_formats.html is defined"
006f9587c3531474fc3fe9f8c92067150a0cc7c3,"      path: ""{{ overcloud_deploy_script|default('~/overcloud_deploy.sh') }}""","---
- name: Unsupported OS version on undercloud
  fail:
      msg: ""InfraRed supports updates for OpenStack version 11""
  when:
      - not (undercloud_version == ""11"")

- name: Checking overcloud_deploy_file
  stat:
      path: ""~/overcloud_deploy.sh""
  register: overcloud_deploy_file

- name: Deployment script not found
  fail:
      msg: ""Overcloud deployment script not found. Expected path: ~/overcloud_deploy.sh ""
  when: not overcloud_deploy_file.stat.exists or not overcloud_deploy_file.stat.executable

- name: Checking file with overcloud credentials
  stat:
      path: ""~/overcloudrc.v3""
  register: oc_credentials_file

- name: Overcloud RC not found
  fail:
      msg: ""File with OC credentials not found. Expected path: ~/overcloudrc.v3""
  when: not oc_credentials_file.stat.exists or not oc_credentials_file.stat.readable"
a23b05977d26aa8213231277ba424789189257f8,"        ""OS::TripleO::CephStorage::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""","---
ceph_role:
    name: CephStorage

    resource_registry:
        ""OS::TripleO::Compute::Net::SoftwareConfig"": ""${deployment_dir}/network/nic-configs/ceph-storage.yaml""

    flavor: ceph
    host_name_format: 'ceph-%index%'

    services:
        - OS::TripleO::Services::CACerts
        - OS::TripleO::Services::CephOSD
        - OS::TripleO::Services::Kernel
        - OS::TripleO::Services::Ntp
        - OS::TripleO::Services::Timezone
        - OS::TripleO::Services::TripleoPackages
        - OS::TripleO::Services::TripleoFirewall
        - OS::TripleO::Services::SensuClient
        - OS::TripleO::Services::FluentdClient
        - OS::TripleO::Services::VipHosts"
9c9b2889e8ff8e51e635a17318b89d65b2167838,"        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Aide{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::AuditD{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Collectd{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Ipsec{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::LoginDefs{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::Rhsm{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 12 %}OS::TripleO::Services::RsyslogSidecar{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""","    networks:
        - InternalApi
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::CertmongerUser{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Docker{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Securetty{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::Tuned{% endif %}""
        - ""{% if install.version|default(undercloud_version) |openstack_release > 11 %}OS::TripleO::Services::ContainersLogrotateCrond{% endif %}"""
5ba614532b20886f7bd075c602eeb618f6350a37,"- name: get controller flavor
  shell: ""source ~/stackrc; openstack flavor list -c Name -f value | grep 'controller-'""
  register: controller_flavor

  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:{{ controller_flavor.stdout }},node:controller-{{ item.0 }},boot_option:local'""","- name: set additional propertiesx
  shell: ""source ~/stackrc; ironic node-update {{ item.1 }} replace properties/capabilities='profile:baremetal,node:controller-{{ item.0 }},boot_option:local'""
  with_indexed_items: ""{{ groups['controller'] }}"""
da4880568831da64ebd742f1e7a9ff9374774c0e,"- name: Override postgres params for CentOs or RedHat when ovirt >= 4.2
    - ansible_distribution in ('CentOS', 'RedHat')
    - ansible_distribution in ('CentOS', 'RedHat')","- name: Include postgres params
  include_vars: default.yml

- name: Override postgres params for CentOs or Red Hat when ovirt >= 4.2
  include_vars: postgres95.yml
  when:
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')

- name: install psycopg2 requirements to run ansible modules managing postgres.
  yum:
    name: ""python-psycopg2""
    state: ""present""

    name: ""{{ postgres_service_name }}""
    name: ""{{ postgres_server }}""
- name: scl enable
  shell: 'scl enable rh-postgresql95 bash'
  when:
    - postgresql_status|failed
    - ovirt_engine_version >= '4.2'
    - ansible_distribution in ('CentOS', 'Red Hat')
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version < '4.2'
  tags:
    - skip_ansible_lint

- name: run PostgreSQL DB config
  shell: '{{ postgres_setup_cmd }}'
  args:
    creates: ""{{ postgres_config_file }}""
  when: ovirt_engine_version >= '4.2'
    name: ""{{ postgres_service_name }}""
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: '{{ postgres_data_dir }}/pg_hba.conf'
    dest: ""{{ postgres_config_file }}""
    dest: ""{{ postgres_config_file }}""
    dest: '/usr/lib/systemd/system/{{ postgres_service_name }}.service'
    path: ""{{ postgres_config_file }}""
    name: ""{{ postgres_service_name }}""
- name: create DWH DB user
  become: true
  postgresql_user:
    name: ""{{ item.user }}""
    password: ""{{ item.password }}""
    - user: ""{{ ovirt_engine_db_user }}""
      password: ""{{ ovirt_engine_db_password }}""
    - user: ""{{ ovirt_engine_dwh_db_user }}""
      password: ""{{ ovirt_engine_dwh_db_password }}""
  when: ovirt_engine_dwh_remote_db == True
- name: create engine & DWH DBs
  become: true
  postgresql_db:
    name: ""{{ item.db_name }}""
    owner: ""{{ item.user }}""
    encoding: UTF-8
    lc_collate: en_US.UTF-8
    lc_ctype: en_US.UTF-8
    template: template0
    - db_name: ""{{ ovirt_engine_db_name }}""
      user: ""{{ ovirt_engine_db_user }}""
    - db_name: ""{{ ovirt_engine_dwh_db_name }}""
      user: ""{{ ovirt_engine_dwh_db_user }}""
    name: ""{{ postgres_service_name }}"""
1093eb53e840684a254f7530655a89a96c8a3e79,name: postgresql,"
- name: check state of database
  service:
    name: ovirt-engine
    state: running

- name: check state of engine
  service:
    name: ovirt-engine
    state: running

- name: restart of ovirt-engine service
  service:
    name: ovirt-engine
    state: restarted

- name: check health status of page
  uri:
    url: ""http://{{ovirt_engine_hostname}}/ovirt-engine/services/health""
    status_code: 200
  register: health_page
  retries: 12
  delay: 10
  until: health_page|success"
ddea3ac3ef62d3c3d5e369153f8b0e09c610e98f,"- name: Set runner executor section
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^\s*[runners.{{ gitlab_runner.executor|default(""shell"") }}]'
    line: '  [runners.{{ gitlab_runner.executor|default(""shell"") }}]'
    state: present
    insertafter: '^\s*executor ='
    backrefs: no
  check_mode: no
  notify: restart_gitlab_runner


#### [runners.docker] section ####
    line: '    image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*\[runners.docker\]'
    line: '    privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*\[runners.docker\]'
    line: '    volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'

#### [runners.ssh] section #####
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
    insertafter: '^\s*\[runners.ssh\]'
","    regexp: '^\s*limit ='
    line: '  limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    insertafter: '^\s*name ='
    backrefs: no
    regexp: '^\s*url ='
    line: '  url = {{ gitlab_runner_coordinator_url | to_json }}'
    insertafter: '^\s*limit ='
    backrefs: no
    regexp: '^\s*executor ='
    line: '  executor = {{ gitlab_runner.executor|default(""shell"") | to_json }}'
    insertafter: '^\s*url ='
    backrefs: no
    regexp: '^\s*image ='
    line: '  image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*privileged ='
    line: '  privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*volumes ='
    line: '  volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Type ='
    line: '  Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Path ='
    line: '  Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*ServerAddress ='
    line: '  ServerAddress = {{ gitlab_runner.cache_s3_server_address|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*AccessKey ='
    line: '  AccessKey = {{ gitlab_runner.cache_s3_access_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*SecretKey ='
    line: '  SecretKey = {{ gitlab_runner.cache_s3_secret_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Shared ='
    line: '  Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketName ='
    line: '  BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketLocation ='
    line: '  BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Insecure ='
    line: '  Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*user ='
    line: '  user = {{ gitlab_runner.ssh_user|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*host ='
    line: '  host = {{ gitlab_runner.ssh_host|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*port ='
    line: '  port = {{ gitlab_runner.ssh_port|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*password ='
    line: '  password = {{ gitlab_runner.ssh_password|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*identity_file ='
    line: '  identity_file = {{ gitlab_runner.ssh_identity_file|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no"
036db67dfd0c210bacd9dfe94fa411aacead9d86,when: gitlab_runner_registration_token | length > 0  # Ensure value is set,"
- name: Register GitLab Runner
  include: register-runner.yml
  when: gitlab_runner_registration_token != ''"
08088b5bb7be58aa86b002b1e4be38654394a605,"    regexp: '^\s*limit ='
    line: '  limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    insertafter: '^\s*name ='
    backrefs: no
    regexp: '^\s*url ='
    line: '  url = {{ gitlab_runner_coordinator_url | to_json }}'
    insertafter: '^\s*limit ='
    backrefs: no
    regexp: '^\s*executor ='
    line: '  executor = {{ gitlab_runner.executor|default(""shell"") | to_json }}'
    insertafter: '^\s*url ='
    backrefs: no
    regexp: '^\s*image ='
    line: '  image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*privileged ='
    line: '  privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*volumes ='
    line: '  volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Type ='
    line: '  Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Path ='
    line: '  Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*ServerAddress ='
    line: '  ServerAddress = {{ gitlab_runner.cache_s3_server_address|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*AccessKey ='
    line: '  AccessKey = {{ gitlab_runner.cache_s3_access_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*SecretKey ='
    line: '  SecretKey = {{ gitlab_runner.cache_s3_secret_key|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Shared ='
    line: '  Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketName ='
    line: '  BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*BucketLocation ='
    line: '  BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*Insecure ='
    line: '  Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*user ='
    line: '  user = {{ gitlab_runner.ssh_user|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*host ='
    line: '  host = {{ gitlab_runner.ssh_host|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*port ='
    line: '  port = {{ gitlab_runner.ssh_port|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*password ='
    line: '  password = {{ gitlab_runner.ssh_password|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no
    regexp: '^\s*identity_file ='
    line: '  identity_file = {{ gitlab_runner.ssh_identity_file|default("""") | to_json }}'
    insertafter: '^\s*executor ='
    backrefs: no","---
- name: Create temporary file
  tempfile:
    state: file
    path: ""{{ temp_runner_config_dir.path }}""
    prefix: ""gitlab-runner.{{ gitlab_runner_index }}{{ runner_config_index }}.""
  register: temp_runner_config_keyword
  check_mode: no
  changed_when: false

- name: Isolate runner configuration
  copy:
    dest: ""{{ temp_runner_config_keyword.path }}""
    content: ""[[runners]]""
  check_mode: no
  changed_when: false

- name: Set concurrent limit option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)limit ='
    line: '\1limit = {{ gitlab_runner.concurrent_specific|default(0) }}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set coordinator URL
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)url ='
    line: '\1url = {{ gitlab_runner_coordinator_url | to_json }}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set runner executor option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)executor ='
    line: '\1executor = {{ gitlab_runner.executor|default(""shell"") | to_json}}'
    state: present
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set runner docker image option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)image ='
    line: '\1image = {{ gitlab_runner.docker_image|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.docker_image is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set docker privileged option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)privileged ='
    line: '\1privileged = {{ gitlab_runner.docker_privileged|default(false) | lower }}'
    state: ""{{ 'present' if gitlab_runner.docker_privileged is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set docker volumes option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)volumes ='
    line: '\1volumes = {{ gitlab_runner.docker_volumes|default([])|to_json }}'
    state: ""{{ 'present' if gitlab_runner.docker_volumes is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache type option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Type ='
    line: '\1Type = {{ gitlab_runner.cache_type|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_type is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache path option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Path ='
    line: '\1Path = {{ gitlab_runner.cache_path|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_path is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache shared option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Shared ='
    line: '\1Shared = {{ gitlab_runner.cache_shared|default("""") | lower }}'
    state: ""{{ 'present' if gitlab_runner.cache_shared is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 bucket name option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)BucketName ='
    line: '\1BucketName = {{ gitlab_runner.cache_s3_bucket_name|default("""")  | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_bucket_name is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 bucket location option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)BucketLocation ='
    line: '\1BucketLocation = {{ gitlab_runner.cache_s3_bucket_location|default("""") | to_json }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_bucket_location is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner

- name: Set cache s3 insecure option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^(\s*)Insecure ='
    line: '\1Insecure = {{ gitlab_runner.cache_s3_insecure|default("""") | lower }}'
    state: ""{{ 'present' if gitlab_runner.cache_s3_insecure is defined else 'absent' }}""
    backrefs: yes
  check_mode: no
  notify: reload_gitlab_runner"
df0d6cd00335438da6ee0528cf2ccc3dc67896ab,--cache-s3-secret-key '{{ gitlab_runner_cache_s3_secret_key }}',"    {% if gitlab_runner_cache_type is defined %}
    --cache-type '{{ gitlab_runner_cache_type }}'
    --cache-s3-server-address '{{ gitlab_runner_cache_s3_server_address }}'
    --cache-s3-access-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-secret-key '{{ gitlab_runner_cache_s3_access_key }}'
    --cache-s3-bucket-name '{{ gitlab_runner_cache_s3_bucket_name }}'
    --cache-s3-insecure '{{ gitlab_runner_cache_s3_insecure }}'
    --cache-cache-shared '{{ gitlab_runner_cache_cache_shared }}'
    {% endif %}"
f02f13b9b53a96bea8cd6c68599cfedf3458c42e,insertafter: '^\s*url =',"- name: Set environment option
  lineinfile:
    dest: ""{{ temp_runner_config.path }}""
    regexp: '^\s*environment ='
    line: '  environment = {{ gitlab_runner.env_vars|default([]) | to_json }}'
    state: present
    insertafter: '^\s*url='
    backrefs: no
  check_mode: no
  notify: restart_gitlab_runner
"
756507be5964f6cac48008e49110271ff9250da1,"    dest: ""{{ gitlab_runner_config_file }}""","---
- name: (Windows) Create .gitlab-runner dir
  win_file:
    path: ""{{ gitlab_runner_config_file_location }}""
    state: directory

- name: (Windows) Ensure config.toml exists
  win_file:
    path: ""{{ gitlab_runner_config_file }}""
    state: touch
    modification_time: preserve
    access_time: preserve

- name: (Windows) Set concurrent option
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^(\s*)concurrent =.*'
    line: '$1concurrent = {{ gitlab_runner_concurrent }}'
    state: present
    backrefs: yes
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows

- name: (Windows) Add listen_address to config
  win_lineinfile:
    dest: /etc/gitlab-runner/config.toml
    regexp: '^listen_address =.*'
    line: 'listen_address = ""{{ gitlab_runner_listen_address }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_listen_address | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_windows

- name: (Windows) Add sentry dsn to config
  win_lineinfile:
    dest: ""{{ gitlab_runner_config_file }}""
    regexp: '^sentry_dsn =.*'
    line: 'sentry_dsn = ""{{ gitlab_runner_sentry_dsn }}""'
    insertafter: '\s*concurrent.*'
    state: present
  when: gitlab_runner_sentry_dsn | length > 0  # Ensure value is set
  notify:
    - restart_gitlab_runner
    - restart_gitlab_runner_macos
    - restart_gitlab_runner_windows"
8c93deed508ae6195ab981d2241020a0eb9f2133,"      name: ""{{ item }}""","      name: ""{{ item }}""
    with_items: ""{{ rocket_chat_dep_packages }}""
      name: 
    with_items: ""{{ rocket_chat_dep_packages }}"""
7e52e7a72269fc433a730cfded9eae3142f39395,file:  CentOS-Base,"  - name: Configure default CentOS online repos
    yum_repository:
      name: {{ item }}
      enabled: {{ rock_online_install }}
      file:  CentOS-Base.repo
    with_items:
      - base
      - updates
      - extras

    when: with_elasticsearch
    when: (with_elasticsearch and with_bro)
    when: (with_elasticsearch and with_bro) and bro_mapping.status == 404
  - name: Add broctl wrapper for admin use
    copy:
      src: broctl.sh
      dest: /usr/sbin/broctl
      mode: 0754
      owner: root
      group: root
    when: with_bro

    when: with_pulledpork and not rules_file.stat.exists
    when: with_kibana and rock_online_install
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana
    when: with_kibana and (kibana_config.rock_config is undefined or kibana_config.rock_config != rock_dashboards_version)
    when: with_kibana and with_bro
    when: with_kibana and with_suricata and not with_bro
    when: with_kibana
    when: with_kibana
    seboolean: 
      name: httpd_can_network_connect
      state: yes 
      persistent: yes
        for intf in {{ rock_monifs | join(' ') }}; do
    - name: create kafka suricata topic
           --topic suricata-raw"
c592211c82952498e252868a43ed13310646f739,"      dest: ""{{ bro_sysconfig_dir }}/broctl.cfg""
      creates: ""{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""
      src: ""{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""
      dest: ""{{ bro_site_dir }}/scripts/rock""","      - { pkg: docket, test: ""{{with_docket}}"", state: installed }
        - { port: ""8443/tcp"", test: ""{{ with_docket }}"" }
  - name: Create /opt/bro dir for wandering users
      dest: ""/opt/bro""
      state: directory
  - name: Create note to wandering users
    copy:
      dest: ""/opt/bro/README.md""
      content: |
        Hey! Where's my Bro?
        =========================

        RockNSM has aligned the Bro package to be inline with Fedora packaging
        guidelines in an effort to push the package upstream for maintenance.
        Fedora and EPEL have a great community and we believe others can benefit
        from our hard work.

        Here's where you can find your stuff:

        Bro configuration files
        -----------------------
        /opt/bro/etc -> /etc/bro

        Bro site scripts
        -----------------------
        /opt/bro/share/bro/site -> /usr/share/bro/site

        Bro logs and spool dirs (same as previous ROCK iterations)
        -----------------------
        /opt/bro/logs -> /data/bro/logs
        /opt/bro/spool -> /data/bro/spool

  # - name: Install broctl service file
  #   template:
  #     src: templates/broctl.service.j2
  #     dest: /etc/systemd/system/broctl.service
  #     owner: root
  #     group: root
  #     mode: 0644
  #   when: with_bro
  #   notify: reload systemd
      dest: ""{{ bro_sysconfig_dir }}/node.cfg""
      dest: {{ bro_sysconfig_dir }}/broctl.cfg""
      dest: ""{{ bro_sysconfig_dir }}/networks.cfg""
      path: /scripts
  # - name: Set permissions on broctl scripts dir
  #   file:
  #     path: /opt/bro/share/broctl/scripts
  #     owner: ""{{ bro_user }}""
  #     group: ""{{ bro_user }}""
  #     mode: 0755
  #     state: directory
  #   when: with_bro
      dest: ""{{ bro_site_dir }}/scripts/README.txt""
      dest: ""{{ bro_site_dir }}/scripts/rock""
      dest: ""{{ bro_site_dir }}/scripts/""
      creates: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""
      src: """"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}""""
      dest: """"{{ bro_site_dir }}/scripts/rock""""
      path: ""{{ bro_site_dir }}/scripts/rock""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
      dest: ""{{ bro_site_dir }}/local.bro""
  - name: Add bro aliases
  # - name: Create bro utility symlinks
  #   file:
  #     src: ""/opt/bro/bin/{{ item.src }}""
  #     dest: ""/usr/bin/{{ item.dest }}""
  #     force: yes
  #     state: link
  #   with_items:
  #     - { src: 'bro', dest: 'bro' }
  #     - { src: 'bro-cut', dest: 'bro-cut' }
  #
      job: ""/usr/bin/broctl cron >/dev/null 2>&1""
    command: /usr/bin/broctl install
      name: bro
    notify: reload broctl
      export kibuser=$(getent passwd 1000 | awk -F: '{print $1}') && export kibpw=$(xkcdpass -a rock) && echo -e ""U: ${kibuser}\nP: ${kibpw}"" > /home/${kibuser}/KIBANA_CREDS.README && printf ""${kibuser}:$(echo ${kibpw} | openssl passwd -apr1 -stdin)\n"" | sudo tee -a /etc/nginx/htpasswd.users > /dev/null 2>&1
    seboolean:
      state: yes
    shell:
      service: name=broctl state=""{{ 'started' if enable_bro else 'stopped' }}"""
034cd599a84a4c9b0b36c7931e5eb7525c55b5cf,"    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}""","    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'started' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}""
    state: ""{{ 'restarted' if local_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | first | bool else 'stopped' }}"""
fe9bd594f3dab88042b91b664c7c45cc7ff92418,"  apt:
    pkg: fail2ban
    state: latest
    update_cache: true
    cache_valid_time: ""{{ apt_cache_valid_time }}""
  template:
    src: ""{{ item }}.j2""
    dest: /etc/fail2ban/{{ item }}
  service:
    name: fail2ban
    state: started
    enabled: yes","---
- name: ensure fail2ban is installed
  apt: pkg=fail2ban state=latest update_cache=true cache_valid_time={{ apt_cache_valid_time }}
  notify:
    - restart fail2ban

- name: ensure fail2ban is configured
  template: src={{ item }}.j2 dest=/etc/fail2ban/{{ item }}
  with_items:
    - jail.local
    - fail2ban.local
  notify:
    - restart fail2ban

- name: ensure fail2ban starts on a fresh reboot
  service: name=fail2ban state=started enabled=yes"
c544d731a1661d88fd4c1609517197786263f3ff,"  local_action: command ansible {{ inventory_hostname }} -m ping{{ (inventory_file == None) | ternary('', ' -i ' + inventory_file | string) }} -u root","  local_action: command ansible {{ inventory_hostname }} -m ping -i {{ inventory_file }} -u root
    ansible_ssh_user: ""{{ (root_status.rc == 0) | ternary('root', admin_user) }}""

- name: Announce which user was selected
  debug:
    msg: ""Note: Ansible will attempt connections as user = {{ ansible_ssh_user }}"""
07047a23840dd6da83acff3bee27ef67efd91d91,"project_subtree: ""{{ project.subtree }}""","project_subtree: ""{{ project.subtree | default(False) }}"""
0d80939ea114d9e5ec2a82b8bfd1e4bf7d38fe17,"  when: sensu_redis_server
  when: sensu_rabbitmq_server","---

  - name: Ensure the Sensu group is present
    group: name={{ sensu_group_name }}
             state=present
             
  - name: Ensure the Sensu user is present
    user: name={{ sensu_user_name }}
          group={{ sensu_group_name }}
          shell=/bin/false
          home={{ sensu_config_path }}
          createhome=yes
          state=present

  - name: Ensure the Sensu config directory is present
    file: dest={{ sensu_config_path }}/conf.d state=directory recurse=yes
          owner={{ sensu_user_name }} group={{ sensu_group_name }}

  - name: Ensure Sensu dependencies are installed
    pkgin: name=build-essential,ruby21-base state=present

  - name: Ensure Uchiwa (dashboard) dependencies are installed
    pkgin: name=go state=present
    when: sensu_include_dashboard

  - name: Ensure Sensu is installed
    gem: name=sensu state={{ sensu_gem_state }} user_install=no
    notify:
      - restart sensu-client service
    
  - name: Ensure Sensu 'plugins' gem is installed
    gem: name=sensu-plugin state={{ sensu_plugin_gem_state }} user_install=no

  - include: ssl.yml tags=ssl

  - include: rabbit.yml tags=rabbitmq
    when: rabbitmq_server

  - include: redis.yml tags=redis
    when: redis_server

  - include: server.yml tags=server
    when: sensu_master

  - include: dashboard.yml tags=dashboard
    when: sensu_include_dashboard
    
  - include: client.yml tags=client

  - include: plugins.yml tags=plugins
    when: sensu_include_plugins"
ba02d8c58d205b67b0abac16c2511f5fa5f8c760,enabled: yes,"- name: Deploy Tessen server configuratiuon
  template:
    dest: ""{{ sensu_config_path }}/conf.d/tessen.json""
    owner: ""{{ sensu_user_name }}""
    group: ""{{ sensu_group_name }}""
    src: sensu-tessen.json.j2
  notify: restart sensu-server service

  service:
    name: ""{{ sensu_server_service_name if not se_enterprise else sensu_enterprise_service_name }}""
    state: started
  enabled: yes
  service:
    name: sensu-api
    state: started
    enabled: yes"
84c68805f7cca85223301e90249b74f1548d1709,istio_git_repo: https://github.com/istio/istio.git,"istio_git_repo: https://github.com/istio/istio.git
istio_git_branch: 0.6.0

istio_playbook_release_tag: 0.6.0
istio_playbook_auth: false
istio_playbook_jaeger: false
istio_playbook_delete_resources: false
istio_playbook_cluster_flavour: ocp
istio_playbook_dest: /home/istio
istio_playbook_namespace: istio-system
istio_playbook_addon: grafana,prometheus,servicegraph
istio_playbook_samples:"
aaf56e4d8949c3505ab644af8a7c2a142c91e357,value: sb-2.1.x,"apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: s2i-springboot-example
spec:
  taskRef:
    name: s2i-jdk8
  inputs:
    resources:
      - name: git-repo
        resourceSpec:
          type: git
          params:
            - name: revision
              value: master
            - name: url
              value: https://github.com/snowdrop/rest-http-example
  outputs:
    resources:
      - name: image
        resourceSpec:
          type: image
          params:
            - name: url
              value: quay.io/snowdrop/spring-boot-example"
d1118f351fce6039b8a78aca8c92f0cb41b5b0f8,"- name: ""Setting service_name fact from config""
    splunk_service_name: ""{{ splunk.service_name }}""
    - ""'service_name' in splunk""
- name: Set Splunk service name
  block:
    - name: Setting SplunkForwarder service
      set_fact:
        splunk_service_name: ""SplunkForwarder.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role == ""splunk_universal_forwarder""
    - name: Setting Splunkd service
      set_fact:
        splunk_service_name: ""Splunkd.service""
      when:
        - ansible_system is match(""Linux"")
        - splunk_systemd
        - splunk.role != ""splunk_universal_forwarder""
    - name: Setting splunk service
      set_fact:
        splunk_service_name: ""splunk""
      when:
        - ansible_system is match(""Linux"")
        - not splunk_systemd

    - name: Setting splunkforwarder Windows service
      set_fact:
        splunk_service_name: ""splunkforwarder""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role == ""splunk_universal_forwarder""

    - name: Setting splunkd Windows service
      set_fact:
        splunk_service_name: ""splunkd""
      when:
        - ansible_system is not match(""Linux"")
        - splunk.role != ""splunk_universal_forwarder""
    - splunk_service_name is not defined or not splunk_service_name
    - splunk.enable_service","---
- name: ""Setting service_name to SplunkForwarder.service""
  set_fact:
    splunk_service_name: ""SplunkForwarder.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd
    - splunk.build_location is search('forwarder')

- name: ""Setting service_name to Splunkd.service""
  set_fact:
    splunk_service_name: ""Splunkd.service""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and ansible_system is match(""Linux"")
    - splunk_systemd is False

- name: ""Setting service_name to splunkd""
  set_fact:
    splunk_service_name: ""splunkd""
  when:
    - splunk_service_name is not defined
    - splunk.enable_service and not ansible_system is match(""Linux"")"
ffa3fc6315c62a2ade0f8cbdc8e24421f61b929d,"  changed_when:
    - splunk_cluster_bundle_result.stdout.find('Applying') != -1
    - splunk_cluster_bundle_result.stdout.find('bundle') != -1","---
- name: Get indexer count
  set_fact:
    num_indexer_hosts: ""{{ groups['splunk_indexer'] | length }}""

- name: Get default replication factor
  set_fact:
    idxc_search_factor: ""{{ splunk.idxc.search_factor }}""
    idxc_replication_factor: ""{{ splunk.idxc.replication_factor }}""

- name: Lower indexer search/replication factor
  set_fact:
    idxc_search_factor: 1
    idxc_replication_factor: 1
  when: num_indexer_hosts|int < 3

- name: Set indexer discovery
  uri:
    url: ""https://127.0.0.1:{{ splunk.svc_port }}/servicesNS/nobody/system/configs/conf-server""
    method: POST
    user: admin
    password: ""{{ splunk.password }}""
    validate_certs: False
    body: ""name=indexer_discovery&pass4SymmKey={{ splunk.shc.secret }}""
    body_format: json
    headers:
      Content-Type: ""application/x-www-form-urlencoded""
    status_code: 201,409
    timeout: 10
  register: set_indexer_discovery
  changed_when: set_indexer_discovery.status == 201

- name: Set the current node as a Splunk indexer cluster master
  command: ""{{ splunk.exec }} edit cluster-config -mode master -replication_factor {{ splunk.idxc.replication_factor }} -search_factor {{ splunk.idxc.search_factor }} -secret '{{ splunk.idxc.secret }}' -cluster_label '{{ splunk.idxc.label }}' -auth 'admin:{{ splunk.password }}'""
  register: task_result
  until: task_result.rc == 0
  retries: ""{{ retry_num }}""
  delay: 3
  notify:
    - Restart the splunkd service

- name: Flush restart handlers
  meta: flush_handlers

- name: Apply the cluster bundle to the Splunk cluster master
  command: ""{{ splunk.exec }} apply cluster-bundle -auth admin:{{ splunk.password }} --skip-validation --answer-yes""
  register: splunk_cluster_bundle_result
  failed_when: >
    (""No new bundle will be pushed"" not in splunk_cluster_bundle_result.stderr)
    and (""Rolling restart of the peers"" not in splunk_cluster_bundle_result.stderr)
    and (""Applying bundle"" not in splunk_cluster_bundle_result.stdout)
  changed_when: splunk_cluster_bundle_result.stdout.find('Applying bundle') != -1

- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml"
7c5201612c9a8642e6170afa26cfb6d29abcb980,"  register: set_symmkey

- include_tasks: trigger_restart.yml
  when: set_symmkey is changed","- name: Set general pass4SymmKey
  ini_file: 
    dest: ""{{ splunk.home }}/etc/system/local/server.conf""
    section: ""general""
    option: ""pass4SymmKey""
    value: ""{{ splunk.pass4SymmKey }}""
  notify:
    - Restart the splunkd service"
18d0d7343cc4df5e2d9c655fe008cbb5520ad099,"- include_tasks: prepare_apps_bundle.yml
- include_tasks: initialize_cluster_master.yml
","- include_tasks: initialize_cluster_master.yml
- include_tasks: ../../../roles/splunk_common/tasks/enable_forwarding.yml
- include_tasks: ../../../roles/splunk_common/tasks/provision_apps.yml
  when:
    - splunk.apps_location
- include_tasks: push_apps_to_indexers.yml
  when:
    - splunk.apps_location"
4cdc3adcd79e4ce03ff45bbca5dfac78f4b55adb,"    path: ""{{ splunk.app_paths.deployment }}""","- name: Gather all deployment server apps
  find:
    path: ""{{ splunk.home }}/etc/deployment_apps""
    recurse: no
    file_type: directory
  register: deployment_apps

    section: ""serverClass:all:app:{{ item.path | basename }}""
  with_items: ""{{ deployment_apps.files }}"""
8ad553f34e2ff2c8dcb022ef5b37d5d076414742,- include_tasks: ../../../roles/splunk_common/tasks/check_for_required_restarts.yml,"
- include_tasks: check_for_required_restarts.yml"
3dd50d8cead943c582f3c099af644f3019d2ba62,"- name: ""Wait for port {{ splunk.svc_port }} to become open""
    port: ""{{ splunk.svc_port }}""","- name: ""Wait for port 8089 to become open""
    port: 8089"
9df7ea424878b36811bdf02739429079da46b1b6,"  license: MIT
  min_ansible_version: 2.0.1","---
galaxy_info:
  author: Justin Leitgeb
  description: Base image and common roles
  company: Stack Builders
  license: Proprietary
  min_ansible_version: 1.2
  platforms:
    - name: Debian
      versions:
        - all

  galaxy_tags:
    - sb-base

dependencies:
  - role: kamaln7.swapfile
    become: yes
    become_method: sudo
    remote_user: administrator
    swapfile_size: ""{{ swap_file_size }}""
    tags:
      - bootstrap

  - role: ansible-role-unattended-upgrades
    become: yes
    become_method: sudo
    remote_user: administrator
    unattended_origins_patterns:
      - 'origin=Debian,archive=${distro_codename},label=Debian-Security'
    unattended_mail: '{{ uu_email_alerts }}'
    unattended_automatic_reboot: false
    tags:
      - bootstrap

  - role: nickjj.fail2ban
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - bootstrap

  - role: geerlingguy.ntp
    become: yes
    become_method: sudo
    remote_user: administrator
    ntp_timezone: UTC
    tags:
      - bootstrap

  - role: jdauphant.ssl-certs
    become: yes
    become_method: sudo
    remote_user: administrator
    ssl_certs_common_name: ""{{ inventory_hostname }}""

    tags:
      - nginx-http
      - nginx-https

  - role: jdauphant.nginx
    become: yes
    become_method: sudo
    remote_user: administrator
    tags:
      - nginx

  - role: ANXS.postgresql
    tags:
      - basic-postgres
"
adc41d115f09c73cb5ae3939332f2aae920d46f4,service: name={{openvpn_service}} state=restarted,"---

- name: openvpn restart
  service: name=openvpn state=restarted"
fcbeab7f3e88a8077cbacdf67b8977c87d85f6c7,"- include_tasks: system/firewall-deps.yml
  when:
    openvpn_open_firewall | bool
    or openvpn_route_traffic | bool
    or openvpn_client_to_client_via_ip | bool

- include_tasks: system/open-firewall.yml
  when: openvpn_open_firewall | bool","- include_tasks: system/forwarding.yml

- include_tasks: system/firewall.yml

- include_tasks: system/routing.yml
  when: openvpn_route_traffic | bool
"
431a1e5d5d87944a1e09930207b646e4a44326c5,when: threatstack_hostname and threatstack_policy,"- name: Cloudsight setup default
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }}
  when: not threatstack_hostname and not threatstack_policy
- name: Cloudsight setup policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --policy=""{{ threatstack_policy}}""
  register: setup_result
  when: threatstack_policy and not threatstack_hostname
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and not threatstack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret

- name: Cloudsight setup hostname/policy
  command: cloudsight setup --deploy-key={{ threatstack_deploy_key | mandatory }} --hostname={{ threatstack_hostname }}
  register: setup_result
  when: threatstack_hostname and threastack_policy
  args:
    creates: /opt/threatstack/cloudsight/config/.secret"
7b468fae79817115e97d087cbaebf59daecaa60b,"    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    secret: ""{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""","---
- set_fact:
    resource_group: ""Algo_{{ region }}""

- name: Create a resource group
  azure_rm_resourcegroup:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    name: ""{{ resource_group }}""
    location: ""{{ region }}""
    tags:
        service: algo

- name: Create a virtual network
  azure_rm_virtualnetwork:
    resource_group: ""{{ resource_group }}""
    name: algo_net
    address_prefixes: ""10.10.0.0/16""
    tags:
        service: algo

- name: Create a subnet
  azure_rm_subnet:
    resource_group: ""{{ resource_group }}""
    name: algo_subnet
    address_prefix: ""10.10.0.0/24""
    virtual_network: algo_net
    tags:
        service: algo

- name: Create an instance
  azure_rm_virtualmachine:
    secret: ""{{ azure_secret | default(lookup('env','AZURE_CLIENT_ID')) }}""
    tenant: ""{{ azure_tenant | default(lookup('env','AZURE_SECRET')) }}""
    client_id: ""{{ azure_client_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}""
    subscription_id: ""{{ azure_subscription_id | default(lookup('env','AZURE_TENANT')) }}""
    resource_group: ""{{ resource_group }}""
    admin_username: ubuntu
    virtual_network: algo_net
    name: ""{{ azure_server_name }}""
    ssh_password_enabled: false
    vm_size: Standard_D1
    tags:
      service: algo
    ssh_public_keys:
      - { path: ""/home/ubuntu/.ssh/authorized_keys"", key_data: ""{{ lookup('file', '{{ ssh_public_key }}') }}"" }
    image:
      offer: UbuntuServer
      publisher: Canonical
      sku: '16.04-LTS'
      version: latest
  register: azure_rm_virtualmachine

- set_fact:
    ip_address: ""{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].properties.ipConfigurations[0].properties.publicIPAddress.properties.ipAddress }}""

- name: Add the instance to an inventory group
  add_host:
    name: ""{{ ip_address }}""
    groups: vpn-host
    ansible_ssh_user: ubuntu
    ansible_python_interpreter: ""/usr/bin/python2.7""
    easyrsa_p12_export_password: ""{{ easyrsa_p12_export_password }}""
    cloud_provider: azure
    ipv6_support: no

- name: Wait for SSH to become available
  local_action: ""wait_for port=22 host={{ ip_address }} timeout=320"""
43aafdfce1e17a4469e505234b4728519d23a9b6,"- name: Build python virtual environment
  import_tasks: venv.yml
- name: Include prompts
  import_tasks: prompts.yml
- block:
    - set_fact:
        algo_region: >-
          {% if region is defined %}{{ region }}
          {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
          {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

    - name: Security group created
      cs_securitygroup:
        name: ""{{ algo_server_name }}-security_group""
        description: AlgoVPN security group
      register: cs_security_group

    - name: Security rules created
      cs_securitygroup_rule:
        security_group: ""{{ cs_security_group.name }}""
        protocol: ""{{ item.proto }}""
        start_port: ""{{ item.start_port }}""
        end_port: ""{{ item.end_port }}""
        cidr: ""{{ item.range }}""
      with_items:
        - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
        - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

    - name: Keypair created
      cs_sshkeypair:
        name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
        public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
      register: cs_keypair

    - name: Set facts
      set_fact:
        image_id: ""{{ cloud_providers.cloudstack.image }}""
        size: ""{{ cloud_providers.cloudstack.size }}""
        disk: ""{{ cloud_providers.cloudstack.disk }}""
        keypair_name: ""{{ cs_keypair.name }}""

    - name: Server created
      cs_instance:
        name: ""{{ algo_server_name }}""
        root_disk_size: ""{{ disk }}""
        template: ""{{ image_id }}""
        ssh_key: ""{{ keypair_name }}""
        security_groups: ""{{ cs_security_group.name }}""
        zone: ""{{ algo_region }}""
        service_offering: ""{{ size }}""
      register: cs_server

    - set_fact:
        cloud_instance_ip: ""{{ cs_server.default_ip }}""
        ansible_ssh_user: ubuntu
  environment:
    CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
    CLOUDSTACK_REGION: ""{{ algo_cs_region }}""","---
- block:
    - name: Build python virtual environment
      import_tasks: venv.yml

    - name: Include prompts
      import_tasks: prompts.yml

    - block:
      - set_fact:
          algo_region: >-
            {% if region is defined %}{{ region }}
            {%- elif _algo_region.user_input is defined and _algo_region.user_input | length > 0 %}{{ cs_zones[_algo_region.user_input | int -1 ]['name'] }}
            {%- else %}{{ cs_zones[default_zone | int - 1]['name'] }}{% endif %}

      - name: Security group created
        cs_securitygroup:
          name: ""{{ algo_server_name }}-security_group""
          description: AlgoVPN security group
        register: cs_security_group

      - name: Security rules created
        cs_securitygroup_rule:
          security_group: ""{{ cs_security_group.name }}""
          protocol: ""{{ item.proto }}""
          start_port: ""{{ item.start_port }}""
          end_port: ""{{ item.end_port }}""
          cidr: ""{{ item.range }}""
        with_items:
          - { proto: tcp, start_port: 22, end_port: 22, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 4500, end_port: 4500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: 500, end_port: 500, range: 0.0.0.0/0 }
          - { proto: udp, start_port: ""{{ wireguard_port }}"", end_port: ""{{ wireguard_port }}"", range: 0.0.0.0/0 }

      - name: Keypair created
        cs_sshkeypair:
          name: ""{{ SSH_keys.comment|regex_replace('@', '_') }}""
          public_key: ""{{ lookup('file', '{{ SSH_keys.public }}') }}""
        register: cs_keypair

      - name: Set facts
        set_fact:
          image_id: ""{{ cloud_providers.cloudstack.image }}""
          size: ""{{ cloud_providers.cloudstack.size }}""
          disk: ""{{ cloud_providers.cloudstack.disk }}""
          keypair_name: ""{{ cs_keypair.name }}""

      - name: Server created
        cs_instance:
          name: ""{{ algo_server_name }}""
          root_disk_size: ""{{ disk }}""
          template: ""{{ image_id }}""
          ssh_key: ""{{ keypair_name }}""
          security_groups: ""{{ cs_security_group.name }}""
          zone: ""{{ algo_region }}""
          service_offering: ""{{ size }}""
        register: cs_server

      - set_fact:
          cloud_instance_ip: ""{{ cs_server.default_ip }}""
          ansible_ssh_user: ubuntu
      environment:
        PYTHONPATH: ""{{ cloudstack_venv }}/lib/python2.7/site-packages/""
        CLOUDSTACK_CONFIG: ""{{ algo_cs_config }}""
        CLOUDSTACK_REGION: ""{{ algo_cs_region }}""

      rescue:
      - debug: var=fail_hint
        tags: always
      - fail:
        tags: always"
f0283856ad52780157e5111ee8dc786cdae8e2d8,"      -out crl/{{ item }}.crt
      creates: crl/{{ item }}.crt","
- name: Get active users
  local_action: >
    shell grep ^V index.txt | grep -v ""{{ IP_subject_alt_name }}"" | awk '{print $5}' | sed 's/\/CN=//g'
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
  register: valid_certs

- name: Revoke non-existing users
  local_action: >
    shell openssl ca -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt &&
      openssl ca -gencrl -config openssl.cnf -passin pass:""{{ easyrsa_CA_password }}"" -revoke certs/{{ item }}.crt -out crl/{{ item }}.crt
      touch crl/{{ item }}_revoked
  become: no
  args:
    chdir: ""configs/{{ IP_subject_alt_name }}/pki/""
    creates: crl/{{ item }}_revoked
  environment:
    subjectAltName: ""DNS:{{ item }}""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""

- name: Copy the revoked certificates to the vpn server
  copy:
    src: configs/{{ IP_subject_alt_name }}/pki/crl/{{ item }}.crt
    dest: ""{{ config_prefix|default('/') }}etc/ipsec.d/crls/{{ item }}.crt""
  when: item not in users
  with_items: ""{{ valid_certs.stdout_lines }}""
  notify:
    - rereadcrls"
65056289aa6ed85b6952e9a3ebc26ecc733afe2c,"    name: ""{{ opensmtpd_extra_packages }}""","---

- name: Enable opensmtpd_service
  service:
    name: ""{{ opensmtpd_service }}""
    arguments: ""{{ opensmtpd_flags }}""
    enabled: yes

- name: Install opensmtpd_extra_packages
  openbsd_pkg:
    name: ""{{ item }}""
  with_items: ""{{ opensmtpd_extra_packages }}"""
c7f4241b1f701519ec20730e465a860b1e410548,"    mode: 0644
    group: 'root'
    owner: 'root'",mode: 0600
c26588a6e72e9341561ca1aae6bc6e4ac3322011,- irods-rule-engine-plugin-python-4.2.8.0-1,"- name: Ensure iRODS 4.2.7 packages are absent
      - irods-uu-microservices-4.2.7_0.8.1-1
      - irods-sudo-microservices-4.2.7_1.0.0-1
      - davrods-4.2.7_1.4.2-1
      - irods-server-4.2.8-1
      - irods-runtime-4.2.8-1
      - irods-database-plugin-postgres-4.2.8-1
      - irods-rule-engine-plugin-python-4.2.8-1"
66cc4f4ce53b16dd3b70e39d7c9f6f6555624f2b,when: ruleset.install_scripts and install_rulesets,"  when: ruleset.install_scripts == ""yes"" and install_rulesets == ""yes"""
b898fe05db131b9aaadc3352d8d0aa4c2ecab865,description: Install and configure viasite/zsh-config and oh-my-zsh,"---
galaxy_info:
  author: Stanislav Popov
  company: Viasite
  description: Install and configure popstas/zsh-config and oh-my-zsh
  license: MIT
  min_ansible_version: 1.8
  platforms:
    - name: Ubuntu
      versions:
        - trusty
        - xenial
  categories:
    - system
dependencies: []"
459ac4631034c75a4e3f9c9963d9bb742edd6c92,"      - wazuh_manager_config.cluster.node_type == ""master"" or wazuh_manager_config.cluster.node_type == ""worker""","    when:       
      - wazuh_manager_config.cluster.node_type == ""master""
      - wazuh_manager_config.cluster.node_type == ""master"""
a21392fe58991ac5e0645e69afd38e57f43fcc74,"    baseurl: ""{{ wazuh_manager_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}-5""
    baseurl: ""{{ wazuh_manager_config.repo.yum }}""
    gpgkey: ""{{ wazuh_manager_config.repo.gpg }}""","- name: RedHat/CentOS 5 | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}5/""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}-5""
    - (ansible_facts['os_family']|lower == 'redhat')
    - (ansible_os_family = ansible_distribution_major_version|int <= 5)
  register: repo_v5_manager_installed
- name: RedHat/CentOS/Fedora | Install Wazuh repo
    baseurl: ""{{ wazuh_agent_config.repo.yum }}""
    gpgkey: ""{{ wazuh_agent_config.repo.gpg }}""
  changed_when: false
    - repo_v5_manager_installed is undefined"
ccbc8f5213603b17a3fdce8ff54fff5e16972d10,"          od_node_name: ""{{ elasticsearch_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""
          od_node_name: ""{{ kibana_node_name }}""","
  - name: Configure node name
    block:
      - name: Setting node name (Elasticsearch)
        set_fact:
          od_node_name: elasticsearch_node_name
        when:
          elasticsearch_node_name is defined and kibana_node_name is not defined

      - name: Setting node name (Kibana)
        set_fact:
          od_node_name: kibana_node_name
        when:
          kibana_node_name is defined

      - name: Setting node name (Filebeat)
        set_fact:
          od_node_name: filebeat_node_name
        when:
          filebeat_node_name is defined


      - ""{{ od_node_name }}.key""
      - ""{{ od_node_name }}.pem""
      - ""{{ od_node_name }}_http.key""
      - ""{{ od_node_name }}_http.pem""
      - ""{{ od_node_name }}_elasticsearch_config_snippet.yml""
      block: ""{{ lookup('file', '{{ local_certs_path }}/certs/{{ od_node_name }}_elasticsearch_config_snippet.yml') }}""
      -h {{ hostvars[od_node_name]['ip'] }}"
3f85c38a8ac21cbaeea355bcf11beeed7dee5729,"  - {
      role: geerlingguy.repo-epel,
      version: 1.2.3,
      tags: [
        ""dependency"",
        ""dependency.epel""
      ],
      when: ansible_os_family == 'RedHat'
  }
  - {
      role: srsp.oracle-java,
      version: 2.19.1,
      tags: [
        ""dependency"",
        ""dependency.java""
      ]
  }
  - {
      role: gantsign.maven,
      version: 4.0.0,
      tags: [
        ""dependency,"",
        ""dependency.maven""
      ]
  }
  - { role: andrewrothstein.terraform,
      version: v2.2.10,
      tags: [
        ""dependency"",
        ""dependency.terraform""
      ]
  }","dependencies:
  - { role: geerlingguy.repo-epel, tags: [""dependency""] }
  - { role: srsp.oracle-java, tags: [""dependency""] }
  - { role: gantsign.maven, tags: [""dependency""] }
  - { role: andrewrothstein.terraform, tags: [""dependency""] }"
